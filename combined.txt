 What we want to do with this, this is a short introduction to the goals, the motivations of what we want to do with this overall day long, over two half days course. The description, the objectives, the summary, and talk a little bit about open source. Now really what are we going to do with this day? My goal is with the introduction to energy machine learning to basically run fast, run fast, run forward, and basically build machines. That's my goal is to demystify and give you a sense of it's not that hard to do. You can load your data in, you can try some things out, and it's something we call in academia, experiential learning, this idea of learning by doing. I'll introduce the basic concepts necessary to get you started, but then immediately we just start doing things. We'll do fast start to building machines. Many of you all know me and Shibron's family to me, I'll tell you, I really enjoyed my time there. If you don't know me and you look at my name and you're a bit confused, well so is almost everyone else I meet. I just pronounce it like perch and we're good to go. My family spent 100 years in Canada since the Ukraine and I think we don't know how to pronounce it either. I've met Ukraine and told me we pronounce it wrong. I have practical experience, I have about 13 years within Shibron years before that of consulting, but I work in the area, I worked in the area of teaching, consulting, industrial R&D, in statistical modeling, geostatistics, reservoir modeling, uncertainty characterization, and so forth. I'm very flexible so as we go through this, if you have any feedback or ideas of how we can improve this, if we should spend more time on a topic, if you did not understand a workflow and you'd like us to run it again, I am completely democratic. That's why I tell my students, in fact, I'm known in my undergrad classes for the fact that I even have mutinies where the students just say, no, we want to cover this topic and I'll tell you what, I'll go with that because I'm all about your learning, I'm not driven necessarily by the schedule. I'm available, I have an open door policy. My attitude is once we have a class together, we're family. A try to, try to give me a call once we're back on campus, but drop me an email and you'll find that I, in fact, respond. I had an executive from Noble Energy told me recently, said, you're one of my favorite professors. I call you actually pick up the phone. I am very much engaged with former students and people in industry, I'm all about trying to help out. I'm a junior, but I have a mining engineering degree, which was more geologic. My PhD definitely became half geology, half engineering, and even within Chevron, I spent 13 years in the Earth Sciences R&D, ETC group, so I do speak geo. Now I'm also very active in outreach, so if anybody here on Twitter, no one better for you. I'm interested, I'm the geosatts gone Twitter, every day I share technical knowledge, information, lectures on Twitter. In fact, to anybody here frequently use YouTube to learn new things, maybe using YouTube lectures, I record all of my university lectures put them on my YouTube channel. So anyone in the world can join in in my university classes, and I put all of my workflows for my university classes on GitHub, so you can follow along. If you want to learn more about some of these topics through my university classes, so I'm all about outreach and supporting people. My goal is to support and partner with people from industry, people from the working professionals and the students, we all win together. That was something I learned within Chevron, we all win together, and I believe that's true. Now, I should say a couple words about Dr. Foster. Dr. Foster and I are together within the department of petroleum and geosatts engineering, but he's also jointly appointed over in the department of aerospace engineering and engineering mechanics. And so while I'm the person here talking to you in the next two half days, and then even afterwards if you're attending the next session, John is the one who, what time were you working until yesterday, John? Was it till midnight? Well, I was, no, I wasn't quite that late all day long basically from seven to seven or so, but yeah, a lot of that was just maintenance on the, on the, you won't, you won't ever see what I did, you know? So the reason that we can all go on the cloud and work within these environments and get the job done is because of all this work that John's done. John is truly more of a data scientist. And so even in our course offerings, John does have a three day course, which is really, really data science. He teaches, he teaches people how to do build up dashboards, how to make things that automate your work, how to work with desk, he real big data type applications. John is covering all of that in his courses too. John, did I represent that right? Perfect. Okay, go to awesome. Okay. Um, he also is active on outreach, anybody who's watching and learning on YouTube. Actually, what's really cool is John does like his engineering mechanics, numerical optimization and so forth, explained like matrix math, explain in a very intuitive manner on YouTube. And so I think just recently, you start to put all this content together on one single channel. So if you learn from YouTube, consider checking out some of his content and he's also on Twitter. How we will accomplish this goal of a run fast forward and do some machine learning is we will have this introduction right now. And this whole point here is just to motivate you, get you excited, you know, psyched out about this excited. Then we will spend about an hour, hour and 15 minutes or more on an overview of machine learning. We will introduce all of the concepts of terms, some of the limitations. And I will even introduce a very simple machine. And I will challenge you to prove to me that it's not a machine. Spoiler alert, it's going to be linear regression. And I'm going to say prove to me that's not a machine and we'll use that as a demonstration of what machine learning is. What we'll do is and today, this is where we'll finish up is we will cover some prerequisites for machine learning within Python. And so in order to do that, you need to be able to load your data, visualize your data, you need to be able to look at histograms, location maps, you know, scatter plots and so forth. And so we'll cover that very quickly. We'll just and we'll then we'll play around for data set. We'll load some things in. And that will be where we'll finish up today, I believe. And then to more morning, we will jump into a very short lecture where we'll cover different types of machine learning very fast. And then we will spend a bunch of time working in a workflow on the cloud with a bunch of machines. We will do a run through together. And then I will challenge you to build a better model than I did. Now spoiler alert, you should be able to beat me. You should be able to beat me. So it'll be a good opportunity. And then we'll talk about new opportunities, limitations. I want to kind of leave you with not, I don't want you to have an inflated sense of, you know, security and confidence. I want you to realize that there's, there's some cautionary tales, there's some pitfalls. And then I want to inspire you and I'll show you some really cool new machine learning, which I think are cases of machine learning revolutionizing revolution, revolution, causing a revolution in our geoscience and engineering applications, operational capability. Every one of you, when you learn new skills and you put new tools in your toolkit, you're able to add more value at work. And so as you face more, more technical problems and you have more technical solutions, I know everybody innovates. Everybody comes up with really cool solutions. You may have had previous methods and this is not drawn into scale. Maybe, you know, somehow exaggerated scale. We're used to that. But you had, you were adding value with everything you did. Now, as we teach you new theory and methods, my anticipation is you will add incremental value, more value than you did before. What I hope will happen though, is that what you'll find is you'll learn a new way of thinking, which means that there will be a variety of step changes in what you do at work. You'll be like, hey, that problem I worked on before, maybe I should try some type of machine learning, automatic classification to augment or maybe I'm working with big data and it'd be kind of cool to detect a pattern or something because it's too much professional time to try to parse through that data. So I hope that you'll find many new opportunities and you'll go on YouTube and so forth, you'll learn new things that will change your mindset. Okay. Now, so today we're going to build from zero. So when I asked, is there anybody here who's done anything with machine learning and there were a bunch of people who said, haven't really done anything, I wasn't afraid. I'm not concerned. Actually, I'm happy to hear that because we're going to build from zero. We're going to provide overview of the methods and then hands on experiential learning. Now the learning is very safe because we're in the cloud. The environment set up. It just runs. We've tested the workflows, the well documented. You'll have a chance to play with the workflow to change things, but it should work very well. It should just run. Now, of course, full workflow development would require time to investigate the problem, the available data. As we said with an ETC, everything we do should be fit for purpose. And so I want to acknowledge that right now. We also do have a lot of content for more advanced topics. If you're interested, more advanced things, there's tons more we could do. If anything, we have aired on the side of simplicity for accessibility for an introductory course. So if anybody here, it's almost like every time you teach, there's going to be that one or two people who are experts. You know the ones who are going to be the troublemakers, not kidding. The ones who are going to challenge and so forth, this course may not be the advanced course you may be looking for because of that. Now I thought I saw a question. I saw a hand up. It was just a relic of a previous question. Okay, remember, jump in if you have any questions or comments. Now there's much more we could cover as we get into these building blocks of how to do machine learning. Well, there's a lot more you can do that re-implement them and to expand them to address a variety of problems. In fact, really cool. One of my students just yesterday submitted to me a project where they actually took the type of methods we're talking about today, specifically a support vector machine and fit a model for failure of pipe junctions based on different flow rates and temperature differentials. This idea of some type of turbulence with thermal cycling can cause fatigue in the pipe wall. And it was fascinating. It was really cool. And it was something completely non subsurface, but energy focused, nonetheless, and quite an important problem. There's so many other problems you can solve with machine learning, not just the subsurface geo science and reservoir engineering. Now there's much more we could cover. There's additional theory. There's much more we could do of hands-on, experiential learning. In fact, there's a topic I was kind of almost going to talk to you about. Feature selection. This is the idea of I have a data set. Anybody here ever worked in a problem where they have many different features or variables that they could work with? You're in the subsurface and you've got all of these variables you could look at and which is the right one. I remember Mike McCurnery over and he was in the Haynesville for a while. I believe he's still an MCBU. Maybe I hope he is. He basically, he would tell us everybody's looking for the secret sauce, the secret recipe for production and non-commentions. And part of that problem is which of the features should you work with? Now we could have spent two hours talking about the statistical analysis and expert judgment aspects of feature selection. Another topic actually, ExxonMobil had me spend two hours with them. There was about a hundred people dialed in, so I don't know how many people were there. And I just spent two hours talking about feature engineering. Now if you're a geoscientist or an engineer, there's a lot you can do to put your knowledge into the variables that go into the model. In other words, anybody heard the adage, the old adage that 90% of the project is data preparation. Anybody ever heard that? I've said that before and thank you, Alessand. And other companies have actually argued, no, it's 95%. It's seriously. So I spent two hours talking to ExxonMobil about how you can do data preparation with your features to be able to incorporate geology, geophysics and engineering into your model. There's a lot we can do with the features. Okay, so those are just two topics that I think would be extremely practical. But unfortunately, we don't have time to cover in this short session. Workflow development. There's so much we can do workflows. Also, basics of Python. Once again, Dr. Foster teaches in Exxon class. He spends, while I will talk about NumPy and about pandas for literally just like 15 minutes today. Dr. Foster goes into some really extensive exercises on how you can work with your tabular and gridded data sets. Model tuning and model QC. You're going to see right away that hyper parameter tuning is essential to machine learning. But unfortunately, we just do it by hand. Now I think that's great because you'll get a little bit of experience with it more directly by doing it by hand. But still, there's a lot we could do with like really good like Kfold Cross-Valitation, other types of methods that are much more automated. And more ways to integrate geo-science and engineering. The plan will do interactive lectures. So thank you to all of you who are showing yourself on video. It does improve the interactivity. Thank you to everybody who's been putting their hands up. Let's keep this as interactive as possible. I really wish I could be hanging out there. I really do enjoy the visits. Demonstrations of methods and workflows in Python. Every time I show workflow, I'll do it first. And I'll do it really slow so you can see what I'm doing. And then I'll ask you to follow along and try things out. And at the very end, I'll ask you to try to do something on your own. Give you time to work on it on your own. So you get your own experiential learning. That's how we're going to get the job done in the next day and a half. Now it is a bit of an ambitious schedule. In fact, we'll adjust for success. So if you're lost or stuck or something's not working. Now if it's a technical problem, please, Avi provided his email contact. If you're on the Slack channel, just jump in and talk to Avi. No one should be stuck. No one should be struggling. No one should be, I can't connect or something's not working. Now if we find that we're doing an exercise, if you're just sitting there and nothing's happening for you and you're not learning, tell me and we'll immediately switch back to a demonstration and all or part of the class could continue forward. And I could go back and do a demonstration again. Feedbacks always welcome. No one should feel lost. Okay, who here right now works in Python? Who codes in Python? Dr. Foster likes to go to Google Trends and show that Python's winning. And then compared to like R, which is used to be kind of the premier statistics lane, which the work in. Definitely Python starting to take over. The other thing about Python that's very interesting to me is I code, I code in 4 trans, C++, visual basic. I've done lots of coding over the years, like full stack development. In fact, and I'll tell you what, in C++, your attitude is to code something up, to make something to program it. And Python, the attitude is much more workflow development. In fact, what you do in Python is you leverage packages a lot. Now I was, I had an interesting situation. I was trying to build a trend model. Anybody here ever built a subsurface trend model? You know, we're trying to figure out the porosity trend by depth, porosity trend, errorly over the reservoir. I was trying to do that in full 3D with an dense data set was I needed to do sparse data convolution. Now when I went to the standard sci-py and other packages, I couldn't find that. So I looked and I looked and I looked. I ended up using astral pi. Anybody ever used astral pi? It's an astral physics package. And it had incredible functionality for sparse data convolution. In other words, building a smooth trend model from sparsely sampled spatial data. In other words, guess what? Space is space. Space out there is the same as our space in the subsurface. And for modeling, I use astral pi astrophysics to be able to build my reservoir models. I think that's wonderful. Actually a funny thing is when I taught at Anadarko, they started laughing at me because two of the data scientists in the room were astrophysicists. And they actually used the package. One of them actually co-authored the package, which was super cool. Okay. We can leverage the world's brilliance. Bill Gates tells us about certainly there's a phenomenon around open source. In fact, Dr. Foster is pretty strong in his language. He'll talk about the fact that open source software has quality that rivals the software from CAN software. They standard kind of canned commercial software packages. Because open source is being reviewed and continually improved by an entire community. You know, everybody's helping out. Now what I would say is 20 years of C++ and 4Tran, but with Python, I code much less, but I get much more done. And I think that's beautiful. What do they say? The only error-free code is the code you didn't have to write. You know, like in other words, write as little code as you can. Because ultimately, it's just a lot of work to maintain and work with code. And Python allows you to do that. Okay. Anybody here used Jupyter notebooks before? Okay. There's a bunch of you have never used Jupyter notebooks. So let me make a comment about that. First of all, Jupyter notebooks are awesome. And you might also be using Jupyter Lab. It's all good. But what you do is imagine something in a browser where you have a bunch of windows or panels. And each one of the panels you make a choice is it going to be text or is it going to be code? And so you can put together a workflow, a nice workflow that's web-based. You can read it and look at it within your browser. And we'll do that right away. And you can have an explanation, introduction. The text can give links where you can get to different resources. Anybody here ever use the latex? Anybody ever? I know of some PhD advisors or specifically Krull. And I'm joking around, but my advisor insisted that I wrote my PhD thesis in latex. Latex is, and when I wrote the book, we wrote it in latex. It's basically the language of publishing. And so you can write really nice formats, equations. We don't use latex here, but we use markdown, which is basically a watered down simplified version of latex. And so we're able to build really nice documentation with the equations, the math, the explanations. Then right here, you see this right here? That's a block of code. And so then you can run that code. So I could put documentation. This is what I'm going to do. Here's the code. Then it's live. The person can just run the code, see the output. And then you'll have more documentation, more code, more documentation, more code. And look at that. The output is a bunch of histograms. And you can just see them. Or the output might be a matrix scatterplot and so forth. You can build beautiful workflows that go in sequence with great documentation and live code. And you can share it with others. Now what's really cool is you can use containers like sandboxes. You can put this workflow online, maybe on your internet or on the internet, if you wanted to. And other people can run it. And so this is a wonderful vehicle to be able to deploy. Because I say specifically Python in Jupyter notebooks. Now what's really interesting is that Jupyter was designed to be this workflow development type environment where you can put documentation and code where you can prototype very easily. But the name is kind of a little bit of an interesting acronym. JU stands for Julia. Anybody here ever code it in Julia? Anybody? I had my Julia moment. I had about a month where I started to code a little bit in Julia. Julia is kind of a new language being developed out of MIT. The point of Julia was to be like Python, but to be as fast as C, the C programming language, to be very quick and optimized. But Julia is still a little immature. Anyway, Julia, PY is Python. Now I'm, what am I forgetting? Oh, oh, no, no, Python. And then they are is an R, the statistics language. So basically the name is just kind of they put it together. Now I think it's also like, I forgot. There was some other context behind it. What does that mean? If you open up Jupyter and you build a workflow, the very first thing you decide is what language you want to work in and you can choose. Now, the native Jupyter that you download with Phanna Conda will only give you the Python auction. You can download the R, the Julia. I think there's even C and Fortran kernels. And then you can actually select and use different languages to work in Jupyter. Okay. Any other questions, comments? Have I sold it to you guys? Are you excited to use Jupyter now? Okay. Let's talk a little bit more about coding. Now when I teach coding, Dr. Foster teaches coding more kind of fundamental coding and good coding practices. Actually, he goes all the way to good software engineering, I would say. I don't teach that way. I teach more workflow focused. I'm all about, let's introduce well documented workflows, get you started. If you look at all the code when we show it, notice the fact that I'm not going to be like concerned if you don't understand every line. I'll show you how you can learn or interpret it, but I'm not going to be worried about you understanding every line. Okay. We're noting that Jupyter is very easy to use internally. Can install everything you need through the, through the GIL options panel. Practical approach, we basically code in order to get a job done. I'm really looking for awareness, appreciation for what can be done, the level of complexity. You know what's funny is Dr. Foster taught his course on data science. And the very next week we were hearing about people automating reporting to their managers and to their community, their stakeholders. Every week they had to put together this report where they gathered a bunch of data. They built, there was a template and they shared it with a bunch of people. And they had automated it using these techniques that Dr. Foster taught in data science. And they didn't have to do that anymore. They were saving themselves literally two, three, four hours that they spent every week doing that. Sometimes it's just efficiency and that matters. Okay. We'll talk about basic workflow design just a little bit. Is there anybody here? Okay. Let me ask the positive question. Who here put your hands up would say that you code? To those of you who are kind of on the fence, can I suggest the couple of comments to push you off the fence onto the side of coding? When I, would you believe that when I teach my undergrad classes with 70, 80 undergrad engineers that many of them don't want to code? Could you believe that's possible? I know a lot of us have this perspective that engineers kind of always want to code. These undergrad, they don't want to code. We have to as professors kind of get them to code, right? So this is what I share with them. Also the geoscientists. Actually, my classes are getting to be half geoscientists. I was just appointed over in the Jackson School. So I'm a professor in both engineering and geoscience now. Okay. Transparency. No compiler accepts hand waving. You ever been in a room and somebody's telling you how we're going to solve the problem and you kind of think maybe they don't know how to do it. Like that really won't work. When you code, you have to lay bare your logic. And it either works or it doesn't work. In fact, Dr. Foster, when he hands out his assignments and his classes on HPC and on, you know, numerical methods, they submit it and it goes to GitHub and it is marked as a unit test. I believe that's the right term. So basically it's either right or wrong. It's binary. Okay. Coding is perfect. It's beautiful for that. Reproducibility. You run it. Get an answer. That's a scientific method. Notification programs need numbers. If you make a program, if you make a numerical method, it will drive you to use your data more to use your data better. Open source. You can leverage the world of brilliance. I'm telling you right now, if you need to get a job done, somebody else has already done it. And that open source code is available for you to use. You can get a lot done very quickly. You can break down barriers. If you're a geoscientist with expertise in subsurface modeling and they're making a brand new tool to support reservoir modeling, isn't it great? If you can sit in the room with the programmers and you have some understanding of what they're trying to do, you understand what you want your tool to do. You have the domain expertise, but you can, you actually know what's possible and you can make sure that your expert knowledge gets into the app. That's much more powerful. Anybody ever heard the term? There's always time to do it again. You know, often we say it's always time to do it right, but there's always time to do it over. How many times have you built a workflow and ran it just once? Who here has done that? Who here has done it perfectly the first time it never had to happen again? Isn't that beautiful when it happens, but most of the time would you agree that you have to do it two or four times, six times seven? How many times have you built a workflow and the business unit actually drills a couple more wells and what do they want to do? They want to update with the new information, right? So remember, if you build a workflow, if you coded up, you automate it and now you can just rerun it with new information, you can make a repair, you can fix that and rerun it again. In fact, let me ask you this and this is a, I'm kind of trapping people a bit. What is the methodology for documenting workflows right now? Do people still write in those scientific notebooks? And Thomas, I forgot his last name, Thomas Martin, I think from over at Colorado School of Minds, PhD candidate over there, said, good documentation is a gift to your future self. Or to the future person. Okay, the main point, I rambled too long about that. When you build a workflow, it's actually a very good form of documentation. Most of the time, because you have comments in the code, you saw the exact workflow, you see the steps at least. So it's very helpful. Jupiter notebooks are a very good way that we can document workflow for our future self and for other people who pick up our project. Okay, good. There's all kinds of other stuff, stuff about coding. I'll tell you what, the efficiency aspect, I like to say with data science, engineers and geoscientists spend more time doing engineering and geoscience and less time making reports and doing the things you can automate. Okay, now I want to put some caveats out because when I show that slide, many people think, okay, perch is kind of being anti qualitative. And what I'd like to say is, or perch thinks everybody should do C++. So I think any type of coding, scripting, workflow automation is matched to whatever you're trying to get done to add value is great. I spent actually, I learned C++ to take an internship with Chevron and California when I was in my PhD. And I spent probably about 10 years at Chevron doing C++, what we call like full stack development back in front in and everything within GoCAD. And I'll tell you what, not everybody needs to do that. I think to be honest, Python is probably much more efficient for many people to prototype. So I don't encourage that. I respect the experience component of geoscience and engineering. I believe that there is much of what we do that actually does go beyond coding. That we can't, I don't believe in the geoscientists or engineer in a box that we can somehow replace ourselves with robots or anything like that. For judgment, there will always be some level of subjectivity because we work with the subsurface. We can never do everything fully objective. Anyone who claims to be 100% objective in the workflow is just sweeping subjective decisions under the rug because they're still there. They're still there. Okay. Or they're creating models that are much too deterministic and there's not enough uncertainty for sure. Okay. How was that for an introduction?
 You have the same slides. Good. So far so good, right? So far so good. So what we'll do right now, my motivation right now is to just provide you with what's the motivation, what's the goals, general content will cover, kind of a rationale of what we're going to cover over the next four half days. And talk about the objectives of summary, make some comments about open source, try to motivate people if you're interested in coding, if you're interested in open source, why this could be useful to you. Okay, so first of all, a little bit of introductions. Now, actually, some of you I recognize you, so you know me, but those who have never met me before, my last name, I do know that it is a clump of consonants that makes no sense at all. Don't be worried about it. Just say perch. If you say perch, you're saying it just as well as my family does, we lost the ability to pronounce that about a hundred years ago when we came to Canada from the Ukraine. Now, I do have practical experience. I'm not actually a typical professor. I'm actually a person from industry just having kind of an unusual academic experience for the last couple of years. So I'd have many years of experience of doing this, teaching, consulting. I used to teach inside of what I'm Chevron when I was working there in fact. So I'm used to teaching within the industrial environment and also at a period of time I was leading an R&D team that was conducting research in this field and also doing our own work and developing our own products and reviewing projects around the world. I'm very flexible. I like to tell my students that I believe in the principles of democracy. It means that any moment you guys could have a mutiny. You could say, um, this topic we want to spend more time. Let's slow down and go through some more examples. Also, I do have a lot of content and I did warn, obvious and canal and john about this. I said on a lot of content. So at any time like during the breaks, we'll talk about it. And if somebody says, well, you know, Michael, I would really like to learn about support vector machines. We could kind of, um, put that in. We could kind of fit that into our discussion over the next four half days. So we'll talk about some of that other content. We could kind of go off the rails. If there is a, you know, grassroots movement to say we want to hear about a topic, not on the current list. Now at the same time, we have an ongoing thing together. So I'm more than happy to cover something like that during office hours or during some type of short kind of a short lecture. We could hold at a later date. So we can get into a lot of different topics. I'm available. I do have an open door policy with all of my students, which means that and you're now my students if you attend my class. And so fire me an email directly. You can, you can also, um, DM me on Twitter or something. I'll respond to you. Um, I'm also, I'm an engineer. I am formally trained as an engineer, but actually a kind of cool life goal. Just last month, I was appointed as a professor in the Jackson School of Geosciences formally. So I'm now an associate professor in both Cockro School of Engineering and Jackson School. Um, I have a strong geosciences background. I speak geo. I much of my work bridges geo and engineering. So I do cover the geo side here. Um, so just super excited about that. And I'm very active on social media outreach and so forth. Actually, I should mention just a little spoiler here. I'm about to send an announcement. I'm anybody here in the AAPG. I'm about to send an announcement. I'm going to be the editor on a special issue of the AAPG bulletin on data and analytics and machine learning for next year. So if you have some ideas, a good case study or something, get a jump on that. Let's get that in there because I really do want to have a chance to showcase good case studies. And I know you guys are doing a lot of good work. All right. So, um, really what am I saying? I'm not bragging here. I'm just saying I'm very much committed to supporting working professionals and developing new skills and data analytics and machine learning. I'm very motivated around that. And I love participating with the community to do that. Now, I've kind of spent a little bit too much time kind of bragging about myself. I think I gave myself like three or four slides. Hey, John, are you still on the phone? I know, John. I'm here. I'm here. Hey, John, do you want to jump in and talk about yourself a little bit? I can talk about you. Sure. I'll say a few words, even though you won't be hearing from me too much during these next few weeks. Dr. Perch is going to lead the way, but I'll be around when I can be and possibly pipe in and say a few comments here and there. And then for those of you that are going to take the energy data science course and that starts the first week in May, I'll be teaching the majority of that with kind of reversing roles with Dr. Perch. One thing I did want to comment earlier, can all announce that Dr. Perch is our geostats guy. He's not our geostats guy. He's the geostats guy. So if you're following on Twitter, YouTube or anywhere else, you'll know what I'm referring to there. But anyway, yeah, my name's John Foster, associate professor and petroleum engineering. Also have an appointment in aerospace engineering and engineering mechanics and at the Odin Institute for computational engineering sciences. So my background is primarily in physics-based computational modeling. I used to work for Sandia National Labs for seven years and wrote physics-based codes there that kind of exploits some of the world's largest computers. And it was that world that brought me into data science a handful years ago, kind of bringing that expertise. My interests now are really in kind of combining physics-based modeling and data science and machine learning where applicable. And of course, like it's like was mentioned earlier, I have a lot of experience in software engineering. So I bring definitely that kind of software engineering flavor to the courses. So when you see me in a few weeks, we'll definitely learn a lot of the software engineering side of the data science world to allow you to write testable real production quality type code. Yeah, so as mentioned there, I do have a social media presence, although not quite as large as Michael's. I do have a lot of content. I haven't been very organized about how to put it up, but I'm trying to kind of unify all my content under a YouTube channel that's just associated with my name, John T. Foster. So there's quite a bit of content on just introduction to Python and numerical methods there. And I'm in the process of moving all my content from courses taught in reservoir geomechanics, reservoir simulation, things like that under this kind of new unified channel. But you should be able to find everything. If you're interested in anything that's out there, just contact me, email, Twitter, YouTube, however you want to do it, reach out. And anyway, I'll be in the background for this week, but I'll hear more from me in May when we talk about the energy data science class. Thank you very much for that, John. In fact, I want to give John full credit while I'm sitting here talking over the next three half days. I hope everybody realizes that John is behind the scenes, making everything work, the fact that we can do everything remotely in the cloud on the server. That's very much John actually just this morning. I was texting him saying, hey, can you check on this, check on this? And he's been really, thank you so much for all that, John. Now we've broken this up into each of the days as a half day. And for remote learning, that makes sense because if you spend a full day remote learning, I think people find that attention is very challenge. Even somebody who has all the best intentions, it's very hard to sit there in your own home and learn for a whole day. But we think the half days work well. The first day we'll get into prerequisites and perspectives, basic probability and statistics, data analytics, data preparation, and we'll cover spatial models and spatial estimation. Now, now for the first section, we're going to stop probably, I think we'll make it through data preparation. Okay. And then we'll get into more geospatial data analytics on Thursday during the morning. And so we'll cover spatial models, spatial estimation, spatial simulation, uncertainty modeling. Day two, we'll do a machine learning introduction. We'll get into dimensionality reduction clustering. So the third session is going to be introduction to machine learning plus we'll cover a inferential use of machine learning, the inferential methods. Then on the last section, we'll get into the use of predictive modeling and machine learning. We'll spend a bunch of time and tree based methods, they're highly intuitive. They're really, really powerful methods when you use them as an ensemble tree approach like bagging random forest. And we'll cover that and then we'll get into artificial neural nets. You'll be able to play around. We'll do some deep learning because we'll do like multiple hidden layers. We'll experiment with that a bit. Okay. So this morning, we'll start with prerequisites, perspectives, we'll do basic probability and statistics. So that's what's on tap for us right away here. Now every time I teach a course like this, I like to remind people that everything we do is about adding incremental value, making you more powerful at your desk, doing your job. That's our motivation. In my old company, we call that building operational capability. That is giving you new skills. Now what I would say if you can imagine, if we give you more and more technical solutions to existing problems on this axis, over time, you'll be able to keep using those skills. And whereas you were previously adding value, and I'm sure lots of value, you know, this is not drawn to scale, I anticipate that with some new theory methods, you could add an incremental amount of value, you would add more value in your job. And I think that's very powerful. But what I would aspire to is the idea that you learn a whole new set of theory and new ways of seeing problems. So you find various new solutions and methods that you can use to do your job better. And so you'll find that you have the step growth of being able to add more value, add more value, you'll find more and more ways that you can apply it. It becomes a person with a hammer, sees the whole world as nails. If you're a person with new data analytics and machine learning skills, you guys start looking at all of your data sets, all your problems and thinking, why shouldn't we use these technologies here? Over the next four days, whenever we do this, we always build from zero. We provide overview of the methodologies, hands-on experiential learning, we'll have well-documented workflows. Now, Dr. Foster, when you get into his material, he will teach coding. He teaches how to get the job done in Python. By the end of Dr. Foster's course, you'll be able to build out dashboards, you'll be able to automate reporting, like really, really kind of like focused on doing the code to get the job done. My job is to teach you about workflows, to teach you about methods. And so, we have great content to teach you basics of coding. I'm not going into that over the next four half days. Instead, I'll provide well-documented workflows that are already in place in Python, but my focus is more on the building blocks and the parts of the workflow, and walking you through and helping you understand how to do the methods safely, and how to interpret and understand the results. Now, we have a lot of content for more advanced topics, to be honest. And so, what I would say is because we made the choice to kind of build from the ground up, I would say if we have erred, we've erred on the side of simplicity for the purpose of accessibility, ensuring that everyone come along and we could all learn together. Now, there's always the opportunity, as I mentioned before, we can get into advanced sessions on a variety of topics. In fact, I teach graduate level courses as does Dr. Foster, and I teach graduate level courses on data analytics and machine learning of a lot of content, advanced content to share. There is much more that we can do. The building blocks that we're providing you in the next four half days, they could be re-implemented, expanded, used in a wide variety of workflows. We're not able to cover all of those opportunities. And there's additional theory we could cover. I did see a couple of people put the thumbs up, and we could get into more deep theory. Deep theory results in a deep understanding results and more impactful use. More hands-on experiential learning. I think that's really, really fun. I'm going to show you today and over the next section sessions, I've gone into a little bit of a hobby of making interactive Python workflows. And this is really improving our opportunity for you to be able to experience the methodologies. You can pull a slider and watch the machine move and change, see Bayesian statistics up close and personal, things like that, pretty powerful stuff. Basics of Python and R, you will cover that with Dr. Foster. Sorry, not Python. Yeah, I won't talk too much about R right now. Model QC, this is something that we really should or could spend more time on. And that's you're building a model now to check the model. And there's a lot we can do to improve that and methods to integrate more engineering and geoscience. You'll hear from us, John and I, our fans of, or we believe that the main expertise is critical even with the data driven approaches. We never see this just put the data in the hopper and see what happens. Okay. Here's the plan. We'll do interactive lectures, discussions to cover the basic concepts. That's how we're going to get the job done demonstrations where I will walk through and show you a workflow. After I walk through a workflow, I will give you time to play with the workflow to try something. I'll give you a challenge or a question, something I'll ask you to try or check out. So that's the approach we're going to use for the next sections. Now, this is an ambitious schedule. We're trying to cover a lot of content. We will adjust everything for success. So let me know, you know, we have a good number of participants and I really do appreciate people committed to spend the morning with us today. That's so awesome. But if you're stuck, if you're lost, if you're not learning, if it's not working, speak up. And we can adjust. If we're doing a workflow, and I've asked you to do something and you're just stuck and you're not getting anything done, it's not helping you, I'll switch it into a walkthrough and I'll just start walking through and we'll do it as a group. Or we could do it as a smaller group and let other people work at the same time. We can do anything like that. We can switch it up if people are filling their not learning. Feedback is welcome as we proceed. Communication is critical. Now, let me just say, like, why do we want to work in Python? Why is Python useful to us? Actually, let me ask you, anybody here used the program in another language and they've converted to Python? Can you maybe you'd like to speak up and just say, what was the advantage? How did they see that? I would say that the use of Python, the thing for me, has been the change in my mentality. When I coded in Fortran and C++ and I did full stack development for quite a few years, I would often think of a problem and I would just start writing the code. In Python, I think of a problem, I go online and I find a package that does it and then I implement it into my workflow. What I would say is we leverage the world's brilliance, much more of Python. Packages allow us to put together workflows with a lot less coding. In fact, my quote would be 20 years of C++ in Fortran, but with Python, I code much less and I get the job done. In fact, what did they say? The most robust code is the code you didn't have to write. Because often you'll find whenever you have to write large volumes of code, there's always and Dr. Foster will talk to us about this code maintenance, code testing and so forth. Coding is a lot of work and if we can avoid coding and use other people's code, that's very powerful, very, very powerful. And Dr. Foster, I'm going to steal your thunder here, suggest that open source code is getting to a point where it arrivals the quality of many of the more canned commercial software. So now who will hear, put your thumbs up if you're using Jupyter notebooks. I'm curious. Jupyter, and you'll see it when we get online and we work through workflows, is really a workflow hosted on HTML like within a browser in which we're able to have blocks. And the blocks can have, you guys can all see my mouse, right? Sometimes, thank you very much for that Tim. I saw that nod. Thank you, Evan too. Okay, so you can have a block of documentation. Now, the documentation is in a language called Markdown, which is a watered down version of letek. And anybody ever wrote like a master's or PhD thesis, specifically like 5, 10, 20 years ago, I think most of us did them in letek, right? It's kind of the language, republishing a book and you have to kind of use language to include your figures and your formats. Markdown does that, but it's kind of a watered down version, which means you can write very nice equations, you can have very nice formats, you can put hyperlinks, right here I introduce myself and I put a bunch of hyperlinks to my online content. So you can make a very nice documented workflow with equations, explanations of what you're about to do. Then you can put a block of code and then you'll have like explanation block of code and then you have block of code and the output from the code, the graphical output, where it could be tables, it could be plots, it could be anything. And so by doing this, you can put together a very nice nicely documented workflow that you can share with other people in your company. Now let me ask you this, has anybody ever developed an engineering or geoscience workflow in Excel and shared it with co-workers? Anybody ever done that? Yes, I knew it, I knew it, I'll tell you what, I remember in my days in Chevron, there was this individual mic weight and he was magic with Excel. Have you ever seen somebody do things in Excel? You didn't think was possible? Like a deep knowledge of VBA and the charts were super interactive. He made these interactive charts, you could actually like drag things around. It was incredible. And the engineering equations were operating in the background. The thing about it was the documentation was somewhat poor. You had this Excel spreadsheet with like this attempt at a dashboard and then a whole bunch of documentation all over the place. And then one of the biggest problems you had was there'd be multiple versions of the Excel spreadsheet floating around the corporation and you never knew which version you had. There was all these kinds of issues. Well, this is like doing that much better because you can put together your well documented workflow, if all your code with better documentation with a nice sequence of steps for the workflow, great explanations, great charts. You can use the world's brilliance of packages to be able to get the job done without having to code everything by scratch. And you could use containers, a sandbox, hosted online like we're doing. And then everybody can jump in and use your workflow and you ensure they get the right version that they have the right environment. Everything is set up and it just works. And you guys will see that as we work together. So if you don't currently use Jupyter, consider it or Jupyter notebooks or Jupyter lab, consider as an opportunity to put together workflows that you can document and share with others, even host online. Very, very powerful. Okay, I hope I convinced some of you to consider looking at Jupyter. Now, you don't see this because we set up the environment, but we're using standard anaconda except for one or a couple of packages, but one of them is Geostat Pi. So don't be concerned about that. When we run workflows, we will import Geostat Pi all the time. Geostat Pi is my open source package on spatial data analytics. The reason I coded it up last spring in the middle of teaching two courses and basically spending every weekend coding up algorithms was because when I looked out at the world's packages, I could not find a good spatial data analytics package to get the job done. So I forced to use my own package, which I did it to support my students. The good thing is it's becoming quite well tested now. It works pretty well and we'll use it to get the job done in spatial data analytics. It's open source. You could install it anywhere in the world just by typing in pip install Geostat Pi. It's on the Pi Pi repository for anybody in the world to use. So anyone can download and work with it. It's for spatial data analytics geostatistics. Okay, let me just make a couple more comments around coding and then we'll basically finish up with this introduction here. Coding. Now, let me just by show of hands, who here considers themselves a coder? Who here would say that coding is a skill that you have? I see Brandon, you have a checkbox, a checkmark, are you still? Is that from last question, Justin, you're checking it. Paul, okay? Scott, Sarah, good. Very cool. Now, there might be some humility going on here, or maybe in posture syndrome, but basically some of you may be, you know, well, I do I dabble in a little bit. Let me try to convince you about the power of coding. Now, first of all, what I'd say is when I talk about coding, I'm very workflow, workflow focused. In fact, let me be honest with you and people see this. If you come work with me, you'll see I'm like many coders. I have my dialogue open, my IDE, my integrated development environment, my Jupyter notebook or lab is open and I'm coding on the other screen. I have stack overflow or I have some type of program, a website open where I'm basically able to ask questions about code and get assistance, get snippets of code that I can use. Remind myself, oh, where did the bracket go? Do I need to use the semicolon there or not? Many of us are like that. I have to admit, sometimes when I'm coding a Python, I mix up the context, you know, the syntax of C++, Fortran and, you know, Visual Basic with my Python, because you know, I mix a match between the languages. So many of us code, but we're workflow focused, we know enough code to get the job done. There's a very practical approach where you focus on coding as a means to an end to get the job done. Now, there are some people go down the journey to learn coding at a very deep level and I would look at people like Dr. Foster who clearly has a understanding of coding at a very, very, very deep level so that he's able to produce great quality code and teach people to do coding at a very high level. I'm very strong level. Okay, expectations. I'm really focused on awareness. What can be done level complexity, basic workflow design, experience with the tools. And so getting a chance to play with and use the tools that are available to us. Now, if there are some of you who are not coding right now or dabble and coding are interested to now make the jump to coding at a much deeper level and this is good motivation. A segue for Dr. Foster's course on data science that will really push you to a deeper level of competency as far as coding. These are my top reasons why I think everybody should code. In fact, when I put the soda on Twitter and I'll link then, I said, I think everybody, every engineer, every geoscientist should know coding. And what was interesting, I got a little bit of pushback, but not that much. I think we're getting to a point where people realize, you know, it's kind of like eating your vegetables. You know, I love the barbecue too, but we really do need to have a little bit of vegetables. And so coding is kind of like that. It's kind of good for all of us. Okay, so if you don't mind, if you can hear me going through a couple of these, first of all, transparency. Have you ever been in a meeting where somebody asked you to explain a topic or you've seen somebody else explain the topic and this starts happening. Kind of just hand waving, you know, we'll do this, we'll do this. The beauty of coding is no compiler will accept your hand waving. Coding forces your logic to be laid bare for everybody in the world to look at your workflow, to look at your method and judge it. Is it accurate? Does it work? Does it get the job done? I love that about coding reproducibility. You make code, you run it or use someone else's code, you get an answer, you hand it over to your colleague, to your peer, they run it, they get the same answer. That's a principle of scientific method, right? The reproducibility of results. Coding gives you that because you have a method, a set method to get the job done. Open source. I strongly believe in this. I was recently actually about a year ago, I put together a spatial trend model in a sparse three-dimensional data set. To do that, I needed a method that was efficient. It was still a lot of wells for sparse data convolution. So a convolution operator, you know, an averaging window. And I just needed something that would work with sparse data. Most convolutions assume densely sampled regular grids of data in some dimensionality. I ended up using astral pi to get the job done. Astral pi is a package for astral physics used all over the world, like in the observatories. And here I was using their package to build a subsurface trend model. And it worked perfectly. It was so efficient because guess what? Space is the same in space and in the subsurface. It's still a spatial problem with sparse data. Deployment. I had an annual review at the end of the year. And one of the things that really helped you was when there was a whole bunch of stakeholders who said, I used something that perch made to get my job done. If you use code, you can deploy your ideas widely. And to me, that's a huge opportunity to add value. You know, it might be altruistic. Maybe you want to help others, but it doesn't have to all be altruistic. It turns out everybody rises together, right? When other people succeed, you all win together. Let me just one more. How many times have you built a workflow? Done the job and it was perfect. You did it by hand and you never had to do it again. Or you never made a mistake and you didn't have to redo it. Has anyone ever done that? Has anyone ever worked an entire workflow out and it just worked and you were done? I find often you have to run it two, four, six times, right? And even if you did it perfectly right the first time, there's always new data. We work in a business where we keep gathering new data all the time. If you code something up, you script it up. Now that means that when they give you the new data or you find the mistake as they're often is, you just make the correction, you rerun it, you run it on the weekend if you have to. It's so much more efficient if you script it up. Now there's another thing that people, I think people don't think about too much, but when you create a workflow that's scripted and automated, it also provides an audit trail, documentation. How many people are documenting their projects by PowerPoint nowadays? And what do you think of that? Is that the best documentation? Are anybody here still writing like full documentation and word with like sections, explaining all these assumptions and everything? People still doing that? The great thing about scripting and automating is you provide a much stronger documentation. I'm telling you, I've had the experience of doing a workflow going away for two years to work on other stuff coming back and I can't remember what I did. I think we all kind of suffer from that. Was there a question or comment, please? All right, all right. I thought I saw somebody jumping in. Perfect. At any time, feel free to jump in place. Okay. Now when I talk about learning the code and I kind of lean on this a bit, I hope I can make a couple of comments to maybe soften my comments and make it more practical. That is any type of coding scripting workflow automation matched your working environment is great. I don't think we all have to be C++ experts. I respect the experience component of geoscience and engineering. I know that there are some things that we just can't completely automate because it turns out that some of it's just we got to be able to integrate and understand so many complicated things going on. Some expert judgment remains. Okay. That's the end of the introduction.
 There's a kind of outline of what we're going to talk about here, the digital transformation. I think Dr. Perch probably laid that out for you really well and why you energy is unique. I know he showed this slide, which references this Deloitte study that talks about the maturity of different technical sectors or sectors of business and where they fall in terms of this digital transformation. Here's energy, and it's kind of in the middle, not the lowest, not the most in terms of readiness. But obviously you all are here because you're willing to learn Python, apply these skills to data analytics. I have some slides coming up that describes what a data scientist is. You could argue that if you've been using Excel for the last 20 years, you've been doing data science. I don't think that necessarily is incorrect. However, there are many more powerful tools you can use. And hopefully by the end of this course, you'll next time you reach for Excel, you'll stop yourself and begin to realize that you can do a lot of these things much quicker and a much more automated way in Python. This is so much more charming when Dr. Perch mentions this slide. I know you all saw it last week. We want to make the argument that energy is unique among the energies. Energy is unique among different industries. Primarily because we actually do have big data. We've had big data for much, you know, a long time before big data existed actually is a sort of buzzword, particularly when it comes to seismic data. But what's truly different about, you know, us is that our data is sparse and highly uncertain. We only sample maybe one one trillionth of the subsurface, you know, basically the area right around the wells and everything else is unknown to us. That's very, very different than a lot of other problems. You know, we also. A lot of other problems, particularly in the space of social media, right, where a lot of success and machine learning has been shown recommendation engines, I'm talking about like your spot a fly and Netflix recommendation engines and other things adds, you know, Amazon, these kinds of things. You know, they're there are low consequence, right. So Dr. Dr. Purchase always likes to point out that or she shows that this is a spot up spot recommendation system from last summer. And he's a Canadian. So he listens to a lot of Canadian music like Neil Young and other things like that. And he said that one day Spotify started that guess because he was Canadian they recognize that and they started to play a lot of nickel back. He hates nickel back, right. So the consequence of that recommendation engine being wrong is basically nothing right you just skip the song and it uses the reinforcement learning algorithm. So hopefully if you skip it enough, it learns that you don't need to that you don't like nickel back and it quits doing that. But we're we're never going to make decisions in the subsurface like that like, oh, let's just go drill well. And if it's empty, oh, well, you know, we're wrong. Let's move to the next one. We can't do that. We can't afford to do that. So the decisions that we have to make are much higher consequence than most of the applications of machine learning where we're a lot large large success has been seen in the last few years. So, you know, this is we have expensive high value decisions and we must support them with data. And a lot of times that actually leads us to using simpler machine learning models because the simpler models are more interpretable. And if we can interpret the model, then we can defend our choice of, you know, we can defend our decision. Right. And so that that leads us to, you know, the aspect that that a real data scientist needs to have domain expertise. Right. And so that the models can be interpreted correctly and other things like that. So what is a data scientist? Well, this is a chart you may have seen before, Venn diagram. Right. And, you know, one circle is statistics or mathematics, you could just say more generally. Another circle is coding. And the last circle is domain expertise. And it's at the center of these where we have a data scientist. Right. And so, you know, somebody with a computer science background is really they're going to have likely a good knowledge of statistics, a great knowledge of coding. But they're going to be lacking the domain expertise. And so the models are all going to appear like black boxes to them. And they won't necessarily know that it's producing the incorrect answer or have any domain knowledge to interpret the results in any meaningful way. And so it was part of the reason we started data. You guys have the domain expertise. If you're an engineer, you likely have enough math and statistics already. So what we're trying to do is expand this circle, expand the coding circle. And, and, you know, really, not just, you know, sometimes you'll see hacking. And so, you know, you know, you can see that it's written in place of coding in the circle. And I really don't like that word because it implies hacking implies like to me that maybe there's some lack of quality in the in the in the software. But I hope to convince you over the next few days that in fact, when done correctly. And the work that we do in data science and with a lot of the tools that we're going to that we're going to show you over the next few days. Well, in fact, be of much higher quality than anything you're currently producing in certain, in a lot of cases, any kind of commercial software that you'll be using. So, you know, I don't like the word hacking that you see appear in the circle. We're not we're not going to be hacks. We want to be software engineers, if you will, produce high quality code. That's what we're going to try to grow this circle for you guys so that you can call yourself a data scientist if you'd like to. So, I think Dr. Perch covered this a lot. I'm going to skip to this slide and reinforce this idea that there's value and simplicity. Right. And so, you know, the state of the art, what everyone's excited about machine learning is neural networks. Right. It's all you hear about. And you know, if you if you took Dr. Perch's class last week, which much of you did, you got to play around with the neural network. And you know, he likely told you talked about the the bias variance trade off. Right. And I have another sort of tool that I used to describe the bias variance try not to be.
 to put it into the course notes permanently. But simple models like for example linear regression have high model model bias, but they have low variance. That is, they're fairly insensitive to changes in the data or retraining. And so what I have here is just a set of data points, the blue lines, the blue dots are either, or what I'm using to train a linear regression model. Okay. That blue line that passes through them is the trained model, right? It's the output of the model. Over here, I have the test residual error. So the red dots are my test data, right? So these are, this is, you know, I use the blue dots to calibrate my model. And I use the red dots to assess the predictability of the model, right? So basically what I want to, you know, I hold out the red dots and then I say, okay, well, how good is my blue line here when I compare it with the red dots? And that's what I'm plotting over here. And it's the total residual error. Okay. So each of these, they're just plotted against their index. So for the most part, I've got the scaling fix here. You'll see why in a second. But for the most part, there's low error with respect to the data or low with respect to what I'm going to show you in a second, right? So this is basically the simplest machine learning model we could come up with, right? Linear regression. And here's what I mean by low variance. So if I move the training data around, right? So you can think of this as like taking a different sample set. So I'm, I'm taking those blue dots and I'm just wiggling them around in space. But you could also think of them like those are separate data points. I move them. You can think of that. Those are a different set of data points that I'm using to train my model. And you see that I, as I move them around, the blue line, the model, doesn't change that much. It's fairly insensitive to the data, changes in the data. Okay. Now let's see what happens when we go to a higher order polynomial, right? So here we're adding complexity, right? So in this case, I'm just going up polynomial order. So instead of using a straight line, I'm going to use a cubic line to describe the data. Okay. Okay. In this case, still not so bad, it doesn't change terribly with respect to the data, you know, the variance is fairly low and the error is still fairly low. Okay. And I could go on to the fifth order, but let's go ahead and look at seventh order. Right. So if I have a seventh order polynomial now, you notice that the seventh order polynomial passes through every blue data point exactly, right? There's no error with respect to the blue dots. There is error with respect to the red dots and the prediction. And it, and that's plotted over here. And again, you can see that already there's more error with respect to the prediction than we had before. Okay. But let's look at what happens when I move the data around. So as I move the data around, you see that the error grows enormously with respect to the prediction. So again, we don't care. You know, it's so funny. We've been trained our whole lives and to talk about our squared values and things like that. But all that, all that does is tell us how well we fit the data that we already have. It gives us no ability to predict anything. Right. So you can have the greatest fit in the world. And here's one of them. Right. This is a great fit to the blue data, but virtually unable to predict the red data in any meaningful way. So probably the best is somewhere like a third or a fifth. I think the third is actually the best, right? And it's not great. But you see there, even with the third order polynomial, we're not fitting the blue dots exactly. But in general, we have the lowest mean squared error with respect to all of the data. And this is why it's really, in my opinion, you should always start simple. There's a concept called Occam's Razor. Is anybody familiar with Occam's Razor? Can you maybe hit your thumbs up or yes in the, if you've heard of the Occam's Razor? Okay. So Occam's Razor, let me see. I'm trying to navigate zoom here where I can see the there we go. So I have a couple of people to say yes and a couple of people say no. Thank you for that. So Occam's Razor is a concept that basically says amongst competing models, the correct choice is always the simplest one. And the reason for that is that it has to do with how we fundamentally validate models. We use the word validate often. But in truth, we never validate the model. We actually only, we can never say a model is valid. What we can do is we can test it and try to falsify it. And we can certainly say that it's invalid. So we start with a simple model. And the reason we prefer simple models to complex ones is that they're most easily falsified. If we start with a linear regression model and it doesn't fit our data at all in any meaningful way, it's easy to falsify. And then we can go on and add complexity. And so Occam's Razor stated simply is basically saying that amongst competing models, the correct model to choose is always the simplest one. And the reason for that is begin because of just the process, the way we do the scientific method, the way we set out to validate models. We never actually validate. We can only say that they're not invalid. So we often use the word valid as a substitute for not invalid. In other words, we try to falsify the model. And in the absence of being able to falsify it, all we can say is that it's not invalid. We can't, we can't say with certainty that there's not some scenario, some possible combination of parameters or some possible real data set that would invalidate our model because we don't know. And so again, just to bring home the point here, like I think Dr. Purchan and I are really on the same page with that. I'm not saying that there's not a place for neural networks. I use them in my research. But I don't think we should start with a neural network.
 Let's go ahead. Let's talk about some fundamental general comments. I'm going to make some general comments. I'll get Phil Sophical at times. I think there's a context we should provide around data analytics and digitalization. Now, one thing that I told I recently presented to another company. What I said to them was sometimes when I talk philosophy, the reason I'm doing it is I'm trying to help you communicate with others. I feel like to some degree that this is an opportunity, when we go through this, look at the way I communicate some of this and see if there's an opportunity for you to communicate with your managers, with your stakeholders using this type of terminology. It's sometimes the way we explain to people who are uninitiated. It can be very powerful and useful. So consider adopting some or using some of that data analytics and geostatistics machine learning statistical learning prediction and inference. I think it's good for us to start out and just kind of set some terminology up and then we're good. When we talk about basic probability and so forth, it's good that we know where we're going with it. I don't want to spend time talking about frequentists and basing probability. And then, you know, you're just sitting or thinking, why are we talking about this? There's no context of where we're going, what's the ultimate goal of talking about that? There are lots of other resources available. If you look on the learning management system that you have access to, you'll see a lot of recorded lectures, a lot of other content that can be helpful to you. So if you're interested, you can always review, we'll be recording this right now and providing you a product that you can look at afterwards. All right. Good. Good. So goals of this lecture, motivation. I want to motivate you, my biases, but I think the biases are useful communication methods, definitions and terms, introduced concepts, and then we'll dive into data analytics and machine learning. We'll give you an overview, and then we can do some basic probability. Okay. The first comment I'll make right now is it turns out that we're not alone. Digital transformations are going on everywhere. I was asked recently by Deloitte to not, sorry, by Pricewater as Cooper to be on a panel. I'll talk about that in the next slide. But there was a whole bunch of VPs who were interested to learn more about what's where I'm digitalization. I was on that panel. Every sector in our economy is going through digitalization right now. In fact, every energy company I visit and last year I had the real cool opportunity to teach, I think, about 20 separate engagements and meet with people from many different energy companies. And what I found was every single one of those companies has some type of initiative. And it varies. There's one company that's got this massive initiative to try to train up 700 citizen data scientists. Another company I met with was trying to hire a bunch of data scientists and so forth. There's all kinds of different things going on. Energy when it comes to readiness, according to the recent Deloitte study, is somewhere in the middle. There's other sectors which are more of the technology, minors, sectors which are much higher maturity and there are other sectors like health which we're finding out right now. We're finding out that some of these systems are really, really old computer systems that aren't quite expandable and able to handle some of the challenges we have right now. And government and public services are less ready for this. So we're somewhere in the middle. Every sector of our economy is facing this digital challenge right now. Now my biases, when I showed up at that panel with Price Waterhouse Cooper and I stood with these other experts and talked about opportunities and digitalization, this is what I say. I say there's more we can do with our data. We can get more done with our data. We can ring more information out of our data. We can do better with our data preparation. Okay, there are opportunities to teach data and analytics statistics machine learning to engineers and geoscientists to improve their capabilities to give them another tool to go in their toolbox to give them another lens by which they can see the world around them. Geoscience and engineering knowledge and expertise remains core to our business. Now this was kind of a fun panel. I'm a very polite Canadian. I get along with everybody, but I got in a little bit of an argument just a little bit. It was a friendly argument. There was another individual on the panel who was basically saying we needed to hire a lot of data scientists and kind of do a replacement of some of our workforce. And that's when I had the chance to argue back, no, the great innovations that happen in our field are because of geoscience and engineering knowledge. And from my personal experience, we do very well at teaching engineers and geoscientists how to do data science. They pick it up quite well. Now at the same time, I do believe a sprinkling in of data scientists who have very deep level understanding of the coding and implementation and methodologies can still be very useful to an organization. Okay, data and analytics, let me just comment on that. Big data. Who here is working with big data right now? If you want to know if you're working with big data and you Google that right now, what is big data? What you'll find is that the criteria for big data are the Vs, a bunch of Vs. And so the first one is volume. You have a large number of samples, large memory requirements. It's difficult to work with the data, difficult to visualize. You open it up, you can't render it, you can't look through it, you can't parse it, you can't visual, it's very difficult to work with. Some tech people will argue it shouldn't be able to fit on a hard drive. I don't worry about that. But what I will tell you is that I know that during a Gulf of Mexico survey, seismic survey, our data collection rates rival the collection rates and the transportation requirements of that data of Google, of JPL, of NASA, of everybody. In fact, I would suggest when it comes to the volume, we have cases where we are winning. We have amazingly large data sets, velocity. The tech people will often argue that for to have this V of velocity, it needs to be real time data. I don't agree with that because what I would say is that relative to the complexity of our workflows and our decisions, we have a continuous data acquisition. Think about the last time you worked on a project and you're doing seismic processing, you're trying to reinterpret brand new seismic and at the same time you're waiting to drill a couple new wells. You have a lot to do in those in that timetable and I would say in that case, we have velocity too. High rate or continuous relative to our decision making cycles. Variety. Data from various sources with various types and scales from poor to drainage radius for production data, to basin analysis for exploration, we win at this. We go all the way across the scales with so many different types of data, variability. Data acquisition changes during the project. We also have this completely. I've seen many, many projects where we start with a couple of exploration two-dimensional seismic lines, 3D seismic reprocess to seismic again, 4D put ocean bottom nodes in and we're shooting 4D now. Like it, we clearly have to put together multiple types or ventages of data that are completely very different from each other and veracity. The data has various levels of accuracy. I tell my students in my classes that we don't have any hard data. None of our data can be considered hard. That's what I tell my students. If you really look at it, even a core has to be extracted from the subsurface from the in situ conditions where it's likely been disturbed. There's so many issues around veracity of our data and then you consider seismic and the scales and so forth. What I like to tell and I actually did this, I was on a conference call with Google. They have a team here that does subsurface in Austin and I had a really cool conference call with them and they told me they said, Michael, can you help us communicate with the energy companies because they're just not willing to adopt the new technologies? And I said to them, I said, listen, you need to listen more than you're talking because I said, energy has been big data long before tech even learned what big data was. And so I really do feel that strongly. We know something we can learn from them, but they should also be learning from us. Big data analytics, methods to explore and detect patterns and trends and other useful information from big data to improve decision making. Everything we do in statistics, data analytics, machine learning is about impacting a decision. If you don't impact the decision, I tell my students every day in class, you don't add value. Okay, so who here has seen a Venn diagram recently or who here knows what a Venn diagram is? Okay, thank you very much. I'm seeing I saw 100% out of people I can see and I see a bunch of people who are, I got yeses old, we're lighting up the board right now. Okay, very cool. Okay, so Venn diagram. It's a probability tool. It communicates the idea of marginal and joint probabilities, conditionals, but really in this case, it's communicating association, the domains of knowledge. Okay, so if you take all of statistics, we would draw a big circle. In fact, I have people all the time come up to me and say, your professor who teaches statistics, do you know about blank, blank, blank this? And I'll have to say, I know something about that area, but who can really know all of statistics? It's such a vast area. It truly is. Statistics is collecting, organizing and interpreting data as well as drawing conclusions and making decisions. Everything we talk about will be going to the decision. Geostatistics is a branch of applied statistics, so it fits inside of statistics as a subset. And it's all about the spatial geologic geoengineering context, the spatial relationships, volume support and uncertainty. That's why we care about spatial data analytics and geostatistics because of these critical components of spatial volume support and uncertainty. Data analytics is the process of examining and very data sets big data to discover patterns and make decisions. Now, I want to point something out to you right now. And that is, if you look at the definition of data analytics and you look at the definition of statistics, I can't find a meaningful difference. I honestly believe that data analytics and statistics are really the same thing. In fact, right now, if you do statistics in your job and you do visualization to support the data decision making, I would suggest that you can update your CV and say that you do data analytics. Okay. Spatial big data analytics, geostatistics, plus big data. So we have big data and we overlap with geostatistics. We're doing spatial big data analytics and I really wish I would have written that book. Somebody else beat me to the book. Big data analytics is the expert use of geostatistics on big data. Okay, so that's how the traditional areas, we know geostats. We've been doing that for a long time. That's how that fits into modern context of data analytics geostatistics. I'm only going to cover a couple of concepts right now on the second half down Thursday. We will dive in and do a bunch of spatial data analytics and geostatistics concepts to support what you do at work. Really, the overall approach of geostatistics is to support reservoir subsurface modeling. We're going to integrate all possible information sources into our model and we're going to be able to build multiple models that span the uncertainty. We represent uncertainty in the subsurface and oh, there always is uncertainty in the subsurface through a ensemble of models. Then what we're going to do is we're going to apply a transfer function, which is really just a term that represents whatever it is you have to do to the model to get it to dollars, to calculate a metric that is related to a decision, a decision criteria. Okay, so that might be volume metrics flow simulation connectivity analysis. Maybe it's just the proportion of sand. It doesn't matter. It's whatever you need to get to dollars. Then you'll take the distribution of outcomes from the multiple models with the calculation applied and that will give you the decision criteria distribution. From that, we can use decision making in the presence of uncertainty, decision analysis to make decisions such as number of wells. Yes or no, develop, go forward or leave and sell. Whatever we need, whatever decision we need to make, we support it. That's the idea of geostatistics, the 40,000 foot view of what gets done. Now, when we spend Thursday morning talking geostats and we're talking about barograms and spatial continuity and so forth, sometimes people in engineering or geoscience get confused by the terminology. The thing I want to assure everybody is that geostats spatial data analytics was developed from the practice of subsurface estimation and modeling in mining and the theory was added later on. In other words, geostatistics is a very applied science and the theory was just added in afterwards to basically expand the area. It's very much focused on the practice. Statistical quantitative descriptions of the concepts from geology and engineering of the subsurface. Geostatistics is a practical approach for subsurface to support decision making. Now, what I can commend you is that every concept in the subsurface has a geologic expression could also be related to an engineering concept and could be related to the equivalent geostatistical expression. It's very much applied. In fact, you could imagine that when you develop a reservoir, you can have a discontinuity. The reservoir is here is different from the reservoir over here. There is a significant change in the reservoir. These changes within the reservoir are represented in a geologic expression based on this idea of architectural complexes or complex sets. The geoscientists will often adopt a architectural hierarchy that uses these building blocks to explain that or to model it. The geostatistical expression, if you were to load up go-cad or patrol, would be you have regions, subsets of the three-dimensional geomodel over which you use distinct statistical and deterministic models to model that phenomenon. In other words, these regions represent this concept of a hierarchy within subsurface. A geoscientist would have an interest in the major directions of continuity. That's essential to us in the subsurface. If I drill a well here, I want to know how far away can I make a good prediction. I need to know what were the directions of deposition? Did it go in this direction? Did it go in this direction? What was going on? The geologists would talk about the paleo-flow directions. They're looking at the different scales of deposition. What's the provenance where the source material comes from? Where did it get the positive down dip in the basin? The geostatistical concept would be a major direction of continuity in a geometric and a stroppy model, which is a spatial model for all directions and distances. All of these concepts of the subsurface can be related to engineering and geoscience and then related to geostatistics. It's all very much applied once you learn the concepts and we will cover some of that on Thursday. Here's a fundamental problem. We are in data analytics and machine learning going to build statistical models. We're going to make fundamentally statistical inferences about the subsurface. Now, we'll talk more about inference shortly, but what we mean is that we're going to need to collect samples. Then we take the samples and we're going to formulate a statistic to try to represent what's going on in the subsurface. Now, any statistical calculation requires multiple samples. You need multiple samples. You need sample, come back sample, come back sample. In error and water and agriculture, I did a paper once on weeds and agriculture. You sample multiple times to build a distribution. Now, the problem is with our geospatial problems repeated samples are not available to us. Okay, so let me ask you a question. You go down on the subsurface, you drill a hole, you put the core barrel on and you extract a core sample. That core sample looks just like that. It comes up, the porosity is 13%. Now, after you have taken that sample, if you go back to that location in the well, you sampled 13%. If you go back to that same location in the subsurface, what is the porosity now? I know. I would agree though, it is still 13% at that location. I agree. Okay, what I'm trying to illustrate is you only have access to one sample at every location. Repeated sampling is not available to you in the geospatial context. Okay, now we're stuck. We're stuck in the hole in the ground, but that's not okay. We got to get the job done. So instead of time, having a time interval with multiple samples at that same location, what we need to do is we need to pull samples over space to calculate our statistic. Because one sample does not make a statistic. I cannot calculate a mean. I cannot calculate a variance. I cannot calculate anything with one sample. So the decision to pull data over space is called the decision of stationarity. It is the decision that a subset of the subsurface is all the same stuff. Excuse my use of extreme terminology there are guys, but it's all the same stuff. That's the concept of stationarity. Okay, now we will get into stationarity more on Thursday morning as we get into concepts of spatial data and lyrics, but we need it in order to calculate a statistic to build a predictive model. Now, the thing that the reason I also covered that is because next week on Tuesday, when we're doing machine learning, we need to work with data. We need to work with statistics because machines are statistical methodologies. We will need to go ahead and get distributions and samples. So we need to think about stationarity there too. So it's going to be universal. Any subsurface data and lyrics is going to be considered stationarity. Okay, has anybody ever told you that all of your data sets are biased? I know I'm kind of rude. I'm sorry, I told you it was a polite Canadian, but then I come into your business and I tell you all your data is biased. I'm just very rude. It turns out that virtually all subsurface data sets are biased, specifically sparsely sampled well data. The problem is that data is collected to answer questions to resolve risk to maximize value. Not one time did I mention there that we collect data for statistical representivity. We don't. We collect it. And in fact, the practice should not change. That's the best economics is to collect data to resolve uncertainty. Okay, or to maximize value. Okay, we might we must mitigate the bias in our data sets. And so this morning we will talk a little bit about how do we de bias our data sets? Okay, spatial context. Let's talk about the spatial context. Spatial continuity matters. In fact, what I like to do is I show my students this model right here. This is a model of permeability going from one to 100 milli d'arcy. And this is another model. They both have the exact same distribution of permeability. This right here, I don't show the students this right away. I just show them the two models. And I ask them is it better to have longer continuity or shorter continuity. And all the students say I would rather have a reservoir with long continuity. Then what I do is I show them the time of flight calculation from the fast marching methodology. It's a fast flow proxy with the permeability data. It's a permeability weighted time of arrival for earliest arrival approach. And I show it to them and I say which one of these is getting better recovery. This is the injector. These are the four producers around. It's a five spot. Okay. And what's funny is that this right here with the short scale continuity of very short noisy continuity actually has better recovery. It's what we call homogeneously heterogeneous. You can use that at a dinner party kind of sounds cool, doesn't it? Okay. So spatial continuity matters. We need to quantify and post spatial continuity. And guess what? Many of our problems in the sub surface sound something like this. I want to drill this wall right here. Can you tell me the uncertainty pre drill at that location? There's huge value if you can do that. And if we know spatial continuity, we know how to do that. We know what uncertainty to put around it. Okay. Sources of uncertainty models. We'll talk about this a little bit tomorrow. Thursday I should say. It turns out our data measurements at the calibrations are all uncertain. Decisions and parameters that we use in the model are uncertain. Spatial uncertainty because we're moving away from sampled locations. We don't know absolutely what's going on. If we got a well here or a well here, it doesn't matter if you know perfectly well. The type of depth situational setting, the distribution of porosity, permeability. You still have uncertainty. Just spatial uncertainty because you're going away from where you sampled. Uncertainty is doodar ignorance. It's not a function of the subsurface. There is no objective uncertainty model. Not in the subsurface, not in data and analytics, not in machines. There is no objective uncertainty model for you. That's cool. It's a model that you have to come up with that has practical use. You know, you remember that phrase, all models are wrong, but some are useful. The box phrase. That goes just as well for a model of uncertainty. They're all wrong. Uncertainty in the uncertainty. Andre Jornel and one of the forefathers of geostatistics got asked that at a major conference, and he said, just don't go there. There is uncertainty in the uncertainty model. But you get to a point where you're so far out on a cancel lever. It's not really helping you out. It's not a useful exercise. Just considered the first order, the first order, uncertainty, and ignoring uncertainty is assuming certainty. We'll talk about this more on Thursday. But remember that. That ignoring uncertainty is often a much more extreme assumption than even your somewhat limited attempt to get an uncertainty model. If you're interested in more deeper theory on the spatial data and analytics, go ahead. You could consider the book I wrote with Clayton Deutsch, Professor my PhD advisor up at the University of Alberta in Canada. I kind of kick myself. If we'd written the book five years later, we would have called it Data Analytics and Geostatistics. Because it really is, spatial data and analytics. If you want to dive into the theory, but you're willing to sacrifice accessibility, you could consider this book right here. They do pride themselves in being much more theoretical. But we wrote our book to be accessible as a handbook to a practitioner. So our book should actually not put you to sleep too quickly. You should be able to pick the pick concept.
 By the end of this, I want you to be thinking number one that we're big data. We were big data long before Tech even was, you know, drop the mic like that. I also want you to be thinking that machine learning for subsurface is different than any other machine learning application, i.e. we're unique. We have unique challenges. Okay. So spoiler alert, that's where I'm going to basically get to by the end of this. Okay, so let's talk about a general overview. Anybody heard the term big data? Anybody been living on an island and never heard about the, everyone's talking about big data right now. Okay. Is there, are you working with big data? A show of hands who's working big data right now? Okay. Yes. Yes. I want to see a lot of hands up. I want to see a lot of hands because we are big data, my friends. Okay. So how do we know we're big data? If you go online and you type, do I have big data? What is big data? And that kind of question. What you'll find out is that the criteria for big data are the V's. Volume, velocity, variety, variability, and veracity. Those are the criteria for big data. Now volume, volume means that you have a large data set. Now I went to a tech meetup and they said it's not big data if it fits on your laptop. It's got to be so big. It doesn't, it's got to be many terabytes. It doesn't fit on your laptop. I'm not that picky about it. What I would say is if you have data that's difficult to handle, difficult to visualize, then you have big data. Now if the tech people give you attitude about that, what I would say is that when we do our seismic acquisitions or we're doing our seismic surveys out in the Gulf of Mexico, our data transmission rates rival the transmission rates of Google, JPL, all of these major data handling type of organizations. So we definitely do work with large big data. Okay, velocity. The tech people say that it should be real time data. What I say is we have a, our data collection is continuous relative to our decision making cycles. In other words, have you ever been on a project and you just integrated that new seismic processing, but they just drilled a new well? And now we got to incorporate that. How long does it take to integrate a brand new well into a subsurface model? Can we do that in afternoon? It takes time. There's so much complexity in our workflows. I say that we have real time from the perspective, it's continuous relative to the difficulty and challenge of our decision making cycles. Okay, variety. What types of data do we have? Do we have variety? Just somebody list off, give me some types of data we work with. Ponditate the stratigraphy type models where you have like these analog models with Tao's son, and he's like figuring out how the cementation happens in super high resolution. Anybody ever seen the work of Chris Peola and all those flume experiments where they're looking at Jurassic Tank? Anybody here ever been to an Elkrop? Oh, those are the days. I'll tell you what I spent time on the Elkrops of Morgan Sullivan and repost some material. It was those were amazing trips. All of this data, there's so much data. Anybody here ever work with thin sections looking at the poor structure? We go all the way from the poor structure. Thank you for that, Meg. Data acquisition changes over the project. Now, this is incredible. I'm working on an induced seismicity project right now with the Bureau of Economic Geology. Our biggest problem is that the sensors, the detectors for seismic change over time. One goes down, one goes up, the resolution, the accuracy changes everywhere all the time. So it's incredible. How many people worked on a project with seismic information in an exploration setting with like 2D seismic? And then you start to get 3D seismic. And then you do a seismic reprocessing. And then you start to put ocean bottom nodes in and now you got 4D seismic. Like it's the same kind of information, but the acquisition changes over the project. Is it easy to integrate that together? Anybody working 4D? Maybe not as much in the terrestrial setting maybe, right? More, maybe more marine type setting for 4D, right? Okay. Varacity. The data has various levels of accuracy. Do you guys know what I mean if I say hard data? What does hard data mean? Hard versus soft data. Anyone who wears a modeler? What's our hard data? I thought I... Oh, sorry. The hard data would be data that we consider to be highly accurate to the point where we didn't have any significant. We don't have significant uncertainty with regard to that data. What's our hard data in the subsurface? You know, I've heard that. Now I would agree with that. What comes out of the ground is really the only hard data we have. That's the only thing we know certainly. And I really appreciate that. How about when it comes to understanding like trying to model the reservoir? Are our core samples, are they hard data? Are they perfectly accurate? Do they have in situ conditions where they disrupted or disturbed during the core cutting process? Isn't it incredible? How about the scale that they represent? Is it meaningful or do we need to be at a larger scale? What I would suggest is when it comes to the veracity, all of our data, besides production, has different levels of accuracy. Okay, so because of this, next time you meet with anyone from tech, I want you to be able to say the following. Energy has been big data long before tech even learned what big data was. We've been big data for like decades. Okay, so just remember that guys. We are big data. Okay, statistics and geostatistics. If anybody's taken basic statistics, you'll remember it's all about collecting organizing interpreting data as well as drawing conclusions and making decisions. Now, so if we had a Venn diagram, do you guys remember Venn diagrams? Everybody, I see people nodding about Venn diagrams. Really, it's a model of probability, joint probabilities and so forth. And so from the size of the circles, you kind of see the probability or the space that's covered. Statistics is vast. Geostatistics is a branch of applied statistics. It's all about spatial geologic context, spatial relationship, volumetric support and uncertainty. It's an applied branch of statistics. Now, here's something really interesting. Anybody ever looked up the definition of data analytics? It turns out we're all supposed to do data analytics now, right? Anybody being told you should learn about data analytics? Anytime I look up a definition, it talks about statistical analysis of the data to support decision making. Or sometimes data analytics, they'll talk about business analytics, they talk a lot about visualization. And they'll talk a lot about pooling the data and drawing inferences and so forth. When I look at the definition of data analytics, it really sounds like it's the use of statistics with kind of a focus on visualization. And so what I argue is if you know fundamental statistics and you use it at work, you're doing data analytics. So go ahead, update your CB, update your profile on LinkedIn. You're doing data analytics if you use statistics and what you do. Okay, guys, because I, anybody here argue against that, that maybe there's a little bit of rebranding going on. It makes sense. Okay, very cool. I think we're going to have some people who are going to be like putting data analytics on their list of their operational capability, their skill sets, for sure. Big data analytics is the process of examining large, very data sets to be able to explore and discover patterns. Spatial big data analytics is when you do the same thing, but you do it with spatial data. I kick myself that I did not write this book. There was a book that came out called spatial big data analytics. I wrote a book on geostatistics and I kind of kick myself because I should have called it geostatistics and data analytics or spatial data analytics or something like that because that's what it is. Data analytics is the use of statistics. Remember that. How about machine learning? Anybody ever Googled what is machine learning? Because we're all supposed to be doing machine learning now. What does that mean? Right? Well, I did what everybody else in the world did. I Googled machine learning and I found the Wikipedia article and I read the Wikipedia article. And this is a quote taken directly from the article machine learning, the study of algorithms and mathematical models. Okay, so stop there. It's a toolkit. It's not one method. It's many, many methods, a toolkit of methods of computer systems that progressively improve their performance on a specific task. Okay, it's a learning method. It's going to improve its performance on a task. The build mathematical models of sample data, known as training data. Okay, so it's learning from training data in order to make predictions or decisions without explicitly being programmed to perform that task. In other words, it's general. A machine should be general. It should be able to solve many different problems. Okay, so a toolkit, they're learning, they're training and learning through data and they're general. They can be used on many different problems. That's what machine learning is. Now, if you keep reading the article and I think a lot of people don't, I think they read that part and they're like, okay, I know machine learning, check that box. Right at the very end, it says where it is infeasible to develop an algorithm of specific instructions for performing the task. You hear that? It's not a panacea solution. This is a problem with machine learning applications right now. People jump to machine learning. If you're an engineer, if you're a geoscientist, if you understand the subsurface, if you understand the inversion methodology, if you understand how to do to do the flow forecasting, it's much better to use the engineering physics, the geologic physics, the geophysics physics to solve the problem than to jump to a machine. It's going to be more accurate to use the physics. Okay, so remember that. Actually, I remember there was a professor who told me don't use it as a crutch. Don't jump to machine learning. Let's still understand our systems. Okay, anybody here works with variables. We all work with variables, prosody, permeability, mineral concentration, saturations, everything. These are all spatial variables we work with every day. Production is spatial, but also like a time variable too, right? So spatial temporal variables. Concentrations will saturations are also spatial temporal because the reservoir will change over time as we do water, flood, or whatever we're doing. In data mining machine learning, change your definition or change your your nomenclature. We're going to call them features. So anytime we're working with variables and machine learning, we're going to call them features. Okay, the population. The population is the exhaustive finite list of properties or features of interest. Generally, the entire population is not accessible. If you were superhuman and you had x-ray eyes, you could visualize the reservoir and you could look down on the earth and you could see all of the reservoir. You would be able to see at every single location, at a scale required to solve your forecasting problem or your volumetric calculation, whatever you're doing, you would see the property of interest at every single location. Okay, but that's the population and that's not generally available to us. Otherwise, we'd not have very much to do at work indeed, right? We would be able to just know the answer. A sample is a set of values at locations that have been measured. So the population is the entire reservoir. The sample would be along the well bore, maybe to the depth of penetration of perpetual physical measure, or maybe the entire reservoir at a low resolution for the case of seismic information and much low, low resolution, of course. Okay, parameters are summary measures of a population. Population mean population standard deviation. We rarely have access to access to this. This is what we would call an inferential problem. And I will explain that right away. Now, model parameters are different. And I will use the term model parameters in machine learning. Those are going to be the dials we turn to fit the data. And we'll talk about that shortly. That's training model parameters. Statistics is summary measure of the sample. So if we have a well, that's our sample available from the population. If I calculate the mean, it's a sample mean, a sample standard deviation, and so forth. So we specify that. Is there anybody here who's thinking what's going on? I thought this was going to be machine learning. You remember the part where I told you that data analytics is statistics. And data analytics is kind of a core knowledge base that's really required to build machines because they are data driven. So we have to explain a couple of statistical concepts. Now, I have to admit, when I teach a full course in machine learning, spend time, I actually spend almost a morning talking about probability and statistics. Because in order to build machines well, we really need to understand statistics and probability. But I just, or this fast sprint forward, we'll just introduce a couple of concepts. But don't worry, I told you we were going to cover machine learning. Look, here it is machine learning. So from now on, from the next half to half days, whenever I talk about machine learning, this is what I want you to imagine in your head. Okay. This is all a machine. My goal is to demystify machine learning. This is a machine. These is to your model. If I'm predicting oil in place, what would be my predictor features? So the inputs, the predictor features are those inputs to the model. The model, now, F should be the natural setting. It should actually be the system. But we don't have that instance. F and F is going to be a machine that takes us from the predictor features to the response. And the response features are going to be the outputs back in the Gilday's statistics. We call these the dependent variables. Okay. But on features, there's so much rebranding going on. Actually, you're going to find out machine learning people love cool terms. The decision trees are being pruned, which is really, really cool. Machine learning is trained. Artificial neural net is trained over multiple epochs, like they really love using, they use inertia as a cost, a loss function for doing cluster analysis, like they do all kinds of cool terms. Okay. Machine learning is all about estimating this function for two purposes. The first mission for machine learning is inference. Inference is trying to learn the population. You're trying to learn about the relationships between all of those predictor features. You want to understand this porosity goes up. Mace saturation is affected by that or the correlation structures. What is this population really doing? The shapes of the relationships. I've heard often that Brittleness has a sweet spot that you don't want your reservoir to gooey. Sorry, that was not a very good rheology term. But you don't want it too soft and you don't want it too hard. You want it to be able to frack nicely and you want the fracks to remain open with profit. Right. And so there's all kinds of sweet spots and relationships. Okay. And they may depend. There may be really fractions going on. Now, what is the inferential statistics? Let me just give you a simple example. Given a sample, describe the population. If you do that, you're doing inferential statistics. Now, part of my job is to kind of get you guys using termin- and all parties, both. Okay. So I want you to be able to say that I'm doing an inferential approach right now. Okay. So, give an a sample, describe the population. I'll give you a simple example. I look at, I show you a coin and I flip the coin 10 times. I get three heads, seven tails. And I ask you, what's the probability that this is a fair coin? That's inference. I gave you a sample. Now, tell me the population. What's the population? The coin. The coin is the population. You see that? Okay. So that's inference. Now, don't worry. I give you a simple example. It could have been you drilled 10 wells and you had three successes in exploration. Tell me the true probability success in your exploration program or the uncertainty in that. That would also be an inference problem. Inferential statistics. You have a sample. The well. Now your job is describe the reservoir, the population. I got to tell you geoscientist, geophysicist, engineers were great at inferential statistics. We do it all the time. This is a problem we face all the time. What's the fraction of the reservoir we sample with well data? Volumetrically, what fraction, what proportion of the reservoir do we observe directly with well data? I mean cores and logs. The data scientists said I built a machine with high accuracy and it did a great job making predictions and I don't understand the machine and I don't understand why and I don't care. Okay. If anybody here ever says that, I will immediately be conjured as an angry spirit and I will show up in your office and go, no, we don't do that. You know, we, you're, I'm preaching requires we say in Texas, we need to understand the system. Otherwise, we make big mistakes. When we do prediction, we still want to make sure we're working inferentially. In fact, if you think about it, it's all drawn first but from inference. We have to first understand the basically what we're doing predictive statistics is we're giving an assumption or model of a population. Now you predict the next sample. The next sample will be the pre-drill assessment for the next well. What's going to be the production rate? What's the production rate for that pad in aggregation would be your prediction problem? Or if I break it down to a simple example, I tell you this coin is fair. I have a fair coin in front of me and you trust me. And then I ask you the question, I say, what's the probability of three heads and seven tails? That's prediction. That's moving forward. Inference was, I tell you the outcome and the sample and you tell me the population, that's working in reverse. Prediction is working in the forward problem. Any questions or comments about prediction inference, running with predictor features and response features? So far so good. Are we together? Is there anybody here kind of going, what was inference again? You guys are good. If you ever get stuck on inference, go back to the coins and imagine every time you think inference, somebody showed me the sample. They showed me the coin tosses and they asked me if it's a fair coin. They showed me the wells and they asked me what's the reservoir doing? What is the population? That's inference. Okay. All right. Now anybody is interested. I do have a numerical example of a Bayesian statistics for doing inference on that coin problem. And I'll tell you that is super fun. We'll have some time during the next session and we'll actually dive into that. Okay. Predictive statistics. So once again, you make a model of the system, the population, and you predict the next sample. So I have the original well. I had the information to build the reservoir model. Now I'm doing pre-dual production at a proposed well location. That's prediction. And to be honest, this is something I learned at Chevron. Everything we do comes down to that prediction because that's how we add value, right? What is going to be the next well? And that choice, that decision is going to impact how many slots in the facilities, how big of a facility, and so forth. Those are the big decisions, the billions and billions of dollars in decisions. Now when we do this, we have deterministic now, if you have engineering judgment, if you have geoscience knowledge, if you know what's a deep water turbine, or if you know a shallow marine and it's going to be deltaic, you can use that to try to fill in and understand. In fact, if I had a sparse dataset, and this is space or time, if I know that it's topography, then I know it's going to be smooth. It's specifically if I'm in an area like much of Texas, where it's pretty smooth topography, right? It's not Colorado. Now if I'm working with temporal data set of daily interest rates, I can come up with a very good deterministic model because I know the daily interest rate, changes day by day, it steps. But there are many problems which we have to use the statistical model, a more data driven statistical model. We can't purely use our expert knowledge. This is the space of machine learning. This is also the space of geostatistics. In fact, how many people here use geostatistics in their job? Anybody do that? Anybody here build reservoir models? Ben, Nicholas, Megan, thank you very much. You guys do that. It's a very cool. It's the same mindset. You ever had to defend your geostatistical model? You ever had somebody look at it and say, that doesn't look exactly what I thought it should be. Those objects aren't exactly what I expected. Sequential Gaussian simulation seems a little pixelated, a little bit noisy. You've had to explain to them that we made an abstraction to a statistical model to capture uncertainty, right? But you did it data driven with some expert knowledge like trends. This is the same attitude we have of machine learning. And so just remember that. It's an extension. It's a data driven approach, a statistical approach. Now, deterministic models are beautiful. And I never knock them down because let's face it, integration of physics and expert knowledge, that's determinism. Integrated various source of information, often quite, but the problem is often quite time consuming. We typically do a pretty poor job. Do you guys agree that when we come to deterministic models, deterministic models, we often fail to incorporate uncertainty. Have you guys ever encountered that? We get a little certain. And it's hard because it's really hard to build multiple models. So you'll start thinking, well, if I had multiple geologists, I would have multiple deterministic models, that's very costly and expensive to do. Statistical models, they're very fast. They have an uncertainty assessment. They report significance, confidence intervals, prediction intervals, they're very powerful. On or many types of data, it's a data driven approach, disadvantaged, limited physics, and you have statistical abstraction or assumptions. And so we do this all of the time too. This is the mindset of machine learning. Remember that. It's a statistical model. We have estimation models. We have simulation models. Remember this. The estimation problem is very different than simulation problem. We in oil and gas are experts on this. The geostatisticians, the people who do reservoir modeling know this problem. Estimation is like creaking. You're trying to get the best estimate at every location. Simulation, you're trying to get a good estimate at every location, but you're trying to capture the global distribution, global uncertainty, you sacrifice local accuracy for global accuracy. This is what I want to say is that in our business, we usually prefer simulation because which one of these will flow more realistically. While the one that has the right level of heterogeneity will flow better, it'll have a more accurate prediction. A smooth estimation map will typically be not a conservative estimate of recovery factor. Well, remember this because when we do machine learning, we often work in an estimation context. We're often trying to get the most accurate estimate. So just remember that that all of these good things we know in oil and gas about estimation, simulations, statistics and geostistics, it carries on into machine learning. We don't abandon it. Leave it behind uncertainty modeling. Well, wait, wait, everybody, don't look at the slide. So we do realizations. Fancy, a fancy thing we do is we do a lot of scenarios nowadays. You capture that low mid-high. We consider how the decisions could change. Sometimes we even say the depositional setting could change. So we do all of that. We should still think about that in machine learning. Don't leave that behind. Think about all those uncertainties. Don't abandon the good things we know. Okay. Do you guys feel like I was on a soapbox for a while? Okay, as my kids tell me I should chill out sometimes. So I'm going to chill out here for a bit. And let's just talk about different types of machine learning models. We can work with parametric models. A parametric model makes an assumption about the functional form or shape. We gain from simplicity. We can model with very few parameters. And so we can work with sparse data. Okay. Here's an example. A linear model. That's it right there. Okay. And I did your feature one, two, three, the outputs here. We have coefficients. And if you look at the model relative to any one of the predictor features, it's aligned. And if you look at it in multi-dimensions, it's a hyperplane. That's all it is. Okay. Now the problem with a parametric model is right off the bat. You assumed a linear model. That means if your phenomenon is highly non-linear, you're going to be wrong. You're just going to be wrong. So this is going to be problematic. A non-parametric model makes no a priori assumption about the functional form or shape of the natural setting. It makes no decision about that. It's more flexible. It's going to fit the shape. It's going to find the shape for you. Now there's less risk that your estimate of the function will be a poor fit. They're going to need much more data. That's the trade-off there. Do you see the trade-off? A parametric model simpler, easier to understand, use less data. Okay. A non-parametric model much more flexible, but you'll need more data to strain it. Okay. And so you can fit whatever shape or form that the data take. This is a depth versus a normalized porosity or a Gaussian transform porosity measure. Okay. Non-parametric is actually parametric rich. So from now on, if somebody tells you we did non-parametric machine learning, I want that to pop up on your head. I'm hypnotizing you now. That this will pop up in your head and say, oh, non-parametric is really kind of a misnomer. It really means parametric rich. There's just a bunch of parameters hidden inside. If we were to talk about a artificial neural net, I build one in the class that we'll have in the next session. We'll do it together. It has 500,000 parameters in a simple artificial neural net. They often have many, many parameters. Okay. Now let's talk about in one slide how machine learning gets done. This is going to be really, really heavy. I'm going to introduce a couple of con. Introduce the pieces of this concept of each all talk about each one of these concepts. Okay. So first of all, we take our data. All that data prep data cleaning already happened. It was a wonderful 90% of the project. It was beautiful. We feel confident to move forward now. Okay. We take our data. And in this case, I'm going to use a lot of low dimensionality problems for visualization. This is predicting permeability as a function of based on a data. Okay. I've got all of my data. The x's are the training data. The solid circles are the testing data. So I'm going to separate my data, split it into train and test data sets. That test data is going away. We'd not allowed to look at it for the next step. It goes. You can't look at it. You're not allowed. No peaking. Okay. Now what we do is we're going to build and we're going to train the model parameters to fit the training data. Do you see there's no solid circles anymore? I remove them. I told you there's no peaking. They're gone now. We're going to take a model that's a first order polynomial, a third order polynomial, a fifth order, a seventh order, a ninth order, 11th order, whatever we want to do. But we're going to train these models to fit the training data as best as we can. Fit it. Just fit it. If you can get zero error, I'm happy. It's just get it completely fit. Then we're going to do this on multiple levels of complexity. Okay. So increasing complexity from a first order model to a fifth order, seventh order model there. Now when I say complexity, I could also say flexibility. You see how the first order model is a linear model and it can do this. But it cannot fit the data perfectly because it has very limited flexibility. It's a low complexity model. Do you see the seventh order model? Woohoo. Look at that thing. It's just whipping around everywhere. It can fit the data perfectly, right? We're going to fit all the data. So we're going to this is the training the model parameters to fit the data as best as we can for each level of complexity. Now the next step is we bring the testing data back in. And we compare the best fit model to training data to the error at the testing data locations. You see that? We build the distribution. This is the error at the testing data locations. We could also calculate a statistic like the variance explain the mean squared error, the residual sum of squares, whatever we want to calculate, calculate a measure of goodness of fit with the withheld testing data. You see how that's a test of the model. Then we look at each one of these measures and we say the third order model was the best at fitting the withheld testing data. So guess which model we're going to pick? We pick the third order model. That right there, my friends, is called hyper parameter tuning. The hyper parameter controls the degree of complexity and we pick the one that fits the testing data best. That's hyper parameter tuning. The training data being used to train the model to train the model parameters is this step. And this step right here is the hyper parameter tuning. We're getting the best degree of complexity. How is this model perform at the testing data locations? Is that a reasonable model of testing data? How does it perform at the training data locations? Do you see the trade off here? With a sufficiently flexible model, you can perfectly fit the training data. You have zero error. Now if we put all the data in the very first time, we would do something that's very, very bad. It's called overfit. We would overfit the data and then what is the quality of our model for making predictions into the future at unobserved locations that were not observed? How well would do? Then I agree 100% on that. You know, has anybody ever done history matching? Have you ever done, okay, wait, wait, wait, I shouldn't ask that. Have you ever seen somebody else like a friend do history matching just by changing the permeability at the well location? Anybody ever seen that happen? What is the quality of that model for making a forecast at the very next drill? Well, does it do a good job? Yeah, it's a toss up. It may not do a great job, because you're just doing a little local tweak. You're fitting it. It's kind of like this in a degree to some degree, that's overfitting. Okay. All right. So let's, let's, I mentioned model parameters, hyper parameters, training, tuning. Let's refine those concepts. Let's have some definitions. Model parameters. And that's a, that's a difficult thing. We will, I will cover a couple concepts in just a couple slides. Okay. Okay. Thank you very much. I appreciate you bringing that up. Okay. We're going to be fit during the training phase to minimize error at the training data. Now, there's a lot of other concepts there like what is error? What is the measure of, of, um, precision or accuracy in the model? And we could talk about L2 L1 norms and all of these different consequences, but those are topics we won't get into right now. So if I have a third order polynomial, that was the one that we liked. I have a B3 B2 B1 and a constant term for the intercept, right? And by changing those parameters, I can create a line that does this, a line that does this. I can change that line dramatically. I could run an optimization, minimize the error terms at all of the training data locations. And when I do that, I get a line like this. All of these other candidates were not as good. That step of fitting the third order polynomial to the model is model parameter training in machine learning. Okay. So that's model parameter training. And these are the model parameters are just these coefficients. The choice of the degree of complexity, the first order polynomial, third order polynomial, fifth, sixth, seventh, whatever we want to work with. The hyperparameter is different than a model parameter. It constrains the model complexity, not the fit, the complexity. We select hyperparameters that maximize accuracy with the testing data that are with hell. That's the work's best with hell data locations. Now let's step back and think about what we're actually trying to do is we're trying to simulate the real world condition of making a prediction at new locations. That's what we're trying to do. And so Rachel, getting back at your question, testing training split is supposed to be a simulation of the real use of the model. We're actually trying to say, how would I use this model? Now let's see how it would perform as like a dress rehearsal. Okay. Now these are very important concepts. Any questions about hyperparameters, parameters, training, and testing. This one right here is this idea of a polynomial fit with multiple orders in which we would expand the number of orders to increase the complexity. Now there are many other problems. I was to give this a simplest example I could think of. It would be an example like, has anybody here ever done, creating? Built a creaking map. When you do creaking, you make a choice about the search window. You decide the verigram range. When you change the verigram to use a very small search window, what happens to your creaking map? When you use a very small search window, your model becomes very high resolution, very detailed. It will honor the data perfectly. It'll be very non smooth. That right there is a hyperparameter for a creating estimation map. If you use a very small verigram range, you get a very flexible model that's going to be very specific. If you use a very smooth, in fact, if you do creaking and use very, very long verigram ranges and huge search windows, you'll estimate with the global average everywhere. That's a low complexity model that would be underfit. That's an example from the subsurface. If when we do K nearer astrology for prediction, your hyperparameter is K, the number of neighbors to use, which is analogous to subsurface modeling and mapping. We'll have a bunch of different examples actually for every machine we build will focus on the hyperparameters. I hope that was helpful to you. One of the most important concepts of machine learning is known as the model bias variance trade-off. This is essential. Machine learning is all about this model variance bias trade-off. Now, if I want to calculate the goodness of my machine, I would calculate and please, this a little bit calculate the expectation of the square difference between the true values in the response at unobserved locations. In other words, testing or real world predictions. Minus the estimates I would get from my model. These are all of the predictor features at that unsampled location. This is the model that we built. The difference here is the error, the truth, versus our model squared. The expectation is just a probability weighted average. In this case, we could just think about as the average error. If we took this quadratic expanded it and did some simplification, and James and I will develop this within their textbook, you would be able to see that it breaks up and simplifies into three. Do you guys know variance is additive? That is one of the most powerful concepts in statistics. Variance is additive. Remember that. It's really cool. It turns out there's three variance terms that are all the parts of the error in our model in testing. Model variance, model bias, and irreducible error. Model variance is the error due to the sensitivity to the data set. Now, if you build a linear regression model, if you change the data, your linear regression model will move around a little bit. It's a very simple model. So at low model complexity, this is low model complexity. This is high model complexity. What we have is at low model complexity, we have low model variance. A simple model will not be as sensitive to the data. A ninth order polynomial will actually be very, very sensitive to the data. You change one or two data points, and your model will whip around all over the place. It will change dramatically. More complicated models have high model variance. So we're going to find out model variance in fact really is what each or lunch. It's what really does impact model quality. Okay, so increasing complexity of your machine, you have higher and higher model variance. Now, it turns out that model bias is kind of the opposite. It's the error due to using an approximateive a two simple, your model's two simple. If you have a linear regression model, but the natural phenomenon is non-linear, you can have high model bias. Okay, so if you have a very simple model, you can have high model bias. As you increase the complexity of the model, the model bias goes down. Now, you notice this is additive, so they're going to add to each other. There's one more term, irreducible error. Irreducible error is a limitation of the data. Through NING's lectures from Stanford and Google on machine learning, on YouTube. I highly recommend. Andrew NING is one of the world's experts in machine learning. If you could, and this would be fortunate for you, get rid of me and replace me with Andrew NING, and he'd be not get rid of the reducible error. The world's greatest expert in machine learning cannot defeat a reducible error. Irreducible errors due to the fact that you'd not sample all of the features. Have you ever done that work with a model and you're like, if I could have measured that, I would have done better, but you don't have it. It's because you didn't measure the full range of all the features. It's because you missed lots of information. Okay, so irreducible error is irreducible. You can't get rid of it. Okay, if you add the mold to get function that looks like this, what does that tell us? What's the best level of model complexity to work with when you build a machine? The most complicated? It turns out for many problems, the optimum, something that's not the most complicated model. Please remember that. Don't jump to complexity. Now, is anybody here wondering how far are we going to go for the break? Machine learning overfit. If you increase the complexity, what usually will happen is that we will reduce the error in training, like we saw with the polynomial. The more flexibility you can fit the training data, the error will go down. In fact, at some level of complexity, you have no error. If you look at the error and testing, what often happened, it'll come down to, but then it starts to go up. This zone right here of complexity, resulting in greater error in perfect model. I've been to tech meetups where people have suggested 90% of our machines are overfit. I think their model is overfit. I don't know if my models are ever overfit, but it's true that we have to be always be concerned about overfitting and we need fair testing and training. Okay, let's just quickly talk about a small machine and then we'll finish up here. Okay, and we'll have a break. Okay, I challenge you right now, prove to me that linear regression is not a machine learning methodology. So that's the challenger. What is machine learning? It's a toolkit of methods, mathematical data driven methodologies, statistical models that learn from data, support it with expert knowledge, not explicitly told how to predict it learns from the data. General method that may. Okay, so here's my data set density versus porosity. Why would we have to do that? Well, we can measure density, but we care about porosity, those types of things, right? Porosity gets us poor volumes, gets us oil in place. Okay, so let's build a model. This is some Python code right here. We're going to use a side-pile statistics in order to calculate a linear regression model. I have my density data predict porosity from it. I do my linear regression model. It comes back. I've got a function like this. It's a simple linear regression model and voila. That's my model right there. Okay, how's my model doing? Is it look pretty good? Could I have done better? Is the problem non-linear? I don't think we would have done better for more flexible model. Okay, but this is a linear regression model. Now, let me ask you a question. If I change the data, will this model change? If I got new sample data over here in a cluster, would it change my line? It would clearly move around. So I would suggest it's adaptive. It can learn, yes, Shazad, this thing would learn from the data. Okay, so far, so good at learning from the data. Now, what does the model look like? How do we fit the model? Well, there's about two or three pages of derivation, but there is an analytical expression for the L2 norm, this squared different, squared error norm, where I can minimize this equation right here. And I will get this analytical expression for the slope term and for the intercept term. So we can solve for this, not a big deal, super easy to do. And we're minimizing the error at the data locations, the squared error at the data locations to get the slope and the intercept right there. Okay, so that's how we calculate it. When we do artificial neural nets, it's kind of the same thing. We're minimizing error, but in that case, we're doing back propagation a little more complicated, but not too bad. So there are some assumptions. Did you guys know that linear regression has a bunch of assumptions? Anybody ever looked at them? Who here has done linear regression to solve a problem for? Anybody ever done? I think it's so common. Many people use linear regression. Thank you, Nicholas. Okay, the first assumption, error free. You assume that there's error in the response term right here. You have the error here, but you assume that there's no error in the measures, the predictor features, the density measures, I mean, the density measures right here that they have no error. The dots cannot move back and forth. So there's no error there. You assume linearity, which is a no-brainer, you're assuming that the parametric form of a line fits this natural setting. That's cool. That makes sense. You assume constant variance. Anybody ever heard the term homoscedasticity? You guys, I'll tell you what, every day I have a goal. It's kind of like eating vegetables to say, homeowner and heterskadasity at least once. It's literally my favorite terms to use. Heterskadasity or homoscedasticity, homoscedasticity assumes that the degree of variable over the range of possible densities. This is homoscedastic assumption that it's not more error here, less error over here. You see that? That's a big assumption. In fact, anybody ever done a log transform of permeability to linearize it in order to then fit a linear trend? Anyone? The problem with that is you often violate this assumption of homoscedasticity and not often becomes an issue. Okay, constant variance, independence of the error. You assume there's no correlation between these error terms. Did you know that? So she makes the model wrong and then validates it. Multi-colonarity. You assume that none of the predictor features of your multiple features are linear combinations of each other. They have to be somewhat non-redundant of each other. Okay, that's a linear regression model. You had to optimize to train it to the data and you had all of these assumptions. That was only linear regression and it had all of these assumptions built into that many of us did not understand unless you dig into the details, right? The other thing that's really cool about this data-driven model, our machine, I'm going to argue its machine learning, is you can calculate a measure of fit. The R squared values, the strength of the model, the proportion of variance explained. We can calculate R squared values very readily and they'll tell us about how much variability we understand based on the model and what's the part not understood? Remember variance is additive. So whatever part of the variance is understood is removing from the total variance and the remainder is not understood. So we get to partition variance. We can calculate confidence intervals in the model. A data-driven statistical model, you can get out of confidence interval, the uncertainty in the slope in the intercept. You can actually do prediction intervals. I can tell you given a density, what's the uncertainty in the velocity, not just the estimate, but the uncertainty too. And there's an analytical form when it comes to linear regression. All of these concepts should be used in machine learning. We demonstrated linear regression, but we could do all of our data-driven machines. Okay, do you think I did a paper with Tim McCarg? Anybody remember Tim McCarg? He was a strategist for Work2OfMorgan Sullivan and that team in ATC. I worked with him quite a bit and he actually did produce some plots where there were correlations and there's only three points. Have you ever worked with sparse data? Can we build a model when we only have two points? Is that dangerous? Can I extrapolate up here? Can I extrapolate down here? Is that always safe to do? All of these concepts of the sufficiency of data and extrapolation, interpolation, how safe are you with that model? It means with machine learning. We need to keep all these concepts. So what do we learn from the simple machine? Our simple machine was flexible, fit the data, learn from the data, minimizes error with the training data. There were important assumptions about the machine that we should know to use the machine safely. The model can be tested for significance, proportion of variants explained and we'll do that. It includes uncertainty in the model. You can predict based on new data with uncertainty, we can do that too. And there's issues with overfit and extrapolation. Every time and Dr. Foster did this once in a faculty meeting, so everybody had said machine learning, machine learning, machine learning, he said every time you hear machine learning, think of a advanced form of linear regression. It's really no different. It's a data-driven model that we're going to build. There's a book on machine learning called statistical learning. And the reason they do that is on purpose, James and all, is because they're saying these machines are really statistical data-driven machines. It's the same concepts. Okay.
 Why I'm doing this is I want to give you communication tools. I want to give you concepts that you can use to communicate to management and to other working professionals. Anybody ever heard of the 4th Paradigm of Scientific Discovery? Has anyone ever welcomed you to the 4th Paradigm of Scientific Discovery? If not, let me be the first to welcome you to the 4th Paradigm of Scientific Discovery. It turns out there's been 4 paradigms of scientific discovery. The first one was empirical science, observation based science. You know what's interesting? You know the Egyptians were doing this? Now I don't like, I'm glad I was not around for some of their medical experiments. There were some pretty crazy stuff going on back then. But they were trying to do things with the natural world and observe the response and that's how we were able to do incredible engineering feats. Okay, the second paradigm was theoretical science. Try to find out the classical laws, the relationships, the analytical expressions for things, right? And so that was the second paradigm, you know, kind of this renaissance and discovery and so forth. The third paradigm was computational science. That's when we realized that our reality is heterogeneous. Our reality does not fit the analytical expression. It was better to simulate the heterogeneous system. Okay, so we had these continuum models, computational dynamics and so forth. The computational power forced them. They allowed us to do it. The fourth paradigm is data driven science. This is where we take the data itself and this is a world with advanced machine learning and data analytics methodologies. A world with amazing data sets and a world where not necessarily could we ever figure out the physics. There's no physics behind your preference when it comes to Amazon and what you're going to buy next. I'm sorry, some of that stuff is just data driven. And so we got to this now this fourth paradigm of scientific discovery. Now, if you find that Chevron is trying to do a bunch of digitalization, you're not alone. Every single company and I said, I thought I think 20 different places or something last year was crazy. There's a lot of different companies. Every single one of the companies I visit is working on digital transformations. In fact, the Lloyd did a study of all of the different sectors of our economy and what they did was they rank them by readiness. Are they high maturity, the lower maturity? It turns out that energy was about in the happy medium. We were in the middle. In most of the time, we're not at the trailing edge and we're not at the cutting edge. Okay. And so tech was definitely on the more cutting edge while life sciences and healthcare was on the trip. We're finding that out now. They're talking about some of the antiquated systems they use in the health sector, right? Okay. Now, I have some biases and I should be honest about them. I was on I've been on panels and I discussed with experts and I have my biases, I share. And so for instance, I think that with digitalization, there's an opportunity for us to do more with our data. There's opportunities to teach data analytics and statistics, machine learning methodologies to engineers and geoscientists. Now, when I was on this panel, the other people in the panel, some of them were more tech people and I'm not going to like identify the person or anything like that. And they were suggesting that we should kind of hire a lot of data scientists and really kind of get away from as much geoscience and engineering as we can. And I stood up on the panel and a bunch of executives there from all and gas companies and I said, that's the wrong answer. I said, we do our innovation is geoscience and engineering driven and we do a very good job if we teach our geoscientists and engineers how to do data driven methods. In fact, they do a great job at that. And so I think we need to integrate all paradigms, new tools, add value to old tools. In other words, we augment the new scientific paradigm of data driven on top of all the other science. When we discovered the theoretical, we did not stop the observations. When we discovered computational science, we did not throw the theoretical away. And so we're building on top of our toolbox for scientific discovery. Data driven science needs data. Data preparation remains essential 80% of any subsurface study is going to be data preparation interpretation. We continue to face massive challenges with data. Anybody have problems with data, data curation, where's the data? Can't find the data. Who did that interpretation? How did they do that interpretation? Which one of these interpretations should I use? Does that ever happen? I think that happens all over the place. And don't be ashamed. That happens all over our business. Large volumes of data, large volumes of metadata, which were unique for that. A variety of data scales, collection, techniques, interpretations. We have so much going on transmission controls and security. You ever had data we're not allowed to have the data? You have to go to the country. That happens too. Right? You're not going to go to the country to look at the data. I can't leave. Clean databases. Our prerequisite for all data analytics and machine learning. We must start with this foundation, garbage in, garbage out. Often we'll talk about having a well-formatic clean data table. We're talking about the features and all of the samples nicely dealt with. I want this to be something you can use as a tool going forward. From now on, energy is unique. It's different and it needs unique solutions in data driven. Now, we have a sparse, uncertain data complicated heterogeneous open earth systems. Would you believe the people who do Google Maps, people who do Amazon recommender engines? They have all the data. They see all of the phones. They see all of the people clicking every click. They have exhaustive data. We don't have that. High degree of necessary geoscience and engineering, interpretation and physics. We know a lot about the subsurface and we can use that information to add value. We have expensive high value decisions that must be supported. How much does it cost? Can you tell me, roughly, to drill a well in deep water Gulf of Mexico right now? What do you think? Anybody working the Gulf of Mexico? How much does it cost to drill a new well in the Permian Basin? Now, let's compare and contrast that assessment. In Gulf of Mexico, when I was working that back during the high, it was about $150 million with a production test to punch a hole in the Gulf of Mexico into the Wilcox. It was incredible. Those are expensive decisions. High value decisions that must be supported. Anybody hear you Spotify? You know Spotify has a recommender engine. This is the recommender engine from the summer of 2019. This is sharing some information about me. I'm Canadian. I listened to a bunch of Canadian music. Would you believe every once in a while Spotify recommends and plays nickel back? Now, I'm just saying, on behalf of Canada, no Canadians like Nickelback anymore. I don't like Nickelback, but by guilt by association too many Canadian bands Nickelback is going to show up. I promise you that that's the Spotify recommender engine getting it completely wrong. What is getting it completely wrong when we're drilling wells? We could drill a dry hole. The cost of getting it wrong with a Spotify recommender engine is what? What's the cost? One song. What am I going to do? I start tapping my foot, I bob my head, we all do. You got to admit it. You do Nickelback. It does. It catches you. It's very good at that. And then you have that realization, oh, it's Nickelback and you fast forward it. The cost is zero. You fast forward it. I don't cancel my app. I don't go on yelp and make an ugly review because the value of that decision is zero. It makes no sense for human intervention. If I had my own personal DJ, I don't know, my a millionaire at that point, that's silly, right? So there's no reason to have human intervention there. We got to be a critical user consumer. A lot of this technology was developed for these low value exhaustively sampled low physics problems. And that's not what we work in. Remember this concept. Don't jump to complexity. Excuse the font here, but remember model variance plus model bias plus irreducible error gives us the actual error in the performance of our machine. The curve looks like this often. It looks like this. A lower complexity model often outperforms a high complexly model. And in fact, a low complexity machine is easier to interpret. So you guys like seriously, I see this all the time. Many people will be like, Hey, let's jump to deep convolutional generative adversarial networks. No, don't you might find that you can actually explain the problem much better if a simpler method. Actually, Bruce Power, one of my mentors working in ETC, if you know Bruce Power, his comment was Michael. There's many complicated ways we can tell ourselves what we already know. Don't jump to complexity. Okay, anybody ever seen this example right here? The wolves and the dogs. Anyone seen it? Well, it's a great day. I get to share this wonderful example with you. It is a good case study on interpretability. Okay, so here's my problem with machines that are very complicated. Interpreteability may be very low. In fact, it'll be very hard to understand what the machine is doing. The application of the machine as a standard workflow may become routine and trusted. Anybody ever worked on a project where there's an established standard workflow? Have you ever violated the workflow? Done something different? Bifurcated from the workflow and then presented it in front of say the RAM team or in front of some type of review process or tea or whatever it might be. Are there questions? Do you have some explaining to do? Why didn't you use the standard workflow? Right, Megan? Exactly. And so this is my problem. If the machine becomes routine, it becomes trusted and it becomes why didn't you use the machine? Okay, now if the machine is trusted, but its interpretability is low, it becomes an unquestioned authority. And that is very dangerous, my friends. That is very dangerous indeed. Okay, so our friends, there was a study done by Roberto some years ago and all, I think 2016, machine learning everything's kind of newer, right? And what he did was they took a bunch of pictures of wolves and dogs and they trained a logistic regression methodology to basically be able to give you a probability of wolf and dog from a picture of the animal. And what was very interesting was he trained the machine up and it worked very well, high accuracy, but then he gave it this picture. And it came back as 99% or high probability of wolf. That's a wolf. Okay, now anybody here a dog person? Is that a wolf? Anybody here think that's a wolf? That's a husky, right? Is that a husky? Okay, so these huskies, right? Not a wolf. And so the scientists, the data scientists looked at the picture and they're like, what is it about this that looks like a wolf? Is it the snout, the eyes, the ears or what? They went back to their machine and they said, tell me the pixels that gave it a high probability of wolf. And this is what came back. Why did it look like a wolf? The dog was standing in snow, surrounded by snow and so when they took the pictures and they put the pictures in the machine, I don't know about you guys, but every time you see a wolf, they're standing in snow, they're standing in northern Canada and those scary parts of the Arctic, right? And so the machine had actually not learned to identify dogs and wolves, it had learned to identify snow and no snow. And that's basically what they built. This is the danger of high complex the low interpretability machines. It may look like it performs well with your data, but then when you go to something else, it actually did not learn. You don't know what it learned. Okay, Peter Haas has an excellent TED talk and I recommend it and he actually cites these pictures right here from the Robero study. And he said, even the developers that work with this stuff have no idea what it's doing. Sometimes it's like that. And he also says these systems do not fail gracefully. When they get it wrong, they think it's a wolf when it's a dog. Okay, anybody ever seen the hype cycle? This is the hype cycle from Gardner, a management systems and this is a common thing that occurs with new technology. This is time after discovery. This is expectations. Triggering event goes up. Peak of inflated expectations comes down. Troph of disillusionment, slope of enlightenment and plateau of productivity. Okay, let me ask you. May I ask? I ask every company. I hope it's okay. Not a single company has put themselves here. Would you believe so? You're not alone. Now, I have, and this is how I know I went to a company where they said, yeah, we're around the peak. And then afterwards, I had, I'm not mentioned in the company, the manager took me aside when I visited them and said, Michael, can you help us show some value? We need some good examples to show some value. Where do you think they were? They were starting to come down the slope. And they were needing to show some value because there was a lot of investment going in that kind of thing. So anyway, remember this that we're going to get to a more moderate level. And it's not going to be where we think we are right now. We're going to reach a point where we become more reasonable. A data scientist, if you're a data scientist, you advertise yourself as having domain expertise, statistics and coding. That's the unicorn, the person who kind of has it all. Now, I would argue that many times when you have a data scientist, the domain expertise will be weaker. The coding will be very high and the statistics will be moderate. That's the typical data scientist I would suggest. The coding, the statistics is very strong in the machine learning, but they may not have all the fundamental statistics. And I'm just saying, universally speaking, this is what I think we should do for operational capability. I think we should build on domain expertise. We have excellent geoscience and engineering skill sets. We should continue to build and harness that. We have, we could do more with our statistics and our data. I think we could build up this. And I think we could grow our capability in machine learning, the code being able to build the workflows. I think this is the best way forward for how we can harness the value from these technologies. Statistics to mitigate cognitive biases. There's going to be cognitive biases in everything we do anchoring, recency bias, confirmation bias. There's a common phrase here. I would not have seen it if I hadn't believed it. If you ever worked with Henry Postman here, this is one of his favorite phrases. I wouldn't have seen it if I didn't believe it. Be aware of your biases. Let me just show you really quickly and then we'll move to code. I promise it'll happen. Okay. Which one of these data sets? What is the difference between statistics and data analytics or is there a difference? I would say that there's synonymous. I would go that far. I would say that data analytics is really the same as statistics because any definition I look for is about working with data to support decision-making. And for both of them, they say that data analytics often has that business data analytics which German towards business decisions, but I think applied statistics should do the same. So I really do fill as I said before and I'll say again, if you're good at statistics and you do statistics to get your job done, you do data analytics. Which one of these data sets is random? These are two different points sets. Which one of them is random? Anybody? Who thinks that the left image is random? Okay, left random. Okay, good. Martin, Ian, Skyler, I saw your hand. Thank you very much. Anyone else? Okay, who thinks the right data set is random? Any more vote? Allison? Okay, anybody else think the right data sets random? Come on. There's zero cost to getting this wrong. You might as well throw your hand in. Okay. What's really interesting is this data set is random. This data set is actually anti-clustered. It in fact is more regularly spaced than random should be. Naturally and random phenomenon patterns occur. You should train your eye and realize that random phenomenon often have patterns. Don't overinterpret them when we build our machines. Remember, anybody work with Morgan Sullivan? I did not put this simply because you were Chevron. I quote Morgan Sullivan in my undergrad, undergrad classes, my graduate classes, and in multiple companies. Fellow now for Schmitt Bigger, if you have it, right? He always said the difference that makes a difference. In other words, no one and pattern that you observe when it really matters. And in this case, these patterns are just random. So we got to train ourselves. A lot of statistics is protecting ourselves from the law or the belief in the law of small numbers. It's the belief that randomly sampled things from the population will always be highly representative. In other words, if I have a bag and I have 10 red balls and five blue balls and I draw from that bag, I'm going to expect to see two thirds and one third in every sample set, but it's not going to happen. You could be very lucky and they could all come out as red. They could all come out as blue. If you do two few samples, now if I do many, many samples, the proportion will be right. It will converge on the right proportion. Remember that you should expect fluctuations on certain needs due to small data sets. Working in high dimensions, let's not get into this right now. I'll skip that, but what I will say is don't lean on data analytics and machine learning. Do you get the sense I'm being a little negative today? Just a little bit negative. I think it's good to have a critical perspective. I promised that data driven approaches support traditional hypothesis driven scientific methods. Mizuki said the following big data distributed computing and sophisticated data analysis all play crucial role in discovery of the Higgs boson, but the discovery of the Higgs boson was not data driven. It required scientific innovation. A machine could not have done that. Engineering and geoscience experience for means core. A professor in my department said the following don't use data and analytics. She's learning as a crutch instead of learning and grasping it the intuition and concepts behind the physical laws of the natural system. Always remember that. Okay, that's the end. I promised this would be a little bit lengthy. Any questions, comments? All of these concepts of it as a function that we're going to fit. Parametric and non-parametric models, the fourth paradigm welcome to the fourth paradigm, everyone, and don't jump the complexity. The variance bias trade off. Please, when you see someone build a very complicated machine, ask them, did you build a simpler machine? Did you tune the hyperparameter as well? Okay. How's your energy level right now? Are you guys good to get into some Python?
 So I'm still going here good. Let's talk about machine learning. First of all, I just showed you a book and so let's show a couple more books. Now, just in case you have not been experienced or exposed to James and all, which is an introduction statistical learning of athletes. I find it to be one of the most accessible treatments of the fundamentals of machine learning. They call it statistical learning, but it is machine learning. In fact, I do like using that term because it reminds people that machine learning is a statistical model. It's data driven. And so the book is extremely accessible. In fact, what I like to tell people is chapter one and two, really chapter two will change your life. If you haven't read it, you will go through it and suddenly some of these key concepts within machine learning will click in your head. And you'll be able to, you know, kind of communicate. I think it'll improve your communication. It'll help you with communicating. So please consider that. Now the book is available free from springer. It's it's actually just available. So you can go to that link right there and look at the book and read through the book on your laptop on your computer. Okay. Now, Michael, one other thing we've included it on the learning management system under pre-rex. So it's a link in there too. You guys can access it pretty quickly. It's all within the hub. So pretty easy. Please consider it as a even before we get to next Tuesday and we start diving into machine learning. Please consider maybe on the weekend. Just reach chapter one and two. And I think you'll find it beneficial. Now, this is a accessible treatment of a subset of a more complete treatment known as the elements of statistical learning. In fact, the elements of statistical learning, if you look at the author list, it does include some of the same common authors. James decided to basically try to extract and create something more accessible. The elements of statistical learning does go for rigor in theory, but definitely sacrifices accessibility to a high degree. It is very difficult. I do use it within my graduate level course. My undergraduate students just really struggle with it. Okay. So some treatments of it. Now, just basic terminology. A lot of people use the terms artificial intelligence machine learning and deep learning almost interchangeably. And I don't get too concerned if I drew the Venn diagram, I would say that artificial intelligence is the big circle. It's a bigger area because it's a theory in the development of computer systems where the computer performs a task that usually a human being would have done the task. Now, the thing I would recognize is the fact that many people do artificial intelligence. They really are working in those fields of, you know, speech recognition, translation of languages, decision making, self driving vehicles, and so forth, in which it really is focused on a task that a human being would have done previously in a machine not machine learning is a specific area of artificial intelligence. It's a system that's able to automatically learn and improve through experience with new data without being explicitly programmed to perform that task, a general system that can solve many different problems. Deep learning. You know, there is a little bit of branding going on all of this stuff and clearly deep learning is one of those cases. If you take a neural net and you put more than one hidden layer, you get to tell people you're doing deep learning. Now, admittedly, many of my PhD students, I have 12 students now working with me work with deep convolutional generative adversarial networks in which deep in that sense is a system that's so large, so vast, so many interacting parts. I understand why they differentiate from more of a traditional artificial neural net. But anyway, not a big deal. We'll even do some deep learning hands on the last morning that we work together is part of this course run. Okay, if you want to get a really nice concise definition of machine learning, I do what a lot of people do. I go to Wikipedia, just the first thing that shows up on Google. If you look at the definition, it has a great definition. The definition is it is the study of algorithms and mathematical models. Okay, stop there. It's a toolkit. It's not just one method. It's going to be many different methods. In fact, you go through the statistical learning applications and are you going to find chapter upon chapter upon chapter, each of them with sets or classes of methods. So many tools you can work with. The computer systems use progressively improve their performance. They're training. They're learning as you're working with them on a specific task, the machine algorithms build a mathematical model on sample data. So they're learning without being known as training data in order to make predictions or decisions without being programmed explicitly to perform that task. The concepts of a toolkit learning from data and general. They can work on a wide variety of problems about being programmed specifically on performing that task where it's infeasible to develop an algorithm specific constructions for performing that task. Okay, so this is critical right here. This is the concept that it's not a panacea. It's not a solution that's going to work for every possible case. In other words, if we know the geoscience or engineering solution to the problem, I would suggest we're better off using that solution. In fact, only if maybe we need to speed up, maybe we need to run a proxy that's very fast. If we know the solution, we're better off using our engineering knowledge. Let's just kind of let's mention a couple of concerns or comments about machine learning. And so I like to give the positives and the negatives a critical perspective. Who here is ever seen this picture or this example of the wolves and the dogs. Here's the issue. Rediro and others wrote a paper back in 2016. What they did was they trained a logistic regression classifier with 20 images that were standardized the images were the same size, the animal, the dogs and the wolves were all in the same location, the picture, that kind of thing like they got them nicely set up and prepared. They trained this machine and the machine had a lot going on, but on the output, it just gave a probability of wolf, a probability of dog and they sum to one. That's a logistic regression classifier. So they ran all these images through they trained the model very well. And when they ran this image through the model came back and said very high probability of wolf. That's a wolf. Now anybody here think this a wolf. Any dog people out there. I actually went walking on a trail just a little while ago and I met a dog that looked like a wolf. And the owner told me that it was in fact was 50% wolf. I didn't okay. I don't know. Maybe it's one of those things from one of those websites or something. But anyway, you know if it's got wolf in it. And this clearly is a husky. It's not a wolf. Right. Okay. So they went back to the machine and they said, okay, there's something wrong. This is showing up as high probability wolf. What's going on. So it's a classifier of an image. So they went back to the pixels and they said show us the image that pixels that give a high probability of wolf. And when they did that, these are the pixels that came back and said high probability of wolf. What's going on there. So what's going on there. Is standing in snow. Because if you think about it, most of those images, Google search wolves right now. And I'm telling you what it's all up in northern Canada. In some scary Arctic tundra place with the wolves standing around in snow. And so the fact that that dog happens to be standing in snow, got it classified as a wolf. And so it's a long snow. Maybe it's kind of wolf like the pointy ears. What was it. It turns out it was a snow. Now here's the problem. The problem is that was researchers. And they were experts. And they were able to go back to the machine and a nonstandard way and interact with it. Most people who apply this technology do not have the know how to say tell me which cells made that happen. Okay. They wouldn't have that ability to run that diagnostics. Here's the problem. As the machine gets more and more complicated, interpretability goes lower and lower and lower. Now I worked inside of a company and I was I was a research program manager. And I used to deploy technology to the entire company. So I would deploy workflows. And my experiences within a company, if you have a standard workflow, it becomes routine. It becomes expected. In fact, have you ever done a project and not use the standard workflow and presented the results to management. Have you ever had that experience is uncomfortable. And people ask questions. They're like, why did you not understand the standard workflow. Maybe you don't have the capability to use it. Is was there a reason. How do you justify that? What if something goes wrong. The problem is applications of workflows become routine and trusted. Now, if you have something that you can't interpret and it now becomes routine and trusted, the machine becomes a trusted. An authority that cannot be questioned. And so this is a very disturbing thing. This can be very dangerous. Now, I'm not going to raise a bunch of examples right now. But there have been examples of this already happening in society. Examples are there are some states that are in fact using machines to determine the risk of reoffence for offenders to determine what to do with a parole. And what they're finding is that these machines are making decisions that they're not quite sure of what the biases might be going into the machine. And this is affecting people's lives, of course. So there are so many examples of how, but I know for us, it's going to be very impactful because I know the importance of our decisions more on that later. Who here has seen this curve before. The hype cycle. Okay, I told you I was going to be a little bit critical this morning, right? Okay, I hope I warned you. I should have had one of those viewer of eyes or warnings at the beginning. You know, this will be right at disturbing at times. Okay, so the hype cycle. The whole idea here. And it's really cool. If you go to the gardener website, they have a whole bunch of different hype cycles and they go into details. I'm sorry, this is very much like a very weak treatment of this topic. But what we have is a new technology trigger and then we have time. And then we have visibility. And I think in many organizations, we'll see this effect new technology boom spreads like wildfire. Everyone gets excited. What's going to happen. The visibility goes very high. You get to the peak inflated expectations. We come down to the trough a disillusionment. That's where we realize that maybe it's not as ready. Maybe it doesn't deliver as robustly as we thought. Maybe there are some issues. Slope of enlightenment. We start to realize, wait, wait, wait. There are some things we can do with it. It can be their practical applications and the plateau of productivity. Now every time I show this curve within the organization, I ask the question, where are you when it comes to data analytics and machine learning. I have encountered people all over the field and I've asked the same question. I've never heard anybody up here. Nobody said that we're up here now. The other thing that I've actually had happened, which was really cool, is I had people from the same organization. Tell me they were here, here, here, and here all from the same organization. Would you believe that? That different teams within your company are different points on the hype cycle. Yeah, I agree. I agree. And so clearly this is there. And so why do I bring this up? I bring this up because we have a communication challenge. I think anybody who's gone down here, gone here knows that this is not the pancia, that this is not going to solve every problem that we still need expert knowledge and that there are still our weaknesses and challenges in the world. In fact, when we talk about machine learning next week, we will get into some of these limitations and talk about some of the unique challenges that we have. Now, I was a little negative there. So I always like to end on a positive. I say something negative. You know, when I parent it or my kids are still growing up, you're set after one negative. So I give a couple of positives. It turns out that there's many applications for which machine learning is having a massive impact on society immediately. And it makes a lot of sense that it's having huge value in the short term. Specifically, driving directions are amazing. Hey, did you guys see the person who was walking around for wagon full of cell phones causing traffic types? You see that was not cool. That was an art display. Did you I read the article they said it was an art exhibit. But they basically took a hundred cell phones put in a little red wagon and put them all on driving directions on Google and then walked around and stopped in the middle of a block. And it caused the big red spot on the map looking like a big traffic triumpt to tie up and it caused traffic to be rerouted around. And there's almost no cars driving on those streets as a result. Is that crazy. Anyway, so the point is clearly there's areas where this working and there's not a lot of people walking around wagons with cell phones. So apparently this usually works most of the time. Air traffic routing spam filters. You guys are getting less spam. I'm getting less spam. They're getting better at that. So we're doing some plagiarism checkers. Our students don't know it, but we know when they're cheating. We're catching them. We're doing a pretty good job at that now, right? We also have translation computer credit card fraud. What I always like to tell people if you don't believe that there's a machine watching your credit card. Take your credit card. Not right now. We're social distancing. It was typically around one trillionth volumetrically is extracted as core. Even if you go to logs, it's still going to be one 100 billion. We sample so little of the subsurface directly. We have uncertain data. What we do sample is uncertain. We've complicated heterogeneous open earth systems. Have you ever been surprised by a reservoir? Has noble ever drilled something up and they're like, we know what's going on. And then you drill a couple more wells. I remember when I worked in deep water. This saying we had our mantra was we have sheets when we drill one one well. We drill two wells. We have channels. We drill three wells. We don't know what we have. That was kind of the, that was the saying we were always surprised what happened in the subsurface for fun. Go back and look at the initial models that were published for Cuido block you know off of west coast, Africa. It's incredible. And now look at the most modern models. You'll see it totally changed what that was happening there. Sand to sand wall to wall sand and now very, very complicated channels. We have high degree of necessary geoscience and engineering interpretation and physics. I would say not only is all of our data soft. But I would say that most of our data is secondary. In fact, what we really need to work with is not what we sample directly. We just think about a well log. We don't actually detect things directly. We have to go through steps of petrophysical calibration and calculation to get to the thing seismic data. We don't really care about amplitude and the reflections. What we really care about is getting to accuse the competence and trying to relate that to porosity or some type of rock quality or something, right. We have expensive high value decisions that must be supported. Can I ask what does it cost to drill a well in the Gulf of Mexico these days. I remember at the high with a production test during you know, back during the days of $100 barrel and so forth. It was like $150 million to drill single well and get a well test. It was incredible. But even terrestrial wells, unconventional wells were getting much more efficient, but they're still huge, huge investments. So we've expensive high value decisions that must be supported. I like to contrast this concept of energy is different versus a common application of machine learning. And that common application would be something like the Spotify recommender engine. Anybody hear you Spotify. Do you ever let Spotify just go and see what will happen. See what you'll start listening to. I started listening to some Ramstein. That was pretty crazy. Some German heavy metal. Anyway, so the main point is this is that this Spotify recommender engine will watch what you choose. And then we'll recommend music to you. And this was my recommender engine from last summer. I was having a good summer. There's a little Neil young, the traveling will buries. There was a lot of cool stuff going on. But I'll tell you what I'm Canadian. So I listened to a lot of Canadian music and what happened from time to time is put nickel back on. Now, I don't know if anybody here likes nickel back last time I taught actually I taught for the AAPG. It turned out out of I think 40 people can all of you can correct me. I think there was two strong hard core nickel back fans that I think I offended with the following joke. So if you're a nickel back fan, please forgive me. It's just a joke. It's a kindhearted joke. But I don't think anybody in Canada likes nickel back anymore. In fact, there was a RCMP detachment that was threatening drunk drivers if we catch you drunk driving part of your punishment while you're in the back of the cruiser, we will play nickel back on the way back to deliver you to prison. So the main point I'm trying to make is that I don't like nickel back, but every once wall, my recommender engine puts nickel back on. That's the equivalent of the machine getting the answer completely wrong. I mean wrong. I mean literally drilling where there's no reservoir. Okay. And what's the cost of that? I fast forward the song. I catch myself tapping my foot. I think, hey, that's kind of I like that. It's catchy. And then go, oh, yeah, I know who that is. And I fast forward. I don't cancel my count. There's no consequence. There's no consequence to Spotify. In other words, the decision of what song you listen to next will make no sense to have human interaction. It's so low valued. The same thing is your driving directions. I don't know. I don't know which one of us is personally wealthy to the point where we could all have a chauffeur who drives this around. I don't think any of many of us are would be in that position. Okay, my recommendations for machine learning and energy supports subsurface development when the volume of data is too large to be queried by hand when you have a box of hard drives. And there's no way you can have one of your professional spend weeks and weeks and weeks trying to look at that data, get a machine to go through and try to find patterns to find where you should spend professional time. The system is high dimensional and you can't visualize it. When we talk multivariate, I'll talk about the curse of dimensionality, which is really, really crazy stuff. High dimensional is very difficult to deal with machines do a very good job at high dimensional when the task is routine that automated report that you send every week. So if we can do that, if it's routine and repetitive, guess what? You get to spend more time doing engineering and geoscience because you don't have to spend as much time doing that. I love that cup. I saw the long horns there. Very cool. Thank you for that. Thank you. All right. So we can support ourselves with systems that streamline and automate support expert and system interaction. I love that. I love that idea of immediate feedback as you're working. I will often talk to people when I do this class and I'll talk about the fact that turbo tax. Now, now anybody here, anybody here use turbo tax? We do turbo tax the very first thing you do and we're delayed this year, which is kind of nice. We have a little more time. But when you do it, the first thing you do is you put in that one form from the government. I forgot what it's called. But it tells you your earnings and the amount of tax you paid and the first thing that pops up on the top right is a dollar amount. What was that dollar amount? Then what happens after that is you make a series of decisions and it's an expert system. Every time you make a decision, you see the dollar amount changes, right? Now, if you're like me, it was like, I'm getting all this money back. There's your for you. But then you start to do things and you realize, oh my gosh, it all starts to go down. You see it go red. It goes negative. You start to oh again. The main point is that a machine that provides you with expert feedback on the impact of your decisions is very, very powerful. I would love to see machines that while I'm building a reservoir model and I'm putting a fault in and I'm deciding on the fault throw and the heterogeneity, the permeability on the fault blocks. I would love a model that has a little dollar amount in the top or has an amount that says recovery factor from that planned well or tells me what's the oil in place recoverable with that new heterogeneity. That would be very powerful because I know if I should spend a bunch of time figuring out if that fault is ceiling or what is the actual offset in that location. The other thing I think would be very powerful is in tear things that can be interrogated with excellent visualization diagnostics, a machine that's like the wolves and the dogs, but it tells you immediately which pixels told you wolf. I think those machines are very useful because they help you interrogate your data. All right, let me just wrap this idea up around some example machine learning applications and energy that I think are very useful that are being done right now feature detection guided interpretation and dense data sets. I look at what's going on with you know software around seismic interpretation pre stack post text seismic. They're already doing the machine learning they've been doing it for a long time. I see the seismic interpreters back in the good old days and not so good old days many of them had RSI injuries because they were click click click click click in 3D along every surface to make every interpretation. Now they click a location and the machine spreads out they see the location and the machine says is this where that goes. And then you can interact with the machine you say no bad machine you got it wrong. No how don't do that OK, and then you could go back and you can kind of tell it no correct this correct this the seismic interpretation is machine guide that's very powerful. It helps save human health as far as your wrist and all that clicking. But it also helps us do better interpretations more consistent with the data more consistent expert systems that detect anomalous operating conditions. Dr. Van Orr actually rented a theater and watch deep water horizons or deep harrow deep water horizons with his students and with a bunch of faculty and spend an hour afterwards lecturing the geomechanical aspects of the disaster and what could have been done differently and how the movie was correct or incorrect. I'm telling you what that was the best date night ever with my wife going to movie theater to have like an engineering lecture it was so awesome. OK, the point was that he said as part of his lecture he said if they'd had better expert system. Oh, he develops those he works in artificial intelligence he says they could have known they could have done things better they could have potentially stopped that disaster. I think that's great when it comes to human safety health environment and safety model feedback. I'm optimizing our optimizing our decisions and field development that's critical so we incorporate all of our best knowledge. We'll feed back with fast proxies that's the turbo tax but in the subsurface development world significance consistency efficiency for more impact machines will help us. In fact, when it comes to machine learning, I'm a very optimistic person. I believe that it will lead to more efficient workflows where we do a better job and geoscientists and engineers get to spend more time doing engineering and geoscience, which is what we love to do. We're also talking about the more mundane components of our tasks. OK, meta data data and data sets. Let's just talk about that just very quickly. We could spend in fact, if you are interested for Exxon mobile and I did a lecture on data preparation data engineering and it was really a fun discussion with them. We could come back and just do a tech talk and we could do a discussion for an hour all around the issues around data preparation and feature engineering. It's a very interesting topic. But it turns out 80% of our effort within a project is data preparation feature engineering interpretation. In fact, I would suggest what usually happens when I say that is there's arguments from people within a company saying no, it's 90%, 95%. We're very large proportion of our time. We face continued challenges with data. It hasn't gotten better. It's still there. Data curation. How do we store? How do we work that data? How do we keep it reliable? Large volumes, large volumes of meta data. I'll talk about that next. Variety of data scale collection and interpretation. Transmission controls and security. I've worked on projects where you can't see the data unless you go to the country. We have challenges that I think a lot of people in the tech industry can't even understand. I met with some Google engineers and they said we were amazed. We showed up and said give us the data and that just couldn't happen. We had that there was so much time up front to get the data ready. Data bases are prerequisite for all data analytics and machine learning. I saw 10, 15 years ago a wave come through. There's been multiple waves around all of this. Multiple waves of machine learning. We're like in the fourth or fifth wave right now. There was a wave that came through our industry that said we need to do a better job with our data. We need great databases. What was interesting was I've gone back to those companies that I know had done that. And what I'd say is that that never finished up. Like are we to the point where all of our data is ready for use? Is it standardized? Is it centralized? Do we have the standards set up so we know that everything from every project can talk to each other? I saw thumbs down. Hey, appreciation for the honesty there. I do appreciate that. So I would say that we still face a lot of challenges and data. And my main message here is guess what? Data bases are prerequisite for all data analytics and machine learning. Data machine learning and data analytics don't solve those problems. What I like to tell my students is garbage in garbage out is still an effect. Okay, so we need the data ready. Meta data is a huge challenge that we have specifically within our field. And the reason being is we have so much interpretation so much none of our data is primary data. It's all required some form of processing to get it ready. So metadata is a set of data that describes or gives information about data data about data is metadata. So basically how was the data collected? What are the limitations of the collection? How was the calibration done? The uncertainty? The transformation? The standardization? The interpretation? The corrections? The devising? It goes on and on and on. This is all metadata and it's essential. Do you guys know what I mean if I refer to the concept of corporate memory? Anybody heard that concept before? Oh, I hear somebody but I don't quite hear you. No, okay, I thought I heard somebody. So corporate memory. Anybody worked on a project for about three, four, five years and then moved on to a new project? In general, that's what happens and what happens is key critical personnel move on. We don't do a great job of storing and tracking metadata and all of the project documentation. What typically can happen if you've ever been in circumstance where you come into projects you inherited and you can't quite figure out which data was used for what? That's happened. And there's no shame in that. I find that in every single company I go to metadata and better use of databases with tracking a metadata would be critical around this. Now we could get into kind of philosophical philosophical discussions about the PowerPoint documentation generation and how we can improve that. But we do have massive amounts of metadata we're challenged. Just like spatial statistics and geostatistics statistical learning machine learning is a set of tools. It's a toolbox as a geoscientist or engineer you're going to use these tools each is very dangerous to use as a black box. You need to understand what's underneath the hood expert use really requires you to understand the tool the methods of workflow assumptions limitations. There's also trade offs between the tools. I had a student actually just recently do something very fascinating that built a classification model to determine the risk or the probability of failure of an intersection of two pipes that are flowing with different pressures, diameters and temperatures due to the idea of the turbulence causing cycling of the temperature down pipe. If you got two different temperatures of the cause a thermal fatigue on the junction or even down pipe. And so what was fascinating was we looked at the form and the features and we looked basically at the result of the of the data set or kind of the predictions we're trying to make. We could see a very nice shape or form to it. So we decided to use support vector machines because they work very well at putting parametric surfaces into use as a decision boundary does a very nice job. If you understand what's going on in the hood, you'll pick the best method something that's very robust. You'll do a better job of getting the job done. Now the main thing I'd say is if you're thinking about tool use and think about your tools in machine learning as a toolbox or data and lyrics as a toolbox, please be inspired by the trades people. Anybody here by show of hands is related to or knows well somebody who works in the trades. And me with them, if you sit with them, my twin brother is a machinist. I have a huge amount of respect for machinist now because the level of knowledge he has when it comes to working with metal is really it's amazing 30 years 25 years of working with his tools. He knows how to make beautiful things and in the industry, he knows how to solve very hard problems. We should be inspired by those people. So remember, create a toolbox for yourself that has the common tools that you know very well how to use that can solve a variety of problems you encounter commonly. Don't over and come back your toolbox of a bunch of tools you don't use avoid using tool. I mean that you don't know and don't use and try to really be inspired by the level of expertise of these people who really know these tools well that trades people we know well. Okay, Hadley, Wiccom Chief Scientist R studio, he is dedicated his life to making statistics useful and practical to scientists and to the common common people working in finance, even or anything. And so his attitude is teaching we need to rethink the way we teach statistics because we're going to become irrelevant. The reason being is we're teaching statistics as if it's statistical abstinence. Now he wrote a little bit of a fun tongue in cheek paper about statistical abstinence teaching safe stats, not statistical abstinence. And he comments on the fact that we teach that statistics is dangerous, something we should fear unless we're with somebody who's a PhD in statistics. What we need to do is teach common practice that's safe. We need to also provide tools that are safe for use. And that's what he tries to do through R. I do believe through Python and what we teach we can make statistics more practical to people. Let's introduce the concepts. Let's just introduce very quickly the concepts of prediction and inference and then we're going to jump into some concepts around data preparation. Okay, the terminology we're going to use we keep building models. We're going to build a model all the time here. And so we're going to have predictor features. The predictor features back in the good old days were our independent variables. In other words, if I have a model, why is equal to a function of X1 through M. My predictor features are going to be the inputs for the model, the X's, what goes into the model. Now, we'll also note in the models that will always have some form of random error term. Otherwise, if we don't, we're overfitting our models and we're going to talk about that. We'll try not to overfit. The response or dependent variable. That's the output variable. And so if the X's are the predictor features, the response feature is going to be the Y. That's the output. And there can be more than one Y, two or three or four Y's. Here, we just show one Y. We'll work with one Y most of the time. Statistical machine learning is all about estimating that F in order to go from the inputs, the outputs from the predictor features to the response features for two purposes. One is prediction and the other one is inference. Let's just cover inference very quickly. Yes, question or comment? I heard a voice. So we want to the first one is inference. Now, there is value in just understanding the system. In fact, what I like to say is inference is about taking a sample and trying to understand the population. Or in other words, take a limited sample of the subsurface and try to say something about what's happening in the subsurface. That is inference right there. Now, what's the relationship between the predictor features? That's important too. If I have X1, X2, X3 through XM, understanding how they're related, maybe they're very related with each other could be very powerful. The sense of the relationships, their sweet spots, we might find that in the system, there's an optimum level of each one of the predictor features. That's important. Inferences about learning about the system. Prediction is about taking what we know about the system and using it to get the most accurate predictions of why. In fact, we're focused on getting the most accurate wise. We may not perfectly understand the system. I hope we do endeavor to try to understand the system. I've been to tech meetups where people have said, I don't care about the system as long as I make good predictions. I don't think we should ever be there. That's not, we want to be data driven, but we still want to understand the geoscience and engineering. We're concerned about the relationships between the X's individually with the Y, predictions modeling the system to make estimates or forecasts. Now, there's two different types of models we can build. Parametric models and non-parametric. This is critical. Parametric models, there's a trade-off. We're going to make an assumption up front, a priori, about the functional form of the F. We're going to say, yeah, I think the F is going to be a linear. We say it's linear. Now, because we made that decision up front, we gain simplicity in the advantage of having only very few parameters, very efficient. We can train on very few data. Here's an example of a linear model shown right here. But when we make that decision up front, we risk the fact that the model is just wrong. In other words, we assume a linear model, but the phenomenon is highly non-linear. Therefore, we have what we're going to call model bias, and we'll introduce that right away. Non-parametric models, when you think of machine learning, we often think non-parametric. Because we make no assumption about the form of the function, we let the model learn the form from the data. That's the power of machine learning. It's more flexible to fit a variety of shapes of F. F could be very non-linear, and you'd hope that your function would find that. In fact, the artificial neural nets will cover two weeks from now on the Thursday there will, in fact, be known as a universal function approximator. In fact, we celebrate the fact that it could fit any form. We brag saying it could do that. Now, but there's less risk that your function is going to be a porthit for the function, you know, that estimate of the function, the estimate of F, F hat, that it'll be a porthit for the actual natural phenomenon because it was flexible. It could find it. But there's always no free lunch. You're going to need more data to train that model. Now, this is what I like to say. Non-parametric is actually parameter rich. When we build artificial neural nets, you can find out there's actually thousands if not tens of thousands of parameters behind that model. Yeah. Okay. There's a training and testing approach that we're going to employ when we build our models. We have to do this to avoid overfit. We're going to take all of our data and this could be Euclidean space X and Y or this could be a multi feature space. It doesn't matter. But we're going to have training data and we're going to have testing data. The testing data is with help. It's removed from the data set and we now train the model parameters with the training data. At this step, we are maximizing accuracy. We're trying to get the best fit to the training data. Then what we do is we train a variety of different models with different levels of complexity. And we test those models against the withheld data. We bring that data back in and we say, okay, we perform best on this model with this level of complexity or fit. This is where we're going to tune the hyper parameters. Parameters are trained in the training with the training data. And we use the testing data to tune the hyper parameters. Now, I've just said parameter and hyper parameter. Let me refine the definition so everybody's together on that. The parameters. Now, if I have a simple third order polynomial and I'm trying to train to this data set right here, I think we all can see that the parameters are the B3, the B2, the B1 and the C. Now, I change the C. I move the function up and down. I change the B1. I can change one component. I change the B2. I change. And I can actually make this shape change completely. I can flip it upside down. I can move it around. I can change the slope overall by changing these parameters. I can set those such dim minimized the error at the training data locations. That's called training the parameters of your model. Now, it turns out that there's a hyper parameter too if we're thinking about a polynomial model. The hyper parameter controls the complexity of the model. It's not the fit. It's the complexity. Okay, so we had a third order polynomial. You could have a second order polynomial, first order polynomial, a fifth order, a seventh order polynomial. That choice of the order of the polynomial is my hyper parameter here. It controls the degree of complexity of the model. Now, what's really cool is if I take the training data and I fit it with the best fit model for each one of those orders of a polynomial, that's what we're talking about when we say train a variety of levels of complexity. Now, what we can do is we can position the testing data that was withheld and we can see which data fit best to the model trained not with that data. But you can see what that'll do is they'll protect us from overfit. Okay, so let me ask you this. If I was to change the hyper parameter of complexity or order of the polynomial, which one of these models, the first, third, fifth, or seventh order, will do a better job of fitting the training data. If you look at the seventh order model right here, the error terms at each one of the training data is zero. You perfectly fit the data. In fact, if you set the polynomial basically equal to number of data or plus one, you can basically, you can perfectly perfectly fit that, right. Now, how do you think you would do for this model with withheld testing data? Would you do well? I'm seeing some faces that are suggesting you do pretty poorly. In fact, you could imagine with this data set, if I had a withheld testing data, data value right here, here, and here, what would happen is this model right here is probably making predictions, probably at the floor of my office right now. It's going through the monitor to the bottom of the floor there. It really would be a poor model to make predictions by doing picking the complexity based on fit with unseen data. We protect ourselves from overfit. Now, I will explain overfit right away. Interpretebility explainability. This is critical. Your ability to understand the model, how each predictor is associated with the response. For example, in a linear model, you can perfectly understand the model. If it's a linear model, I have the coefficient. I know for every incremental change of one feature, how the predictions are going to change. But now, imagine the interpretability and complexity of a feed forward. And we'll explain this on the Thursday after this one. A feed forward fully connected neural net would have something like this. So, basically, what you can see is that there are so many parameters, so many connections, it'd be very difficult to parse through and figure out exactly what that model is doing. So, when you build more complexity, you often sacrifice interpretability. And what I would say in our business interpretability is critical. We need to understand how the machine made the choice. So, let's talk about the balancing act for us. Let's talk about flexibility complexity versus accuracy. Typically, what we'll see just like that first, third, fifth, and seventh order polynomial is that what we'll find is that the error, this is error right here on this axis, this gray line line is the error in training. So, if we increase the flexibility or the complexity, the error will continuously go down. In our model, we've reached the point where we got to zero, zero error because our complexity was high enough to fit all of the data, the training data. But if you look at the error in testing, what you'll find is that often that will go down and then it's going to start coming back up. So, what it comes back up again is the point at which you're becoming overfit, you're overfit to the data. You have you result in a model that's locally too specific, locally too accurate, too complicated, then more complicated than what's justified by the data. Okay, now, don't worry about it because when it comes to model goodness, we have a very good metric to work with and it breaks down to some fundamental concepts that we can use. So, the expected test mean squared error would be a very good metric for us to use. It's going to be the expectation or the average squared error. Okay, that was a legacy thumbs down. Good, a response to some other question. Hey, hey, it's super cool. Feel free to interact. I do appreciate it. The average squared difference between the true values in testing and the estimate at the testing locations, this F hat is the estimates at the testing locations squared. That's expected test mean squared error. Now, it turns out if you expand the quadratic and work out the expectations, we don't go in that derivation here. It's available to you in the hasty book. Well, you'll find that there's an additivity of variance interesting thing going on and that is you have three components of variability or of the error with your system model variance model bias and irreducible error. Now, this is central to every model we built model variance is the variance if we had estimated the model with a different training data set. In other words, if you train the change the data, does the model change dramatically a linear regression model if you it's a very simple model. If you change the data, it'll move a little bit a seventh order polynomial. If you change the data, it will whip around like a whip. It will go totally different that previous model would dramatically change based on the data. That's model variance sensitivity to the data and it's a source of error in your model. It impacts performance when we get into decision trees and we go to random forest. It's all about fighting a battle against model variance to improve model performance model bias is the error due to using an approximate model. The simpler models will have higher model bias model bias is the opposite we have a linear model a simple model, but the phenomenon is non-linear. You can't fit it very well. That error is due to the factor models too simple. Irreducible error is due to missing variables limited features samples can't be fit can't be fixed with the model. In other words, what's going on here is the world's expert of machine learning can't improve this. Because you can't you can't pick a better machine. You can't do anything because it's because you did not sample all of the features or variables you need it. It's because the data didn't span all the combinations of features. You don't have enough information. It turns out well, that's additive. So we can plot it for a given system. If we have model by a model model model bias shown right here complexity shown right here as we get more complicated model bias drops we can fit the complexities of the system. The model variance goes up because as you get more complicated your model becomes more sensitive to the training data. And the irreducible error is constant. It doesn't matter. It's due to what you have to work with the data you have. So if you add those all together, you'll get the total error in testing the mean squared error in testing. And what you'll find and this is a very important concept right here and why spent all the time on it. Is that if you pick the optimum degree of complexity or flexibility most often it's not over here. You see that. Because the model variance most often a less complicated model will outperform the most complicated model. Okay, and I just want that to kind of stick in because a lot of times people get excited I'm going to go to the most complicated machine learning methodology. In fact, it may be the worst choice when it comes to accuracy or performance of your model. Okay, what have we done. Well, we spent some time here and I think it was useful time to cover data analytics statistics geoscience engineering data to make some general comments and so forth. I think there's good deliverables from this discussion. The opportunity to think more broadly about using statistics as data analytics to improve our knowledge around that and much of what we cover kind of goes in the fundamental statistics. The idea of open source packages and Python parametric and non parametric the concept of parametric being more efficient for data, but you have to make a decision and lose flexibility upfront. Model bias model variance, irreducible error. Those are all critical concepts.
 How many of you have done something simple is try to open and excel notebook that contains millions of lines of data. It's impossible, right? The next one possible. It locks up your whole computer. So use Excel with caution. Don't open up a large data set. It's even better than the equivalent commercial tools. This is a very true statement, and I hope to convince you of that over the next few minutes and next few days for sure. For one, you're all using open source software every day. I mean, every time you log into a web page, that web page is being served by a Linux-web server, right? All of the top, every single one of the top 500, you know, as fast as computers in the world, super computers are Linux machines. Anyone who's done anything in Linux is using open source software. Linux is the most used open source software project in the world, largest in the world. So of course, we're all using that. At least that. Even under the hood of many commercial software packages, there's lots of open source software. For sure, all of the basic linear algebra that is done in any kind of physics-based simulation tool, they have libraries underneath, Bloss, basically, linear algebra services, LAPAC, things for doing solving linear systems of equations. Those are all open source software that have very liberal licenses such that commercial software is able to incorporate it in them. So we're going to use a lot of open source in this class. In fact, every tool that we talk about is going to be open source. And what that means for you is that you'll be able to use these things immediately in your day-to-day work. And hopefully, we'll convince you that in many cases, they are of better quality than equivalent commercial tools. And over the next few weeks, as we see some more software engineering, when we talk about unit testing and continuous integration and other things, and then you go to GitHub and you look at these open source packages and you see the amount of testing that's done on them, an automated testing that's run upon every single character of code change. This commercial software is not held to that kind of quality standards. And so I hope to actually present some evidence that shows that open source software is a better quality than commercial tools. So why? Quality security reliability. Given enough eyeball, I can't talk today. Given enough eyeballs, all bugs are shallow. This is something called Linus's Law. So Linus is Linus' tour vault. He is the original creator of the Linux kernel. The Linux is sort of a conjoined word of Linus's name and the word Unix. So Unix existed before Linux. Virtually all Unix machines are Linux machines now. There's some form of Linux kernel, right? So the idea here is that by being open source and having thousands of people use it, interact with it, it doesn't take very long for bugs to come to the surface. And as long as you have an adequate system of documenting and fixing those bugs, then the quality of the software will rise. And this idea combined with quality version control software system, we'll talk about that later. In fact, it was really trying to manage the thousands of contributions that were coming in to the Linux kernel that caused Linus's tour vaults to also right get the version control software, the underlying namesake of the website GitHub, which we'll talk more about later. So Git is a version control tool also written by Linus's tour vault, used to manage the Linux kernel project. And it's, we'll talk more about it when we talk about Git and GitHub. But it's really is the ubiquitous kind of tools for version control. And hopefully you'll be using it regularly after this course. Modern software engineering practices, what I mean by that is things like automated testing, continuous integration, automated documentation systems, other things like that. And notebooks, use of notebooks for reproducibility and documentation, other things like that, all kind of falling into this modern software engineering practices. And of course modularity. And in the open source world, we often just build on top of other pieces. Even if we set out to do our own open source software project, we're often just building on top of other packages. So for example, like I mentioned earlier, Bloss and LAPAC, these are like the most fundamental and core linear algebra services available. And so no one really these days goes through the extent to write their own like matrix vector prod highly optimized matrix vector product routines. You're just going to use Bloss and LAPAC, which are written in assembly and nothing will be faster than those. In fact, if you're using an Intel machine, there's a software which has all the Bloss and LAPAC capabilities called MKL. It's Matt in Intel's math kernel library. Now the math kernel library is not in fact open source. So maybe I shouldn't be talking about it here under open source, but it is freely distributed. But the reason I mentioned it is again, it's distributed by Intel. And because they have their own expert knowledge of the hardware, the software that's written into MKL really can't be any faster. It uses internal knowledge of the hardware to make those type of matrix vector or matrix matrix product operations as fast as possible. And so if you're ever doing any kind of linear algebra, you really should be using some of these fundamental libraries. Of course, it's all wrapped under the hood from Python. We'll be using them and we won't even know it as we go forward. So this idea of modularity, building upon the tools of other people so that we can write less code and get more done. There's another reason for using open source. Lower cost of ownership. Free is in speech, not beer. So what we mean by that is that somebody had to write the open source software. So it's not completely free because someone's labor is tied up in it no matter what. It's not free like beer. But what we mean when we talk about free with respect to open source software is freedom to use it however you please. And it's one of the reasons we try to only use open source software in this class, even write down to our learning management system here. This learning management system is based on something called Jupiter Lab, which we'll talk about in more detail later. But this itself is open source and you can install this same exact environment on your own computer. That I can think of, we do not use any or there's not one piece of kind of closed source software that we'll discuss over the next few weeks. And so everything you do here, you should be able to reproduce on your own in your own environment. And of course you'll have access to our learning management system for quite a long time, at least a year, but nevertheless, you can reproduce all of this in your own environment as well. So that's what we mean by lower cost of ownership. This one might be my surprise you, but community, it's actually one of the things that makes Python so strong. And if I haven't talked about Python, let's go do, I like to just do this live. Let's go do a Google Trans Search and look at Python data science. And let's expand the region since 2004. So if you look at Python data science, and I put the data science in there because, well, let's not for a second. Let's just remember how many of you got started programming using, say, MATLAB. Can you give me a thumbs up or a yes or something, a hand and a head nod. This would do a comparison. Python versus MATLAB since 2004. So MATLAB is pretty steady. By the way, you see how, particularly with MATLAB appears to be very cyclical, nice tools built into MATLAB to do data science, but virtually no one does it. And even the same thing, we could probably, another popular language for data sciences are, it's a statistical programming language. So if we look at R, I think you're going to, no, that's actually surprising. I thought it would be, I thought R would be a little closer to. Hey, John, I think it might be putting a space between data and science. Negative? It is two words. I don't know why I wrote it like that. There we go. I apologize for that. I don't know why I wrote that. Yeah, so this is kind of what I was expecting. Again, no one really does data science with MATLAB. I mean, everybody uses Excel as doing some kind of data science, so maybe I shouldn't be that critical. But let's say machine learning. No one really does machine learning with MATLAB, even though there's quite a few tools built there. A lot more people use R, and of course the popularity of Python is really high. And it's one of the reasons that, you know, this is really the reason that we do advocate Python because there are other good language. There's R is a great language. Not such a fan of MATLAB because of the close source nature of it. But nevertheless, returning to this bullet community, what that means, what the fact that all of these people are, I mean, obviously, these are Google trends, so it's like search queries. So it's basically people searching for things related to Python and data science. But nevertheless, you know, part of the reason that we advocate for Python is because of this aspect of community. So the more users of a language there are, the larger the community. And what that means for us as learners is that there's more help available. So if you go to stack overflow, stack overflow is, if you don't know what it is, it's a website for asking questions about coding usually, quite a bit more there. But if you go to stack overflow and you were to look at the tags, maybe we have to go to a question. I don't recall it off the top of my head. But if you were to go and look at the tags, here we go, this one meant to be Python, there's for Python, there's currently over Python 3, there's currently like 132,000 questions on here. For pandas, which is something that was a Python package, we'll learn about later, 127,000 questions on stack overflow that have been, most of them likely been answered already. So if you need help learning Python, there's the larger the community, the better it is for you. And also, you know, Python is a general purpose programming language. It's one that, you know, we will use for data science and machine learning, but you need system administrators often use it for doing system administration tasks, things that they might have formerly used to the bash shell or something else for. And I'm just making the case that the community being larger leads to more help ultimately. And by the way, if you don't use stack overflow, I mean, you know, I tell my students in class, and you know, often when I'm teaching program, I often won't give them all the information. I will purposely in an assignment, leave out something and force them to go to Google, force them to go to stack overflow. I've been coding for, you know, daily for close to 20 years. And, you know, for at least seven years of that time is basically my job. I do it all day every day. And you know, at any given time, you see my computer screen, you'll see a terminal window open, you'll see a Jupyter notebook open, and you'll see stack overflow. I mean, I'm on there as much as anyone and I've been coding a lot. And it really is just an essential skill. It's part of learning to code is learning to search Google quickly, search stack overflow quickly for answers. So what that means is learning to correct keywords, you know. So if you just type Python in the Google, in the Google search bar, you're just going to get pictures of snakes, right? But if you, you know, if we teach, if you type say numpy or, you know, Python pandas, then in that case, you're going to get something a lot more meaningful. And so, you know, it's really just as an essential skill and don't feel bad or feel like, you know, that you're not an adequate programmer. If you have the search Google and search staff stack overflow, a lot to find answers. We all do it. It's part of it. And that's part of why we advocate Python is because Python for sure has the largest community beside any other language except for possibly JavaScript, JavaScript being the language of web pages. So and I think the point I meant to make when I went to stack overflow is to say that the last time I checked, Python was the number two tag in all of stack overflow. Again, JavaScript being number one. And it's quickly approaching JavaScript. The last thing is build a real resume, right? What I mean by that is, you know, there's another thing I tell my students, like no one cares. If you send them a PDF of your resume and at the bottom of that, it says that you know C++. No one cares. Not today. You'll never get hired at any tech company doing that. I know you guys aren't looking to get hired anywhere, right? You're just looking to up skill at your job at Noble. But nevertheless, when you work with and contribute to open source software, when you follow good software engineering practices, when you embrace this idea of openness, openness so that you have fewer bugs and a lower cost of ownership and a community, what you also do is a byproduct as you build a real resume, right? And what I mean by that is again, I showed my GitHub earlier. If you go to my GitHub, again, you can see, you know, in this kind of grid view, all the commits I made, but let's look at something more specific. You'd also go to my repositories. You can click on any one of those repositories and you can see, you know, the code that's in there. But let's go to a piece of code that I've contributed to over the years called paradigm. This was a code we started when I still worked at Sandia and it was open source in 2012 and we've been maintaining it on GitHub ever since. And these are kinds of problems that we solve with paradigm. I actually use it in my research at UT to do hydraulic fracturing. But here you can see it's a general purpose fracture predictive code. You can see here, like some simulation, this was a cylinder that's been exploded, fragmented into many pieces. So it's very good at doing computational fracture. And so if you go out here to the contributors and you click on it, right here, you can see that I'm the fourth largest contributor to this code base. I've written 61,000 lines of code and I've deleted 32,000 lines of code. Of course, most of that was some time ago, 2012, 13, 14, and then maybe a little bit in 16 and a little blip out here a few months ago. But what we can do, you can even go further than that if you're really interested. If you were to, sorry, didn't mean to do that. But if you were to actually look at the commits I made, you can go in, you can see every single commit I made to this code base, commit me like a snapshot in time. So you know, there's a message there, but if you go in here, you can actually see the code that was changed. So this is C++ code. And so over here on the left was the original source file and over here on the right was the changes I made and committed to the repository. Everything is in the open. Every piece of work that you do when you follow these practices is in the open. And this is the stuff that people care about. You know, if you're looking to make a career change, you know, if you, this is the stuff that gets you hired at Google and Amazon and Facebook, okay? And I hope that one day we can say the same thing. I hope, you know, my goal, my vision, part of my desire and sort of what motivates me and one and part of the reason I wanted to start data and teach is I really love to see a culture change in the oil and gas industry. We have so much opportunity that if we would just collaborate with each other, work in the open on a lot of projects. And you know, the real value of most large companies was in the assets that they're saying they're in the ground. It's not in the crappy software that they're writing. So I really hope to see a day when we can collaborate on software work together. And let's really focus on officially getting the real, you know, where the real value is, the stuff that's in the ground out. Of course, I should caveat that when today's oil price is maybe the real value is not what's in the ground. So kind of last thing we're going to talk about with respect to open source and Python is this the Python data science ecosystem. So you might hear me say that you might hear me use things like pi data, Python data science ecosystem, those kind of words. And what I'm really talking about is what the things you see here, the individual third party models, of course, Python itself. And also the models that we use to build software around them, Jupyter notebooks being a core functionality of that. All of these will learn in this class. I mean, GitHub is not really related to Python, but it's where we store our Python code and with something we will cover here. All the rest are Python libraries. NumPy, this is our core numerical computation library for doing array computations. Pandas is sort of our drop in Excel replacement. Pi pi, this is where all our numerical algorithms like root finding and interpolation, integration, routines are contained in SiPi. MatplotLive is a plotting library for making production quality plots. BoKay is another plotting library really targeted for web based and streaming applications. One is a dashboarding application built on top of BoKay. And of course, scikit-learn is the workhorse machine learning framework. Probably the biggest, most used machine learning framework in the world. And those of you that took Dr. Purchase class got a little bit of exposure to it and we'll be doing some more in this class as well. So we're actually not going to write a whole lot of Python code today, some review. But before we go on and talk about something else, I'd like to go ahead and ask your Python expertise.
 Reminder, data analytics, the use of statistics and visualization to learn from the data and reminder about the venn diagram of all these areas of geostats, statistics, big data analytics, and so forth. Okay, let's build some concepts right now. Basic probability. First of all, if we talk about our data types, let's be really, let's be really cognizant of what they tell us. And if I look at all of our data types, I really want to understand resolution, coverage, and information type. This is critical because when we build machines and we put things together, let's be cognizant of the fact that we have dramatically different scales, dramatically different coverages. We have issues around bias. Some we don't have as much issues around bias. We should be thinking about our data like this. We should not be just having the data as a title and a column on a data table. I think we should really be cognizant of this. I put that up there for reference. The other thing too is there's basic definitions we need and statistics are variable. A property measured observed in a study, a location, a time, some interval of space or time, porosity, permeability, and so forth. In data mining, machine learning, we call them features. It's just the way it is. I tend to adopt features everywhere now because of that. Population. A population is an exhaustive finite set of values of a property of interest over the area of interest exhaustive at the scale you need to solve your problem. The population is not observable. In fact, we don't have what's the population for us in the subsurface, the entire reservoir. Sample. The sample is going to be a limited subset of the population. It's the one trillion, the one million, whatever it is of the subsurface that you have available to you and its sparse. It's generally going to be sparse or it's going to be low resolution if it's seismic information. The parameters a summary measure from the population, population mean, is this starting to fire some neurons from your statistics 101? Maybe, right? Okay, population mean, population standard deviation. We don't have access to it. When we do hypothesis testing and I do have lecture content on use of hypothesis testing in the subsurface, we actually do make decisions with regard to the, with regard to population statistics or population parameters, I should say, that we don't have access to the statistics summary measure of the sample sample mean sample standard deviation. That's what we have to work with. That's what we use for our models directly. Now, how many of you are frequentists and how many of you are basins statisticians or do you work in Bayesian probability? Anybody here working in Bayesian probability right now? So clearly when you work with Bayesian methods, it does offer new advantages and opportunities. Let me explain the frequentists, then I'll explain Bayesian and you'll see what I mean. Frequentist approach is the idea of getting to probability by counting and ratios. If I want to know the probability of event A happening, I'll do the limit of N goes large enough, N to infinity or large enough observations of events and I count N, the number of times A occurred, N omega is the full set of possible outcomes. If I take that ratio, I'll say now I know something about the probability of occurrence based on frequencies. That's why they call it the frequency is frequentist approach. Now, what's very interesting is we do frequentist probability all over the place. In fact, it's probably one of the most common ways to get things done, but it is, it has some interesting limitations. I'll talk about them right away. A strict frequentist will tell you there is no probability outside of that very experiment. Now, I get away from that because I'm a geostats person and so I assume stationarity and I assume that once I've sampled that that I have enough samples, I can say something about the entire subset of the surface. And so I'll say by frequencies, I know probabilities in the subsurface. There's arguments about that. But for practical purposes, that's what we all do. The Bayesian approach is different. With the Bayesian approach instead of just counting, what you do is probability is interpreted as a reasonable likelihood representing a state of knowledge. Now, what's really interesting is they accept belief. If you go back here, there is no probability for the frequentist outside of the experiment and the observations. The Bayesian probability says, I will accept the idea that I can specify my prior belief and I can update with new information. The P of A, just like we had before, is now going to be my prior belief. Now, what's really cool is that can be your experience. It can be your knowledge, your scientific knowledge and so forth. The P of A is the information you got from new data, new information and the PA given B is your updated, your updated probability given prior belief and the new observations that you just had. Now, what's very cool about this, if you get into and we will show you some Bayesian approaches and machine learning, like naive base. What you'll find is that it accepts uncertainty at every step in the process. Well, the frequency is approaches really do make some pretty strong assumptions about knowledge. And so we do appreciate the fact that it does allow uncertainty to go all the way through. Now, John, have I missed anything? I know you're a very strong advocate for Bayesian approaches. Now, you're good and just a reminder, as always, my advocacy is to teach Bayesian statistics to engineers. Right. If you're a professional statistician, you can do whatever you want. And so what what Dr. Foster is saying is he feels that Bayesian approaches are safer. Right, John, because of the fact that you are accounting for uncertainty at every step. Well, the frequent as approach, you can get yourself in trouble with that degree of certainty. That's correct. All right. Good. Hey, I've worked with John for a while now. We've had our I tend to work with both methodologies. And part of it is just practical, but I do think that there is a role from both and there can be debates around that. Univariate refers to the fact that we're only considering one feature at a time, one variable at a time. And we've all done this before. You can create histograms, probability density functions. Really standardized histogram or measure of density or likelihood. Cumulative distribution functions are very useful. This histogram converted into a CDF looks like this. It's at the resolution of the data. And we can read directly from it the forward transform, which is the probability, cumulative probability of a specific value. Or we can take a desired, cumulative probability, p 10 p 90 is common in our industry. And we can look up the specific values from it and use that as a predictive model. That's pretty powerful. There's a whole variety of different measures that we're going to use for parameters for a population or statistics for sample. We have the sample arithmetic mean or the just we call the mean or the average, not a big deal. We all do that. Central tendency may be also expressed as a p 50 or the median value, the 50% tell that's safer to do a few of outliers or skew distributions, a log normal distribution or positively skew distribution. The arithmetic mean is going to be pulled very much to a high value. Not a big deal. We can always switch to another measure of central tendency. In fact, we may in fact prefer in some distributions to work with the p 50 or a mode or even other measures like a geometric mean and so forth. Measures of dispersion while the central tendency matters in univariate statistics, but so does the amount of dispersion. In fact, what I would say is dispersion is a measure of uncertainty. Because if you consider if this was the distribution of permeability from my reservoir with a low standard deviation of only 10, that's a very low level of uncertainty. If I make a prediction with regard to a pre-drilled location, I'm saying that there's very low uncertainty. I know what's going on. Standard deviation that would be higher, higher uncertainty. So it's important to quantify dispersion as it relates to uncertainty. Multivariate analysis. We typically need to build reservoir models of more than one feature at a time. We don't get to just work with one feature. We have to work with many. This has gotten even worse or better, depending how you look at it, with whole earth modeling. I've seen companies start in fact concerned with modeling, not just the reservoir, but the overburden to capture more completely the seismic effects or to get at hazards such as any type of failure in the seal or anything like that. Expansion to unconventional has resulted in a whole new set of features. Geomechanical features are now prominent in these models. People are modeling them spatially. Petrophysical properties are being augmented by all kinds of other geologic parameters. People are trying to find out the relationships. Sorry, I have a bit of a tickle on my throat. So there's many different features we need to work with. Now, let me just mention the curse of dimensionality. More features, more variables is harder to do. More difficult to visualize, more data required to infer joint probabilities. In other words, the calculate probabilities with the combinations. It's more difficult less coverage. More difficult to interrogate and check the model. More likely you have redundant or multi colon arid is what we would call that. More complicated, more likely overfit. Very negative, eh? When it comes to working with lots of features, your job is harder. It's not easier. Okay, and we'll say more about that shortly. A confession. Most of the subsurface modeling workflows up until now have been two variables at a time. Typically, what we do is we'll build a faceys model about 90% of reservoirs I worked with had a faceys model. Some of them went right to porosity. We'll have a faceys model. Then we'll go by faceys will build porosity models because porosity is the easiest feature to handle. So we start with the easiest one. Then we'll go ahead will build a model that goes with porosity, co-simulating or co estimating permeability. So we reproduce the relationship. And then we'll do saturations by permeability and faceys. So intrinsically everything has been two variables at a time. So we're getting more challenging because we have to work with many more variables and there are emerging technologies to better handle many variables at a time. We won't I won't get into that during this lecture. If there was interest in how to do simultaneous subsurface modeling of many variables, that would be a lecture set that we could go through an example so I could provide. I have some example workflows. In fact, there's some really cool stuff going on with transformations, just a little bit of a spoiler alert. It turns out that you can actually transform your features, remove the spatial and the multivariate structures, model them independently and add it back in. That's one workflow. There's other workflows like we call super secondary where we combine things together. It's behind beyond the scope of this. We can see the analysis is critical though, because often when you look at variables or features more than one variable at a time, we expand the image. If I had looked at the marginal distribution, one variable at a time for a porosity and permeability, I would never understood that there are these trends related to grain size. There's these interesting structures that occur because of unique lithologies, and that may be something that's of concern to us in the model. Now, when we look at two variables at a time, we are looking for critical structures because they pertain to the model we can use and the choices we need to make down the line with data analytics and machine learning. Now, if we have data like this, acoustic impedance versus porosity, and it has that distribution in the cross plot, that's good news. That's a linear relationship with homoscedasticity. Now, as I always tell my students, every day I get to say heteroscedasticity or homoscedasticity is a good day, because it's like my favorite word to say. Homoscedasticity refers to the idea that if you would have been up on acoustic impedance, the conditional variance, in other words, the spread from here to here, is the same everywhere. In fact, that's a Gaussian multi-gosing behavior is homoscedastic and linear correlation structures, and therefore many of the Gaussian methodologies, the methodologies that assume a Gaussian assumption, and you'd be surprised that Gaussian disease is very contagious. It's all over the place. Many of those methods can be used directly without transformation. Now, you might have permeability versus porosity, it's commonly non-linear and heteroscedastic. I hope you can see that now. The amount of variability and permeability at low porosity is very, very small, but the amount of variability in the high porosities is in fact very, very high. That's the concept of heteroscedasticity. Now, you'll find there's different data analytics and machine learning methodologies that we will violate assumptions with that. Even what's really funny, even something as simple as linear regression requires homoscedasticity. I don't know if many people knew that. And the result is you actually have a bias in your model if you don't have them. Now, there's other things that are interesting. We can make use of that. Of course, you can see if the data is categorical or has some, and you'd be surprised. Many things we think are continuous or have values over an interval, in fact behave in a categorical sense at a certain scale or at a certain, you know, interval of measurement. And it's important to pay attention because that we need to account for that. Continuous, we might also have constraints. There are many different constraints you can encounter between variables. Now, there's some really interesting ones. You might have variables or features for which they have an additivity constraint. This commonly occurs with concentrations. In other words, the more of one feature concentration, the less of another feature you can fit in the system. And so they have a some one constraint that will result in some interesting type of linear features on the plot. We should know about those because we can incorporate them in the model. When we incorporate or we try to quantify by various relationships, the most common first approach is to use the Pearson's product moment correlation coefficient, the correlation coefficient. Everybody uses it. It goes from negative one to positive one. It has all kinds of issues though. It really does. It's very sensitive to the data, sensitive to outliers. And I'll show some examples of that right away. The correlation analysis is employed everywhere. And so one thing we can remember and relate to is the fact that the correlation coefficient, if we look at the equation, is a covariance calculation. Now what's really cool about that is if you look at this equation, it's the same thing as the calculation for a variance, but instead of taking the square, let me change this to a pointer. Instead of taking the square of all of the features minus the mean, you expand the square and you replace the other X with a Y. In other words, another feature. So the way to think about correlation or covariance is that covariance is a measure of how two features vary together. A variance was a measure of how one feature varies with itself. Isn't that kind of cool? I really like that interpretation. It helps me. If you take the covariance and you standardize by the standard deviation of each feature, you get the correlation coefficient. So not a big deal. Super easy. Yeah, so this right here, I'm just explaining this idea that the sample variance can be related to the sample covariance just by replacing one of the X's with a Y. In other words, another feature, a measure of how two features vary together. Now this is something I don't think people do often enough. Who works with the spearman rank correlation coefficient. The reason why we like it is because we do a rank transform on all of the features, not not complicated at all. Take all of your features, put them in order of a sending order and assign the largest value, a rank of one, the smallest value gets a rank of N, where N is the number of data. Okay, so by doing that, what you've done is you sorted the data and you've assigned a value of rank. What that does is if the data is all pooled here, but there's a couple of outliers over here. The outliers are sucked in. They're brought into the distribution. It doesn't matter if the largest value was a thousand units larger in the next largest value. It's now just rank one larger. It's brought in. Now what's very interesting and it's this a little bit subtle. If you have non-linear monotonic relationships, it will linearize that. So what's so it's cool is this is a correlation coefficient calculation that first treats outliers and linearizes monotonic non-linear relationships. Okay, so this provides a much more robust correlation coefficient. Consider this in your next correlation analysis. Now, why do we care about that? Why do we want to talk about limitations? This is why right here. If I make a data set with a bunch of random values with zero correlation, they're independent, but I put one outlier up here. The correlation coefficient is going to be 0.95. You see that one by varied outlier creates artificial correlation very, very dangerous. Okay. The other thing too is if you have a non-linear relationship like this quadratic function and you analyze the correlation coefficient all the way from here to here from negative one to positive one or negative 10 to positive 10. If you had enough samples and you calculate the correlation coefficient, do you have any idea what that correlation coefficient would be? 0, 0, 0. And so which is funny because the correlation is telling you how much information is being shared, but in that case the quadratic would perfectly share information. I know X, I know why, but you'd be fooled into thinking there was no correlation. I like to show this example because I like to illustrate the famous issue that correlation does not imply causation. Okay, now remember, I'm not saying anything negative about churches whatsoever. Okay, everybody keep that in mind as I say, did you know that churches cause crime? You see this plot right here, I show it, right? You have, look at that nice correlation, number of churches, number of crimes. What's going on here? What are we missing? The population. And so what you'll find is if you in fact looked at it, you'd realize that there's small cities, medium cities, large cities. And if you calculate the correlation for standardizing for population, you'd find there be zero correlation. And so this is a huge issue in correlation analysis and this is why I don't necessarily like data mining. Standard data mining may contain or include the idea of just look for every correlation you can find and now focus on that and try to analyze that because often what you find is that you have to control for features carefully. And to get causation, causal analysis requires careful control, careful experimentation where you just very one feature and so forth, right? So we're not doing that here, right? Now has anybody ever gone to the website spurious correlations.com? If you have not gone, I know some of us may have a little extra time with the family these days. I don't know about you, but got three of my kids, including a university kid from UT home with me right now. Everybody's here in the house. Go ahead and cover a little spurious correlations.com. You go to the website, what they do is they data mine from large data sets, usually government type of data sets. And they look for high correlation coefficients, what they found was margin causes divorce in the state of Maine. So in case anybody's concerned about, you know, marriage and all that. Remember margin causes divorce. I personally think divorce causes margin because I could imagine that, you know, I may cook differently or something. Okay, spiders are killing people at a rate that's actually being controlled by the length of the words and the national spelling bee. So I think we should try to shorten up those words and save some of those spider victims. Okay, so anyway, the point is I'm talking about correlation coefficients of point 9926. If you just cherry pick two data sets over a short enough time window. Okay, this is fascinating. I have an interactive workflow on spurious correlations. Now, I don't think I'll do it right now, but I do welcome you to check it out or I will, maybe next day if we have a little extra time, I'll get into it and show it to you. Okay, let me go ahead. I think we've talked enough about correlation analysis. I do want to introduce one new idea to you because I promise with every lecture, my goal is to give you a new tool that you can use right now to do analysis and to get the job done. Who here is used partial correlations. Nobody nobody. Okay, so partial correlation, let me explain it to you very quickly and precisely that goes as following. It's a method to calculate correlation between x and y between two predictor features or between a predictor feature and a response feature. It doesn't matter. You just want to know the relationship between two features. What it does is that mathematically tries to isolate the influence of them on each other. And this is fascinating. So what do you do? What's the steps for partial correlation coefficient? You perform linearly squares regression to predict x from all of the other features. So I had M features available. M could be 20. I want to work with x and y, but I have z 18 other z's to work with. Right. So what I do is I calculate the correlation. I mean the linear regression model of all of the other features on x and that's my model right here. Now I simplified it. Of course, it's a multi-dimensional plot because that would be like a hyper plane, right. Okay, then I calculate the model between all of the other features and the y. Okay, so far so good. I built a multi-linear regression model for each of the two features I want to work with. Then what I do is I calculate the residual. I take the predictions I would make with that model and I subtract them from the actual feature. So now I get a residual and that residual has been detrended relative to all these z's. It's going to be level now. It's a residual with a zero mean and positive and negative values. Okay, so I have removed the influence of all of the other features on x and y. Okay, all of their information has been taken away. Next what I do is I do correlation analysis between the residual y residual x and this is the unique relationship between x and y having controlled for or removed all of the other features. What do you think of that? Interesting. Any questions. That was a little tricky. Okay, now there's always assumptions. The assumptions you make you notice we use the linear regression model. In other words, the relationships we remove are going to be all linear relationships. Okay, so. And the other thing is you remember how I said about homoscedasticity heteroscedasticity. This methodology does assume homoscedasticity between all of the pairwise variables. Okay, just to give you a little warning there. And you could do a transform to fix that if you had to this methodology is very, very powerful. I have a demonstration of doing it. We can walk through it. Maybe after we take a break here shortly. Major scatter plots are also a really good idea. When you look at many, many variables, I always think it's a good idea to produce these matrix scatter plots. Look for constraints, look for heteroscedasticity. See what's going on with your features. This is very, very useful in Python, one line of code and get this plot. Okay, I'm not going to cover the marginal joint conditional probabilities. I'm going to leave this here as an additional topic for you. I'm going to give you a little warning for your consideration. But I will not cover that today. I'll tell you what I'll give you an assignment before Thursday to go ahead and look through these slides. If you have any questions, ask me about them. And if you want, I can cover a little more detail about them.
 So the Jupyter project, so Jupyter was founded as, well, let's just go on to the next slide. The Jupyter project builds tools for interactive exploratory and collaborative computing, and the primary product of the Jupyter project are Jupyter notebooks. Jupyter notebooks were originally started around 2011. They were called I Python notebooks. The Python obviously is a nod to the programming language. The I is meant to be interactive, interactive Python. However, the technology was built in such a way that the front end, the notebook front end was just a web-based front end, and people soon realized that you could detach the compute kernel from the front end. So in other words, it didn't necessarily have to run Python. And in fact, even when the name of the notebooks was I Python notebooks, I used to run MATLAB in them just for demonstration purposes and showing my courses and other things. So I think around 2014, the project was renamed Jupyter, and Jupyter is a nod to essentially the language agnostic nature of the notebook. It's also a nod to in fact, the logo itself is a nod to the moons of Jupyter which Galileo, the kind of father of modern science, Galileo was the first person to see the moons of Jupyter with his telescope, of course. And it's also a conjunction of different names in here. So the JU is a nod to Julia programming the py for Python and the r for r. But again, now these notebooks, you can actually, they'll actually support about 40 different programming languages. And in fact, one of my most favorite ones is that you can actually do interactive C++ with a particular kernel. So in other words, you can write a little bit of C++ and without having to go and compile it, you can execute it in a cell. And we'll talk about what all these things are as soon as cell and whatnot. You can execute that and actually do interactive C++. So for now it's time to get our hands dirty. So if you will go to, you know, you should be looking at something like this, you probably have far less stuff in your file browser over here. But what you're looking at is in fact, a part of Jupyter. And in fact, it's Jupyter hub. So it's a multi-version notebook for businesses, labs, and education. So that's what we built our learning management system on top of. This is Jupyter hub running something called Jupyter Lab. And Jupyter Lab is kind of the third component of the Jupyter ecosystem. And that is the next version of notebooks. And it's something closer to what we call an IDE, a fully integrated development environment. So for some of the users that have used, you know, have done a lot of other coding, you might have used an IDE. IDE typically have things like, you know, debuggers built into them, efficient ways to edit code across entire projects, other things like that. So Jupyter Lab is not a 100% fully functional IDE at this point, but it's moving in that direction. And it is the future of the Jupyter project. One component of Jupyter Lab are Jupyter notebooks. And so let's launch a Jupyter notebook. If you go over here, again, in the file browser, and if you don't see the file browser, you can maximize and minimize it with this folder icon here. So if you were to maximize it, you see the plus there. If you click on this plus, it takes you to what we call the launcher screen. And from here, we can launch a Python notebook, Jupyter notebook. And so is everybody, let me know if someone has trouble or needs me to repeat what we've just done there. So let's see anybody that needs help. So we'll go ahead and just play around in the notebook. So I don't know, you know, I'm a screen real estate you guys have, but hopefully you have enough that you can sort of organize your screen such that you have clustering.datum.org on maybe one half of your monitor. And then you can see what I'm doing on the other half. You know, so have the zoom window on the other half of the monitor. Because what we're going to be doing a lot today is just some kind of handhold coding. So I'm going to type something in you type something. And we'll do that for a while quite a bit today. So hopefully you can rearrange your screen in that way. And now we have in front of us a Jupyter notebook. What's a Jupyter notebook? Well, one, it's a Python coding environment. So we can do things like type two plus two. And we can execute the cell. Now to execute the cell, the way I always do it is I hit shift return, shift enter. So once you type two plus two, if you hit shift enter, that will execute the cell, meaning that actually runs the Python code that you've typed in the cell and reports the result. You could also, in the case that you assign the output to a variable, like for example, we can assign the output two plus two to a variable. And if we execute that code, it does execute and the output is restored in the variable X. But the cell will only print to the screen the last non-assigned statement in the cell. So if you had multiple assigned statements, for example, if you had Y equals 4 plus 4, and we execute, only X will be printed because it's the last not assignment to there. So if we change that to Y, then we get that result. This is a full fled Python programming environment. So we can do things like write functions. So here I have a function a plus a and b. We're just going to have this function return a plus b. And then we can call that. So we have to, you do have to execute any cell that you make the assignment in. And then after that, you can call this function. We're going to write a lot of functions in this class because functions, you know, it's just as a habit. When I have the opportunity, I'm going to hope or try to get you to write good code. And good code is code that's easily testable. And so we really want to write basically all of our function, all of our code, break it up in the small functions that are easily testable. And then eventually we'll add our own unit test to that code so that we can test our code automatically. So we're going to write lots of functions in here. And we'll talk more about that later. So so far, always seeing as just, you know, some interactive Python. There's a couple of other things we can just look at the toolbar up here. There's other things you can do. These are cell operations. So for example, the cut will actually remove that group of cells. And then you can paste it back. And there's also a copy you could paste with those. I mentioned hitting shift enter to run the cells. You can also hit this little play button at the top. So that will run an individual cell. The kernel is, you'll see the name kernel a lot. In fact, in the main fob menu appear, there's there's kernel and some kernel operations. So the kernel refers to, you know, what's behind the scenes doing the computation. And this in this case, it's Python 3. You could possibly switch to kernel. We don't have any other kernels installed on our on our machine here. But you could have other kernels. You could have Julia. You could have, like I said, the possibly even MATLAB if you wanted to. You can have other kernels that you can install for other programming languages. We only have Python here. So this only have to worry about. However, due to the nature of, you know, there's one sort of, Jupiter notebooks are great. They're great for readability. They're great for learning how to code interactivity, you know, exploratory data analysis. But they do have something called hidden state. Let me see if I can describe that. Here, let's see if I, let me do this. So for example, here I'm in a define a variable that's going to store the addition of 2 plus 2. So X is going to be 4. And we're going to add 2 to X. Right. So that's going to be then the answer should be 6. And it is. Okay. However, if, you know, these notebooks could go on forever. I mean, I can go on and add so many cells that they're all, you know, they're off the page, right? So, let's say way down here in this, in this cell down here, I redefine X to be 22. Now, because again, I'm using my notebook for exploratory data analysis. At some point later, I go back up the notebook. Now, this is completely out of view. And the fact that it's out of you or not is not really relevant to what I'm discussing here, I just, I'm just trying to demonstrate a common use case. You have a very large notebook, lots of information. Somewhere down the notebook, you redefine a variable or something like that. And then you go back up here. And now you run this cell, and you get that X is 22. And, or perhaps you even run this cell and that cell. And now you're getting this X is 24. And it can be very confusing because you're looking at this and you're saying, okay, well, in three lines of code there, I have X equals 2 plus 2. And then I'm adding, that's 4. Then I'm adding 2 to that. That's 6. Well, why is my answer 24? These three cells do not add up to, you know, there's nothing in these three cells that would make me anticipate that X should hold the value 24. And of course, it's because we executed the cells out of order. And down here, out of sight, out of mind, we re-evaluated our value of X. This is something called hidden state. And it's the, and it's this idea that the execution order of the cells does matter, okay? And so, one thing I'm going to ask is that when you're debugging your code, before you ask me any questions, a lot of codes, when you're coding in the notebook, a lot of errors come from this fact of hidden state. And so I'm going to ask that you do one thing first before you hit me up for a question. And that is simply go up here to kernel and restart kernel and run all cells. If you do that, this will start at the top, this will restart the kernel, which clears all the stored variables in memory. And this will then run all the cells from top to bottom. And now you'll get, you know, because things were executed in the order you expect, you get the result you expect here without having to, you know, try to figure out what's going on with the hidden state issue. And this is, so this is fairly, fairly common problem. When you're debugging your code in the notebook, whenever something strange appears, the first thing you should do is restart kernel and run all cells. Okay? So, so far we've only talked about cells as place to write code. The great thing about new jubber notebooks, the great thing, you know, for reproducibility and for documentation is that each code cell can be converted into something called a markdown cell. So the way you do that is, if you go up here to the menu and you see where it says code, if you click on that and you change it to markdown, now we have a markdown cell here. What is markdown? Markdown is a very lightweight markup language. So, if you don't know what a markup language is, many of you are probably familiar with at least one and that is HTML, right? It's, it's, it's the language that marks up web pages. That tells you when font should be bold and or a talict or what color it should be or other things like that. And HTML, you know, it stands for, even stands for hypertext markup language, right? That's what HTML stands for. Markdown is a very lightweight markup language. So, if you've ever looked at HTML, it's not lightweight. You have all these tags, you know, basically every time you want to make a cell bold. By the way, markdown cells will actually understand HTML. So, Avi, are you there? Yes, I am John. Okay. What would be the HTML cell tag to make a to make something bold? It's going to be B, yep, B, just B. Yeah, I think so. Let's see if this works. Yep, work. Okay. So, in this case, the markdown cells will understand HTML. So, you can write HTML in there. However, you know, the whole point of writing markdown is that it's a lightweight markup language. So, in this case, the little B tags, right, these are the markup that tell the interpreter that this bit of text should be bold. But in markdown, we can do this with much, in this example, it's not a great one because the cell tag is so short. But if you, in general, the HTML cell tags can be very, very kind of long and ugly, especially as you get more and more complicated styling. So, in markdown, we can replicate this behavior by just using two asterisks. So, there's a little bit of syntax you have to learn. So, that's bold. This is italic. And if you execute that, what you get is rich text, right? This doesn't look like code, like the code cells have, right? This is, this is rich text that we can add it and write explanations here. You can also do things like lists. So, you can make lists. And they come out really nicely formatted, right? None of this nonsense you mess with word, like, you know, constantly changing the indentation and trying to, trying to get the stuff to line up correctly. You can have nested lists. And it automatically handles the labeling for you. In fact, you don't even need to label things in order. So, in this case, I'm just going to, in all, I'm going to, instead of actually putting things like one, two, three, I'm just going to put everything as one. And when I execute this, you see that it's automatically numbered for me. And that's really handy because a lot of times we go back and we want add items to a list. And we don't want to have to re-number everything, right? In fact, I think it just ignores the numbers. Yeah. So, it doesn't matter what numbers you put there. It's automatically going to put them in the, you know, in the correct way, right? You can have other things, can have quoted text. Usually, when we write code in a notebook, we want code that executes. But if for some reason you didn't want code that it can execute, you could write say three back ticks followed by, if you actually say Python, it will highlight the code to be what you'd expect. So, the syntax here is the same as, as, as a attack. In fact, in the newest versions of Microsoft Word, the equation editor and Microsoft Word will even write you, let you allow you to write, let that, instead of using those silly menus that you have to click through with your mouse. So, in this case, if I execute that, of course, didn't work. What happened? There. Maybe I just didn't give it time to render. I'm not sure what happened there. Let's try it again. There it goes. I guess I just didn't give it time to render a second ago. So now, we get nicely formatted equations and equation fonts in our explanatory text. And in this case, I wrote a very simple example, but let me take the time to write something a little more complicated. So, if you just bear with me for a second. Now, in that example, above, I did an inline equation. If you want the equation to be centered in the, you know, the standalone equation that's, say, centered in the text, then you could use two dollar signs and you typically write something like this. So, I'm going to be quiet for a second while I write this longer equation. Just to demonstrate the types of things you can do. um Johnny's it single dollar sign or $2 sign $2.00 for a standalone equation. Oh, I got you. I have a, it must be a bug in my equations somewhere. I think this is a good moment here to show that John also has bugs and error. The person is going to show it. So I don't want you guys to freak out if you can't to get a little bit out. OK, there's part of my equation. Oh, I think I know what it is. OK, yeah, now we're moving. I ride a lot of weird doing with the ampersands, like how you were checking it. I think that's actually an important thing on debugging. Even your equations. Oh, yeah, just commenting stuff out. I don't actually see what the problem is here now. Oh, there it is. OK, I'm making progress. Sorry. I just wanted to do something sufficiently complex to get you to see that you can write really complicated equations in here. I don't think I've ever had so much trouble getting through it. Now that was just working. There it goes. I think some of it, for some reason, it's rendering really slow. I don't know if it's because I'm using up some bandwidth with zoom because it has to reach out to something called a math jack server to render it. I'm not real positive why I can't get this last part to render. So I'm just going to leave it like that. I was trying to write the pressure, the pressure to the specific equation, which of course the petroleum engineers should all be familiar. So I'm missing a term there next to the KMU. But nevertheless, there's a fairly sophisticated equation. I was trying to write full partial differential equation there. But the idea is, hopefully, it's still demonstrated that you can have really nice explanatory text, equations, figures, everything like that, that you can have basically anything you can do with HTML. So you can embed YouTube videos. You can have JavaScript widgets. You can have plots as something I didn't demonstrate. So let's see if I can do this quickly. I'm just going to real quickly make a function that creates a plot given a parameter a. So I'm going to put a plot to plot the sign of X. A times the sign of X. And then let's call my plot. And we get that. So it's not that interesting. But the nice thing about Jupyter notebooks is again, because you have the full power of HTML and JavaScript, we can quickly turn this into something interactive. And so all I have to do is take that same function that I had before, and I put a function decorator on it. And now, if this works, well, it is working, but I didn't fix the scale of my plot. So let me try to do that real quick. So now I have an interactive plot. And all I did was add one line of code. So my original plot, if I just call the function, I get this. But if I just add this one line of code, I can make an interactive plot. And this is really useful. I mean, this is what we mean by exploratory data analysis. Because a lot of times we have models or we have things that have parameters. And we're trying to understand, you know, if we change a parameter, how does that affect the solution? Right. And in this case, you know, it's just a sine wave. Most of us, you know, that have had any trigonometry know that if we're multiplying that, it just changes the amplitude of the sine wave. So we're not learning a lot in this example. But the thing is, this function could be as complicated as you can imagine. Right. It could be the solution of a differential equation. It could be the solution of a system of a differential equations that has 100 unknowns. And you're trying to understand the effect of one input. And you could actually build up these kind of interactive exploratory tools, just like this. And it's literally that simple one line of code. That you know, one line of code over what you would have really written. So another great thing about these jubber notebooks. Now, I will say, well, most of the features of jubber notebooks are kernel agnostic, meaning the the markdown aspect. And the way. Codes. Cells execute this kind of thing is works for any no matter the the kernel. This interact feature is specific to Python. Okay. So don't don't try to write some Julia code and, you know, wrap this interact around it and expect to get this result. It's not going to work. So this is a Python specific feature. But nevertheless, it does work really well. And hopefully, you know, demonstrate some of the things you can do inside jubber notebooks. Yeah. And John, one thing I'd add to that is, it's kind of a native Excel user. What's cool about these jubber notebooks is, because you're able to format text nicely, because you're able to have calculations being done, all in one spot, you can document a full workflow like you saw with Dr. Purchase, you can see that you have a lot of different classes, well, pretty detailed and describe what function is doing what, so that folks can see it pretty readily. And it's a finished work product, right? I mean, maybe you'll talk about your textbook, too. You can do a lot with just these notebooks as kind of an interactive environment that would that go way beyond just using Excel and PowerPoint, you know, hand in hand. Yeah. So, I was hoping to have it ready for this class. It's not currently ready, but I've already shown you, that I have the jubber notebooks that I've re-styled as slides, right? So, we've already seen like this, right? Well, I, I've been working on something, and I think this is a private repository, so you won't have access to it, but I've been working on a book actually that encompasses all of the data and content. And so, let me just show you here. So, so yeah, this is a private repository. If you go to GitHub, you won't see it, but nevertheless, you can see my screen now. And this is just a contents of a book. And in particular, if you look in the content, then there's actually all these Python, you know, the jubber notebooks. The file extension for a jubber notebook is i pi n b. And again, that, that's a historical thing. So, if you recall, you know, I mentioned before that they were originally called iPython notebooks. So, one thing really nice about if you store your iPython notebooks on GitHub, that you can actually automatically render them in a human readable way. So, if you just click on one of these and you look at them on GitHub, you'll automatically render them in a static HTML. So, they'll look just like you're viewing them when you view them. So, this is all. This is all the same kind of stuff. It's, you know, just text some equations and whatnot. And in this case, it actually has some, some interactivity. I'll come back to that. However, what you see here on GitHub is it's just a static HTML representation. So, this code is not actually executable. We can just read it. We can't do anything with it. With this same notebook and just a little bit of additional metadata. I've been able to take these notebooks that I've prepared for, you know, our course and other courses. And I've. Basically, use that as the underlying data that creates this nice web based book. And so, if we look at this is the same section. This is the same section that's contained in this file right here. Now it's been re styled to be. Oh, I don't actually have a, let me, let me go find a. One that we're actually using. So. So, look at this one. Oh, that's, that's one of the ones that's broken. Let's look at this one. So, this is a notebook that we'll be looking at in a couple of days to teach from. And you'll see me teach from it in a couple of days. The role notebook is here, but what I've been able to do. Again, I don't would just basically some, some metadata. Then I can re style that into this really nice web page. So, it's the same content that's just been re styled in a way that makes it very nice and readable. And in this case, it, you know, if you're familiar with the. Gosh, my mind's blank. What's the guy's name? Tuft. Sorry. Edward Tuft is a, is a famous data scientist and visualization guru. And he's really famous for really railing on if you just want to laugh and you hate PowerPoint, go read all of your digs, he talks about like he, he just despises PowerPoint like no one on earth. But, but offers like real criticism of what a really, you know, how badly we use misused PowerPoint, right? Because there's kind of this company culture in a lot of places now where PowerPoint is a reporting system, but it makes for just flat out awful reports. And, and if you try to, if you try to give a presentation from your PowerPoint report, that is one with lots of text on it and other things like that. Well, then it also makes for very, very full poor presentation, right? If you never go watch a TED talk, you don't, you don't see slides full of text like our modern corporate PowerPoints, of course. It's because we're trying to make reports stand alone reports. So these, these pop outs are, are styling of Edward Tuft. And again, is this, this visualization data communication guru and he, and he loves these little pop outs. And if you look at, so for example, this line right here, this highly publicized first image of black hole reproducing that plot line. It appears here. Now, the rendered version on GitHub doesn't show the image, but automatically there's some metadata associated with this line that when I re-style it into the book. Automatically creates this little pop out out here. And you get this nice, really nice human readable thing. And again, something, I was hoping to deploy for this class, but we're not quite ready yet. This will be available to you guys in the future as soon as I can get it all together. But this is a much, much, you know, easily more easily read than, than the other things. Another nice thing about this is I can actually once, once you have a log, once this is available, once you have a log in for any one of these that have a notebook underlying notebook. If you just click on this classroom.datum.io, it automatically brings that notebook into the classroom. Where it is then executable. So now this is the same notebook. It's been automatically deployed into the classroom. It looks a little different, but it's executable the same way. So those are code cells that I can execute and I can change. So again, underlying all of this is Jupyter notebooks, right? So just this ability. And it's the ability to have this nice sort of self documenting code, if you will. And you can take it a lot further to make these beautiful interactive slides, books, anything else. You can also, as you might have seen, you can export it as say a PDF. And so if I export that as a PDF and then open it. Can you guys all see that? So it took my notebook and it turned it into a PDF. I mean, again, obviously it's a PDF. So it's static. You can't execute it. But all the equations look beautiful. The rich text looks beautiful. The code is readable. It's easy to tell what's code and what's not. The interactive stuff of course is not there. Right. The output from the code. And there's all kinds of tips and tricks you can do to sort of control what gets exported. Like for example, if we have this interactive piece in here, we know that's not going to export to a PDF. We could just tag this cell with some metadata and the, the, and have the exporter ignore that cell, for example. So this is a lot of what I do in that book representation. There's kind of a lot of little tricks where I'm tagging different cells to control how they look given the export format. But you can see this is a really nice looking. I mean, at least the equations and the rich text there looks really beautiful. And of course you can intervene your code. So this is just, these are, this is a multi way multitude of ways to share information with Jupyter notebooks. And then on GitHub and automatically render in the static HTML, you can export them to slides is a lot demonstrated to an executable script to mark down to detect HTML to another format rich tech a markup format called asky doc. So there's a lot of ways to share your results when you're working with Jupyter notebooks.
 Why I'm doing this is I want to give you communication tools. I want to give you concepts that you can use to communicate to management and to other working professionals. Anybody ever heard of the 4th Paradigm of Scientific Discovery? Has anyone ever welcomed you to the 4th Paradigm of Scientific Discovery? If not, let me be the first to welcome you to the 4th Paradigm of Scientific Discovery. It turns out there's been 4 paradigms of scientific discovery. The first one was empirical science, observation based science. You know what's interesting? You know the Egyptians were doing this? Now I don't like, I'm glad I was not around for some of their medical experiments. There were some pretty crazy stuff going on back then. But they were trying to do things with the natural world and observe the response and that's how we were able to do incredible engineering feats. Okay, the second paradigm was theoretical science. Try to find out the classical laws, the relationships, the analytical expressions for things, right? And so that was the second paradigm, you know, kind of this renaissance and discovery and so forth. The third paradigm was computational science. That's when we realized that our reality is heterogeneous. Our reality does not fit the analytical expression. It was better to simulate the heterogeneous system. Okay, so we had these continuum models, computational dynamics and so forth. The computational power forced them. They allowed us to do it. The fourth paradigm is data driven science. This is where we take the data itself and this is a world with advanced machine learning and data analytics methodologies. A world with amazing data sets and a world where not necessarily could we ever figure out the physics. There's no physics behind your preference when it comes to Amazon and what you're going to buy next. I'm sorry, some of that stuff is just data driven. And so we got to this now this fourth paradigm of scientific discovery. Now, if you find that Chevron is trying to do a bunch of digitalization, you're not alone. Every single company and I said, I thought I think 20 different places or something last year was crazy. There's a lot of different companies. Every single one of the companies I visit is working on digital transformations. In fact, the Lloyd did a study of all of the different sectors of our economy and what they did was they rank them by readiness. Are they high maturity, the lower maturity? It turns out that energy was about in the happy medium. We were in the middle. In most of the time, we're not at the trailing edge and we're not at the cutting edge. Okay. And so tech was definitely on the more cutting edge while life sciences and healthcare was on the trip. We're finding that out now. They're talking about some of the antiquated systems they use in the health sector, right? Okay. Now, I have some biases and I should be honest about them. I was on I've been on panels and I discussed with experts and I have my biases, I share. And so for instance, I think that with digitalization, there's an opportunity for us to do more with our data. There's opportunities to teach data analytics and statistics, machine learning methodologies to engineers and geoscientists. Now, when I was on this panel, the other people in the panel, some of them were more tech people and I'm not going to like identify the person or anything like that. And they were suggesting that we should kind of hire a lot of data scientists and really kind of get away from as much geoscience and engineering as we can. And I stood up on the panel and a bunch of executives there from all and gas companies and I said, that's the wrong answer. I said, we do our innovation is geoscience and engineering driven and we do a very good job if we teach our geoscientists and engineers how to do data driven methods. In fact, they do a great job at that. And so I think we need to integrate all paradigms, new tools, add value to old tools. In other words, we augment the new scientific paradigm of data driven on top of all the other science. When we discovered the theoretical, we did not stop the observations. When we discovered computational science, we did not throw the theoretical away. And so we're building on top of our toolbox for scientific discovery. Data driven science needs data. Data preparation remains essential 80% of any subsurface study is going to be data preparation interpretation. We continue to face massive challenges with data. Anybody have problems with data, data curation, where's the data? Can't find the data. Who did that interpretation? How did they do that interpretation? Which one of these interpretations should I use? Does that ever happen? I think that happens all over the place. And don't be ashamed. That happens all over our business. Large volumes of data, large volumes of metadata, which were unique for that. A variety of data scales, collection, techniques, interpretations. We have so much going on transmission controls and security. You ever had data we're not allowed to have the data? You have to go to the country. That happens too. Right? You're not going to go to the country to look at the data. I can't leave. Clean databases. Our prerequisite for all data analytics and machine learning. We must start with this foundation, garbage in, garbage out. Often we'll talk about having a well-formatic clean data table. We're talking about the features and all of the samples nicely dealt with. I want this to be something you can use as a tool going forward. From now on, energy is unique. It's different and it needs unique solutions in data driven. Now, we have a sparse, uncertain data complicated heterogeneous open earth systems. Would you believe the people who do Google Maps, people who do Amazon recommender engines? They have all the data. They see all of the phones. They see all of the people clicking every click. They have exhaustive data. We don't have that. High degree of necessary geoscience and engineering, interpretation and physics. We know a lot about the subsurface and we can use that information to add value. We have expensive high value decisions that must be supported. How much does it cost? Can you tell me, roughly, to drill a well in deep water Gulf of Mexico right now? What do you think? Anybody working the Gulf of Mexico? How much does it cost to drill a new well in the Permian Basin? Now, let's compare and contrast that assessment. In Gulf of Mexico, when I was working that back during the high, it was about $150 million with a production test to punch a hole in the Gulf of Mexico into the Wilcox. It was incredible. Those are expensive decisions. High value decisions that must be supported. Anybody hear you Spotify? You know Spotify has a recommender engine. This is the recommender engine from the summer of 2019. This is sharing some information about me. I'm Canadian. I listened to a bunch of Canadian music. Would you believe every once in a while Spotify recommends and plays nickel back? Now, I'm just saying, on behalf of Canada, no Canadians like Nickelback anymore. I don't like Nickelback, but by guilt by association too many Canadian bands Nickelback is going to show up. I promise you that that's the Spotify recommender engine getting it completely wrong. What is getting it completely wrong when we're drilling wells? We could drill a dry hole. The cost of getting it wrong with a Spotify recommender engine is what? What's the cost? One song. What am I going to do? I start tapping my foot, I bob my head, we all do. You got to admit it. You do Nickelback. It does. It catches you. It's very good at that. And then you have that realization, oh, it's Nickelback and you fast forward it. The cost is zero. You fast forward it. I don't cancel my app. I don't go on yelp and make an ugly review because the value of that decision is zero. It makes no sense for human intervention. If I had my own personal DJ, I don't know, my a millionaire at that point, that's silly, right? So there's no reason to have human intervention there. We got to be a critical user consumer. A lot of this technology was developed for these low value exhaustively sampled low physics problems. And that's not what we work in. Remember this concept. Don't jump to complexity. Excuse the font here, but remember model variance plus model bias plus irreducible error gives us the actual error in the performance of our machine. The curve looks like this often. It looks like this. A lower complexity model often outperforms a high complexly model. And in fact, a low complexity machine is easier to interpret. So you guys like seriously, I see this all the time. Many people will be like, Hey, let's jump to deep convolutional generative adversarial networks. No, don't you might find that you can actually explain the problem much better if a simpler method. Actually, Bruce Power, one of my mentors working in ETC, if you know Bruce Power, his comment was Michael. There's many complicated ways we can tell ourselves what we already know. Don't jump to complexity. Okay, anybody ever seen this example right here? The wolves and the dogs. Anyone seen it? Well, it's a great day. I get to share this wonderful example with you. It is a good case study on interpretability. Okay, so here's my problem with machines that are very complicated. Interpreteability may be very low. In fact, it'll be very hard to understand what the machine is doing. The application of the machine as a standard workflow may become routine and trusted. Anybody ever worked on a project where there's an established standard workflow? Have you ever violated the workflow? Done something different? Bifurcated from the workflow and then presented it in front of say the RAM team or in front of some type of review process or tea or whatever it might be. Are there questions? Do you have some explaining to do? Why didn't you use the standard workflow? Right, Megan? Exactly. And so this is my problem. If the machine becomes routine, it becomes trusted and it becomes why didn't you use the machine? Okay, now if the machine is trusted, but its interpretability is low, it becomes an unquestioned authority. And that is very dangerous, my friends. That is very dangerous indeed. Okay, so our friends, there was a study done by Roberto some years ago and all, I think 2016, machine learning everything's kind of newer, right? And what he did was they took a bunch of pictures of wolves and dogs and they trained a logistic regression methodology to basically be able to give you a probability of wolf and dog from a picture of the animal. And what was very interesting was he trained the machine up and it worked very well, high accuracy, but then he gave it this picture. And it came back as 99% or high probability of wolf. That's a wolf. Okay, now anybody here a dog person? Is that a wolf? Anybody here think that's a wolf? That's a husky, right? Is that a husky? Okay, so these huskies, right? Not a wolf. And so the scientists, the data scientists looked at the picture and they're like, what is it about this that looks like a wolf? Is it the snout, the eyes, the ears or what? They went back to their machine and they said, tell me the pixels that gave it a high probability of wolf. And this is what came back. Why did it look like a wolf? The dog was standing in snow, surrounded by snow and so when they took the pictures and they put the pictures in the machine, I don't know about you guys, but every time you see a wolf, they're standing in snow, they're standing in northern Canada and those scary parts of the Arctic, right? And so the machine had actually not learned to identify dogs and wolves, it had learned to identify snow and no snow. And that's basically what they built. This is the danger of high complex the low interpretability machines. It may look like it performs well with your data, but then when you go to something else, it actually did not learn. You don't know what it learned. Okay, Peter Haas has an excellent TED talk and I recommend it and he actually cites these pictures right here from the Robero study. And he said, even the developers that work with this stuff have no idea what it's doing. Sometimes it's like that. And he also says these systems do not fail gracefully. When they get it wrong, they think it's a wolf when it's a dog. Okay, anybody ever seen the hype cycle? This is the hype cycle from Gardner, a management systems and this is a common thing that occurs with new technology. This is time after discovery. This is expectations. Triggering event goes up. Peak of inflated expectations comes down. Troph of disillusionment, slope of enlightenment and plateau of productivity. Okay, let me ask you. May I ask? I ask every company. I hope it's okay. Not a single company has put themselves here. Would you believe so? You're not alone. Now, I have, and this is how I know I went to a company where they said, yeah, we're around the peak. And then afterwards, I had, I'm not mentioned in the company, the manager took me aside when I visited them and said, Michael, can you help us show some value? We need some good examples to show some value. Where do you think they were? They were starting to come down the slope. And they were needing to show some value because there was a lot of investment going in that kind of thing. So anyway, remember this that we're going to get to a more moderate level. And it's not going to be where we think we are right now. We're going to reach a point where we become more reasonable. A data scientist, if you're a data scientist, you advertise yourself as having domain expertise, statistics and coding. That's the unicorn, the person who kind of has it all. Now, I would argue that many times when you have a data scientist, the domain expertise will be weaker. The coding will be very high and the statistics will be moderate. That's the typical data scientist I would suggest. The coding, the statistics is very strong in the machine learning, but they may not have all the fundamental statistics. And I'm just saying, universally speaking, this is what I think we should do for operational capability. I think we should build on domain expertise. We have excellent geoscience and engineering skill sets. We should continue to build and harness that. We have, we could do more with our statistics and our data. I think we could build up this. And I think we could grow our capability in machine learning, the code being able to build the workflows. I think this is the best way forward for how we can harness the value from these technologies. Statistics to mitigate cognitive biases. There's going to be cognitive biases in everything we do anchoring, recency bias, confirmation bias. There's a common phrase here. I would not have seen it if I hadn't believed it. If you ever worked with Henry Postman here, this is one of his favorite phrases. I wouldn't have seen it if I didn't believe it. Be aware of your biases. Let me just show you really quickly and then we'll move to code. I promise it'll happen. Okay. Which one of these data sets? What is the difference between statistics and data analytics or is there a difference? I would say that there's synonymous. I would go that far. I would say that data analytics is really the same as statistics because any definition I look for is about working with data to support decision-making. And for both of them, they say that data analytics often has that business data analytics which German towards business decisions, but I think applied statistics should do the same. So I really do fill as I said before and I'll say again, if you're good at statistics and you do statistics to get your job done, you do data analytics. Which one of these data sets is random? These are two different points sets. Which one of them is random? Anybody? Who thinks that the left image is random? Okay, left random. Left random. Okay, good. Martin, Ian, Skyler, I saw your hand. Thank you very much. Anyone else? Okay, who thinks the right data set is random? What's really interesting is this data set is random. This data set is actually anti-clustered. It in fact is more regularly spaced than random should be. Naturally and random phenomenon patterns occur. You should train your eye and realize that that random phenomenon often have patterns don't overinterpret them when we build our machines. Remember, anybody work with Morgan Sullivan? I did not put this simply because you were Chevron. I quote Morgan Sullivan in my undergrad, undergrad classes, my graduate classes, and in multiple companies. Fellow now for Spendigraphy, right? And he always said the difference that makes a difference. In other words, no one and pattern that you observe when it really matters. And in this case, these patterns are just random. So we got to train ourselves. Lot of statistics is protecting ourselves from the law or the belief in the law of small numbers. It's the belief that randomly sampled things from the population will always be highly representative. In other words, if I have a bag and I have 10 red balls and five blue balls and I draw from that bag, I'm going to expect to see two thirds of one third in every sample set. But it's not going to happen. You could be very lucky and they could all come out as red. They could all come out as blue. If you do two few samples, now if I do many, many samples, the proportion will be right. It will converge on the right proportion. Remember that. You should expect fluctuations on certain needs due to small data sets. Working in high dimensions, let's not get into this right now. I'll skip that. But what I will say is don't lean on data analytics and machine learning. Do you get the sense I'm being a little negative today? Just a little bit negative. I think it's good to have a critical perspective. I promised that data-driven approaches support traditional hypothesis-driven scientific methods. Muzoki said the following, big data distributed computing and sophisticated data analysis all play crucial role in discovery of the Higgs boson. But the discovery of the Higgs boson was not data-driven. It required scientific innovation. A machine could not have done that. Engineering and geoscience experience remains core. A professor in my department said the following, don't use data analytics, machine learning as a crutch instead of learning and grasping it, the intuition and concepts behind the physical laws of the natural system. Always remember that. Okay, that's the end. I promised this would be a little bit lengthy. Any questions, comments? All of these concepts of it as a function that we're going to fit. Parametric and nonparametric models, the fourth paradigm, welcome to the fourth paradigm, everyone. And don't jump the complexity. The variance bias trade off. Please, when you see someone build a very complicated machine, ask them, did you build a simpler machine? Did you tune the hyperparameter as well? Okay, how's your energy level right now? Are you guys good to get into some Python? Okay, let's cover the very basics. Jupiter, Python, scikit-learn package, the tools. Let's cover the idea of importing a package, loading data, checking and visualizing the data, saving the results back out. What's our motivation here? Well, any machine learning should be driven from expertise on coding, cutting edge, machine learning methods really are available to you in Python. And it's open source. You can just use it, leverage the world's brilliance. I can't learn, I'll become your new friend. It's my new friend. It's beautiful. It's amazing. And so this is a chance for us to kind of build up some ideas about how we work with data within Python with Jupiter notebooks and how we grow our capabilities in coding using our machine learning technologies. John, I think it's really important to learn about what variables, what's a list versus a tuple. All of those basics, what's an NDE array? How do you work with multi-dimensional arrays? What's a pandas data frame? We'll cover that today. Data tables arrays, efficient methods for working with these. One event, one idea would give you is if you're working in Python and you use loops, think you're probably doing something wrong. Most of the time in Python, you don't have the loop. There's going to be broadcast methods so you can do it much more efficiently. Use example, workflows, demonstrations that I provide as a recipe. I'll cover a huge amount of what you need to do to just get started. And there's lots of resources online. Use docs for the packages. Who here has gone to scikit-learn or to pandas or to numpy and looked at their documentation? I'll tell you what, don't be ashamed of that. Anybody know Joseph Hovedick? Or Sir Vitale? These people are experts in coding within Chevron, ETC. And you... Who are flow? These are websites where people ask questions about coding problems. And anyone in the world jumps in. And in fact, you get ratings and you get recognized as an expert by answering more questions and giving good answers. And so it's a way people promote and build their CV. Anybody ever been to a tech or coding meetup? The building actually just down the street from 1500. I've been to some of those tech meetups. They happen almost every second week or so. They're really good data science meetups. So go there. Check it out. Check up you two channels. There's so many things you can do. Minimal workflow overview. Okay, so the basics. If you're designing a workflow, there's a lot of considerations that we need to consider. But we're going to skip them to do a fast start introduction. Resources, goals, the scientific hypotheses, trying to be critical, scientific about everything. We should do that. Evaluate the available resources, problem opportunity statements, research questions. This is fit for purpose modeling. I won't cover that here. Data cleaning and preparation. Is our data clean and ready to go as soon as we collect it? I don't think any of our data is ready to go. There's always some issues with data we need to deal with. Be biasing data imputation. That's where you miss the features. In unconventionals, we often are missing features. We have a well here with some measures. We have a well here with other measures. And we don't have everything everywhere. We have to deal with missing features. Data mining. We won't cover that. Data mining feature selection and engineering. Unfortunately, we don't have time to cover that. That'd be really cool. I'm rigorous model checking and hyperparameter tuning. I'm sorry. Today and tomorrow, we will do hyperparameter tuning by hand. And the reason being is it's really good to get experiential learning. We should automate it and do better things. But we're not going to do that. We'll do that in the next session. Okay, what are we going to do? The fewest essential steps that you need to cover today, import packages. We'll cover that and talk about what a package is, how to import it. We'll get it data like a map, tagular data like well data. We'll talk about getting that loaded in, visualize and do summary statistics, histogram location maps, scatter plots, matrix scatter plots, we'll do that. And then tomorrow, we'll build machines and we'll do instead test machines. We'll show how to do that in Scikit learn. Jupiter notebooks. Remember, it's a beautiful thing. You got workflows in these nice integrated. You got the documentation. You got the code. You got the outputs all together. Very professional. Very powerful. We talked about that. The basic setup. So on the Guil option panel, you guys now have, is it anaconda three? This class, we're going to use Jupiter lab hosted on the cloud. So we're going to jump right into Jupiter and be able to look at it. If you use Jupiter notebook, it's almost the same. There's just some subtle differences. You log in. You're going to work in an environment. It's all set up for you. All the packages you need to work in are in fact installed. Now, all the packages we use today and tomorrow are part of the anaconda base. They're just there when you install anaconda. For the more advanced course, we do artificial neural nets. We use tensor flow with keras front end. And that you actually have to install additional packages. Example data sets that I provide are all synthetic. So you can play with them on your own. There's no permission issues. And those are available to you. Well documented workflows. So you can play around. They're all available for you. If you wanted to install anaconda at home, and I don't know about you, but sometimes I like to play around at home and do technical stuff outside of my work, it's not bad. You can go ahead and install anaconda directly from anaconda.com. And you just install Python 3.7 or whatever the current version is. It might have gone up since I took the screen capture. Okay. I'm explaining a couple of concepts. I'm going to show some concepts with slides, how to install packages and so forth. And then we'll jump into a workflow. I want to jump into the workflow. Let's talk about packages. Python encourages the use of open source. The way it does that is through the use of libraries packages modules. Okay. The whole point is this leveraging some definitions. A library is a collection of packages. Okay. A package is the most common unit that we deal with when it comes to bringing new code and functionality into our program. Scikit learn is in fact, it will include it's a, people will term it as a library because there's a bunch of different tools and methods in it, but it could be also considered a package. We'll commonly import these packages and work with them. And they're in fact, a collection of group of modules. Now, so for instance, within Scikit learn, there'll be a bunch of different modules that allow you to do things like decision trees to do cross validation of models and so forth. Now, my advice to you as we import packages or modules into our code is you import only what you need. Be efficient. If you, first of all, if you import everything, if you tried to import the whole world of brilliance at once, your code will take forever to run. That import statement would take very, very long. The other thing is that you could cause confusion and conflicts. Would you believe in the mass of libraries and packages and modules that people reuse the same names for functionality? And if you imported everything and you run a function like if you're doing some type of a fast Fourier transform, you don't know which package did it. You don't know where it actually came from because people use the same names, FFT or something like that. Okay, slow runs and memory requirements and you could also have crashes, conflicts between packages. That's a cool thing about Anaconda. They test for compatibility and they make sure it all works together. How do you import a packaging code? Super easy. Import name of package. That's all. Now, common thing to do would be import the numpy package and you could say, you give it a name that you want to use. You would say as np and this is very common. And so from now on in the code, you can type np.and access numpy functionality. Import pandas as pd. Everybody does that. And import matplotlib.pyplot which is all the plotting functionality based on the same as MATLAB available to you within Python as PLT. So from now on, if I type PLT. I can make a histogram, a scatterplot. I can do all the stuff you would have done in MATLAB. Okay, so the general use will be I will do an import package name. And then when I want to access the function, I'll say package name function. Import package name. And then I could say specifically from package import a certain module. And then I in my code, I would give the module name and the function. And I could do import a package as a new name and then just give the new name and the function dot the function. And I can access the function. Tabular data. We all work with tabular data. Well data, soil samples, pseometers, whatever it might be is all tabular data. We have samples in space with multiple features. So the samples are going to be the rows of the data table and the columns are going to be all of the features. In this case, index x, y, the faceys, porosity, permeability, acoustic impedance. This is a map, two dimensional map data. Okay, now if you work with comma delimited files, it's generally the raw file looks like this. It's going to be the value for the first feature, comma, the value of the next feature, comma. And these will be multiple samples going down the rows. Now what's interesting about comma delimited files that are structured like this, if you double click them and you have excel on your computer, by default, excel will open them up. And they'll open it up as a table, which is very convenient if you want to edit or work in excel. We all know excel excels very common. Okay, comma delimited files. We're going to use comma delimited files within our for our tabular data. How do we load tabular data? What we're going to load it into a pandas data frame. Data frames are the data tables most commonly used within Python. Who here has used pandas data frames before? Anybody use them? Megan's been using them. Anybody else? Nicholas, you've been using them, but many of you have not been using them. Okay, now here's the idea. The data table class, it has a lot of built-in functionality and it's very efficient. Underneath the hood is fast, optimized code for data checking, summary statistics, visualization, data manipulation, management, sorting, searching, querying, and so forth are all built right into a pandas data frame. And that's why we use it. It's very easy to do things with your data in pandas data frames. Okay, so one of the first things we could do when we're doing a workflow is we could set the current working directory. When you open up a Jupyter notebook, the default is the directory that you launch from, but you might want to set the directory to be somewhere else. This is how you would do it. You would use the OS package, which allows you to interact with the operating system, and you could do a change directory command and you could say, let's go to my D drive to a certain folder on my D drive. Now what you could do is you could do a PD, which was, wait, what did PD stand for? When we did an import on the slide before. Okay, so we did an import pandas as PD. Now we do PD, we can access pandas functionality. And there's a built-in function called read CSV, the name of the CSV file, and it'll read it directly from the current directory. And it creates this object right here. That object is a data frame, I call df for data frame. That's my data table. So that's all I have to do to load in my data. Now once we've done that, we have all kinds of functionality built in. We have date df head would be a function with parentheses. It tells me it's a function. And so that function will take the first six samples and it will show me a nice display so I can preview. Now you can put a parameter in, you can take more than six samples. What do we notice about this? The first observation is you see the 0, 1, 2, 3, 4. It's the first five samples. I'm sorry, I said six. You notice how it goes 0 from 4? Python is indexed from 0 to n minus 1 for everything. Okay, some languages go from 1 to n. Pandas goes from 0 to n minus 1. Just remember that for everything we do. For Python. Okay, any questions about working with tabular data loading it in. The reason I like comma delimited personally is because the delimiter is explicit. You can see it. I have run into plenty of problems working with tab delimited data where I think that's a tab, but spaces. You see what I mean? And you can cause mix-ups or so forth. So I like common delimited. Now, please, let me, anyone working with really large data sets, I don't recommend asky-based common delimited. We should be working with binary and more efficient type of formats. So I just, but for basics here, we're working with these simple common delimited. Okay, any other questions or comments? Now, what if we have missing information in the sheet? Whoa, what do we do then? Okay, the main thing is this. If you have comma delimited, you'd want to make sure that you have coded missing data or you know, that you still have a space to indicate that something's missing. Now, the great thing about pandas data frames is that it's data science and they expect missing data. Missing data happens all the time. So all the built-in functionality actually can account for missing data. So there's tons of methodologies to do summary statistics and so forth that will extract the missing data first. Okay, now if the question is more about how do we do data and mutation, that's a whole different challenge. How do we re-estimate missing data, which is often important? We want of time to cover that today. Okay, good. Pandas. Now, the great thing about pandas, I mentioned there's a lot of built-in functionality for data checking and that includes summary statistics. If I load in my data frame, I can do a DF describe and I can actually directly calculate a bunch of the summary statistics I want to work with. There's so many methodologies in Python to quickly do summary statistics and to understand our data. And we can check for data issues. We have negative values. What's the min and the max? What's the P90? You can actually change this so you can display P10 P90 and so forth. Okay, we'll cover that. Gritted data. What's an example of gridded data? Well, maps, seismic volumes. Those are gridded data. The gridded data we're talking about here is regularly spaced gridded data. Okay, so this is not unstructured gritting right here. If we want to work with gridded data, we can also have a comma delimited file with gridded data. Now, one way to do it would be like I'm showing an excel where the columns are all of the x cells and the rows are all of the y cells. And so this is really a map with equally spacing on the samples or the samples are representative of cells with a certain scale that's uniform across the entire map. Okay, so you could imagine if you were to work with acoustic impedance, it could be stored in a, you know, if it was a map for a unit of acoustic impedance, it could be like a 100 by 100 mesh of equally spaced values. It could be something like that. Okay, so this would be a case of gridded data. Now, of course, we have other formats for gridded data. Now, how do we deal with gridded data on Python? We're going to use numpy. Numpy package has a class known as ND array. ND array is a multi-dimensional array. Super powerful, super efficient, really fast. Okay, so it's really great for loading the data, for visualizing the data, for being able to do checks, summary statistics, data manipulation. You see I'm starting to sound like broken record. The same things with pandas data frames for tabular data is the same thing for working with gridded data and numpy ND arrays. Okay, so we could set our working directory, make sure we're loading the data from the right place. You'll see we'll do more complicated things with data engineering and with feature engineering and loading with intake and such. I'll show that to you in the code. But if you want to do this on your local drive, you just set the working directory, load the data. NP stood for when we load our packages. You remember, we're going to load using numpy NP for us as numpy load text and we say it's a comma delimited. We can set the delimiter as a input parameter for this function. We give the name of the file and seismic is now a object. It's an two-dimensional ND array. And so it's an ND array. It's going to be zero indexed. It's going to go from zero to NY minus one zero to NX minus one. Now, if you ever tried to display your data and you tried to actually work with your grid data, I want to warn you of something. When we work in Earth Sciences, where usually do we have the origin for our map? Is it here? Where's the zero zero index here? Here, here or here? In GoCAD, in Petrell, all of our geoscience software. Where do we put zero, zero, zero? So it's going to be right here. In numpy, the zero, zero is here. Okay, so I warn you about this because I got to tell you your models get flipped upside down, your maps are upside down, you might not even notice you're looking in the wrong spot. So let's be careful about that. It indexes a little bit unusual. Okay, the whole thing too is that a object like a two-dimensional array will automatically have built-in functionality and built-in variables. And one of the built-in variables is shape. You notice there's no parentheses. It's just a variable, not a function. And when I type that, it gives me what's known as a two-pole. A two-pole is going to be like a list of values, but you're not allowed to edit. You can't change them. Okay, so 100, 100, it's telling me that it's 100 Y and Y number Y cells, number of X cells 100. So I would expect this to go from zero to 99, zero to 99 in Y and X. Just be careful. Y is here, X is here, and if you have a three-dimensional Z is here. So this may make you up to, anytime when we work with arrays and other types of context, we expect this to be XYZ is YXZ. Okay, just a reminder of that. Okay, visualization with map plot live, we're able to do histograms, we can do location maps, we can do pixel plots, so our tabular data will do location maps. Our exhaustive gridded data will do pixel plots, and so we can do all of these. I want to walk through these types of displays. Okay, I'm going to cover how do we save and visualize data in more detail. I want to jump into code right now. Let's play with code packages. So what do we take away? Our takeaways from this section, this discussion, packages, there's many open source packages available on Python. We import and utilize a variety of packages. Don't load them all. Load the ones you need for your workflow. Data types, while we have tabular and graded data, that we're going to work with, we have numpy for gridded data pandas, data frames for tabular data, loading data, there's tons of functionality to load in many different formats. Actually, what's really funny is Dr. Foster is teaching a very short half day course on Python for, I think he called it as Excel addicts. So those people can't quite quit the Excel addiction. I do it too. I'm addicted to it. It's okay. I'm on Michael Perch. I'm an Excel addict. And I'll tell you, there are lots of functionality built into pandas to be able to interact with Excel.
 So this is our first time together opening a workflow. And just by show of hands, how many time, how many of you is this the first time this is the first time you've ever opened up a Jupyter notebook? This is truly a great day. This is truly a great day that I get to share this experience and discovery if you guys this is so wonderful. Okay, so let's go ahead and look at what this is. Okay, so first of all, just for fun, you can take any one of these cells and you can double click it. Can you guys do that with me? Double click the cell. And if you do that, what you're actually seeing is you're seeing the mark and down code. If you hit the run arrow up here, it will recompile the cell. If I double click down here, you can see exactly what I wrote and mark down to make this documentation. It's not a big deal at all. We can easily put equations in there using the dollar signs. We can put links in it. We can make it very powerful. It can be very good documentation. We can do all the formatting. Okay, so so far that's mark down. I also introduced the idea of the run arrow. I like to use the run arrow. There's lots we can do that we can actually go up here and we can go to like a run and we can run all of the cells, some of the cells. We can run up to the active cell. You don't have to get an RSI. You don't have to run, run, run everything, but we're going to do it like that so we step through things. Okay, now the next thing is if you ever run code, what you'll notice is that this do you see this circle right here? When you run code, it will go solid meaning it's running, the kernel's running. Also down here, it says idle, the Python kernel is idle. When it's running, it'll tell you it's running. Now, if it hangs and it takes too long, you ever done programming and you've run something and you not sure if it's just stuck, you always have this square right here. You can just exit. So if you really get stuck, you can exit. You have a, you can pull the rip cord and escape. Okay, you can do that. Okay, now this is our first block of code. Now I said these are well documented and I didn't joke around. There's no joke about that because it actually says this is we're loading the required libraries. Now all of these libraries come in standard anaconda. So if you open up a Jupyter notebook and you do this, I guarantee you money back guarantee that this is going to go ahead and run. And so now I want to tell you something. When you load the sheet for the very first time, nothing had run. So if I do not run this block of code and I go down here and start running this code, I'm going to in fact get errors because the fact that I have not imported the, the, the actual packages, not until I run this code. So click in this box, the box is active and hit the run error. I ran it to simply demonstrate the idea of writing markdown and what it is. Okay, so we run this right here. We've now imported now. I hope some of you feel more intelligent because you've just imported the world of brilliance into your workflow. In fact, if you guys see in the matrix, this is kind of like that whole jacking like where they put the thing in and they can teach you out of fly helicopter, that just happened. Your code now has access to the world's best stuff in working with gridded and data tables and visualization. Okay, so we ran that. Now what we're going to do is we want to go ahead and read our data in. Now if we do that, we can run this command right here, show you ever seen a cooking show where it's like I have a pie. I have the ingredients making a pie. Let me put the pie in the oven. Oh, here's the pie already cooked. You've seen now the cooking show where it's already cooked. Kind of like just jump ahead. Here's the product. Okay, so I'll tell you what, let's not do it like that. Let's go ahead and reset the kernel. Come up here, kernel, restart kernel and clear all outputs. Okay, do you want to do that? If you do that, nothing's been run, nothing is shown. There's no output and then we'll know we ran it because we'll see the output appear and we'll know that it ran. So let's go ahead and restart the kernel. Okay, now if you look through here, there's nothing, there's no outputs. Everything disappeared. That was an output. Okay, restart kernel and clear all outputs will get the sheet in a native form without anything being run. Okay, so let's go ahead and we can run this walk again. And now we've imported those packages. We're more intelligent. We're ready to go. Now the question was, how do we know we ran it? This is interesting because I always face that problem of, did it really run like anybody here a little suspicious or critical when they do things? We should be, right? Okay, so if you have that feeling, okay, so have you guys ever heard the old adage? No news is good news. Okay, so let's go ahead. What would happen if I said I want to import a package called num pies, which doesn't exist? Okay, let me go ahead and run that. Do you see a gun error? Error is there's no num pies package. So that's the first way you know it ran was you don't get an error. So when I run this line of code and nothing happens and this circle is empty and the processor is idle, the kernel is idle, it ran okay, it's done. The second thing you can do, if you want to confirm that it ran okay, you see the plus symbol up here? You can go ahead and you can type that in and you can just simply say something like you could say numpy.0's 100. Okay, numpy has a built-in function called zeros where you can create an one-dimensional array with 100 zeros. It's a 1D array with 100 zeros and so the fact that I could run that code tells me that numpy was important properly. You see that? So you can't always test like that if you wanted to, you could do a little check. Now I added in that cell, I can do the cut command, be careful with the cut command. You could cut if you need. If you ever do that, you can go edit and undo the cut, but you can do them. Okay, so the question was how do you know the code ran? You can test by actually trying to access some of the functionality you're looking at the outputs. Any other questions? Michael, in the top right, that circle goes black right when it processes as well. Exactly, thank you very much, kernel. The circle's black while it's processing and the down here will indicate that it's in the middle of processing. Okay, thank you very much for that. Okay, so let's go ahead. We're going to read in some data. Now I want to show you something about errors in Jupyter notebooks. So we're going to do pd read CSV. You remember that command? It's going to load a comma delimited file from the current working directory and it's going to load it in as a data frame. Okay, so now we're working in this environment. We already have the working directory set at home. You might explicitly want to set your working directory. We're going to load in a file with the wrong name. Can you run that line of code? Okay, look at that. You guys see that? You guys get the same problem? Okay, so what happened here? We created an error. We used the wrong name for the data file and now we got the... Now I want to kind of teach you something that's really important. Jupyter gives you that are in fact known as an error trace. Have you ever heard the term error trace? What that means is it gives you at the very beginning the line of code at which the error happened in your code. Okay, so the line... The error right here was on line number one from this code right there. But then what it does, it jumps into pandas and it tracks all the way through pandas. Track, track, track through all the pandas functionality to the ultimate location where the system messed up, where it actually got the error. And then at the very end it gives you the actual error. File wrong name does not exist. Okay, so at the very beginning it tells you where in your code it messed up, then it traces through all of the open source to where the error occurred and it tells you what the error is. So here's your lesson. If you have an error like this and it's long and ugly, go to the first line, find out where it happened in your code. Go to the last line, find out what the error was. The problem is, or not the problem, this is a feature, is Python Jupyter is assuming that you're a programmer who's developing open source. They assume you want to know exactly when it went wrong and where it happened. Let's go ahead and correct this error right here. We're going to load the proper data file. Okay, here we go. Can you please run this line of code right here? And we load our data file in. So Panas has read the common delimited file into a data frame. This is the data frame right here. How do we work? Anybody feel kind of suspicious? Well, the first thing is we ran a ran. You could see the circle up top. If I run it again, oh, it's so quick. You can't even see the circle. You didn't get any errors in it. How do we know it worked? One of the best things we can do is we can use the built-in functionality like the head command to look at the first couple of samples. So let's go ahead and run that line. If you run that line, you're going to see, in fact, the data table, the data frame first zero through four samples, the first five samples. Did you guys run that? Did that work for you? All right, awesome. Michael, I appreciate the nodding. Thank you very much for the interaction. Martin, I see your check mark. Thank you for that Martin. Appreciate that. Okay. Now, let me teach you one more thing that's really important about functions working with our packages. Okay, you see this function right here called head. The reason I know it's a function is because it has the parentheses, meaning it takes parameters, input parameters that can go into this function. Now, can everyone do me a favor and type, shift, hit tab, put your cursor in the parentheses and do that. Did everybody do that? Okay, this is known as doc strings. The package has been coded properly will include built-in doc strings. The doc strings will tell you the parameters that are expected for the function. In this case, it's n where n is the number of samples to show and equal to five tells you the default parameter for this command. If you don't say anything, it assumes n equals five. Then what it should have, it explains what the function does. Then it explains what the parameters are. It tells you what's being returned. This is what you'll generally see in an doc string. Okay, so we now know that there's an n. So you could type in n equals and my favorite number is 13. It's very unlucky. I like that number. So I could type in n equals 13 and rerun the command and look, I see the first 12, well, 0 through 12 for apples of this data table. Okay, so are there any questions about the idea of doc strings and the parameters for the function and defaults? Are we good? Okay, good. Now, built-in to data frames are also summary statistics. And so you can run that describe function. So let's go ahead and run that function describe. So what describe is going to do is it's going to give us all of the summary statistics for all of the features x, y, faceys, porosity, permeability, acoustic impedance that count. Now, somebody asked about missing values. If you have null values, not a number of values, n numbers, missing values, what will happen is the count will tell you that you don't have a full count. There's 480 rows in the table, but it's actually telling you the number of samples you really have. So it would be less than 480. So that's pretty convenient. Okay, now I want to show you something else. The output from this command, in fact, is a data table. And so when you have that command that creates a data table, you can do this. All right, so what I can do is I can chain another command. The output from this is a data table. So I can do a transpose command on top of that. So I can chain together my commands and Python. Again, output, apply another function to it. I get a new output. And you could keep going. You can, in fact, make a very long command that does everything you want. It could be very compact. So if I run that, look what I did. Do you see difference? What's different? Okay, so basically that was transpose. So now the rows are the columns, columns are the rows, and you might like to visualize what the features on the rows. So you can chain together commands and all of them have their parameters and so forth. Okay. Now if you wanted to access certain certain components of a data table, and I will not spend time on this, this will be Dr. Foster in this data science course. You can slice up a data frame and you can actually grab a certain feature and you can look at the very first 13 samples by using command like this. This effectively what we're doing is we're slicing the data table. We're actually observing the very first zero through 12 samples of porosity. So you can access parts of a data table. You can slice it. You can query it. You can sort it. I'm not going to cover that, but there's so much we can do with our data frames to manipulate and visualize and understand our data tables. Okay. Now what we'll do is we'll do a little bit of visualization based on the data frame. And so what we can do is at any point, we can do something like creating a histogram. And so this line of code PLT, what did PLT stand for? You guys remember PLT? Exactly. It's our plotting functionality from MAT plot live. MAT plot live, pi plot, Martin, that's precision. That's an A plus on that answer. Yeah, you nailed it. Okay. So that's exactly it. We said import MAT plot live package, this specific module of pi plot as PLT. So now we can access all of this MAT lab type. Now this data frame with the name of the feature is going to pass a one dimensional array with only that feature. Now you don't have to believe me. You don't have to trust anything I say because you can just do this. You can go data frame, try this out. Pyrosity like that. Try typing in that command. Just create a brand new block with the plus symbol right here and then type that in right there. Try that out for fun. Specifically, if you haven't had experience with data frames. Okay. Now run that line of code. What do you get? Anybody done it? You actually get all of the prosody data from the data to frame as a one dimensional something. Now what is that? Anybody know what this is? It's an array, one dimensional array and it has a name. Do you know what that's called in Python? It's a named array. Anybody know? Guess what you can do? You can do a type command on that. What is it? It's the fact that it's in fact a named. It's got the name too. So that's a series. In fact, many times you can use a series for a list or one D array. So all of them are usually compatible. It can be compatible. So you can see the type is a series. Now sometimes the series is just going to be a one dimensional ND array with a name added in. Now there are times where you want to make sure that you're actually working with just the you want to remove the name and have just a one dimensional array. You could do that. And if you just do this extension right here, you're accessing just the one dimensional array and you're not working with the series anymore. So you could do that too. So there's many ways that we can work with data with data frames. If I want to make a histogram, this command is just passing all of the prosody data to the histogram function. Okay, that's what it is. It's a series. And if you ever wonder about exactly what is that thing? What did I just make? Use the type command to check it out all the time. Okay, that's enough talking about that. Let's go ahead and make a histogram. Ta-da. Are you guys able to make that histogram? There are so many things we can do with histograms. You see right here, we give the range. We tell it where to start, where to end. If you wanted to, you could change that to 0.3. You can change the number of bins. We said we want 10 bins. You could use 30 bins and you could rerun it. You see, there's so many things we can do, the work with the histograms. We might decide that we want the alpha level to be like 0.9. We don't want it to be so transparent. We rerun it and now it's red. The alpha level is the transparency of the bars. You can plot multiple bars on top of each other and have them semi-transparent so you can see multiple data sets at once. You have all of the labels. You can do all kinds of labeling. You can show it. Any questions by histograms? Super easy, really. Really cool. You can do so many things with making them. You can remove the edge color, whatever you want to do with histograms. Okay, histograms. Another thing we want to be able to do is look at location maps. A location map will use the plot scatter functionality x, the x coordinate, y, the y coordinate. s is the size of the marker. We're going to have a marker on a location plot. You can make them. Anybody here ever use spot fire? Who used a spot fire? Anybody does scatter plots in spot fire where you can change the size of the dots? That's pretty powerful. That helps you visualize multi-dimensions. You can visualize x, y, as two variables. The color as another variable. The size of the dot as another variable. You can even have different shapes as another categorical variable or something like that. You can do a bunch with spot fire is very powerful. You can do the same thing here. Much of the same thing here. You can also control the alpha, the transparency. The minimum maximum of the variable so you can set the color bar to be appropriate. If you're a porosity between 0 and 30% or something like that, the line widths on the edge colors and all of that. Okay, so we can go ahead and we can run that function. Now when you build a plot, it's building block based like PLTs or building blocks base. So basically you built the scatter plot, but then you can add to it. You add titles. You can reset the x and y limits. You can add labels. You can set a color bar. You can use a certain color scheme on the color bar and then you can show it. So whenever you do map plot live, please think of the initial inciting incident of your story is going to be making the plot. Then you have all of these use PLT to modify the plot. And then when you're done modifying the plot, you show the plot and that closes the plot. Now you have the plot. Okay, that's the way we're going to plot and visualize data. Now anybody here have a publisher paper, peer review publications, APG Bulletin, you know, interpretation, something like that, right? SB, okay, everyone, a lot of people have you ever had issues with the plots not having sufficient resolution? Have you ever sent a plot and the editor comes back and says we need these at 600 dots per inch. Anybody run into that? Guess what? Python's your new best friend. Because when you create these plots, you can save the plot to any file format and you can control explicitly the dots per inch. And these are vector drawings so you can actually save them as 600 dots per inch. So you know what I like to do? Every time I have an editor who's being difficult with me, you know what my best revenge is? Give them figures at 600 dots per inch. Do you have any idea how big those figures become? Often there are hundreds of megabytes. And I love it because it's really hard for them to work with, right? That's my revenge. Anyway, sorry. So you can do that. You can save to the resolution you need. That was a little wicked. Okay, so now let's go ahead and run that. Now that we understand scatter plots. Okay, there we go. How's my scatter plot look? Looks okay, right? Now, not a big deal. You could change everything you want to change. Do you guys like my color bar? That's inferno. Okay, I love that about working a map plot live. You can control the color bars very well. Anybody here have color, color blindness issues with seeing color? I know a lot of people do and I won't specifically draw people out for it, but I'll tell you what, these color bars are excellent for people with eye issues because the intensity and the tonality change together and that does help color blind people so we can use really good color bars. Okay, is that enough on plotting scatter plots? Are there any questions? Kind of cool, right? You remember when I said no news is good news? When it comes to working with open source, I have a different attitude that even goes further than that. And that is if it ran, it's probably good news. If I check the results and the results are working, it's probably good news. When you work with open source code, there's always these types of warnings. I was going to bring this up. In fact, I could say I did this on purpose. Is there often going to be warnings because of deprecation? What's the term? Deprecation. Sorry, deprecation. I had a moment there, which means that the command you're using at some level, in fact, there's a piece of it that actually has been replaced by something new. Often those warnings are not your fault. In fact, in this case, if you look carefully, you'll see that there's a change now and they're going to be working with a new type of visualization. They don't want you using scatter anymore, but they say we're going to remove this in two or three releases. So you often run into this because open source is continually evolving and changing. It's no different than working in patrol. Your old workflow may not work with the most current version of patrol because there's been updates. We're fine for now, but you'll see lots of warnings like this. Sometimes they're totally not your fault. Sometimes it could be a case of map plot live is using numpy under the hood, and they're using some old command and the warning is not because of you. It's just something to note, but not necessarily something to be too concerned with. Any other questions, comments? Okay. Let's go ahead. We're going to round this out by loading up some seismic information. So we're going to load gridded data. Now, remember, we already showed you a slide where we're loading a gridded data set. This is comma delimited. It's going to have every row is going to be every row, I mean, is going to be a y-sexual column is going to be in the column. So x and y's, and it's going to be equally spaced data. And our data is called its data set number 12. Now, this is kind of a funny thing. I have an undergraduate level course in subsurface modeling, and for every student to get a unique data set and their challenge is to complete a full cycle statistical workflow using open source. And so this is student number 12, their data set. So you get a sense of this is the data set I gave them. Okay, let's go ahead and run that. No errors. Okay. So seismic is now, what is it? What is seismic going to be? Well, we could look at it. You know, it's kind of interesting and Python. When you create something, you can just type its name in and usually it'll show you something. So if I do that, guess what? Can you guys hit enter right here and run that or not enter it the run command and run that? You can see it's an array and it's got two square brackets and you'll know to square bracket, to square bracket, square bracket, square bracket, dot, dot, dot, dot, dot, dot, it's truncating it. So it doesn't, if it's too much data, it doesn't want to fill up the whole screen and you can never look at it, it gets crazy. So it's truncating it. The fact that it has two squared brackets with this nesting tells you it's a two dimensional array. Now you might be wanting to know exactly what that thing is. So you can use the type command again. If you do the type command, it's a ND array, numpy ND array. So now you have an ND array, your data set, and you can do all kinds of functionality on it. Now one of the most simplest things that you could do, the most simple things you could do, is you could look at the size of the data and say, well, how many X's, how many Y's, how big is this thing? If you use the shape, shape is a built in variable for a ND array, if you type that in, look at that. It's a 100 by 100 array of data points. So that's exactly what it is. Okay, any questions about this ND array and using the shape command? Good. Now what's interesting is this, do you see how these numbers come out with round brackets? That means that it's a tuple. In fact, if you do the type command, right there, and you ask, well, what is that thing? It's a tuple. A tuple is a list of values, which is immutable. It cannot be changed. You're not allowed to change it. Okay, basically because the shape of the array is the shape of the array is static, you can't, you know, that's the actual shape of the array. So it's a tuple. Now what's interesting is you might write code at some point where you want to access one of those components. So you could do shape and index zero and you get 100 and that would be the number of why values within your, within your table, within your grid data, your two dimensional ND array. You could do NY equals to this. And now from now on, if I'm doing code, I could say, oh, the number of Ys in my grid data set is equal to this first index of shape. And I could do this to get the second index. And I could say nX is equal to that. And that would be perfectly correct. Now you notice NY then nX is the order it goes in. And you also notice that everything in Python is zero index. The very first value in the tuple zero, the second value is one. And it goes all the way up to N minus one. Okay, any questions about that, y'all? Good. All right, let's do, let's visualize a grid at data. Let's take a look at it. So I put a lot of documentation in here. PLT, which stands for map plot, live, pi plot, has an IAM show, which is designed to visualize gridded information on a regular grid. So you're able to visualize it. The variables are the parameters for it. I should say the parameters are, first of all, a two dimensional array. It expects a 2D ND array. The interpolation option. Now I do interpolation none because if you do interpolation, it'll create this weird kind of effect where you have like pondering. I would rather visualize the cells, see the data cell by cell like pixelated, because that's what my data set is. The extent you can control the extent of the data set, the V min, V max are the range of values from minimum to maximum of the values you're visualizing. This is acoustic impedance. So it'll have its range and the color map. Okay, once you've done that command, it's the same pattern every time. Create the image, modify, tailor, customize the image with other commands and show the image. That's what we do in map plot live every time. We can add axes, labels, you can add a goal, you can set the color bar and you can put a label on the color bar. Okay, so let's go ahead. That's the code right there. We run it. Did you guys get the same result? We're very good with that. What do you guys think? Do you think map plot live is complicated? So one thing that's super cool about map plot live, I'm not going to demonstrate it right now. You could actually combine a scatter plot and a pixel plot. These kind of, you know, I am show plots. You can combine them because right here I made a plot, but I could actually put in right afterwards a scatter plot with the same extents and they would superimpose on top of each other. So you could have this type of plot a seismic with the well data on top of it. So you can combine plots together. You can do subplots where you have one plot and then another plot and another plot all laid out really nicely. And once again, you can save to a file and you can create a specific resolution any file format. So it's really good for making figures for your papers or any type of presentation where you need high res. The other thing you can freely draw on this figure. So I could actually draw circles and lines and position them where I want them to be. I can put text labels anywhere I want to. So you can make highly customized figures. Okay, now when you build a Python Jupyter notebook, you always got to do output because it's not going to be just inside the notebook. You want to send things elsewhere. You want to create a product that can go elsewhere. You want to do some cool data science stuff and you want to write it back out. So you need the ability to be able to write back out again. And so you can take a data frame and you can do two CSV, you remember read CSV, you can do two CSV and you can create a brand new data frame. So if you manipulate the data frame, you make modifications, you can write it back out. If you create a brand new, if you're a brand new data frame, if you're workflow, you can write it back out. With the NDA array, NumPy has a whole bunch of options to be able to save it back out. And so the parameters are the files names, the two dimensional arrays equal the seismic, the delimiter. If you want to, you could do space tab delimitation, whatever you want to do. And you can save it to the file. You can even save as binary files. You can get much more advanced with all this loading and saving. Okay, there's many more things we can do with pandas and NumPy and MatplotLive. The documentation is exceptional. These are the building blocks for all of data science and data analytics and Python. Everybody's using these methodologies.
 So the next thing we're going to talk about is some basic Unix Linux file system commands. And the real reason that we're going to look at these is because this is how we're going to interact with our version control system get. So we're not going to try to become Unix experts, but we do need to learn how to navigate around on a Unix command line terminal so that we can perform our version control actions. And we're going to learn quite a bit about get the version control system. And we'll use it throughout the course in particular as we go on and begin to get into our exercises on Wednesday and Friday. I have sort of hidden in the version control the solution to many of the exercises. And so if you want to access the solution later, you'll have to at least know a little bit of get in order to do that. Okay. So I guess by show of hands, how many of you ever used or have experience with Unix or Linux? Yeah, so I'm okay. Good. So this might be kind of boring because this is the most basic stuff here. Just sort of how to navigate move around on the fall. And so again, this is a scenario where it's best if you can range your screen where probably for most of the rest of the course, try to keep your screen arranged where you can have the classroom.datum.io and one half and then what I'm doing, what I'm sharing on the screen on the other, because we'll be going back and forth a lot to type things in the terminal in this case. Ah, okay. So of course, you know, Unix is an operating system and I use the words Unix and Linux interchangeably. Nowadays, you know, in the early days and the 60s and 70s, there were variants of Unix, but Unix these days really refers to a standard, meaning a set of rules that describe how an operating system acts. And so there's this, you know, the most common one is this POS, POS, IX, standard that basically, you know, describes what Unix machines should do. And then people have various freedom to implement, you know, exactly how they do those things. But nowadays, almost all Unix machines are Linux machines. Linux just being one flavor of implementation of that standard. On a Unix machine, again, so I'll use the words Linux and Unix interchangeably. When I say one of them, I can mean the other. In our case, you know, we want to understand like how to navigate around the file system, right? In Unix, everything is a file. Directories, hard disks, you know, DVD roms, we don't have too many of those anymore printers. Those are all files and basically the categories and three types of files, an ordinary file, which might just call our regular file. This is just where we store our text. So this is like our source code, things like that. We have directory files, you know, otherwise we call those folders. And that concept is very similar on windows, right? They look like folders when you work with them on the graphical user interface. And then like I said, these device files, which could represent a device or peripheral, like an external hard disk or possibly a printer or something like that. The files are arranged in this sort of inverted tree. So everything starts at root and that's this slash here. Okay? And so this would be called, if you're familiar with it coming from the windows world, this is comparable to the C colon, right? So it's the base of the root, the word root, you know, even comes from this tree like idea that it's at the base of the tree. Okay? And then everything, the next level down is inside sort of the root, right? And so a standard UDX machine will often have these kind of folders. And while nowadays there's no limitation on the length of names of folders or programs or variables or anything like that, there's really no limitations on those kinds of things. In the old days, when space was very, very limited on computers, you know, and the, the units was originally written in like 1969. So, you know, in those days, when space was very limited, they had these very short variable names, very short folder names. So these are all, you know, directories or folders that are inside root. These would be common ones on a unit's machine, bin, it's kind of short for binary. And even though there's no limitations these days, we've held on to these conventions. So most units machines will have these kinds of folders in the root directory. The ones that are, you know, particularly stand out our home, of course. Home is, you know, from the home directory is where all of your user account directories are. So on our machine, we of course have a root, we have a home, and then all of you have your own user accounts on that machine now. And this example here is Romeo and Juliet. All of yours will be something like Jupyter, Dash, your GitHub username. Okay. Those will be your, your, your user names. And then inside of those, you can, you can have your own folders and files, right? And we'll create some files just for practice. We'll create them throughout the course. And then of course, there's also some folders already in there, like the introductory course notes folder and other things. So Ben is a, you know, where you often put system binary files, live is for system library files. Another important one is like user local. So the thing about user local, this is an area where you often will put files that you want all users to have access to. However, you don't want this to be, to be touched in an upgrade. So if you were to upgrade the operating system, there's no guarantee. Well, let's say there is a guarantee that user local will not be touched. So it will not be affected by anything in a system upgrade, whereas all these other folders could contain things that will be upgrade. Not the home folder either. Those would be left, your users would be left alone, but, but anything. So, so from this kind of route, we can begin to construct what we'll call file path, right? So for example, if we wanted to stick a file inside of user local, then we would have to start with slash type about usr type another slash type local. And then we'd be inside user local. We'll talk about, we'll, we'll, we'll practice that here in just a second. So let's go ahead before we move on. This, this lecture is going to be a lot of kind of just going back and forth between slides and the terminal window. So hopefully you arrange your screen such as you can see this. Let's, when you're back with in a Jupyter lab, just like we launched the Python notebook earlier, over here in the file browser at the top, if you hit the plus to take you to the launcher screen, then notice down at the bottom, there's one called terminal. If you click that, that will open a Unix terminal, right? And it should have your username and, you know, maybe some other information there, but your username would be something like Jupyter, Dash, and whatever you gave us for your GitHub username, okay? So now we're on the Unix command line. And so if you type LS, that will list all the files, the LS is short for list, that will list all the files in your directory. And you can also be more explicit about what you, so if you type LS without any additional arguments, it's going to list the files that are in the directory that you're currently working in. And if you want to know what directory you currently working in, you can type PWD, PrintWorkingDirector. So that will tell you the PrintWorkingDirector. And that has the absolute path of the directory you're working in. So slash home and your username, okay? So if you wanted to say list a specific directory, not necessarily the directory you're working in, you could type LS and give an argument, where the argument is the absolute file path. So in our case, like for example, home. So if you just list home, then you'll see like all of the users and basically everyone who's ever taken our course. Now, you won't have access to see what's inside those folders. You may not, do you have access to even list that directory or do you get a permissions there? You can list the directory, right? Yeah. So that's why I thought, however, if you try to list, like my home directory, so if you type Jupyter, John T. Foster, I think you guys are gonna get a permissions there. Can you see what's in my home directory? Do you get a permissions there? I got a cannot access. Yeah. Yeah. Yeah, so you won't be able to, you'll only be able to list or view things that you have permission to access, okay? But that is an example of using the absolute, what we call the absolute file path, right? So if you type, you know, slash home slash Jupyter, and your GitHub username, you should be able to list what's in your home directory. And again, that's an example of using the absolute file path. Absolute means from root. You're gonna type out all of the subfolders from root. Okay? There are some relative paths, and those would be, there's some shortcuts for those. Like for example, the dot means the directory you're currently working in. So that's the same if you type LS with no arguments, or you type LS dot, you're gonna get the same thing. And in this case, these relative and absolute path names will work with lots of unix commands. And so in this particular case, there's no utility in typing LS dot versus just LS, but there could be for other commands. So the dot is just a shortcut for the current directory that you're working in, right? And again, if you wanna see that, you type Pwd. Okay, so back to the command line. What we're discussing was the difference between absolute and relative paths. So one relative path is the dot, it means the current working directory. Okay? Another relative path is double dot. So if you type two dots, that means the directory above the one you're working in. So in our case, the directory above your home directory is slash home, which is the directory that lists all the users. So from where we are, again, print working directory, you should be in your home directory. From there, we can type, we can, we can get to listing the home directory in two ways. We can type the absolute path, which is from root slash home, or we can just simply type LS dot dot, which the dot dot means one above us in the tree. Now where I get above, again, it corresponds to this kind of, this kind of structure, right? So if I'm juliette and I wanna list home, then I type LS dot dot and that lists home. That's one folder above me in the tree structure. Okay? Yeah. So a single dot represents the current directory, two dots represents the parent directory, the directory above you. And I think I give some examples here, but we've already seen that. So let's practice making and removing some directories. Any time you wanna clear your screen, you can just simply type clear, and it will clear the screen. So to make a directory, there's a command, and it's called mkdir. Now again, we, even though there's no current limitation to the length of program names, we still hold on to that historical units aspect. And a lot of the units commands are very, very short. I think mkdir is probably the longest, one of the longest ones, most of them are three letters or less. So make directory, and then we just give it a directory name. So say test underscore directory. It's in general a good idea to avoid putting white space in your name. And that's because of something called, it just makes tab completion more annoying. It looks like I already have one. So I'll remove it real quick. Now you shouldn't get that error. So if you type makedir test directory, and then you list the director, you'll see somewhere, you should see test directory. So there it is for me right there. That's an empty directory, there's nothing in it. But there's something all Unix machines have called tab completion. So if you wanted to change directories, so say we wanted to move our working directory into test directory, what we would do is type cd test, and we would start to type it. And if you hit tab, you'll see that it auto-complete. So I'm just going to type t-e tab. You see that it auto-completes up to the underscore? And that's because I have other folders in there that I so I have a test directory and I have a test theme. So if I want to move into test directory, then I need to type a little more of what is there, test dir. So I can just type the d and hit tab, and it'll auto-complete to the end. And then I just hit return, and now it brings me into the test directory. So if you type print working directory, PWD, now your working directory is inside test directory. OK? If you want to change directories to the one above you and use a shortcut, how would you do it? So now print working directory, we're back in the directory where we started, our home directory. By the way, you can always get to your home directory by just simply typing cd with no arguments. This cd with no arguments will always take you home, no matter where you are in your directory structure. OK? So we've created this test directory. Now let's remove it. Well, the only way you can remove a directory with the command I'm about to show you is if it's empty. So if you, there is a command remove directory, kind of like make directory, but it'll only remove a directory if it's empty. So there, we just remove the directory made, and it didn't give us any kind of errors. OK? So I'm going to go ahead and make test directory again. In fact, I'm not only going to make test directory, but I'm going to make a directory inside of test directory that I'll just call test directory 2. So test directory 2 will be inside of test directory 1. And if I hit enter right here, I'm going to get an error. And the reason I get the error is because it tells us, no such file directory. What it's complaining is that test directory itself does not exist because we removed it. There is an option we can give to make directory. By the way, you can get back to all of your old commands by pressing up and down arrows. So if you press up on your keyboard, it will take you back through all of the old commands that you have in history. If you want to see a listing of all your commands, you can actually type history. So now you have a listing of all the commands. And in fact, you can even run the commands from here, if you want, by typing an exclamation point and the number associated with the command. So if I want to run that and make directory, test directory slash test directory 2, if I run that, of course, it gives me the same error. But that's one way I can access my old commands, the up arrow arrow or the history. So what I'm going to do is up arrow to this guy, make directory. And I'm going to add an option dash P. The dash P stands for make the parent directories if you need to. And so now we don't get any error. If we list our home directory, you see that test directory is there. And if you list test directory, you see that test directory 2 is inside of it. So in that, we've covered how to make a directory, how to make a directory inside of another directory. Now, if the parent directory exists, you don't need to put the dash P. So you can say test directory. And then inside of it, we can put a new directory, say, test directory 3. And there they are. So if the parent directory exists, you don't have to use the dash P option. If it doesn't exist, it will create it on the fly for you. So we've covered how to make a directory, how to remove an entry directory, how to make a directory inside of another directory. Let's return to the slides for a moment. So we covered all of those. Now, the last one we did in cover, so we know that removed directory works only if it has an entry directory. Now, we can use a more general command, remove, which works not only on directories, but files and other things. And if we give it the dash R option, the dash R stands for recursive. So if we wanted to remove those directories and everything inside them recursively, we would type remove dash R, test directory. And again, that's going to remove not only test directory, but everything inside of it, and it's gone. You have to be very careful when you do this, because if you had the right permissions, you could do this. So I hit enter and I had the right permissions, what would that do? And it would do it. It would not stop you from doing it. Again, if you had the right permissions, now none of you have the right permission. But you do have permissions to remove things in your own home directory. For example, you could recursively remove the introductory course, not its folder, and it would be gone. All you'd have to do is log out and log back out, log out and log back in, and you could get it back in that case. But that's not always the case. So be careful when you're removing things. We'll talk about a way in a second that you can help yourself possibly not make those kinds of mistakes. So let's talk about copying and moving. So returning to the terminal, let's go ahead and make our test directory one more time. And this time, in addition to making the test directory, we're going to use a command called touch, which just makes empty files. So let's make a couple of empty files. So let's make a couple of empty files. So let's make a couple of empty files. So let's make a couple of empty files. So let's make a couple of empty files. Say test1.txt, test2.txt, test3.txt. So now if we list the directory we see we have test directory, and we have the three test.txts. And what we can do is we can move them inside of test directory. And the way we do that is with the command MV. So move. Let's start with just moving one directory, test1.txt. So the syntax for move is what you want to move and where you want to move it to. And they can take absolute or relative file paths as long as it valid. So in this case, the this file path to test1 is relative. We're not typing out the whole file path from home. We're just typing out the name of the file. And then where we want to move it to. Well, let's move it inside our test directory. And to do that we type out test directory. And it's not necessary to do this, but I always like to remind myself of just putting a dot. Remember dot means right there. So test directory slash dot means move the test1.txt file into test directory. And if we do that and then list the directory, we'll see that test1 is gone from our home directory. We can also list test directory. And see that it is there test1.txt. Okay. So that's moving a file. We moved a file from one location to another. We can also use move as a rename utility. So let's say I want to rename test2.txt to be test1.txt in my home director. Well, then I'll just use move. Move test2.txt. And I want to move it to test1.txt. And so you see now that I list the directory test2 is gone. Test1 is there in replacement. So we can also copy files so we can copy, say let's get a let's copy. Test3.2.txt to test2.txt. So we've created a copy. Now they're all three there again. We can also use something called wildcard substitution. So I purposely named those three files to only be different by the integer character at the end of the name that are test so that we can do this. So what I want to do is in one command, I want to move those three files. Or let's say copy in one command, I want to copy those three files into tester test. The way I would do that is type test asterisk.txt. The asterisk becomes a wildcard that will pattern match anything that could go there. So anything that matches that starts with test and ends with dot txt will get pattern matched. And you'll be able to copy that. And again, the syntax is the same. What you want to move. In this case, it's a regular expression pattern there. So it's going to catch multiple files. But nevertheless, the syntax is the same. What you want to move to where you want it to move it to. And we want to move this into test directory. And so then if we list the test directory. There are the three files. And we also copy a directory. But in order to copy a directory, we have to use that dash R for recursive. So in this case, let's copy the test directory. And just call it something else test directory to, for example. So now we have test directory. And test directory to. And if I LS test directory to. You can see it has the three files in it. Because the dash R means recursively. And not only the file, the folder, the directory, but everything inside it as well. If you ever want to get help commands for any of these built in Unix commands, you can type man say man CP. So man is short for manual. So there's the manual entry for. Copy and you can just press. The space bar to take you down the screen. And then when you're done, you can type q to quit it. And we type man copy. You see the one that says dash i right there. Interact prompt before override. So that's how you can protect yourself. So if you have a file test one dot TXT. I'm going to create it by touching it. So it's in my home directory there. If I want to copy that or we could say, yeah, copy it. Test one dot TXT. Into test directory. Then obviously we've already done it that way. But if I put the dash i option on it, it will interactively prompt me. Do I want to override it? And you can answer yes or no. So that's how you can protect yourself. So there's also. Other patterns that we can use and instead of just using the wild card. Like for example. If I type this. You notice I have the one in the two inside brackets. That means pattern match anything with a one or two there. So that would copy test one dot TXT and test two dot TXT into test directory. While leaving test three dot TXT unchanged. And we can verify that by using the dash i to be interactive. So see it's telling me. Do you want to override test one? Yes, I do. Do you want to override test two? Yes, I do. There's a whole set of regular expressions that you can use for pattern matching. That can be very, very complicated. It can allow you to do very, very complicated tasks. You know file system tasks, renaming files, moving files around with very simple short syntax. I'm not going to go into all the regular expressions, but you can be aware of them. So I'm going to return to the slides. I think we covered everything here. Again, these are here for your reference. So we already talked about how to remove a directory. We can we can remove the files. And using the same command as we used to remove directory. So let's go ahead and CD for change directory. Let's go into one of the directors we created, for example, test directory. So CD space test directory. Now you can print the working directory should be inside of the test directory inside your home directory. So we can remove some files. We'll go ahead and do that now. And the way you do that is just remove and file name. Like so. You can also make this you can use the same types of commands. So the pattern matching was not specific to the copy or remove command. It works for most units commands and remove is one of those. So for example, if I wanted to remove test two and three, I could either use a wild card. Or I could use that bracketed syntax like I showed you before. If I put the dash I that will make the remove interactive. Do you want to remove this file? Yes, I do. Remove that file. Yes, you do. So now my test directory is empty. There is one thing you can do. So you can create your own aliases to commands. So if you're so nervous about deleting files that you want everything to always be an interactive remove. What you can do is you can type alias remove equals remove dash I. So what that's going to do is it's going to create an alias named Rm. That's actually going to call Rm dash I. And so now if I say for example went back to my home directory and tried to remove the three files that we originally created. Test one two three by say pattern matching. Like that. If I run it because I've aliased removed to remove dash I. Then it's going to automatically be interactive. So I'm going to answer note to all these. You can see what your aliases are by just typing alias with no arguments. So there's some that are that are kind of standard aliases eGrep fGrep grip L L L L L. So a lot of people use. So for example. If you type L L it's it'll give you a listing of the directory just like L. S does but it has a lot more information. It has the file permissions the owner and the group owner of the file. When the file was last modified the size of the file all kinds of things like that. So that's L L has been aliased to be LS minus LA. Again if you want to print your aliases you can type alias. If you want to remove the alias so for example the one associated with remove I don't like it to be interactive like that. Then I can just simply type on alias. Remove and then if you type alias again you'll see it's gone. So as you become a super user you can you'll create your own sort of collection of aliases and shortcuts and stuff like that. And there's places within the operating system where you can store those so that they'll be permanently available. Okay there was a question about how to remove directory I think it was answered but I'll recover it just one more time. Again use the regular remove but with a capital R for recursive and then it's gone. Returning to the slides we already talked about remove. And so that's really all we have to talk about with respect to Unix plans. So again just to recap we make a directory with mkdiR test directory to. We move files by the way you can move entire directories in the same way so for example if I wanted to copy test directory to. And all of its contents into test directory one I can do it the same way I would copy a file. I can copy files or move files into directories three deep so for example or you know in deep. So for example if I wanted to copy all of those test.txts into test directory and inside of that test directory to I can do it just like that. And then if we list test directory test directory to you'll see the three files that are in there. Really again the main reason that we'll use this will be to interact with Git commands Git is version control system that we're going to talk about next. And you have to move move around and move into the repo to be able to even execute those commands. So the main thing we'll use it in this course for is just simply you know changing directories moving into those directories that contain the Git repositories so that we can execute the commands. Now the next thing we're set to talk about is actually get and GitHub. And that usually takes about an hour to go through without stopping. And it's definitely probably you know I think most people that take this course feel like that part of the course is where they really feel like they're drinking from a fire. So just so much new. And that they're not used to so many new concepts and ideas.
 And it will open up this workflow. Now, please, what I would ask you to do is just run the blocks of code. So this is our first time doing any type of Jupyter Notebook. And I mentioned already that it's blocks of text, which is documentation, and then blocks of code. If we could go down here to this first block of code and just click on this and click on the arrow up top. So click here. You get a blue line here. It means this block is selected. Click on the arrow at the very top. What you should notice is there was a flicker up here indicating that the machine was running. There was a running time there. And some of our things we run will take more time. And what this did was import all of the packages that we need to work with to make this thing work. Now this is using a bunch of side pie for statistics, using I pie widgets to make interactive plots and so forth. Then what we'll do is we'll click on this code. This code right here is going to create a dashboard where we can do our spurious correlation demonstration. Click on that and you run it. You see how there's filled in circle up top. It took a little bit longer to run. It's done. As soon as it's done, the circle up top is no longer filled in. And it says idle down here for the processor. Is everybody with me? Are you able to run that code? Now what you do is the last one to run is the display. When you run this, it's going to create this display right here. Who's here with me? Who has this display right now? Now what I've done, this is fun. I can use a Gaussian distribution. I can set the sample, mean, the standard deviation, the min and max, whatever you want to set. The triangular distribution, uniform distribution doesn't matter. Then what you do is you pick the number of samples and the number of features and they're all randomly sampled from those distributions. And the distributions will have a distribution with that. Oh, that's the correlation coefficients, my bad. I thought I put the distribution into. Then what will happen is it does the Pearson product moment correlation, coefficient calculation for all of the pairs. Okay. You have four, let's say we have about 500 samples. And if I, let's increase the number of features, let's make like 20 features. Okay. Right there. Is it just, it might, oh, you see what's happening is I think everybody's running at the same time. So now the server is being slowed down, right? Okay. So I'll be very judicious about my changes. Okay. It's a little difficult to read. But the most important thing to realize is that I color coded them. If you have less than negative point eight or greater than point eight correlation coefficient, which would be a correlation coefficient that would set you off, right? That would be like, oh, oh, oh, something's going on there, right? It will light it up as red or blue. Now when you have 400, 500 data, it's not going to show up as being any correlations. Now drop that down to like three samples. Or something like that four samples, something like that. I bet you it's going to take a little bit of time because we're all doing that. There. Did anybody get any spurious correlations? Is it done running? Okay. How many spurious correlations did you get? How many times did you just randomly, because the fact that you have two few samples, get something that appears to be correlated when it in fact was from a random phenomenon? If you look at my example right here with three data, I'm already to the point of having probably of maybe a third, maybe a quarter of my pairwise correlations look like they're strong correlations. Are you guys getting similar results? Okay. Now you could play around. If you make this higher, maybe six data, you'll notice that some of the spurious correlations start to drop away. And by the time you get to 10 data, 20 data and so forth, it becomes less likely for you to see any spurious correlations, but they could still occur by random. Okay. This is a reminder that even random phenomenon can have correlations. If you have enough combinations of features and not enough samples, now if you play with the distributions, using a strongly skewed log normal distributions, going to blow this up to because the log normal distribution will create outliers and those outliers will create a parent correlation. Okay. Was this a useful demonstration? Did you guys like this?
 Get and you know if you've been listening to me say get and you thought it was spelled G-E-T because that's also how as a text and I would pronounce G-E-T get it that's not how you spell it it's G-I-T I don't know the or the true origin of the word I think G-I-T is some sort of the derogatory term in England and the creator of this version control tool get his name is Linus Torval he's actually the guy the same guy who wrote Linux the Linux kernel in fact he wrote get to handle the thousands of contributions he was getting for bug fixes and other things with respect to the Linux kernel so he wrote get the tool get to help manage the open source contributions to the Linux terminal terminal Linux kernel anyway Linus Torval is a is a British fellow so perhaps it means something to him get hub which we'll discuss later and all of you at least have a get of account and we I showed you you know my get up profile last time and you know my thoughts about how software development is done in the open and open source and how it relates to all of that sometimes I say that get hub is the the Facebook for coders right so it's it's where we interact with each other and I guess in a slightly social way but but more than that really in a productive way right so we there there is tools and we'll see them later to you know comment on changes that we make to code and other things and but it's you know it's it's a cloud-based repository of source code and with a suite of tools that allows us to help you know collaborate on source code with each other more effectively so as I mentioned before get is a version control system it's it's you commonly used by large code development projects to track and commit changes additions to the code base think I mentioned last time I actually use it with my students even to version control reports and documents that we work on in collaboration with each other I already mentioned it was written by Lana store vault and you know it's sort of a follow-on or a refinement or a better version than some of the older virtual control systems which have been around for a while like CVS and SVN there's another one called mercurial so what CVS and SVN or I guess what get is has different going for it is that it's a distributed virtual control system so in order to explain what that means what distributed means let's first talk about how CVS and SVN would have worked CVS and SVN were centralized version control systems that were not distributed so let me give you an example the one I mean by that so with an old say CVS repository the repository would sit in some remote location perhaps on a corporate server you know this is kind of before the days of the cloud so imagine how to repository of you know master repository of your source code that sat on a server somewhere and as a user I would want to make changes to this repository what I would do is I would pull down the most recent version of the code and so inside this repository would be a bunch of commits that correspond to changes in the source code and you know just in the interest of time I'll just label them rather dumb and it rather dumb way say v1 v2 v3 v4 so if I was using a CVS version control system repository when I would pull that down to my local computer right so this is now sitting on my laptop it's my laptop so this is now sitting on my laptop and what is sitting here is the most recent version so version 4 now so this is me john's laptop and let's say that I set out to make some changes to the code and at some point while I'm working I discover that I'm working on a feature of the code that maybe someone else wrote perhaps my friend obi is out there in the audience so at some point I discover that I need obvious assistance on this code so what I would have to do back then is and whatever state the code is you know whatever changes I made from version 4 I would have to make a commit we'll call it version 5 and I would have to send that commit back to the central repository so now the central repository contains this version 5 and obi would have to pull down that version so that he would have version 5 and he could make the changes and then once he was done he'd have to submit those back and then for me to get them you know so let's say he makes changes now he's on version 6 he sends it back to the central repository I pull them down again and now I have version 6 now there's some inefficiencies in this and one of them is that in the example I described version 5 was an incomplete you know the code was incomplete at that point it was an incremental commit and and by putting that incremental commit back into the central repository perhaps someone else would come along that didn't know what we were doing obvious myself in our plans for collaboration and in the meantime before we sort of can move from version 5 to version 6 which has the more complete feature somebody else pulls down a version and now the code's broken and they don't know why and other issues so this is this kind of idea of a centralized version control system and then that there's one centralized master repository and everything must pass through it okay on the other hand get is a distributed version control system so now let's imagine we have a code base in the cloud again just for lack of creativity we'll say it has a few commits in it commits by the way our snapshots in time of the code base changes to the code base so now in in my example I pull down a copy the action of pulling this down at least for the for the the first time that we pull it down this this is called a clone so I I'm said to clone the repository and now I actually have instead of having just a copy of the most recent commit I have the entire version control history so I have v1 v2 v3 stored locally on my laptop by the way get is smart enough that if for example let's imagine that the only change between v1 and v2 was just simply a typo literally one letter in one variable changed that what get does is it's smart enough to only store those changes so in other words v2 is not an is is not an entire copy of v1 with one letter changed it only has the instructions to change the letter from v1 to v2 and and likewise on down the down the way so it stores this information very efficiently another and not not much larger you know if we're only talking about textual changes between v1 v2 and v3 the size of the entire repository is not much larger than the size of the initial initial commit v1 okay so this is me and I have the entire repository here and likewise I'm going to collaborate with Avi and and use our example from before let's say I make some commit I make some changes to the repository I make an incremental commit v4 but again v4 doesn't fix the code I need obvious help to finish it so in most cases I wouldn't want to push this v4 commit back to my central repository putting some incremental or broken code back into the central repository not only the more efficient it just keeps the repository cleaner if Avi could just pull directly from me and because we both have full repositories you know the entire history of the repository on our local computers then he can pull directly from me again the pool would only include the changes so if he has the entire repository that I had and only v4 the commit v4 has changed when he pulls directly from me it's very efficient process and that it only gets the changes to be before it doesn't create a copy of the entire code bits so and this matters because the history version control histories for projects at last several years can get quite large and it can be a slow process so so he pulls the changes fixes them say you know makes the changes makes it commit v5 and then and then sends it back to me or I pull it back to him so that now I have v5 sitting here I go on and make changes and ultimately I send you know what I'll call v5 plus you know what I got back from Avi plus anything that I added I send that back to the central repository and only that gets stored you know so only v5 plus is there the v4 is missing because it was an incremental commit and we don't want it in the in the main code bits so this is a very simple example but the reality of you know instead of just two people like for example the Linux kernel has thousands of contributors right so there's thousands of folks who all have the freedom to collaborate with each other directly and eventually those changes can be merged back into the main repository so that's it's kind of a hopefully an illustrative example the difference between distributed version control and centralized version control that everyone I assume everyone was able to see my drawings okay perfect so in order to create a repository let's go ahead and do that so let's switch over to the command line again if you haven't done it already it's it's probably wise to position the windows on your screen such that you can see my screen on one half and have the classroom dot datum dot i open on the other half again if you haven't already go go over here to the launcher and open a terminal window now later we'll discuss how we clone repositories from the cloud but what we'll start with here is we'll just start with creating one from scratch okay so what we need to do the repository always resides within at least one directory so what we're gonna do is we're not the creative directory so we know how to do that make directory and we'll just call it my test repo so once we've that now that's nothing more than a unit's directory or folder right and so let's move into that directory and once we're inside the directory again verify that you're inside the directory by typing pwd trend working directory you should see you know whatever you decided name your repo my test repo or whatever then to create a git repository now this is nothing but a empty repo at this point we'll we'll add some files to it in a second but to create a git repository then we're just gonna type git init and what that did it didn't appear to do much because if you list the if you just do a regular listing of the directory there's still no files in it however if you do a long listing of the directory if you do an ls-la you'll see that there's a hidden folder called git this is a directory right so if you if you were to list dot yet then you see that there's actually things inside of it you never want to edit anything inside this folder this contains all of the version control information this is specific to the version control system so you never want to edit this fault you know anything inside there directly but however it is it is there and it lives there i don't think i discussed this last time but any folder that has a dot in front of it so it's valid to put a dot in the file name the folder name directory name in this case any folder that or directory that has a dot in front of it is automatically hidden so what that means is that they're not listed if you typed ls you have to type ls-la to see the hidden folders it also means that you wouldn't see this in the file browser so if we go over into the left hand pane where the file browser is and you were to say double click on my test repo you don't see anything in there again because the dot git is hidden that works for directories and regular files so with this let's go ahead and create a few files okay so we're going to do that by just saying touch file 1 dot txt touch file 2 dot txt so let's create two files and if you haven't done it already go ahead and navigate to the repository in the file browser so that you see the files that you create show up over here so let me pause there for one second is anyone have any questions or is everyone okay with what where we're at so let's go ahead and put something in those files so for example in the file browser if you double click if you named your file dot txt if you simply double click on it it will open that file and a text editor it understands that the file extension txt is a regular text file and then we enter a text editor and at this point we can type some text so you know obviously later we'll expect what we're typing in these to be some type of code but here you know just any text will be able to demonstrate what we're going to do so I'm literally just going to type some text text and then I'm going to save that file you can save it by typing either command or control s depending on if you're a Windows or Mac I guess Windows would be control s Mac would be command s that will save it or you can also save it by going to the file menu and going files save file so now we have some text in the file in fact I'll go ahead and close the file returning to the terminal window you can you can have the terminal actually print out the contents of the file with a command called cat so if you say cat file 1 dot txt it will print out and because there's only one line in this case it kind of prints out funny but there is the line some text and unfortunately it kind of prints out funny such that it mixes the text that it prints with with the command line I think if we would have added a carriage return then that wouldn't be the case but there was no carriage return in that file I guess so anyway there's the text from the file so what we want to do is we want to explicitly add this file to be tracked in our repository and the way we do that is we type yet add in the name of the file file 1 dot txt we can verify that this did something by typing git status so there's kind of two commands within git they can give you a lot of information one of them is git status so the other ones git log I'll talk about that one next but for now git status tells us that we have you know it and even does some color highlighting here so it says you have changes to be committed there's a new file file 1 dot txt and it also tells us that in this directory there's some other files there's another file called file 2 dot txt and then there's this hidden folder called i pi nb checkpoints this is this is a autumn this is sort of an automated an auto save feature of Jupyter notebooks so it jubyter will automatically snapshot at points in time versions of your notebooks and at some point yeah I must have had a I must have had a notebook open in this in this report in this folder anyway so we we have this file that is it's it's said to be staged for commit when it's in this kind of green state so we haven't actually made a commit yet we've just readyed it to be committed okay so the next thing and I want to I'm gonna stop my share for just one minute because my system is in a more advanced I've made some modifications of mine that it's going to do something that you guys won't see so I want to put mine in the same state as you guys so just give me one second here and I'm back okay so so again we're we have a one file staged for commit can verify that would get status so let's make a commit this means a snapshot in time that's going to save the you know save our code base as it is right now and so what we'll do is just say get commit and then we'll put dash in that allows us to write a commit message on the command line so get commit dash in and then you can give it a commit message in quotation marks and almost always the first commit in any repository is just first commit we'll talk about later what a good commit message is and what should go in there but for the first one first commit is adequate now this is this is what I had to undo on my machine so that I would see this message you all should also see this message except for this is something you'll only see one time on you know per computer that you're working on so basically it's just telling you to tell us who you are and so literally what you need to do is just type these these are two separate commands type them just as it's telling you to type them get config dash dash global user. email you can even say copy and paste it if you like however change you you know you at example.com to your actual email and if you use the email that you use when you signed up for GitHub that will give you credit you know what later when we when we actually interact with GitHub if you recall from Monday when I showed you my GitHub profile you could see all of the commits that I had made and all of the contributions to you know all of the different contributions to code basis I made were associated with you know me and the way that it it understands to do that is it uses the same email that you perhaps if you the way that will happen automatically is if you use the same email that you use when you sign up for GitHub so in my case it's probably my personal email you can by the way you can register multiple emails to GitHub like your personal and your work and then it would work with either one so you need to run the first command get config dash dash global user. email and then in quotation marks put your email address and then run the second command and put your name and it could be you know however you want to write your name once you've done that you'll never be prompted you'll never be prompted like that again so after you do this once you'll never have to do it again on our on our system once we have that now we have to make the commit again because it didn't actually make the commit it aired and gave us this you know tell us who you are okay so if you recall I can access old commands with the up and down arrows so I could type get commit dash m first commit again or I could just up arrow until I return to that command get commit dash m and then I'll just hit enter and now this time I don't get an error and that commit was successful I can verify that was successful by typing get log and in this case there's only one commit we'll we'll make some more changes and more commits later but in this case every commit we make will have this unique identifier some long hash string like that that uniquely identifies the commit it'll also have the information so this is the information we input so on yours you should see whatever name you wrote whatever email you wrote as the author and then it keeps the date and time and the commit message right so that's one snapshot in time of our repository so let's go on and make some more changes to our repository so for example let's open file 1.txt again and add a second line with some alternative text anything you want to write just make sure it's on a second line save it go in close file 1 and then go ahead and open file 2 as well and add some text to it again imagine that you're writing some code or something like that so now we have made those changes save them and we're going to type get used to type and get status we're going to type it a lot we're going to type get status now it's telling us that we've modified one file in the repository file 1 and then it has the same information as before there's two other files that are untracked okay now I know we made changes to file 2 but it's still untracked we'll come back to that in a second for now what we want to write is you know before we explicitly added get add file 1 right listen I'm going to show you a shortcut if a file has already been tracked it's already tracked in the repository we can omit the explicit get add command and just type get commit and like before when we wrote dash m m stands for the message we're going to add an additional option dash a what the dash a means is commit all files that were already tracked so it's a shortcut we don't have to do the explicit add to save the changes to file 1 dot txt so we can go ahead and add another commit message in this case and not very clever one I'm just going to say second commit and then I'm going to type get log in again get used to typing get status and get log now we see the second commit there with the unique also has a unique commit you know a unique stream now go ahead and type get status again and we still see that we have this file 2 here I'm going to do something a little bit out of order but I want to demonstrate let me see what the best way to be to do this okay here's what we'll do what this ipi in-be checkpoints that's something I never want to track that's a that's a in fact um in many cases it's useful to just turn it off and Jupiter and perhaps we should do that on our system here but and you know since you guys are actually on your own individual servers does does everyone else see that or is that just on me because I opened the notebook does everyone does anyone give me a yes or no on your own machines do you see dot ipi in-be checkpoints okay that's something we never want to commit into a repository and there's a way to to inform get of that right so what I want you to do is type echo dot and in quotes dot ipi in-be checkpoints and then put a star at the end of it remember the star is a wild card and then close the quotes right so if I just enter that it's going to echo that back to the screen okay so if you just hit enter this is going to echo back to the screen now that didn't do anything hit the up arrow to bring that command back active so echo ipi in-be checkpoints and now what we're going to do is we're going to read it this is called redirecting in unix so we're going to redirect the output of that command which is simply printing that to the screen to a file and the name of the file is going to be called get ignore so we're going to redirect it and it needs to be a hidden file so it needs to be dot get ignore so literally what's going to happen is dot ipi in-be underscore checkpoint star is going to be placed into a file called get ignore and we can verify that if we do a long listing so if you hit enter to execute that command and you do a long listing of the directory you'll see you know so ls-l a you'll see this file get ignore is there now and in fact if you remember we can look at what's inside a file by using the cat command so if we cat get ignore we'll see that there's a long there's one line in that file and it has this ipi in-be checkpoints and now let's type get status that ipi in-be checkpoints is gone right it's missing that's because we that dot get ignore file has a special meaning to get and it's going to ignore anything that's in that file so no matter what you do you'll never be well without forcibly overriding it you'll never be able to commit on accident that dot ipi in-be folder that's that you you only want to keep things in your repository that are relevant to your source code you don't want to keep you know autosave files or you know if you're compiling code some of the intermediate files that were produced in the compilation process you never want to commit do you want you want your source code to be as minimal as possible so that you clone it you can clone it quickly okay so now we have this these two files that aren't tracked and so what we can do is just type get add dot now if you remember dot means the current we're working directory right so what i'm saying is add everything in the repository now this is usually not a not a good idea to do this but in this case it's only two files there and we want to we want to actually track both of them so it's usually a good idea to go ahead and version control your get ignore file as well so if you type get add dot then you type get status you'll see that there are two files staged for commit so let's go ahead and make the commit then get commit and again dash m and this time i'm going to say something more meaningful at least like add get ignore and then if i type get log you can see my three commits there so to move on and demonstrate something new let's make one more set of changes to file one this time let's you know at least remember what our file looks like because we're going to undo those changes via the version control system later so i'm just going to add a third line of text save it return to my command terminal and type get status tells me i have a file that's modified so i'm going to go ahead and make commit get commit this time i'm going to say dash a dash m i'll just say third line changes and also want to show you that you can combine these two so instead of saying dash a dash a dash sorry instead of saying dash a space dash m we can combine these into one so we can just say dash a m and then we type get log so now we have this log of changes and one thing up here it shows you that you know head pointing to master well in a moment we'll learn about branches branches of the code base it's kind of like branches of the tree and i'll discuss that in more detail later in just a minute but i'll just say for now that master is just the default name given to the default branch so when you initialize a new get repository there's only one branch and it's always name master you can change the name you can have other names but master is always the you know the default name and the fact that it says you know head is pointing to master this just means like where we're at in the current code base is you know it's called head and and and it's on the master branch so we'll see what when we make some changes here in a second okay so let's imagine then that we want to go back and look at an older version of the code for whatever reason perhaps the new version is not working and we want to revert to an older version or perhaps we you know made significant changes refactoring things okay looks like I have a question here the the command is get commit dash a m not get commit dash a l okay no problem we're all learning okay so we want to go back to an older version of the code for whatever reason and in this case we just want to go back and look around we're not going to we're not going to try to make any changes or anything like that we're just going to go back and look around so remember these commit IDs are unique that they'll always be different for everyone and that's how we can identify any given commit so if you remember our file one has this third line of text that we added in the last commit if I go back one commit in time to the one where we added to get ignore file that would be identified by this unique identifier and we only need enough of that identifier to be unique so what we can do is we can say get checkout and we just need to type enough letters such that it's unique from all the other ones which in most cases four or five letters is adequate when you only have a few commits you know sometimes two letters is adequate but but almost even even with hundreds of commits if you just have the the first four or five letters it's enough to make it unique so if you type get checkout in this case in my case it's e4 b91 for for everyone else there will be something different so type enough of it to be unique and then hit enter and it's going to give you some information that you're in this detached head state whatever that's not really important here but the first thing let's do is go open file one dot txt when we do that you notice that the third line of text is gone because we went back in time to a state before we wrote that third line of text the other thing we can do is we can type get log and if you notice now we don't see that fourth commit in the commit log right it tells us that head is at this commit it's it's not pointing at master because master is the most recent it's it's also a short name for the the hash string of the most recent commit so that's why you don't see that head pointing at master anymore because head is here and master is one commit ahead of us okay we can actually then go back you know say so say we look around and decide okay well I want to go back to the current you know master so in that case I'm going to say get check out master when I do that and I type get log now you see again now head is pointing at master and that third line of changes is there and if I were to reopen file one dot txt another time my third line of text is return okay well let's imagine that we want to permanently get rid of those changes okay so say we we realized when we added that third line of text that it was it was incorrect and we want to permanently go back in time to this commit there's a couple of ways to do that one of them is get reset hard and then we just say head dash one and if we type get log so that what that head tilde I'm sorry head tilde one means one commit from head right one commit back in time from head we could have also wrote out the whole you know you could have done what we did before and gave it a unique identifier that would have worked as well but now if you type get log those changes are gone forever we've removed them from the code base and if you go back and look at file one you see that in fact you know that third line text is not there okay now let's return to the slides for one second may just make sure I have it yep so we've we've covered get status and get add so these slides are you know basically just reference material for the things that I've already shown you again to add all files in a directory you could do get add dot I would I would recommend not doing that unless you know exactly what you're doing to avoid you know accidentally committing files that that you don't want in your repository we've covered how to add a commit message if you were to ever omit the dash M then what's gonna happen is and and please don't do this because it's gonna put you guys into a state that that well I might have to help you get out of so if you I'm just just for demonstration then let me watch me but don't do anything so I'm gonna return and add a third line of text to my file and you know so then if I type get status shows me that that files modified and so this time I'm gonna type get commit but I'm gonna omit the dash M what it's gonna do is open up an editor and then it would allow me to edit the text here so I could add a clever commit message and usually if it usually if you want to open an editor it's because you're gonna write something really explanatory in the commit message and this is exactly what I didn't want you guys to get into this situation this is a this is an editor I should fix this but this is an editor that I'm not actually familiar with so I can't remember how to get out of here okay so in in every uh unique system there's a set of environment variables you can actually see what they are if you type ENV it will list a long thing of the environment variables and so if you in this case I'm gonna list one and search for one called editor and there doesn't happen to be one there so in my case please don't I'm just this is not really related to directly related to what we're this tangent to what we're talking about here but quickly I'm going to um set my editor to be equal to y and then now if I do that get commit again oh I expected it to open by it's not staged you need a tash a ah thank you there you go so this is the editor I prefer and I know how to use so let me let me redo uh what I was going to say so it open an editor and now I type a short commit message and a longer and a longer uh explanation and then when I close and save the editor that that will get saved into the get history in a long right so in this case you see that the full commit message there so if you will emit the M it opens up an editor and the editor that it opens is the one that's stored in the environment variable called editor okay and in my case I prefer to be the the vi editor we I try to avoid using like proper unix editors in this class because that could be a whole week long course in in in of itself and so for the most part we just use the built-in text editors the ones that you see when you open and when you double click on a file okay so uh that demonstrates use you know omitting the the dash in so for this purposes of this class try not to ever do that just always use the the inline commit message here's just some notes on what good commit message should contain so if you do have a you know a really big commit because you've made major changes to the code then the typical format would be to have a very short kind of one-line commit message followed by in a longer details bulleted points other things like that we've already talked about excluding files in the get ignore file in its role let's look at how we would remove a file from the get repository so let's say that after some time file 2 becomes obsolete it's no longer needed in our repository we can just like we type remove to remove the file what we can do is type get remove file 2.txt and when we hit enter you'll see that and if you refresh the file browser over here you'll see that it actually removed the file it deleted so it's gone it's gone from our repository and it's also gone from the file system okay so if we type get status you can see it's gone now we can we haven't made a commit yet so if we were to make a commit it would be forever gone from the repository going forward however we can get it back from the last commit and the way we do that is to get checkout dash dash means the dash dash means the last commit and then we give it the file name in this case file 2.txt oh perhaps I have to go back farther let's say I can maybe go back to this commit so the one where we added to get ignore command was also the one where we added the file 2 so if I type get checkout give it that commit and then type file 2.txt now you see it's back okay so the other way to get rid of the file or the other scenario would be the case where we want to remove it from the repository but we don't want to delete it from our hard drive because for whatever reason this is often the case where if you accidentally commit a file like one of those ipi nb checkpoints or something like that if you want to accidentally commit something you want to take it out but you don't want to actually remove it from your hard drive then what you would do is type get remove cash file 2.txt so in that case it's not removed from the system right if I do a listing of the directory it's there however if I do get status it shows that that's been deleted right so then if we were to go ahead and make a commit get commit dash a m remove file 2.txt so then it shows that it's still there it's just not tracked for change you know we're not tracking it for changes any longer so that's what the dash dash cash option does now let's talk about another common scenario so it's often the case that we have you know what we'll call our master branch and here's what we're going to talk about branches more a little more so in our master branch we have you know I'm trying to avoid just using something like v1 v2 but you know let's uh yeah let's not do that I don't want to do this but it's just going to be quicker if I do so let's just say v1 v2 v3 and we're working along and someone calls us and said hey I discovered a bug in the latest version on master but the bug is imagine it's small it doesn't affect the overall functionality it's some edge case right for the most part the code still works but it's some edge case that some user identified and so we don't really want to affect master because there may be you know we're collaborating with other folks and and other folks may want to be you know may want to use master the master branch for other tasks and we don't really want to get in there and mess around you know actively with master so it's a common workflow that you never commit directly into master okay so often what we would do if someone called and reported a bug is we would branch off of master so we're going to create a new branch and we'll call this branch bug fix and when we when we create the branch v3 bug fix branch and master our identical there they haven't there's no changes at that point okay so let's let's go ahead and see how we would do that up to this step so the way we do that is just simply type get check out dash b for new branch and we'll call it bug fix if you want to see what branch you're actively on you can type just get branch with no arguments and there'll be an asterisk next to the branch that you're actively working on so as a right now the two branches are identical okay and so let's go and fix our bug on the bug fix branch we would let's go to file one again and just add a fourth line of text fix bug save that file return to the command line get status shows that the file is modified so we'll commit that get commit dash am and we'll give it a commit message we'll just call it bug fix or we'll say fix bug so now we're here we have a new commit fix bug okay and we've tested this code we've verified that it works so we want to get those changes back to master this is called a merge and this will be now here so the way we do that is we type get merge bug fix master so the branch you want to merge in is the first argument and the branch emerging into is the second argument oh I did it wrong I said that backwards hmm see I'm gonna go back to master get checkout master it's verify okay let's try to get merge there we go sorry so I just I moved back to master so I said get checkout master and then type get merge bug fix master so the branch you want to merge and the branch you want to merge into I did say it right the first time I just needed to move back to the master branch so now if we type again if you type get branch you can verify that we're on the master branch and if you type get log you can verify that the bug fix commit has been merged into master so then we can delete that branch if we choose so you can say get branch delete bug fix and it's gone and this is a common workflow for small bug fixes so you you branch and you know make the change and then merge it back and later we'll see how to use some additional help tools that that get hub gives us to do some additional code review so just this alone is a powerful thing to know because it when you when you get used to this kind of you know branching it really gives you a lot of confidence to try exploratory things in your code because I mean I'm sure some of you have been coding some of the language some you've you've encountered a situation where your code works it produces the known expected output but perhaps it's say too slow and you you think you have some ideas about how to make it faster fix it but you're but you're scared to mess with it because you spent so much time just getting it to work right sure everybody's encountered that kind of scenario right and so having a good knowledge of a version control you're safe right you can always go back well first of all even if you're doing a straight from master you can always go back in time right you can always go back to where you work but even more so if you're working with this kind of branching attitude where you're just going to branch off a master and try something and perhaps you know if it doesn't work out you just delete your branch and and go on you got a question so I'm sure everyone can see that can multiple people check out the master here on the different branches and merge all branches together how does get handled multiple code changes of the master on different branches the answer is yes you you can absolutely do that and that's what makes get and distribute a version control very powerful how does it handle the changes well very careful it it it has the ability so for example let's let's uh let's say that we Sarah and I are collaborating on some code together and or we're trying to fix a bug together and it's really high importance we need to get it fixed as rapidly as possible so we're working on it at the same time we we create a branch it's in a centralized version control system or repository like on GitHub where we both both can pull down the branch and then you know she is focusing on a function that lies on line 22 and I'm focusing on a function that lies on line 100 we both make the changes we need we make commits and we merge them back into master get will handle that automatically for us we it is automate there's automatic there's no merge conflict we made changes in different parts of the code and it has the ability to handle that for us in the in the event that we make we both say changed line 22 when we go back to try to merge that obviously there'd be a merge conflict we both simultaneously made changes and get would warn us about the merge conflict and we would have to resolve it we basically someone would have to manually go in and look at and decide which you know which of the changes to keep you know minor minor hers and and so yeah that's called you know merge conflicts again there is a workflow that we'll discuss later related to GitHub which is is actually can assist with all of this because of really nice visual tools that assist with code review and so we'll talk about that here shortly new energy companies tend to set up internal Git networks or use GitHub I think you'd have to talk to the people that GitHub to know what the what most other folks are doing however there is enterprise versions of so and GitHub is not the only player around I mean it's it's it's by far the most used but there are other ones like Bitbucket and GitLab and they don't have different pricing structures they're all kind of targeted at different audiences I would say it with Microsoft's purchase of GitHub last year it's really hard to be anything because you know GitHub Microsoft has the the cash flow behind them to just try to get market share so the they've recently even lowered the pricing for GitHub to to very very you know basically it's basically free I mean it is free for everyone for open source repositories all the time but when you begin to collaborate on you know private repositories or if you wanted some enterprise version that you're going to install behind your behind your firewall then then there is some cost associated with that but those have been greatly reduced since since Microsoft purchase GitHub so yeah you know I don't I don't know what the kind of consensus is in terms of what other companies are doing but they're but there are many other tools and you know it would be you know if you if you don't feel comfortable say even using private repositories on the public GitHub to collaborate which I mean a lot of tech companies is that's what they're doing just just using the cloud-based version of GitHub and private repositories if you don't feel comfortable with that for whatever reason then you could install like an enterprise GitHub you know behind your firewall so let's move on and talk about GitHub I showed you my GitHub profile the other day and when I was sort of advocating for open source and and this idea of you working in the open establishing a portfolio of work on GitHub and other places but let's let's talk about you know the actual mechanism and what we do right on GitHub so what is it it's a cloud-based remote remote repository server and it's simplest form it's just that right it's just a free place to keep your code I say free because it's it's free if you it's free for public repositories meaning everyone can see your work when you start to have private repositories particularly I think you can have individual private repositories that are also free but when you have private repositories that you need to collaborate with other folks with then you have to have some type of paid account which I think currently it's a $4 per month per person on your team so it's pretty cheap right it's this integrated pull request and code review system that is really what makes GitHub so useful and we're going to talk about that and I think the best way to demonstrate this is just with an example by the way I think I mentioned already there there are similar services bit bucket and get lab the you know they just don't have the market share that GitHub has it's by far you know I don't know the actual numbers but it's by far GitHub is the largest of these I mean I guess you know since I mentioned since I mentioned the GitHub is the social media of for coders right or the Facebook for coders I guess bit bucket would be like my space or something right so some some social network that no one uses I shouldn't say no one uses obviously people do use bit bucket just the same way I think people still use my space but anyway so let's uh for this then if you go to the table of contents there is a exercise over here GitHub merge and pull request and so what you might do I may have not shown you this when we discussed the Jupyter lab but you can actually move these windows around so what you might want to do is take the assignment or the exercise instructions if you just grab that tab at the top and you pull it you can sit you can orient it where the terminal window will be above or below or to the left or to the right so in this case I think what I'll do is is place the exercise here on the right and have my terminal window open on the left hopefully things won't get too cramped and of course you can resize these windows and everything might be useful in this case to close the the file browser on the left side to have some more real estate so the first thing we're going to do is we're going to what we call fork right so if you click on this link click on this link it should take you to an external webpage that is GitHub and you'll see that all repositories and GitHub always have you know some either user or organization name and then the repository name so in this case we have an organization data and there's one exercise one repository inside that that we call GitHub merge exercise so it's a really simple what we're going to do to start this and these are in the instructions here but you can just follow along with me we're going to fork this repository it for most of you you won't have a whole bunch of organizations that you're part of and so you won't be faced with this and in my case it actually is telling me that I've already forked this so I think what I'm going to do real quick is I'm going to go just so you see you you guys see exactly what I see I'm going to go delete that fork so just bear with me one second so just returning here again now we're going to fork the repository and then now in my case you should see something like this so what is the fork? The fork is kind of like a clone in the or a copy in the cloud right so the original datum get a merge exercise we just used it as and we copied it into our own you know account right so associated with our account so now you should have your your GitHub username followed by GitHub merge exercise and you should see up here that it says fork from datum get a merge exercise and in fact it even gives you more information than that if you look down here it says that this branch is even with datum master meaning as a right at this moment what is what my fork is identical to the upstream fork you know the upstream version and that's the the datum get a merge exercise so this is a pretty common workflow when collaborating with others on open source code as you would on get up as you would create a fork just so that you can have your own copy and this is sort of like an extra precautionary measure because now even if we're making commits into master they're not on you know the upstream true master the datum master they're just they're local to our own account okay so from here we can clone this to our computers and and our computers what I mean our computers were in our case we're talking about the Jupiter lab session that we're using for the datum course we can do this by going over here to this green button and saying clone or download you click on that it gives you a link if you click on this thing that looks like a paste board that copies that link to your paste board on your local computer so meaning you can you know if you if you then paste it with control v or command v then it will paste that to you know whatever so if you copy that return to Jupiter lab into the terminal I would do this in your home directory so if you remember you can return to home by just typing cd without any arguments that takes you to your home directory so if you do print working directory you'll see that you're in home now you can just type get clone and then paste the link oh I already have it in my let me I'm going to remove it you won't you guys won't see this okay so let me try it again get clone paste the link you'll see some message cloning into and now if you look at you if you list the directory you should see get up merge exercise okay so what we'll do now now is change the directories into that directory now we're in the get repository right so we can do things like get branch they'll tell us the branch we're on we can do things like get log get log that will display a log of the information in fact it also tells us more you know so previously we only had this like head pointing the master now it's also telling us that not only is it pointing to the local master but the origin master is in the same location origin being the version on get up right the the place where we cloned it from so let's go ahead and create a branch you know I'm following instructions now so we we've we've done the first thing right so so we've done this so it looks like a question okay when we've gone to get hub we forked the repository we used the Jupiter lab terminal application to clone the repository to your home directory so the next thing is to create a branch called add email so we'll do that over here in the terminal we're inside our repository again we can verify that with print working directory and what we're going to do is we're going to just say get check out dash b and then the branch name the branch name will be add email now what we can do is if we go to the file browser go back to the home directory by clicking the folder in the next to the root and go back to your home directory and you should see get up merge exercise if you click on that you'll see a file in there email dot md just open that file and what I want you to do is just add your email address so change johnadfake.com to whatever you want and you can make up a fake email address if you want it doesn't really matter you know I'm not storing it or using it for anything this is just for demonstration purposes so once we do that save the command once you save it you can close the file if you wish and then back over here in the terminal window I'm going to type get status and you'll see that that file email dot md is modified so let's go ahead and make a commit get commit dash am and then give it a commit message you can just say change email now we could merge this back into master but for demonstration purposes we're not going to we're going to leave it in its own branch and we're going to push it back to get hub so the way we do that is we type get push origin remember origin is the default remote name in this case it refers to get hub that's where we clone the repository from and so get push origin and then in this case instead of master we don't want to push the master branch back we haven't made any changes in the master branch we want to push the branch add email so if you hit enter you'll be prompted for your get hub username it's the same username and password that you used to log into our system and if you put those in successfully you'll see some information that says it's pushing it back to get hub so now we've edited email we've committed the changes with a commit message and we've pushed the add email branch to our personal get hub account so then the next thing says to go to get hub and open a full request so there's a request that I slow down a little not having to do that there's a question about storing your username and password with respect to pushing back to get hub there's you don't store I guess the short answer is yes yes it is possible but it's not as simple as like storing it locally you have to set up some kind of additional encryption through ssh and there's a process for doing that in get hub if you're interested you could just google um pushing to get hub without a password something like that and it will show you how to set all that up it's kind of beyond it's not that hard to do but it's it's beyond the scope of the class and in this class we won't be pushing to get hub a whole lot so it's not going to be that big a deal so after you push to get hub if you return to get hub again your local fork or I'm not I shouldn't say local your fork on get hub so meaning should be something like get hub.com whatever your username is get hub merge or sub-size you'll you'll notice something like this there's this yellow box that says you know you you recently pushed the branch add email less than a minute ago compare and pull request so let's see what happens if we click that button click compare and pull request now in in this case if you look over here the the base repository on the left is the is the upstream branch that is datum get hub merge exercise for right now I don't want you to don't push to datum okay change this to your own account should be you know whatever your username is get hub merge exercise so it should look something like this where we're changing we're trying to we're trying to merge add email into master now we've already covered how to do this locally what we're doing now is is learning how to use get hub to do it because get hub has some nice features that can help us do the code review so you know if this was a real meaningful change to the code you would write some comment explaining what you want to do what you know what the changes you made did and why you want them you know why they're useful and why you want them committed in the mastery once you've done that you just write some details we'll go ahead and type create pull request now I don't know where what I've always felt like it should be called a push request I don't know where the word pull came from I guess from the fact that ultimately the person who's going to approve your pull request will be pulling it but nevertheless so this when we created it it tells us this branch has no conflicts with the base branch merging can be performed automatically so if there were a merge conflict it would tell us there and it would give us some visualization tools to help understand exactly what the problem is okay we can also do additional code review recurrent sorry additional code review by going up to these tabs up here at the top see one of this is commits checks files changed let's go ahead and click on files changed and what you said see is something like this the old file on the left or the old version on the left the new version on the right and it tells you exactly what this changed right so basically in my case it was just the John T. Foster Jr. Gmail versus John and fake that's all that was changed and you know it would basically highlight all of the changes like this and so there is a common workflow when people collaborate using Git and GitHub that even if you have permission to do so you never commit to master you always submit pull requests like this and have someone else review your code and it it's useful for two reasons I mean of course one you just get another set of eyes on your code to make sure that you didn't do anything even if the code is working for your use case you didn't do something that might affect someone else but the other thing is when you're working with a team of people if a code base has a very uniform coding style it's easier to maintain meaning and I've worked on codes where you have multiple people who all coded in very different ways it can be hard to read that code because you know for just different coding styles and code is really meant it's red far more often by humans than it is by computers so it's it does matter to have sort of a uniform coding style and code base it's far more easily maintained and so the simple fact of having someone else look at your code over time what you'll just sort of organically happen is that the code base will begin to look more uniform because everyone will learn from each other the sort of best practices and coding styles and and sort of commit to using them there are also tools called linters that automatically format your code to give it to a given style talk about that was a little bit on the last day of this class you can also if you return to where conversation you can also have a long discussion and if you go to GitHub and you look at other open source projects and you go to the pool request feature you'll see many of them have long ongoing discussions about whether this feature is really needed or additional changes that might need to be made for it to be merged etc and you know so if you recall yesterday I mentioned that I was going to try to make the case that open source software has a higher quality than alternative you know commercial tools and part of the reason is this you you have an very if people are using and you know basically all open source software development is done on GitHub nowadays and with respect to every feature addition every bug fix you have a very transparent and open discourse that is occurring everything is in open and you can see exactly not not only the code changes that were made but in many cases the conversations that led to the decisions for the code looking the way it does or doing the things it does in addition to that of course are all the automated checks so in our case we you know appear you see this checks well there are none because we don't have any in place okay but if we did what you would see here are a list of automated tests that were run against our code base we'll talk more again on the last day of this class when I talk about code quality and reproducibility I'll give you I'll show you code that is tested but what you would see here in addition to if you return to conversation where this says you know this big green check mark that says this branch has no conflicts with the base branch that's just meaning that there's no textual changes that that there's no merge conflict but it would also if there were checks in place like automated tests that check the validity of the code the outputs it produces then you would also see that here and it would tell you okay the automated checks have been run and they all pass and we're safe to run you know you're safe to merge this pull request okay and so you can do this you can you can just simply you know click the merge pull request or you can there's other options you can do a squash and merge we already talked about what that means however in this case there's only one commit so there's nothing to squash there's a rebase and merge that that's more that's a more advanced feature that I think is beyond the scope of what we're trying to discuss here so I'll skip that but nevertheless if we go ahead and do that and you just go ahead and click merge pull request and then you know if you wanted to leave an additional message or whatever you could confirm the message the merge it and then it actually even gives you the ability if you choose to do so to delete the branch so if you want to delete the branch you can do that here as well and then if we return to you know if you click up job you know where you're using your name and get up merge exercises you click on that returning to the main page right here you'd see there's only one branch now the other we've deleted the other branch and if you click it on email.md you'd see that it is the updated file okay any questions so that's the process of you know in this case we're just merging into our own repository but often you create a pull request for against the repository that you may not have the permission to merge against like you're asking other people who are the maintainers of the code to review your code and merge those features that that's the more you know often use case and an open source software
 So before we build models, let's go ahead and recall some fundamental concepts. Let's make sure that as we're having a really fun discussion in front of the models, we're looking at hyper parameters and parameters. And I say model variance. I want in your brain. I want you to be thinking exactly what model variance is and realizing how meaningful this is. Let's increase the quality of this experience of the hands on. OK, so I won't talk about big data or statistics, data analytics. I think that's pretty fundamental. We've already talked about parameters and statistics. We talked about machine learning. That's not what I'm worried about. I'm worried about kind of more nuts and bolts around modeling. So remember, as we build these machines, this is what we're doing. We're estimating a function f such that we're able to go from the inputs, the predictor features to the response. And every machine I show you will have two predictor features, one response feature. And I do that specifically so we can visualize the machine as a map. We're going to be able to see the entire model as a map. And that's a very good way to look at a machine. You can see how it behaves. OK, so that's the thing. We're going to do everything we do right now is going to be, we're going to build predictive models. We're going to try to put together a model that does a good job of going from the X's to Y's. We're focused on prediction at unobserved locations. It's the same thing as trying to do pre-drill prediction of a well. In other words, I have a combination of predictor features that could be porosity and brittleness in an unconventional reservoir, a geomechanical property. And I'm trying to take the sample data and make predictions at locations where I did not measure porosity and brittleness. I want to be able to interpolate effectively with the model. Now we're going to be working with a variety of models. We will have parametric and nonparametric models. Remember, every time we use a parametric model, we're making an assumption about the functional form or shape. We decided a prying in advance to modeling that we think the functional form, the natural system, is linear. The f is a linear function. And therefore, we said we just need to work with one plus the number of features parameters, right? All the coefficients in front of all of the predictor features. Multi-linear regression super simple. And if you looked at it in any one of over one of the predictor features, you would find it in fact is a linear model. If you look at it in full 3D, 10D, or whatever, it's a hyperplane. It would be a hyperplane. OK, now you gain simplicity of only having to estimate very few parameters. When we do nonparametric for this problem, you'll see it's a lot of parameters. OK, now which means you can work with sparse data. It means that you can have a model that's much easier to work with and tune. It's just going to be very simple first to work with these parametric models. A nonparametric model makes no assumptions about the f. The function that describes it's an approximation of the natural system. It's flexible to fit a variety of shapes to that function. So the good thing about that, there's a plus, is there's much less risk that you get a bad fit because of inflexibility. In other words, your function was flexible enough to fit a variety of shapes. So that's good. You have more flexibility. In fact, a lot of people, a lot of my students say, well, isn't that truly machine learning that it learned the shape from the data? And some of my students feel parametric is not really as machine learning. I don't get too worried about that. They're all, they all have their limitations. Typically you need a lot more data for an accurate estimate of that function f because really every time I say nonparametric, please remember that that's parametric rich. In other words, this description right here had a lot more parameters to get there to fit that. We had to fit a lot more model parameters to get this shape than with this model right here. This model, if we were only with one predictor feature, one response feature, would literally have just been two parameters, an intercept and a slope term. This one right here, oh boy, that's a lot of parameters to fit that. Now, any questions, comments about parametric and nonparametric machine learning? I think you're going to find we're going to do both and we're going to discuss both. Okay. Now, remember, model parameters are going to be those coefficients. It's going to be those, those dials that we get to turn and we'll use some type of optimization under the hood. We're not going to do it ourselves. It's by, it's automatic. We're going to fit. We're going to fit those model parameters or train those model parameters based on the training data. So we're going to have part of the data set that we're going to keep and we're going to use this training data and we're going to do the best job we can fitting whatever that form is parametric or nonparametric. Okay. So when we do model parameter training, we're setting these values such that we minimize the error. So it's going to be a least squares solution. For those of you in, in optimization, it's an L2 norm. Okay. So, and then we have model hyper parameters and they're different. They don't control the fit to data. They control the complexity of the model. And so anytime you're working with a machine and you're trying to ask yourself, is it a hyper parameter or is it a parameter? Ask yourself if it changes the flexibility slash complexity of the model. And if it does that, it's a hyper parameter. And there's many, many hyper parameters. We'll work with a couple today. Okay. So for the polynomial model, the choice of the order was the hyper parameter because we could build and fit the very best model, which was first order, third order, fifth order, and seventh order. And so that's hyper parameters. Now we're going to select the hyper parameters such that we maximize the accuracy with, with held testing data. In fact, when I pick hyper parameters, I'm not going to look at the training data. I'm only going to look at the testing data. Now let me ask you this, what would happen if I was to tune my hyper parameters by minimizing the mismatch with the training data? Which model would I pick? If we were the tune or hyper parameters on the training data, we would always pick the most complicated model. That's why we have to do this train test split because if we don't, we'll pick the most complicated model. And I guarantee you, we will be in a situation which is known as overfit. We're fooling ourselves in the thinking that we know more than we do. And if you have ever seen somebody do machine learning and they report, I've got an R squared of 99%, 99% of variances that is explained. I know everything. I've had that happen all the time. My students do that all the time. It's really fun. And so what I do is I get them to test it more thoroughly. And it always turns out that those models with such great fits usually are overfit. And I perform very badly at new information, new observation locations with held testing data. Okay, that's why we have to do this split. Now we can pick the most complicated, we pick the, can anybody have any thoughts about how much, what proportion of the data should you withhold for testing? You know, it's interesting. I did a survey of literature and machine learning. What I found was everything between about 15 to about 30% with held for testing. That was pretty typical. Now what I like to do when I talk to my students, I have a lecture where we talk about fair test train is you can think it through. What is the consequence? Like let's, the best way to think about a concept is take it to the extremes and see what would happen. Okay, what would happen if I withhold 90% of data for testing? The one thing it is a balancing act. If we withhold too much data for testing, how good is our test of the model? Are we testing a wide range of configurations? Are we doing a good job testing the data? We withhold 90%. Are we doing good job testing the model? It turns out if you withhold lots of data for testing, you're actually doing a good job testing the model you build because you're checking at a lot of different possible locations to see how it performs, right? Now how well are you training the model if you're only retained 10% of data for training? Is your building a really bad model, a bad set of models, but you're doing a great job testing them. You see that? Now, how about the opposite? If you were to take 95% of data and you use it for training and you only extract 5% of data for testing, now you're in a situation where you're building very good models. You're using lots of data to train it. You're seeing lots of different ranges of possible configurations of the data to train it, but you're not sufficiently testing it. You're not testing over a wide enough range of possible locations. You have good models, but you're not really doing a great job evaluating them. That's the way to think it through. That's what leads us to 15 to 30%. People have done studies where they try to optimize that. The answer is, as I said, 15 to 30% is usually optimal and it totally depends on the configuration of the data, the noisiness of the data, and so forth. Is that helpful? All right. So we're in a situation that if we were to increase the model complexity and by adjusting the model hyperparameters, we would be able to reduce the error in training basically to zero. If most cases you get to a point where the model will perfectly fit like this example right here, which in fact is a decision tree. And it's reached a point where you perfectly fit all of the data and there's no error anymore. Now what you'll find is if you look at the withheld testing data though, is that usually the model improves for a bit and then the error will start going back up, indicating that you're in a state of overfit. That's how we would define overfit. What's happening with overfit? You're fitting data, noise, data, idiosyncrasies. You're not capturing the structure of the natural system. You're capturing the noise and the data, not a good thing. Noise complexity will generally decrease the error with respect to training, but will usually result in an increase in error with regard to testing. So we tune hyperparameters so that we pick the model with the best error in testing. These were avoiding this effect of going to high complexity. The result is that usually a lower complexity model wins. The most complicated model rarely, rarely wins. Very rare. And remember, from now on, if anybody talks to you about machine learning and energy, I want you to be able to say that energy is unique. Why was it unique? Don't look at the slides. We showed an example in which it looked at an image, a machine and said 99% chance that it's a wolf. It knew it was a wolf. The cost of looking at the reservoir and thinking that that's the place to drill the well and it's absolutely wrong with high certainty is extremely high. Yeah. What's another difference with energy? Anyone how else is energy different than many machine learning problems? What about our data? The machine learning people from Google. What kind of data do they have? Exhaustive. They see it all. We see one trillion. They see it all. You think they might design a little bit different tools than us? It's totally different. In fact, many of the machine learning methodologies, and I'm going a little bit deeper and I went before, make no accounting for sampling bias. Do we have bias data? Well, you bet every single data we have is biased, right? And now I had a student who did research and demonstrated that if you do training of a machine with bias spatial data and then you test it with unbiased data, that the test will still be biased. So if you imprinted bias into your machine, your machine will be biased afterwards. So we have a paper that's coming out right away. If you guys are interested to hear more about that. So basically, we have to deal with bias. We deal with spatial information with multiple scales. Most of the machine learning people don't deal with all this multi-scale spatial sparsely sampled stuff. Okay. We're very different. Did I miss anything? I did. I missed something so important. The geoscientist and engineers in the room did not call me out for this. Is there an engineering solution to driving directions? Is there a real engineering solution that could be applied to a recommender for what music you like? Remember the definition of machine learning. The final point that was made at the very end of the article, it said where it is not possible to come up with a physics driven solution, a science driven solution. And so if we have engineering and geoscience, we got to put that in. And it's up to us. The algorithms are perfectly data driven. We do not abandon the third paradigm or the second paradigm or the first paradigm because we live in the fourth paradigm. We use all of the paradigms together. Boy, was I on a soapbox or what? I was really ranting. Sorry about that guys. I got carried away. Okay. This final, final concepts here, we'll just finish up by promise I'm having too much fun. Okay. Remember, this error in testing has a mathematical solution. We looked at it, actually we looked at it right here. We looked at this error right here with regard to withheld testing data is what we care about. And it has a mathematical solution in expectation. Visual expectations like a probability weighted average, not a big deal. So the average error in testing locations that we did not train with can be calculated as the combination of three separate components of variance. Variance is additive. So we can put them all together and we get this model variance. Let me just ask you, don't read it. If you already read it, maybe sorry, it's unfair. Let me go back. Visual variance is the error due to sensitivity to the data. A linear model, if you change the training data, will move around a little bit. A ninth order polynomial fit, if you change a couple of data, we'll whip around. You've seen that before. You'll just jump around. It'll go different directions. If I go back to this model right here, if I change a couple of these training data, imagine how much that function will change. Okay. Like seriously, where is this function going? Like I'm pretty sure it's hitting your floor right now. It's going through the bottom of the monitor. It's going right to the floor. Now imagine if I change this data point right here, if I just remove that data point right there, dramatically different model predictions. Okay. So a more complicated model is much more sensitive to the data and that is a source of error in our models. Okay. The other thing is model bias. That's the opposite. A linear model is too simple. It can't fit a non-linear phenomenon. If you work with a linear model, with non-linear phenomenon, you have a high degree of model bias. Model bias, due to the fact the model is too simple. And irreducible error, it's just the limitation of your data. You didn't sample everything. You didn't sample all the features. You missed something. You didn't sample the full range of features. You may not have got all the low prosities, because you drilled in the good stuff. You may not have got all the combinations of the petrophysical and gene mechanical properties you're working with the non-commentials. You're missing part of the data. In fact, one thing we really miss out on today is the curse of dimensionality. When I teach my students, I go into horror stories with Dr. Purchan. One of them is I teach about the curse of dimensionality and we realize in a high-dimensional problem just how little we know. It really is an interesting. In other words, we don't sample enough to really see everything we need to see. Okay, as you change model complexity, if you increase from simple to complicated, model bias goes down. The first order model can't fit the data. The third order can fit it. The higher order and higher orders will do a better job. They're more able to fit. They have less model bias. But they really start to lose out a model variance. They start to become very unstable and sensitive to the data. An irreducible error is constant. If you add them all together, you get a line that looks like that. Now, we have engineers here, right? What do you do when you have a line like that for a result? It has that little u-shaped to it. What does an engineer do? They optimize. They always optimize. They find the minimum, right? And if you find the minimum of a line like that, it's going to be here and it's never going to be the most complicated model. It rarely, rarely is. Okay. Are we back into the mindset of machine learning? Was that helpful to go through a quick review?
 Once again, here's a general discussion of multivariate analysis talking about correlation calculations. Here's the Pearson product moment correlation. Oh, the covariance. Here's the Pearson product moment correlation coefficient down here standardized covariance. And we got the rank correlation coefficient. We provide a little bit of details about different types of multivariate measures. Now let's go ahead and import some packages, use the world's brilliance. So let's go ahead and import geostap pi and we'll get an import on sci-pi for doing some. We'll do a little kernel moving window calculation. We have sci-pi for summary statistics, matplot, live for plotting. Lots of great packages. Is everybody able to run the workflow up to this point right here? I see nodding. I see thumbs up to in the screen. Thank you very much. Okay. Now what we're going to do is we're going to use intake. Intake is a data engineering package. Dr. Foster has set it up to be able to take our data set, a spatial data set, a two-dimensional data set with multivariables and has been able to basically load it up and set up all of the how do you load, how do you deal with the type of file, how do you clean the data. It's all packaged in data engineering with intake. One will talk to you about how to use intake, I believe, during his course on data science. Okay. So we load our data. Now, put your hands up if you've never used a data frame before in Python. John is going to talk about data frames. I don't want to steal any of his thunder because there's going to be plenty of thunder there. But at the same time, what we're doing is we're just basically loading up our data from a file into a very nice table. And the table is going to have every row is a sample, every column is a feature, and it's very organized. Now the cool thing is, as Pandas has these data frames and DF now is a data frame, has plenty of built-in functionality. So you can load your data, visualize your data, do things with your data very nicely. You can combine your data, you can clean your data, you can sort through your data, you can query the data, it's all built in. Okay. So we loaded up the data. How do we know that that worked? Well, I'm not a very trusting individual to be honest. So I like to confirm. And so what I do is I can just say, okay, I made a data frame, give me a preview of the first 13 samples. Head gives you the first 13 samples tail would give you the last samples at the very end of the data frame. If you run that code, do you get that? Is that what everybody gets? Do you get a preview of the data frame? Okay, very cool. It worked. Okay. So what do we have? We have, we could have done this print command and the iLaw command, in fact, what it does is slices. If you want to take a certain column, take a certain row, this slicing command is simply saying take the first six samples and take the, or the first five samples, I should say, and take all of the features. And that's what it's done right here. Or we can use the head command and get the preview. We can look at the data. The other thing that's really cool is that has built in statistical analysis. So we can go ahead and run that and we get all of the common statistical summaries. The mean standard deviation, min, p25, p50, you know, and so forth, Max. These are the features we have x, y, faces. Faces are going to be one and zero sand and shale. Shale is zero. Pyrosity, permeability, and acoustic impedance are all available to us. Okay. Now what we want to do is we want to probably plot spatial data. Now the plot spatial data, you can use map plot live. Now what I find with map plot live is a lot of people get a little confused about it. It's a kind of a lot of things to do at once. So I made a bunch of convenience functions that do location map plotting. And I put them in my package. So when you run those commands, did you get those maps? Did you guys get these maps? I see nods. Thank you, Sarah for that. I appreciate it. Now what's very interesting is that this teacher's a few things. One of them is the idea of using a subplot. Now I'll tell you what, part of this whole Python thing is this ability to get really great plots of your data. And I don't know about you, but when you try to build a reporter, what a good visualizations key to good communication. And so what we're able to do with subplots and make multiple plots put them together. And what's even really cooler than that is you can save them to any file format at any resolution. Anybody ever submitted to a journal and they say it has to be 600 dots per inch. And you struggle because you did your plot in Excel or you did it in PowerPoint. You can't get it to that resolution, right? Well, guess what? You know what I like to do? I like to give them 600 dots per inch and show them how terrible that is. I drop in those like 500 megabyte files right on that share directory. I clog the whole thing up and let make them pay, you know, because they really make you suffer over that, right? Okay, sorry, that was angry. Okay, so we can plot. We can look at all of our plots. We can do a lot of creative things with plotting. I'll show you stuff that gets much better. We can also look at the Bivariate relationships. You remember we discovered that you should be having deja vu at this moment, because everything I'm showing you right now was shown in the slide deck. It's the same data example from the slide lecture. Okay, so we got, hey, what was this called? It's heteroskedastic. It's also a non-linear relationship between the two features, right? If you look at the expectation over the one feature, it's definitely got a non-linear. Well, this right here, you probably has a linear relationship homoskedastic. Very good, very good. Appreciate that. Now, you remember I showed the definition for co-variances? Well, guess what? If you've got your data and you take all of your features, you can calculate a covariance matrix with one command. In fact, spoiler, kind of a spoiler here for John. If you do this, I lock right here. What it's doing is it's extracting a slice out of your data frame. And you can run the co-function right here on it. And what'll come back will be the full covariance matrix. Let's run that. Give that a run. Did you get that? Did you guys get the same thing? It's working. Good. So this is the covariance between all of the features. It's the degree to which the variables vary together. OK, now that's kind of harder to look at because it's non-standardized. It depends on the variance of each one of the features. Let's run the standardized correlation coefficient matrix. Do you see that? So this is really good because now we can immediately visualize which ones of these features seem to have the highest degree of correlation. What's most correlated? Parosity and what do you think? Parosity and facies are pretty strongly correlated. Parosity and permeability, they're some level of correlation. We can look through and figure out kind of negative and positive relationships. Now, you saw that really cool plot where we looked at a matrix plot of our variables. Let's go ahead and run that. Now, this takes a little bit more time. Now, here's another instructive thing. Do you see the red box or the pink box on my display? A little bit of a warning. In general, you'll find that with many different packages, you'll get warnings. Often they have something to do with the factor using older code. They're recommending you to use a newer method, something like that. I don't get too word most of the time. I'll read it and check it out. But you can see it complained, but it's still ran. This is a very powerful display. Simultaneously, you're seeing all of the pairwise relationships between all of your features and its color coded, oranges, sand, blue is shale. So that's really useful to see the relationships. What do you see? Acoustic impedance to porosity. Have a very good linear, homosodastic relationship. It also appears that porosity from the sand and shale both follow the same trend. But they have unique subsets of porosity for which they cover. And you can see that right here. Permability has a much more complicated relationship with acoustic impedance. Makes sense. It's not a very clean relationship in general, right? It's probably more of an artifact of its relationship through porosity, I would imagine. Okay. Any questions so far about these displays? Is this useful to talk through and show these displays? Okay. I see nodding you guys like this. Okay. Now, it also turns out we can produce kernel density plots, which are even more powerful. I think these are really, really cool. Look at that. Every time somebody gives me a scatter plot, have you ever got a scatter plot where there's so much data you can't see anything? You've got a scatter plot where it looks like somebody took spray paint and it's just old dots, right? I would much rather get a by very density plot like this. Okay. Now, look, this was one line of code, pretty straightforward right here to create that plot. Most of this is just putting the labels on the plot. Okay. With that one line of code using SNS. Now, when you look at my workflows and you see SNS dot, you should realize that that was an important package from somewhere else or some code from somewhere else. Go up to the top of my workflow, go down to the imports and you'll see import C born as SNS. Okay. So now when you're working with my workflows and you want to do something on your own, you want to be like, hey, where'd that come from? Where's the documentation? Go to the top, see which the package it is. And you can look up C born. The docs online will be very good. C born, NumPy, Pandas, MatplotLive, all have very good documentation. They're very good at helping you get started if you want to start doing things. Okay. The other thing too is that often when people are doing this, they're kind of a little controlled, confused about exactly what are the parameters to use. At any point, you can put your cursor inside the parentheses for the parameters to run with this KDE, kernel density estimate plot. And you can just do a shift tab. Everybody try that. Just put your cursor there and do a shift tab. Shift, hold shift and hit tab. Did you get this? These are the doc strings. So in every package that's well written, there should be doc strings and you can pan through it. And it's showing you all of the parameters you can put into a method. And here's the documentation to explain the parameters and what you can do with them. Very, very, I'm telling you, this is your new best friend in Python. I love using the doc strings. I love the shift tab. And it gives you lots of great ideas. The other thing too is you see all of this equals, equals, equals. Those are in programming known as defaults. In other words, if I don't say data to equal something, if I don't put it explicitly in my method, it will assume none. It will assume don't do it, okay? Or true or false and so forth. Okay. Any questions about this? Do you guys like that? The kernel density plot? Any questions about it? Okay. You could see how that's much more powerful than a scatter plot. Okay. Now, there's other plots. There's tons of other plots we can do here. I'm really kind of showing off Seaborn. I love this plot too. What we've done is we took that plot right there and we turned it into a kernel density, a contour plot. These are each one of these are probability or likelihood plots, likelihood contours. And so where you have the greatest likelihood is the yellow contours and then going to the lowest likelihood or lowest density out here. And then these right here are the marginal distributions, univariate, just porosity, just permeability or acoustic impedance so you can visualize the whole thing at the same time. These are great plots. I love the joint plots. Okay. There's a bunch of other types of plots we can go through. I'm going to leave it right there. Any questions before we move on to new content? Okay. Okay.
 Okay, we're going to, I think right now we'll cover sample sampling limitations. And I think we'll start to talk a little bit about sample bias. We may need to finish up next time, but it'll be fine. We'll have time to spend on it. Sample limitations. What's the motivation? This is where I say, as I already said this morning, that all of your data is biased. In fact, what you'll find is many of the things I said this morning earlier will come up later on as we get into different topics. And so all of your data is biased because we sample it specifically for a purpose of reducing uncertainty and for maximizing value of a project. Okay. And so biased in biased out. In fact, I've had students work with advanced machine learning methodologies and prove that if you train with biased spatial data, your model is biased. Even if you use it with unbiased data afterwards, that bias gets encoded into the model. So there's significant uncertainty in our summary statistics and models. So not only is our data biased, it's also uncertain. And we've got to account for bias and uncertainty together. Okay, sampling limitations. What's the cause of bias? Now I could just ask people, I could say, how do you decide where to drill the next well? When you work on a subsurface asset team, how did they come up with the idea of where to drill next? Anybody here been involved in that decision of where to drill? I would say that in general, we're always, we're going to be picking well locations based on answering critical questions. If we have a reservoir, I'm always interested how far does the reservoir go? Because at the end of the day, that impacts my ability to book reserves. And that's the value of the company, right? The more reserves we have, I think your ground, the greater the value of the company, we want to make sure we're drilling to add reserves. We want to drill so we are improving our knowledge as far as connectivity and compartmentalization and what's the pressure doing within the system and what will we be able to produce from. And we also want to drill in such a way that we maximize net present value. Sometimes we drill where we get the best production on purpose because we need early production to pay for the project, right? We want to flow in order to add value now. Now, what is my concern? The concern is that that data that you drilled to answer questions and reduce uncertainty. Now we're using it for a different reason. We're using it to build a model of the subsurface. In other words, if this was Y and this is X and this is looking down at your reservoir, these are all of my wells that were drilled to reduce uncertainty, maximize value of the project. But now within the area of interest, this outline indicating the reservoir, I want to determine the average porosity. Now, why do I want average porosity? Because that's exactly how I'm going to calculate oil in place, static oil in place. And I'll use a recovery factor. I'll get to what can be recoverable. Now, what's the problem? Would you guys be comfortable if I was to go ahead and just use the average and I'll call it the naive average of this data to get the average porosity? Would that be okay? It doesn't look too bad. Is anybody is anybody's eyes catching the fact that we have kind of dense sampling here and kind of sparse sampling here? It looks a little like that. What if I told you this? What if now based on geologic knowledge mapping geophysics or engineering production data, I can tell you that this is high porosity. This is the best part of the reservoir and this is the worst part of the reservoir. Now, are you concerned by my sampling? The impact, if I just took the naive average of all those samples, is it would be biased high. In fact, how do you fill now? I'll tell you what, I love good news. The best place to find oil is in the oil field. The best place to get high production is near the high producing wells. I'll keep drilling up that high right there, but the problem is the person building the model is going to use that data to calculate an average and they're going to apply that average over that entire area. So we got to do something about that because what's going to happen is this. Initially, we drilled too many wells in the high area. We were not sampling in a fair manner. The result would be that we would estimate the average porosity above the actual true population average. And so we're biasing high. How much are we biasing high? Well, we drilled a couple wells in the low area. Things got a bit better. But then look, we drilled more wells in the high area, more wells in the high area, and the sample bias keeps going up. If you kept drilling drilling drilling drilling and using that average, we're going to keep creeping that average up higher and higher and higher and the bias gets worse. We do it all the time. How would we fix it? I worked with a PhD with statistics once in my former job, and they were so disturbed over the fact that we don't design our sampling campaign up front with the decision in mind like the doing stats. I told them no, you get to work with the data you get. But what would we do if we had PhDs and statistics and we could fix it? Well, we've only got two methodologies you can use to get representative sampling. One method is random sampling. Now, random sampling is very rigorous. It means that every single single location in the reservoir has equal probability of being sampled by a core or by well lock. Okay, now that would be very hard to do. Okay, so random sampling, regular sampling could also be done. And in some cases in unconventionals, we kind of get towards regular sampling with the pad configuration, not really, but somewhat. Okay, now the problem is regular sampling is less reliable than random because of the fact that we have the possibility of a resonance between the sampling interval and some type of harmonic in the system. If you're sampling just the crest, you would still be biased high. You know what I mean? Or sampling just the troughs of a cycling phenomenon. Okay, so, okay, let's do it. We're going to be statisticians. You and all of us, we're going to go in and we're going to go into our boss's office. And maybe we're not in the Gulf of Mexico anymore, but maybe we're doing something on shore. And the next wallet is going to be drilled by random. How's that going to sound? How's your boss going to react? My advice would be you probably want to, I got an X, I think Justin put an X on that one. I think personally that would be a good data kind of update your CD. You know, right? It's not going to show value. It's not the way we can do it. Okay, so we can never use raw spatial data without doing something to address bias because all of our data is biased and we can't solve it up front. We need to just deal with what's given to us. This is not the best economics or the way we do it right now and we want the best economics. Okay, so let me talk about I'm going to have a little horror stories with perch now. And I told you all your data is biased, but I'm going to tell you has three orders of bias now, not just biased little bit. It's really biased. And what I'm going to say is the well is drilled often in the best part of the reservoir or to answer those questions, right? Then along the well, the core is extracted in the best parts. Okay, often we don't we don't extract core in the worst parts for sure. Then when we take the core and we put the core out, we extract core plugs to send for analysis, right? We're not going to take something that's really bad and send it off. And sometimes it's not it's just the fact that we can't run the analysis on something with, you know, nano permeability. It'll just take too long to run. Or maybe we can't extract the plug because it's not competent enough. It's just breaking up. So it's not necessarily malicious bias. It's just a limitation. Okay, three orders of bias in the sampling. This should horrify anyone with the statistics PhD for sure. Okay, now there's another issue here too. Remember random sampling said every location in the reservoir should have equal probability of being sampled. We can't actually do that. We have infrastructure. We have pad locations. We have geomechanical constraints on reliable drilling. We have rock, biology that tells us that there's certain locations we can drill as well are reliable. Okay, so we have issues with accessibility to the subsurface, even if you wanted to do random sampling, you can't. We can't access every location in the reservoir equal probability. It's not possible. Okay, data is not collected for statistical representivity. I'll just say it like that. Wells are drilled in the greatest probability of high production. We drill for value and to answer questions, reduce uncertainty. Horizontal Wells target stratigraphic zones of interest. They don't go everywhere at random. Chores are taken preferentially from good quality reservoir rock and these data collection practices should never be changed to the best economics. Most data and the most important locations of the reservoir. That's good for us. Okay, but we have to do something to get good statistics like our average porosity to calculate oil in place. And so we need to do something to adjust the histograms, the averages, the p10s, the p90s, the kurtosis, everything needs to be adjusted. Now, the first line of defense to bias is good mapping. In fact, you could build a geologic map based on the data and take the average of that entire map. And that would be a very good representative average. That would be a geologically mapped or secondary variable. You could use seismic mapping or something to get an average mapping is your first line of defense use of regions breaking up the reservoir. So when we went back here and I motivated this, I said the problem was the fact you have high. So you have a high region. Let me make a pointer again. Sorry, I forgot about that. You have a high region right here and you have a low region right here. If you broke it up and dealt with it separately, that would be a good way to deal with the bias. These three data are now used to calculate the average in the low region. And these remaining data right here, the other eight data would be used for the high region. Okay, so you could break up into regions. Otherwise, you can use two techniques. One technique we've taught previously and there's even some folks inside of noble using right now declustering. declustering is a technique by which we wait the data based on the representivity of the area or the volume. In other words, if you're a data and you're surrounded by data very densely, you get less weight. You know less about the reservoir because you're redundant with all that other data. If you're a data point and you have no data around you, you're by yourself, you're all alone, you get more weight because you're representative of a large volume of the subsurface, you know more and you're adding more. Okay, was there a question I heard a mic key. Okay, declustering is about adding weights. Previously, we had a data set with a feature. Now we're going to have a data set with a feature and beside it, the weights for each one of those samples. And I will explain it and we will do devising through declustering together. Now, there's another type of devising technique, which you make a totally brand new distribution based on secondary data sources. Okay, so if you have geophysical information, you use that to map and to get a new distribution. Debicing can be used when you actually are missing part of the sample range. You don't have the low values. You need to get them. You need other information to fill that in. Some people would call it a form of distribution imputation imputation filling in missing parts of the distribution. Okay, let's go ahead and we'll talk a little bit about declustering, but we won't have time to run through the workflow today. Now, let me pause. Are there any questions about this motivation and about the causes of bias in our data sets? I see people shaking their heads perfectly clear. Make sense? Wait, I did see some facial expressions that didn't look perfectly clear. Let me know if you have a question. Okay. All right. Let's go ahead and talk about how we're going to do declustering. Okay. So let's take this example right here. The color bar is cold colors are low porosity warm colors are high porosity high porosity up here. Okay. So you can see that we have denser sampling here spurs to sampling here. It wouldn't be okay to use the average of this data set the naive average to represent the average over this block because clearly we're kind of over sampling or clustering are samples in the high values. Okay. So one way we could address this is we could do regions. Okay, regions would work very well here. I could draw a region of high quality reservoir good porosity. Then I could draw a region of low porosity. This is terrible and everything in between medium porosity. Then what I could do is I could go ahead and calculate the average of the high the average of the medium the average of the low and I would use them only in their regions. This would be a great step to debiasing your situation getting a representative average. Okay. So this is the idea of mapping through regions break up the model into subsets. Now, if you did full mapping, that would be even better. That would be super cool. You could look at the data set and you could say, well, guess what? I think that there's a preferred direction of continuity going like this, where we have kind of a high quality medium quality lower quality and worse quality on the fringes. This whole entire data set is like some type of distributory fan system where we get worse reservoirs. We go deeper into the basin. Okay, a deltay type deposition. That would be totally fine and you could go ahead and map that out. If you calculate the average of that map, that would be a very good debiased average. Because if you look at it, what you'll see is that in locations of dense data, that data gets less influence over the map. You see that it's actually debicing it because over this region here, all of this data is kind of redundant. It's not changing the map too much. Here, this data values by itself, it's going to pin the map. That one data value is going to have a huge impact on the map. So it's actually accounting for debicing. Now, is there a question? Okay, good, good. All right. Good. So mapping for debicing. Okay, declustering methodology is right here. The declustering methodology, let's go ahead and demonstrate it with this data set right here. Everybody can see the dots. Those are the wells. You see the regularly sampled except for if you look at the truth model right here, the background color is the porosity truth model. We gave ourselves a truth model so we could test the declustering. So we have all the low porocities or the darker colors, the lighter colors of the high porocities. Look at what's happening here. You see what's going on? Do we have a bias? We have regular sampling. Ah, okay, okay. And that's there. There's a question about clustering or declustering. When I refer to, let me just say when I refer to clustering, I'm referring to the phenomenon by which we have a higher degree of sampling density in a specific subset of the field. And typically that's tied to the quality. In other words, when I say clustering, I mean that the sample data are more densely sampled in the good or the worst part of the reservoir. Okay. And when I say declustering, I'm referring to the concept or this methodology that I'm about to explain the declustering methodology. So there was a question about the difference between clustering and declustering. Thank you very much for the question. Everybody feel free to either speak up or just throw a question out on the chat window and I'll grab to jump on it. Okay. Thank you. Okay. Hey, awesome. Thank you. I'm glad to hear that clarified it for you. Thank you, NASA. Okay. So let's go back to our declustering example. We kind of we can see that the lower parts of the reservoir, the worst part of the porosity we're missing wells. So we have denser sampling in the better parts we less dense sampling in the worst parts of the reservoir. Okay. Now we're going to go ahead and we're going to try to adjust our histogram and all our summary statistics by not only having the feature at each location, but beside it, we're going to have feature weights. So we're going to set up a set of weights that we can now use and apply to each one of the samples or values. Okay. So these weights are going to go from we're going to have a weight available for everyone, the data one through N. We have them so they're greater than zero and we're going to have them so they sum to N where N is the number of data. Now that's very convenient because if you do that, then there's a way we can look at the weights and interpret them and I'll talk about that right away. Equal weighted phenomenon would be a phenomenon for which all of the weights are equal to one. In that case, if you set all of the weights equal to one and you calculate an average, you get the naive average because there's no waiting going on. The debicing technique that I'll talk about a little bit later will derive an entirely new distribution based on secondary data. Okay, let's do declustering right now. Take the data. We go ahead and we calculate the true distribution from the map. It's that entire distribution right here. The sample distributions right here, do you see the bias? The whole thing is shifted a little bit towards the high. Okay. Now, you don't have to believe me based on the shape of the histogram. The true mean is 10% perosity. The sample naive sample or cluster sample mean is 10.5% perosity. And the error is 5% perosity. That's the relative error. Is an error of 5% in average perosity over a field important. What do you think? I see nods. I see now. Actually, everybody in unison. That was actually pretty cool. So 5% difference in mean porosity or average porosity is 5% difference in oil in place. The static reserves, the static property in place. Now, I have seen in the period of my career over many different companies systematic overestimation and oil in place often on the order of 5 to 15%. And I suspect personally, based on my assessment, that this tree is solved through accounting for this issue of bias in the sampling. I think that people universally, if you look at Patrell or gocat or any of the subsurface modeling software, people typically just take the raw data, calculate distributions and move forward with a predictive model. I think this is a big part of it right here. How does cell-based declustering work? What we're going to do is we're going to go ahead and we're going to take the data set and we're going to break it up into a bunch of cells. Now, what we do is if a data point is by itself in a cell, it gets one full weight. If you have three data in a cell, it gets one third weight. So, if you have five data and so forth, it's going to one fifth, one, whatever one N number of data in the cell. Now, we'll take the weight and we'll multiply it by the number of data divided by the number of cells with data. That's not a big deal. That standardization ensures that the sum of all the weights is N number of data. And that a data point in a location where there's no clustering or anti-clustering. In other words, we have good, almost regular sampling distance. Those data are going to get a sample weight of one. So, 1.0 is a nominal weight. Now, let me stop right there. All we did was put a mesh on our data set, count the number of data in each cell, standardize it so it sums the weights now sum to N and 1.0 is nominal weight. So, there are any questions about this methodology to calculate weights for declustering. Okay, let me ask you a question. Are there any concerns? Is there anything I'm missing right now? What would happen if I move the mesh? Just a little bit. Would it change the weights? Look at this data point right here, all by itself in a cell. If I move that mesh up a little bit, it shares its shell. It shares its cell. You see that? Okay, so clearly there's a sensitivity to the origin of the mesh. Does that make sense? Would you be comfortable if this data can go between one weight or can go to like one fourth weight, one quarter weight, just based on where we put that mesh? That should bother people. That seems very arbitrary. What do we do? Hey, well, I think what 80% of us are engineers. So a good engineering solution, take the mesh, move it around. And then, randomly move it around, calculate the weights, move it around, calculate the weights, repeat enough times, and then average the weights over the multiple random grid locations. If you do that, you remove the sensitivity to the mesh. The mesh side, the mesh location doesn't matter anymore. Okay, so that's the first thing. Let me ask you one more. We're about to run out of time here. We're about to stop for the day. Let me ask one last question. What would you, what about the size of the cells? Does that matter? What would happen if I make the cell so big that there's only one cell and all the data fall into that cell? What would be the resulting weights? All of the data in one cell. Okay, so clearly having a huge cell size would just result in the naive mean, the naive average. Okay, now what would happen if the cell sizes are so small that every data point gets its own cell? So small, very, very small cells here every data point in their own cell. What would the weights be? I saw somebody mouth it just now. There will be equal weight again. So clearly if we go very small, we get naive average. If we go very large, we get naive average and something in between is going to be the optimum cell size. So what I'm going to do is I'm going to leave it right there.
 So let's cover the very basics to support fast start immediate hands-on experience of machine learning. Yesterday was old vegetables. Today is going to be nothing but enter the name of your favorite food. For me, it's going to be steak today. Today is going to be steak. Hands-on experience of machine learning. We got to instantiate the machine. That's fancy coding language for saying make the object that is the machine. Okay, so we instantiate. We make the object. We're going to train the machine parameters training. That's our workflow. We pick a machine. We train it. We're going to adjust the complexity with hyper parameters. And we're going to make predictions with the machine at the testing locations. So by adjusting the hyper parameters by hand, we'll do it by hand. Then we're making predictions at the testing data with held locations. Then we can check the machine with the testing data. We're going to measure the error. Once we measure the error, then we can iterate. The iteration will be us changing the hyper parameter by hand to see if we can improve the quality or reduce the error at the estimation locations. Okay. So that's going to be our workflow. That's what we're going to cover. Now what's our motivation? Well, we need to understand the fundamental building blocks of the code. And Scikit Learn is a wonderful place to start. Let's get good at Scikit Learn. Let's be able to build a variety of models. Scikit Learn is plug and play. Once you understand this workflow, you can just trade the machine. And it'll do the same thing. It's very plug and play. So it's very, very powerful. I'm teaching you four machines today, but you've learned 30 machines. If you know how to, not the details of the machines, but anyway, but you know how to program them and get them to work. Okay. My advice on coding once again is we're going to look at a lot of code. It's going to be more than you understand. I have it well commented and we will step through it. Take it as an opportunity to be focused on the workflow. Don't get caught up on every syntax within the code. But on your own, if you do seek to kind of to improve your skillset here, and I do hope I've convinced you or you already are convinced because you're here, you're definitely a self-selected group of individuals, I imagine. Learn the basics of variables. Learn about tuples and NDE arrays and lists and dictionaries. Learn these basic types of variables and classes we work with. Learn about data tables and efficient methods for working with all of these objects. How do you manipulate them? A lot of this stuff is built in for you. Use example workflows and demonstrations. We provide a lot of workflows for you in our learning management system. We'll cover like 90% of what you need to do for machine learning, really. Use the docs for the packages. Nicholas showed us you can get to the docs by just doing the double question mark with the specific class and you can see the docs, which is very cool. Or go online, use stack overflow. We all do. There's no shame in that. And I'll tell you what, go to meetups, man. Sorry, I really believe in them. Like it's incredible. The world is more than happy to share their knowledge with you. And in this field, more than almost any other field, people established their credibility, not through university degrees. And that's kind of weird for me to say that, right? They establish it by having a GitHub account full of code and documentation. They establish it by sharing their knowledge with others. This field is very, very cool that way. This is very cool and YouTube videos and such. Hey, can I ask, is anyone ever gone to my YouTube channel? Pretty good. Is there some good content? Yeah, hey, thanks guys. I mean, I guess this has become therapy now. I'm glad to hear. And remember, I actually am like a DJ, unlike the Spotify. I will actually listen to your request. If there's something you want to hear more about, I'll throw it a video on YouTube. I think it's pretty cool. Okay, here we go. We're going to do everything based on scikit learn. The scikit learn documentation is wonderful. It's really, really good. Like it's amazing. In fact, what's crazy about scikit learns documentation. Not only does it teach you how to use scikit learn, it actually gives you a lot of good ideas for doing it, for doing machine learning, classification, regression, clustering. They have it all broken up into really nice modules with lots of functionality. Everything is there for you. The wonderful scikit learn. Amazing, amazing package to work with. All right. We already did that. Let me just... Okay. The comma machine learning steps in Python package. We're going to separate the data into train and withheld testing data. We're going to instantiate the machine, set the hyper parameters at that point. In other words, we make the machine and we tell scikit learn the hyper parameters right then. We go ahead and train the model with the training data using those hyper parameters. We had to tell it the hyper parameters or it couldn't done the training. They were going to make predictions with the model at testing data locations and we check model performance. There is functionality in scikit learn to do every one of these steps. In fact, the code to do this is shorter than my description. There's a line for every single one of these things in the code in scikit learn. Okay. Typical models or construction. Construction is automated over a range of hyper parameters for tuning. We typically automate those steps. In fact, we can do k-fold cross-validations, really cool on multi-processors, HPC kind of stuff, high power computing. We can do that very efficiently in scikit learn. We will not do that. We will do it by hand so you get experience. I think it's a good way to get experience. Okay. Let's step through the steps and the code. Then we're going to look at code. There is going to be repetition here again. This is all about trying to build up those neurons or train your neurons so you get thinking this way. The train and test split, one line of code. All you have to do is use the function built into the scikit learn library called train test split. You give it the predictor features as a data frame. You give it the response feature. Today we will use one response. Many machine learning methodologies can use multiple responses. We're just using one for simplicity. We'll only use two predictors so we can visualize the entire problem. These are the inputs the predictor features in a data frame. These are the response features right here or response feature for us. This is the test size. 20% is being with help. You get to decide on that. You could actually change that if you like. And we use random state. Random state is a random number seed. Why do I want to use a random number seed? A seed number. Anyone? We're exactly. We get realizations of the data. Would we all have the same result? We would all have a different training data set, a different testing data set. And then when we look at the results, everybody would be like, my model doesn't look like yours. And so I did that. So we would all have the exact same result. We're forcing it into determinism. We don't want to necessarily have a bunch of different training data sets for us to talk about it together. Now, Kfold Cross Validation would in fact try to use different iterations of the data for a trained test to test across all of the possible samples. We're not going to do that today. Okay, so the inputs and we have the outputs. So what's really cool here is look at what train test split does. And this is something really important. Python functionality, the functions allow you to have more than one output. The commas are used to separate the outputs. When you run this code, you put in one data frame for the inputs, the predictor features, one data frame for the outputs, the response features. And it gives you the input data frame train input data frame test output train output test. So it gives you all of them separated. How does it do it? How does it separate them? Random completely random. You see that now there is a discussion we should have at some point about fair test train. Can I just do mind if I just kind of like see the idea since we're talking about random number seeds. Can I see the idea in your mind? If I had a bunch of well data. Okay, so can we all visualize this a three dimensional volume of well data? What is the spacing of the data in the vertical direction of the vertical wells? Well logs. What's the spacing of a well log? The many reservoirs, if it's conventional, we could be talking about half a kilometer or a kilometer miles, like really wide apart, right? Now when I do random random withholding of 20% or 10% of my data for test, what is the furthest distance you think you're going to be extrapolating for the test? Imagine only 10% of the data randomly withheld. How far away am I extrapolating or making a spatial estimate? On average. Is it going to be kilometers? Do you think it's likely that most of the time you're going to be just taking out one data point and you have data, half a foot, half a foot on either side of you? Only 10% withheld, 20% withheld. Most of the time you're going to have data within a foot or half a foot or two feet away from you. You see that? Now where do you want to use your model? Do you want to use your model to make predictions half a foot away vertically? Most of the time when we use the model, the subsurface, where are we making predictions? Half a foot away? Horizontally, yes. So most of the time your model is going to be used to make predictions hundreds of meters if not kilometers, horizontally, and you've tested it at making predictions half a foot vertically. You see the problem there? Let me see. Okay. So we're all geoscientist engineers type. Let's brainstorm. You guys are now my research team. How would you do a fair test train split in that case? What would you want to do? Martin suggested we remove the entire well. You could take away a lot vertically. You could take out an entire unit vertically. You guys are thinking exactly this is the path we should be on. We should be thinking not random with holding. And so what are you doing? When you do train tests, you're simulating the real world use of the model. You want the test to be as hard as the real world use of the model. That's your goal in life as an expert in machine learning. Everybody cool with that? Okay. But today we will just do random. We're just going to do random, but we're going to understand the limitation of it. Okay. Good. Any questions or comments around train test? No, we're good. So if you look at this right here just to explain it, what I did in the code as I said, give me the length of that data frame, which is the number of samples. Give me the length of this data frame. That's number samples. And then I report it out. I print number of training, the number of training data, number of tests, number of test data, just to show the you guys when you're running, how much data you have, which is 20% of the test and 80% for training. Okay. Now this is the entire workflow in Python for building a machine and making predictions at testing locations and then reporting the error in testing. This is the entire workflow right here. Now let's go ahead and walk through this and you'll see it's not too bad at all. And then we'll go through code with it. Now I put it all in one block of code so we could just hit run once. I don't want anyone to get an RSI having to click, click, click, click, because we're going to iterate. I want to encourage iteration to learn the machine. Okay. First thing we do from scikit learn package module tree import tree. Is the all of the functionality in that module to be able to build a decision tree, which is one of the models we're going to work with today. So step number one, instantiate the model. Now remember part of my job is to give you terminology that you can use at dinner parties and at work that'll sound really cool. Instantiate the model, which literally in programming speak means make an example or an object from that class. Okay. You're going to make that. So tree decision tree regressor and you see the parentheses. Those are the hyper parameters for decision tree. And so you're telling it make a decision tree regressor. Now regression just means it's a continuous output classification means it's a categorical output. We're going to do regression. We're going to work with regressors today. Okay. Everybody good with instantiation? Set the hyper parameters. This is where you do it. Remember, if you don't know the hyper parameters, what are you going to do? What's the trick? What can we do to find all the hyper parameters? Is don't start to complicated start start with something simple work your way up. Don't go straight to complexity. That's it too, right? So what you can do is put your cursor right there in the brackets, shift tab and you'll see all of the possible hyper parameters. And importantly, you'll see the default settings and usually default settings are going to be complicated. So in other words, don't limit the complexity of the machine. It's up to you. You got to rain the machine in. Okay. Thank you very much for that. I appreciate that. Okay. Then you're going to fit the data to fit the model on the training data. It should be fit the model on the training data. You now have a decision tree regressor. You made it with that hyper parameter. And then what you're going to do is you're going to fit it. The fit command takes the inputs, which are all of the trained values. Now there's some weirdness here. And I'm sorry, this is a Python thing. Really what we want to pass in is training data for the predictor features, training data for the response features. And what's really, really funny is that the way it works with Python, it gets kind of mad for some reason when you do the fit. You actually have to do a little bit of a reshape on it in order to get this to work for all possible cases. Don't worry about it. This is a little bit of details. This values is turning this data frame information into an NDE array. And the reshape is just kind of explicitly stating the columns and the rows, the dimensionality so that it's clear for the inputs into scikit-learns. So it's just us, in fact, this code right here is allowing you to run this in the case for which you have one predictor feature. And the problem is if you try to run with just one predictor feature, it'll get confused because a data frame of one feature kind of gets weird. It becomes a series instead of a data frame. And so we're just fixing that. OK, so this is the inputs of the predictor features training. These are the response features in training. And this command is now training that model. OK, then what we do is we want to visualize the model. I made a function that solves the model over a grid of values over a range of all of the predictor feature combinations so we can map the model. We can see how it behaves over all possible values of the predictor features. And so I made a function to do that. Then what we can do is we can predict what the model specifically at the test locations. These right here are the testing data withheld. And what we get are the predictions at the testing locations. And then what we can do is we can use a built-in function within scikit-learn to calculate the R squared. The variance explained from the model by comparing the testing what we got when we're with the testing data that was withheld the truth at the testing locations versus what our model would predict. So this is we're going to get error from this. And then we can go ahead and calculate residuals. We can calculate a histogram. We can look at the distribution of the error. Now, besides some kind of minor details, this is workflow makes sense to everybody. Not too bad. Actually, with what we've already discussed, we're very close to understanding almost all of that. But we can definitely understand the workflow. That's our goal today. OK. Now, what does it look like? We're going to build four models. You're getting four for the price of one today. We're having a four for one sale. OK. And so the models we're going to build include multiliner regression. My advice to you is that whenever you build a machine, build multiliner regression. Because I know within Chevron, every time we introduce a new technology, we're concerned with incremental value. What's the value of the new technology versus old technology? And I'll tell you what, if you build a model of machine learning and you get an R-squared of 80%. And I was your team leader. I would say do multiliner regression. And if you do multiliner regression and you get 78%, which model am I going to use? Am I going to use the convolutional generative adversarial network that got 80% of the data? And when multiliner regression got 78%, no, stick with something highly interpretable that everyone can understand, right? And we know it's going to have low model variance. We know it's going to be more stable. OK. So, boy, that was me ranting again. We can minimize the error with the training day with respect to a linear combination of the predictor features. That's multiliner regression. It's a parametric model. We assume everything's just linear. The hyperparameters, what's the hyperparameters in linear regression? None. There's no hyperparameters. There's no control of complexity. Now, if you're bothered by that, if you say, well, there should always be a hyperparameter, then my response to you is use ridge regression. Has anybody ever used ridge regression? Oh, oh, oh, today's a good day. You're learning about something new again. OK. So ridge regression is a form of linear regression where they add a regularization parameter to the optimization, to the loss function. And that parameter is basically saying, I want the coefficients to be as low as possible. And you can set that as a hyperparameter, awaiting on the impact of having a larger magnitude of the coefficients. So if your line has a slope like this and you change that parameter, it will cause the line to flat note. You get a simpler model that's less fit to the data. It's a hyperparameter. OK. Low parameterization, superinterpretable. OK, let's interpret linear regression right now. I got acoustic impedance and brittleness and I'm making predictions of production and unconventional. This measure right here is gas production and MCF PDs. OK. This is production of gas MCF PDs. OK. Which one of these features has the most impact on production according to this multiliner regression model? Can we do it? How much does production change as I change acoustic impedance? Just imagine traveling a constant brittleness line. Yeah, I see vertical lines. Yeah. How much does production change as I traverse brittleness? You see that? It's a little counterintuitive at first, right? But once you see it that way, think of traversing the lines, changing one thing at a time, and you'll see very quickly you can interpret this model to be highly sensitive to acoustic impedance and not so sensitive to brittleness. Now, one comment I'll make is that's an artifact of the fact that brittleness, look at brittleness, how it's behaving. Look at the training data. See we got low production, low production, high production. Can anyone else see that? In the training data, what's going on brittleness? Lies brittleness suffering with the linear regression model. That's what brittleness is doing. It's a non-linear, non-monotonic, complicated relationship. You see that? And the linear regression model just can't fit it. Okay? Any comments or questions about linear regression? Okay, so this is, and remember I showed you a function where we visualize the model. We visualize all of the possible predictions over a range of the predictor features, AI and brittleness. That's exactly what I'm talking about right there. Okay, we're going to, that's what we're going to do. We're going to build that map for every one of the models. Okay, next model. Who here has ever built a can yours neighbor machine learning, machine learning based approach or model? Anybody ever done can yours neighbors? K you've done it? Nice. Okay, let me ask you. Do you like it? What? Wait, wait, wait, okay. You've got, now you've got to defend that. Sorry, no pressure, but why don't you like can yours neighbors? If you don't mind. Now when you have very sparse data, linear regression works pretty good because it's a simple parametric model with very few parameters. It has let me ask you, I'm putting you on the spot now, is can yours neighbors parametric or non-parametric? Okay, anyone? Any parametric or non-parametric? Oh, let me explain it first. I'm so sorry, I'm jumping ahead of myself. Let me explain it and then we can decide. Okay, it's analogous to spatial interpolation with can yours neighbors in the predictor feature space. Okay, who here is built an interpolation model at some point in their lives? Creeging, convolution, anything like that. Okay, this is a mapping algorithm. In fact, can yours neighbors is completely analogous to a convolution or moving window based average map. If you took the data and just moved a window around, it's the same kind of idea. What we're going to do is we're going to take the data, the training data, in the predictor feature space. You see that? We got bird on this, we got acoustic impedance. I'm going to go to every single location when I want to make an estimate. I'm going to look around myself, a search, a spiral search. I'm going to find the K nearest training data and I'm going to take the average of them. And that'll be my estimate. Or I could take a weighted average based on distance away from the point. Okay, so I'm going to spiral search. So if you think about it, that's the same thing as a mapping method. I'm doing a mapping interpolation in the predictor feature space. A range of acoustic impedance, the range of brittleness. So everybody who's done mapping realized that a lot of machine learning is just mapping in the predictor feature space, not Euclidean space. Questions or comments? Okay, so you get to pick K. Let's see how K is a hyperparameter. If I'm going to this location right here and I want to make a prediction of the production given acoustic impedance of a boat, a normalized 0.5 and bird on this normalized about 0.85, what would happen if I use a very small number for K? I only consider the very nearest neighbor, maybe K equals 1. Okay, now imagine what would happen if I use K equals to a very large number. And I take the average of those values. Which one of those models will be overfit and which one will be underfit to the training data, which will be more flexible, which will be more complicated, which will be more flexible complicated, which will be less flexible, less complicated. Sorry. If you take all of the data and take the average at every single location, we would just estimate with the average of the entire training data set, the average production at all observed samples, training samples. That would be completely underfit. That would be variance explained zero. You don't explain any of the variance within the problem. If you use K equals to one, it's a nearest neighbor estimator. You would be precisely accurate at the training data. At the training data locations, you would pick with zero error, the training value, and away from it, you'd use the same value. That's completely overfit. K, we turn that dial for a very small K, we have an overfit model for a very large K, we have a very underfit model, and we get the tune everything in between. Any questions, comments about K nearest neighbors? This brings up a couple of good comments. First of all, to answer your question, if you have a very small sample set, you run into the problem that K was already mentioning. That is, she had a negative experience with K nearest neighbors, because she had a very sparse data set. It's the same problem as trying to build a spatial map, data driven only with no geology concepts when you have very few wells. You've got two wells, five wells. It's so hard to do anything. You have to bring in geologic concepts, and this is just data driven. The choice of K is different than the choice of a radius. Many nearest neighbor approaches don't use K that use the distance to which you want to average. Okay? Now, K nearest neighbors doesn't do that. There is no radius. It's K the number of data, which means if I'm in a position where I have very dense samples, how far away do I look in order to find K equals five samples? I don't exactly reach, well, I heard that. You don't go very far. I read your lips. That was pretty cool. You don't go very far at all. Okay? Now, if you're out here and you need five nearest samples, you're going to go really far away. What does that mean? It means K nearest neighbors is locally adaptive. In locations of dense data, it's going to have more details. In locations of sparse data, it's going to be smooth, more of a smooth model. Okay? So that's a very interesting thing. K is locally adaptive, the K nearest neighbors approach. Okay? Any questions, comments about that? I pick this as our second machine because it's analogous to mapping. And many or scientists, many engineers are familiar with the mapping idea. And the hyperparameter of K is very intuitive once you get over that hurdle. The initial hurdle of understanding it. Okay? So I think it's a good one. Now, let me bring up a question. It is a low, the model is highly interpretable because it's mapping. But it is a non-parametric model. It really is a non-parametric model. In fact, this is a unique case of machine learning. It's known as lazy learning. Has anybody ever heard of lazy learning before? Okay? Let me ask you a question. Okay? So you've built 100% the data. Do you guys see that this machine learning approach actually is not the model is the data and the K? In fact, I don't calculate the model until I ask the model for a prediction. You see that? I have to say, okay, I want to know what the production is for acoustic impedance of point two and a birdleness of point two. Then I have to go to the machine and say, okay, now give that to me. And it does it by going to the data and looking for nearest neighbors and waiting by distance or something like that. It does all of that at the moment. Easy learner means that there's no really fit model. It just has a hyperparameter and the data and every single time you ask for a prediction, it goes back to the data. Does that make sense, everybody? Kind of cool, I think that's kind of cool. All right. What else? I think there was something else I want to mention about can yours neighbors. I think I'll, I may remember later. If acoustic impedance has units that go from 2000 to say 6000, which is commonly the case, depending how you report acoustic impedance and whatever units, can you imagine if we now brittleness is measured as a ratio as something going from zero to one or from zero to 100, what would be the impact on this model? If these acoustic impedance has a range of 3000 possible for its values, and this has a range of one, when I do distance calculations in this predictor feature space, what would happen? What would happen is every time I go to a location, I'm looking around myself spiraling out and looking for the nearest data. I have to calculate distance. If these units go from zero, this goes from 2000 to 6000, like it originally did, and this goes from zero to one, this whole problem is effectively a flat align. The aspect ratio just flatens down to a line. What would happen is I would get weird striations because I would basically be only considering acoustic impedance, a brittleness range would mean nothing. When we do anything that considers a distance calculation, we have to standardize the features so that they have the same range so that we don't create anz-tropy in the solution, artificial anz-tropy. That's something we're going to have to do. The question that Rachel asked about, if both of them are describing variability components, do we want to consider them? When we talk about feature ranking, we will shortly, we don't want to use the two features if they're highly correlated with each other. If we were in a situation where the training data fall on a line like this, in other words, acoustic impedance and brittleness are almost the same as each other. They're linearly related at a high correlation coefficient. That is not a good idea. That's what's known as multi-colonarity or colonarity. If your predictor features have that, it means that they don't describe independent components of the system. You really do want for a stable model that you want them to do that. The main thing I would look for, Rachel, is I want to make sure that the training data spans this space that the two features are not highly correlated and collapsed on top of each other. That's the main thing. Besides that, I want to integrate these interesting shapes and forms, these complicated interactions between brittleness and AI. It's very powerful to do that. Our next model is decision tree. Who has built a decision tree before? It is a greedy binary hierarchical segmentation of the predictor features space. All we're going to do is start with everything in one region. We're going to estimate with the average of that region. In other words, the very first model for decision tree is every one of the data, the training data in one region, and estimate at all locations with the average of all of the responses. It would be the underfit case of estimate with the average over the entire response, everywhere the same value. Then what you do is you look at the whole thing as a region and you cut it, and it's greedy. You're going to take the very best cut so that you reduce the error. When you cut, if I cut, I'm going to cut just across one feature. I pick a threshold I cut into multiple regions, two regions, and I use the average of the response in each one of those two regions. Then I do another greedy cut in a region, do another greedy cut. I have the whole thing in one region, do a cut. Then I have two regions, do a cut. Then I have three regions, do a cut, and I keep cutting every time I cut, I add a new region. When I'm done, I've got a model like this. In decision trees, we're going to play around so we can actually visualize that. The result is that the more cuts you do, the more complicated the model, and so you get a choice. You get to choose hyperparameters that control the complexity of your decision tree. We're going to play with that. It's a low parameterization, a very compact model. Let me ask you, is it a lazy learner? Nope. It's an eager learner. I'm saying that when I'm done with my decision tree, I actually have a model that I can give Ben. That model doesn't need the data anymore. I can take the data throw it away. Don't do that. I can just give Ben the model. The model will just be a bunch of nested if statements that tell you about all of the threshold cuts to be able to get to each one of the regions and then predict with the average at each region. That's a decision tree. We'll play around with decision trees. When we have the full course on machine learning, if those of you who are sticking around for the longer course, we will go into nuts and bolts and much more details with the methods. Who here is used to support vector machine? Also known as support vector machines, plural. Here's the idea with a support vector machine. Support vector machine is a linear model, okay? But here's the trick. You use a kernel transform to transform the entire problem into a high dimensional space. Then you fit a linear model in that high dimensional space and you project it back to the original space. When you do that, you get a non-linear model. So it was a linear model like imagine, this problem is a two-dimensional problem, right? It's, we got a acoustic impedance and birdleness. It's just two dimensions. We projected into a 100 dimensional space and fit a 100 dimensional hyperplane and we projected back down into low dimensional space and that's what it looks like. Now if anybody here is filling like this sounds like a sci-fi movie, yeah it does. It really does, doesn't it? Sounds kind of groovy, doesn't it? Now I'm going to blow your minds. It turns out there's a kernel trick and the kernel trick works like this. You can solve this problem without ever going to the high dimensional space. You never actually have to calculate the values there. You just need to know their distances between each other in the high dimensional space and you can do that the math works out. Now here's what makes that really cool. You can project to an infinite dimensional space and solve this problem. Anybody having that experience? I can project to an infinite like infinite dimension and project back again to this dimension and it works. Okay. So anyway, that was cool. So it's a little bit of an advanced technique but don't worry. It can be very simple. The polynomial kernel would allow us to take this original problem in two dimensions and projected to three dimensions or four dimensions, not infinity. Don't worry, we're not going to do that. Or you could use a radial basis function to project it to another dimension. It can be very intuitive. It's not too bad. Hyper parameters. Now what's really cool about support vector machines and that's not a spelling mistake, they call it machines because you can plug and play the kernel. You take a kernel, you can put a linear kernel in and you would just get the same thing. It would be a linear model. You can put a polynomial kernel in. You can put in a radial basis function. You could put something more complicated. Every time you do it, you get a totally different machine. That's why they call it support vector machines. Okay, now I'm going to have only one more point because this is getting real sci-fi now. I don't want to go too far down this path. The reason we like support vector machines is because they're very good with noisy data. In fact, support vector machines are very good. Have you ever done a classification problem where you got the data classes overlapping each other? They're noisy? It turns out that it's very good at being able to deal with noisy data like that where the classes do not separate very well. We'll talk about that more in the advanced class. Let's go back to our good old friend, K nearest neighbors. I think we can all agree K nearest neighbors has the most interpretable hyperparameter so far. I've explained it the best. It's really on me. If we put K nearest neighbors with K equal to all of the data, the estimate at every single location, K told us it would be just the global mean. In that case, the response feature predictions have zero variance. Oh, this does raise a point. You remember how we talked yesterday about estimation versus simulation? Simulation and geostatistical modeling of reservoirs tries to reproduce the full variability of the problem. The machines we're using are more estimation machines. They're not trying. They don't build in the idea of getting all the variability back. OK, so what do we learn? What's the takeaways? Multiliner regression, very simple parametric model, extremely easy to understand. We got to start there. I invite you to every time you build a machine, build a multiliner regression machine, just see how you would perform with the most simple interpretable model. If you want hyperparameters, try ridge regression, try last soul. One of my students, or last soul, depends how you pronounce that. One of my students did a really nice example demonstrating the value of ridge regression just recently. I could send that workflow. It's super cool. Canier's neighbors, this idea of analogous mapping, like a nearest moving window average, decision tree, this idea of separating or segmenting support vector machines. Let's build some machines.
 So let's just go ahead and go to the launcher again when you find that by the left hand file browser window, the plus sign will bring up the launcher and then go ahead and open up a Python 3 notebook. So the first thing we're going to talk about is functions in Python. The reason writing functions is so important and in our case all of the exercises that we go forward will ask you to basically put the complete code into a function. And you know the reason for that is because functions are kind of small programs in and of themselves in a sense that they take some inputs and produce an output. And if you have something that takes some inputs and produces an output, then you can test that, right? You can test it to make sure it works correctly. So like when I showed you earlier all those automated tests in NumPy, this is essentially what they're doing, right? They're they have some function that you feed in a set of arguments to and it produces some output. And so you basically test those for, you know, obviously not only, you know, producing the correct numerical values when that's the expectation, but also part of testing is testing for failures and edge cases. So if someone were to put in some funky arguments, you'd want your, you know, properly a well-written code would handle that in some way, would warn the user that that's not a value, you know, valid argument or input or, you know, it's outside the bounds of the valid range. And so, you know, a lot of tests with, with tests those kind of funky or edge cases are other things. Those are called unit tests. And you're basically testing, testing individual functions. And there's a, there's like a philosophical concept and software engineering that relates to this, forget what the exact, I'm afraid I'm going to misstate it, but it's sort of the separation of responsibilities of the single responsibility principle. That's what I was looking for. So it's kind of a philosophical thing and software engineering where the states that you should basically, every function that you write should have one well-defined single responsibility. That doesn't mean it's a single line of code. It just means that the function should do one well-defined thing, you know, take some arguments, manipulate them in some way and produce some output. And, and so really when you write much larger programs, you should do it by just collecting a bunch of small functions that all individually have a single responsibility. This is how you write high quality, maintainable code, okay? So functions are very, very important and we're going to essentially write and test all of our code from function. So it's important that you have an understanding of the different types of functions and function arguments, okay? So in Python, and I know I covered, you know, at least one type of function, very basically when we just talked about Jupyter notebooks on Monday, but in Python, most functions start with the keyword def. So we're going to define a function. And so in this case, we'll define a function, say my add. My add was going to take two arguments, a and b, and it's going to return just the sum of those two arguments. So then I can call this function with numbers, right? Just putting in two numbers, and then it will produce a result. So make sure you execute the cell where you you know, hit shift enter on the cell where you write your function definition. You won't get any visual feedback. I mean, if there's an error, if there's a syntax error in the code, it would tell you about that. But you're not going to get any visual feedback to tell you that, you know, the correctly written function is registered in memory. That can be an issue, right? So if you were to restart the kernel in the Jupyter notebook and try to execute the second line, you're going to get an error that my add is not defined. That's because I haven't executed the first, right? So if I execute the first and then the second, I get, you know, the expected result. So this is this is a standard function in Python. In this case, a and b are referred to as positional arguments, meaning the first position, you know, let me change the, so when I call the function with two and three, the two is always associated with a, and the three is always associated with b, right? So I, you know, in this case, addition is commutative. So if I swap them, it won't matter. But you know, in a different function, it would matter. Like for example, if we said, you know, my subtract, and then we call that, it is going to give us a different answer if we swap these, right? So these are called positional arguments, meaning that they're, they're position in the argument list matters. Okay? The first one will always be associated with a, the second one will be associated with b. There's another kind of argument, and that is a called a keyword argument. So in this case, what I'm going to do is copy my add function down here. And I'm going to change b to be a keyword argument, meaning I'm going to set it equal to something. And whatever it's equal to is going to be its default value. So let's not do zero. Let's, let's say b equals two. Um, so if I run this, I can actually then call this function with only a single argument. So if I omit b, I just call this function with a single argument, single positional argument, able to take on that value and b will be automatically defined to be two. And that's the result will be produced. Now I can, you know, change b to be equal to something else. If I want to, but in general, I could also leave it off. So these are called positional arguments or, or default. And the thing about positional arguments is they can actually, you can change the order of it. So let's say I take my my subtract function and I make both arguments positional. In this case, you know, of course, I could call my subtract without any arguments at all. It's going to just take on the default values and produce two minus one. I can also change the values of both. Right. And additionally, I can change the order of the fun of the arguments. Right. So I could put b before a in this case. And it produces the, the correct result. Any questions? I can also have a variable number of arguments. So in this case, I'll take my add, let's change that to, let's just, we can just write another function. We'll just say my sum in this case takes a variable number of arguments. And what are we going to do? We're going to say that my sum, we're going to initialize it to zero. And then we're going to say four i in I guess I should have, I made a mistake there and then I didn't put a star. So if you have a variable number of arguments and what I, it's best described what I mean by variable by execution the function, you'll see that in a second. But if I want a variable number of arguments, I need the asterisk in front of it. So if I, if I have that, then I can write a function like this that's going to add up and return my sum. All right. So basically, whatever I put in the argument list will be added up in this for loop summed into a variable my sum. And then my sum will be returned at the end of the function. Okay. So if we execute that, then we can call my sum with a variable number of arguments. So we call it with one two, we get three, call it with one two 22. So it doesn't matter how many arguments I put in there. They're going to be, they're going to be brought into the function as a list, the Python list, we'll talk more about lists in a second. They're going to be brought into the function as a Python list and then I can iterate over that list, summing the entries of the list up and then returning that value. Okay. So that is a, those are, those are actually called variable position, our positional arguments. There's an additionally something called a variable keyword arguments, which would be given with, I think the best, we'll just do something very simple to demonstrate this. So my variable keyword args and then we'll put two asterisks and then just say keyword args. And then in this case, we're just going to return keyword args so that we see what happens. So in this case, we're just going to take the arguments and return them. And we'll see what happens here as I input variable number of keyword arguments. So a is equal to two, b is equal to two, hello, c is equal to a list. What happens is whatever num, the variable number of arguments that I put, put there are stored as a dictionary in Python. So this is a Python dictionary. It takes keyword value, right? So a is two is assigned to a, hello is assigned to b, and this list one three is assigned to c. Again, if you're not familiar with those data structures, lists and dictionaries, we'll cover them momentarily. The last thing I want to say about functions or the last type of function we're going to talk about is something called a lambda function. So lambda functions are most useful in kind of two scenarios. First of all, it has to be a really short function, one that's like a one liner. And then they're most useful when they're defined inside of other functions. So it wouldn't be, it wouldn't be idiomatic Python to have inside of a function to have another function definition. I'm not saying you can't do it. You know, you absolutely can do this. But in this case, the function doesn't do anything. But it wouldn't be idiomatic Python because every time you call my sum, you're redefining this new function inside of it. So then the next thing is a lambda function. A lambda function again would be a scenario like maybe like this where you need for whatever reason to define a function inside of another one would be one use case. And the other use case is in a scenario where the function and we'll actually see this later in the class where the function is actually passed into another function. So the function is an argument to another function. Now I can show an example of that here in just a second. So the way you define a lambda function is with the lambda keyword. And then the independent variable in the function is just x. And then it's, you know, it's just a syntax. You say f equals lambda x colon. And then you write the function like, for example, say x squared. And so the way I would call this then is just f2. We should produce four, right? We're squaring it. F4 should produce 16. We're squaring it. You can have, you can also have multiple positional or keyword arguments. Right? So you can say x plus y squared, for example. And in this case, I can call it with a single function and y will default to two. So now I have x takes on the value four and I square it. That's 16. The default value of y is two and I square it. So that's four and I add the two together. Four plus 16 is 20. Right? Produced the right result. I mentioned you can have, you know, a use case for lambda functions is when you want to feed them into another argument, right? So another function. So imagine you had a function that is, you know, called my weird sum. That takes us the first argument of function. And then returns the value of that function at a number plus another number. Right? So in this case, the argument is itself a function that will get passed here evaluated and then we add four to it. So what we can do then is call this my weird function, my weird sum, first by inputting a lambda function as the first argument or the only argument. In this case, let's say, we'll do it again. So we'll say x squared. Right? So what this is going to do is evaluate x squared where x is three. Right? So that should be nine plus four. Right? So 13 should be the output of this function. And it is, right? But then if I wanted to say, call the function again in a different place in my code, but this time I'm going to instead of squaring the function, I'm going to cube it. I get a different answer. So that would be another use case for lambda functions. As arguments, when functions are required as arguments, and we'll see this use case in a real, you know, in a meaningful example later in a class. And then in the other time would be to avoid this kind of thing, defining a function inside of another function. Okay? Any questions about functions and Python? So let me just glance at this to make sure. Okay? So the next thing I want to talk about is data structures, lists, two pulls and dictionaries. Okay? So the syntax for a list, a list is a collection of items in Python. And the syntax uses square brackets. So we often think, you know, for numerical computing, we're engineers, scientists, we can think about numbers. We can store a group of numbers in a list. Okay? We can also store mixed, you know, so in this example, these are all integers, but I could store an integer, a floating point number, and a string all in a list. I can even store a list as one of the items in the list. So in this case, what I'll do is, as I'll store, you know, a different integer, a different floating point number. And then the list why our place is the last argument. And so then if we look at what Z is, it has an item, an integer floating point number and the third thing in the list is why? Okay? Another list. And so you can have like these nested lists and whatnot. We can access items in the list via their numerical index. Python is zero index, meaning things start at zero. So the index is, you know, starts zero, one, two. So for example, can somebody tell me what the zero index of Z is? Two, right? Two, right? So zero, two, right? One would be 3.14 and two would be the list. Okay? But then I can even index into that list. So for example, if I wanted to get a string from it, then that would be the second, you know, zero, one, two, the second thing in that list, zero, one, two, the two index thing, and I get a string. Turn out you can even index into strings. For example, if I just say the zero in first, you know, thing in a string is a. So those are lists and lists support reassignment. So if I wanted to change, if I wanted to directly change a string to something else, like a number, I could do it this way. And now if I go and look at what Z is, it's been changed. In fact, not only is Z been changed, but so is Y. Because if you remember, I defined Z to have Y in it, right? So even though I did the indexing this way on Z, because this Y was just a reference, or you know, this entry in Z is just a reference to Y. In other words, it's not a copy of this thing in memory. Again, in Z, it's nothing more than just a pointer to the address and memory of where Y was stored. So when I index into it this way, I'm changing the original version, right? I'm changing Y. Is that clear to everyone? This is called pass by reference semantics. And it's, you know, if you come from like C or C plus plus, we would understand this. If you come from MATLAB, this would cause you a lot of headaches. Because you probably used to reassign your variables in your inside functions and other things in your code. And in MATLAB that makes a copy. But that big, that copy, especially when you're talking about, you know, large arrays of data, slows down the code, big time, making those copies. So in this case, you know, we're not making a copy. We're just, you know, it's just the address to the original location and memory. And so it's an automatic reassignment. And in both, in this case, it changes both Z and Y. Or, you know, actually, factually, it changes Y. And Z just contains a reference to Y. So in, so lists support this kind of indexing. And we can do something, you know, in that case, I was doubly, you know, using this double index to get a string. But, you know, you can just say, I wanted to change the first value of X to, you know, a floating point number. I could. So there was the original X there. If you understand them well, it's a great thing because you can write very efficient memory efficient code. And it's going to be fast. But if you don't understand it, then yeah, it can cause problems. So there's another type of data structure that's very similar to a list, um, called a tuple. And, and, and so here we'll just, um, I guess, I'll use A, B, and C instead of X, Y, and Z. But tuples, instead of using square brackets, use parentheses. And then again, the entries in the tuple are, um, separated by commas. So again, and, and they can store variable data types as well. So I can have an integer, a floating point number, and a string, and a list, and even another tuple. Right. And I can index into that in the same way, right, that I did with a list. You can also, there's this indexing that allows you to count from the end. So minus one means one from the end. Minus two means two from the end, right. So in this case, tuple, and the, and the list there, you can also, in both lists and tuples, you can have what are called slices. So the, the syntax here is that we're going to go from a value to another, from an index to another index. So for example, if I say, um, go from zero to the fourth value, it's going to take zero one to, it's going to go zero one to three. It's going to return those things. It will exclude the fourth index zero one two three. So that's called a slice. That kind of syntax. You can also add an additional term to the slice that will indicate taking steps. So what this says is go zero one two three in steps of two. So it's going to take the first one, skip the second, and then take the third one, skip the fourth, and then it ends. Okay. So you might ask, well, so far, everything you've shown me about tuples, I can do with a list. There's what's the difference. The difference is with respect to reassignment. So the way I was able to reassign a list, you'll notice that if I try to do it to a tuple, I get a syntax here. It tells me what's wrong. The tuple object does not support item assignment. It, uh, yeah. So, you know, if you have things that you want to, um, the memory footprint of either one of them is about the same. So if you have things that you know, you don't want to ever change the value of, you're going to store them in a tuple for read only access, if you will, then a tuple is a better, better choice than a list. A list would be a choice from something that you wanted to, to, to access. Both lists and tuples supports, uh, what we call unpacking. So let me have a, uh, uh, let me just take a subset of this tuple and I can do this kind of syntax. So this is called unpacking a tuple. So I'm, I'm taking the value of one will be assigned to a, the value 3.14 will be assigned to be and a string will be assigned to c. So I can execute that and then we look at a, b, c. So it's an efficient way to unpack a tuple. Um, let's see. Hey, John, it's, it's, when thinking about tuples versus list, um, like trying to make it tangible in terms of when you'd use one, would tuples be like, if we're, if we're looking at a bunch of production data for, you know, say a bunch of wells, right? Things that you wouldn't want to change are some of the basic metadata associated with it, like APIs or something like that. Is that the right way to think of what, where you'd use tuples as the form of like unchanging, like an unchanging list? Is that? Yeah, I would just say, I would just say anytime you want read only, you want to store a collection of things that are going to be read only. Got it. You're not going to ever try to rewrite, you're never going to try to write to them. Whereas, you know, list, and I didn't, um, let me create another list. You know, we can also, uh, append the things to a list, right? So this is, this is a common use case for a list is where we want to append something to the list, right? Something like that. We, we, we also list support things like sorting, right? So we can, you know, if it makes sense, if, if they're all numbers, then we can call all sort on a list. And it will sort the numbers. So that would be another case, you know, use lists, you know, not necessarily when you want to write to them, but when you want to do these kind of operations, if you're going to, if you're going to add things to the list or you're going to sort the list, those would be also use cases for lists versus tuples. Got it. Both of them support um, both of them support, well, I tell you what, let's, let's, let's talk about dictionaries real quick. And I'll go on to the next point. I was going to say is both of them support iter, you know, iterating over them. But in order to iterate over a list, they need to write a for loop. And that's the next topic I want to discuss. So, uh, let's, let's discuss dictionaries real quick. And then we'll come back to that. Perfect. Um, so dictionary in Python uses, uh, curly braces. And dictionary is a collection of keyword value. Right. So the keyword is always a string, well, it's not always a string, but usually it's a string like something, you know, A, and then it's value. C, and C could be another dictionary. Something like that. So here's our dictionary. And then we can extract things from the dictionary via their keyword. So this makes for really nice readable code, right? Because it, you know, if you're, if you're indexing, if you're indexing into lists, you have to do it via their numerical index. And that's not always as obvious, um, as, you know, using the keyword, right? You know, it's, it's actual label. And later we'll learn about pandas, uh, as a, as a toolbox for data analysis. And it, it uses, uh, sort of under the hood uses Python dictionaries. So, um, yeah, I guess in just the last example, we can, uh, look at C, right? And so we can index into this dictionary as well. So the C keyword corresponds to a value that's another dictionary. And we can index into it as well to get, you know, say the value to, if we wanted to. You can also add items to, to a dictionary in the, in the, with a nice syntax. So if we want to add an item, say D to a dictionary, then I just do it like this. So my dictionary just grew by D, right? So I think the use cases for these things, uh, will become more apparent as we, as we move along into class and we use them more. So the next thing I wanted to talk about was, you know, kind of, uh, looping and flow control structures. Both, well, all of these things can be looped over the dictionaries that take, take a little bit of extra effort, which I'll discuss. But, you know, a list, you know, so up there above, I define the list, right? The list can be looped over just, you know, four i in a list. And then say print i, right? So that's just going to print all the values individually from the list. A tuple can be looped over in the same way. There's some additional things. So in this case, i takes on the actual value of the item in the list. A lot of times we also want the numerical index. So for example, um, well, I'll use a different syntax here. I'll say for i, comma, item in a numerate a list. So what a numerate does is it returns the values in a list into item. And it returns the index, you know, the numerical index starting from zero into i. And so in this case, if I print i and then followed by item, we should see what see what happens there. In this case, you know, so the index zero corresponds to one, the index one corresponds to two, the index two corresponds to four. So that, and again, this would work the identically if if this was a tuple instead of a list, there's no change there. For dictionaries, you have to specify what, you know, if you want to, if you want to iterate over the keywords, the values or the arguments, so, or the, or both. So for a dictionary, if you wanted to iterate over it, what you would say is for item in a dict, and then, and then you have to specify, say keys. So if I do this and print item, it's going to print the keys for a dict. So ABC and D. If I'd rather iterate over the values, I can do that by changing this to values. So in this case, now it iterates over the values, or if I want both of them, right, in that case, I can use the similar syntax to like key value and change this to items. And then in that case, I can print key and print value. So key A corresponds to the value one, the key B corresponds to the value 3.14, the key C corresponds to the dictionary, and the key D corresponds to four. Any questions? So far. So up to now, you know, particularly if you, if you took the code academy course or a data camp or something, you know, you probably haven't learned anything new. And depending on how far you went into those modules, I don't know how, how deeply you got an object or any programming, but that's what we're going to talk about next. And this is really, you know, again, if you're coming from another language, this might be the part that you're, you're, you're not familiar with.
 The way we define, so object-oriented programming, you know, the idea behind it is you have classes which upon instantiation produce objects. And then the objects have data associated with them, we call them attributes, and they also have functions that operate on that data. We call those member functions or class functions, something like that. Depending on the language background, like if you come from Java, they're definitely called member functions. I think Python is just called functions. So let's define again, I think the best way to learn this is to see it in action. So let's define a function, I mean a class that will call well. And all classes, well, not all classes, but usually a class will have a special member function called the knit. And it's underscore underscore, a knit underscore underscore. And then it always has the first argument, any class member function always has the first argument itself. Talk about what that means in a second. But this is a function that is automatically run upon class instantiation. So in this case, what we're going to do is we're going to have one attribute that we'll call well type. And in this case, we're going to set it to none. Talk about why in a second. And that's all the init function is going to do. So we'll just return from there. There's another special function that we can define that is underscore underscore STR. And short for string underscore underscore again, first argument self. And what this does is it is this function is called when you call print on an object. And we'll, so we'll see that in action in just a second. So in this case, what we want to do with this is we want to return the string that we'll call that we'll just say I have the well type of sorry. So this syntax, if you're not familiar, this is a string. And these curly braces here indicate that this is a placeholder for text that I'm going to replace with this format command. In this case, well type, or actually should be self dot well type. So anytime you're referring to a class attribute, you have to put the self in front of it. Okay. So this, you know, basically, if we, and we'll see this in a second, but we instantiate this class, and we call, and we call print on the class, and it's going to print, I have the well type of, in this case, none, which is not that interesting, but we'll make it more interesting in a second. So let's execute that cell and then define the object. Right. So we'll store it in a variable that will just call a well, and we instantiate the class like that. And then like I said, there's one attribute that was assigned upon class instantiation, we can access it with this dot syntax. So well type. Well, it doesn't return anything because it's none. In fact, let's make this this string none. So instead of, instead of having it, none is a, is a special keyword. It's like an empty object in Python. So instead of doing that, let's assign this to the string none. Okay. And so now if we run, if we run all three of those lines again, it, you know, it reports that the well type is none. And if I call print on the object, the object is A well, then it prints out, I have a well type of none. Okay. So again, in this case, well type is the class, is A class attribute, and string is this underscore and score string is a special member function that operates on the data, the data of the attribute being well type, it operates on it by simply using it to create this string. Now here's where things become more interesting. And this is where the utility of object oriented program becomes. We're going to define another class. This time, we're going to call it vertical well. And what we're going to do with vertical well is we're going to have it inherit from well. So in the class definition, when we define the name, we open the parentheses and we, in here, we give it as an argument, the name of our original class. Now, when we, when we inherit from another class, the well becomes the parent class of vertical well, and we inherit all of its member functions and attributes, unless we override them. So I can define this class with nothing in it. And then I can actually instant instantiated. So if I say V well equals vertical well, and then print V well, I have, apparently this class does nothing, but because I inherit from well, it does have some features. So let's make it more interesting. I'm going to override the parent class as a knit statement to find a new one. And in this case, I'm going to set self well type to vertical and return. Now, when I execute that in the next cell, look what happens. I did not rewrite the member function string. I didn't need to because it's already defined. And then, so by simply redefining well type to be vertical, I automatically get, you know, the behavior I want when I call print on it. Okay. Now let's, let's define another function, another member function inside vertical well. We'll call this function compute well, volume. So in addition to writing this function, which we'll return to in just a second, we want to add arguments to the instantiation of vertical well, like for example, depth. We can even give it a default value if we want, but it's also something we can override upon class instantiation, and I'll show that in a second. So what we'll do then is define a new class attribute, depth, which is equal to the argument that you input, depth. And then we can use that in compute well volume to actually compute the well volume. Compute well volume will take an argument diameter. We'll make it a positional argument, which means we have to use it. And then from there, you know, we can compute the diameter, right? So the diameter squared over four times, we have to have a value of pi, right? We can import math, import the math library, and that gives us the ability to use math dot pi for pi. And so yeah, diameter squared over four times pi, that gives us the area across sectional area, and then we can just multiply that by self dot depth, and that gives us a well volume. And now I can instantiate this class with a different depth, for example, just so that I don't use the keyword, and we'll use V well, and then we'll then we'll call compute well volume with, you know, some value. Everybody okay up to there? Let you get caught up. Okay, let's keep doing. We're going to find a new class. I will call horizontal well, and we're going to have it inherit from vertical well. And the sort of vocabulary that we all from think about here is that the horizontal well is a vertical well, right? Me, even if it's one inch, you have to drill down a little bit before you can drill horizontal, otherwise it's not a well, right? It's tight on the surface of the earth, right? So anyway, the analogy may not be perfect here, but you know, a vertical well, I mean, our horizontal well is a vertical well, and a vertical well is a well. So that's the way we think of these things, when you have these inheritance trees. And so again, if we define horizontal well with nothing else, then we can still, you know, even though we haven't defined anything, we can still do things with this. Like, for example, we can instantiate it. And we can print horizontal well, but we're going to get the wrong result here, because it is inheriting this information, because I didn't redefine the init statement. So that's what I'll do next. In this case, I'll define the init statement of horizontal well to have two arguments. So a depth and a length, meaning the length of the horizontal component, right? We'll think of the well as just a perfect L shape, just for purposes of demonstration here. So then we can define the attributes. So self well type equals horizontal. So well type, I'm sorry, self depth equals depth, self length equals length. And so now if I just do this and instantiate and print, again, without having to redefine string, right? So the inheritors tree is two levels deep now, because I didn't redefine string and vertical well, horizontal well inherits the original definition. And it works. So the idea is you have your most abstract classes at the top, and you have a couple like fundamental functions and things that those do. And then you derive a class and you add more functions and you derive a class and you add more functions. And eventually you can get really complicated behavior in your sort of most, your deepest, you know, further you go down the tree, because it inherits all of that other features and attributes, member functions and attributes from the class of the bow. And the idea is that you write less code ultimate. So the other thing about these is you can have something that you know, we call like polymorphism. And so I can redefine and I'll just to be quick, I'll just copy this. And then so we can redefine compute well volume in horizontal well. And in here we just have to add depth plus length, right, to give us the volume of the horizontal well. And so then if I instantiate this class now with giving it a depth of say 10,000 and a length of 10,000. And then I call this member function. Again, I have to instantiate, you have to execute the cell where you define a class and then execute it where you instantiate the object and call the member function and you get the result. And this is called this is called polymorphism, right. So it's another kind of key ingredient of object oriented programming. And the good part of it is, or you know, these compute well volume, again, just as a lesson in software engineering code quality, I mean, use variable names and function names that are meaningful. You know, don't use X and Z. I know I've used them here in the short demonstration, but when you're writing real code, use variable names that are meaningful, that self-document. A well written code in Python needs very little comments. If you use variable names that are sort of, they mean what they are, right. And you know, you might complain as well, I don't want to type that much. Well, you need to, then the next thing is to use a proper code editor and use tab completion, because you don't actually have to type all this. In fact, even here, the jubber notebook will tab complete this. So if I just type another this to be unique and I hit tab, I'm not going to do it for some reason. That should be working. I'm not sure why. But a proper editor, a code editor would definitely allow you to do tab completion there. And so that's what you know, you can have long variable names and they won't cost you any productivity, because you're just going to hit tab after you type a few letters. And if you do that, I'm going to get a chat here. So the question is, is H well not defined to execute that first line? That is true. But I had already executed this one before, so it should have been defined. And therefore, I was thinking that compute well volume should have been defined already. I had already executed this line before. I tried to demonstrate that tab completion. So I'm not sure why that wasn't working. But anyway, you know, so in this case, you have a function name that basically is named what it does. Why would you call it anything else? You know, what's the alternative here? You have two functions, one in the vertical well class called compute vertical well volume. And then in one in the horizontal well class called compute horizontal well volume. Well, they both compute the volume. Why not just call it compute well volume? And they, but they change their behavior based on the object that they're operating on, right? Because I instantiated H well as a horizontal well, then it knows to call this function that's assigned to the horizontal well, including the depth and the length in the computation. So that this is called, you know, it's one form of polymorphism. We have polymorphism is this idea that you can have function names that operate differently depending on their arguments. And you can think of H well as the first argument, you know, that it's kind of like the self there, right? It's the H well as the first argument to this function. So that's called polymorphism. And John, would you agree that like object oriented programming and kind of doing it in this way helps it helps you build workflows that are a bit more modular and more straightforward to troubleshoot rather than kind of like a long list of, you know, code that's hard to debug, et cetera. Do you mind diving into that a little bit? That's certainly the philosophy behind it. Now you can get carried away with it. You know, the philosophy behind it is that because I can have these inheritance trees and, you know, again, usually the further down you get, you know, you're inheriting and using functions from the from the classes above you in the tree. And so since you've already written those functions and you've already tested them, you don't need to write and test them again. So you write less code and the code quality goes up because, you know, you don't have to worry about testing code, or you know, you write less code, you write fewer tests. That's sort of the idea behind it. Now you can get really carried away with it. You know, I would advise that you try not to have ultra deep inheritance trees. You know, like if you have objects inheriting from objects inheriting objects inheriting from objects inheriting from objects inheriting from objects inheriting from objects, you're like seven levels deep. Then you actually, you know, it can make it harder to debug because you have to go back up the tree so far to find out, you know, what the problem is. So I think, you know, two, three, maybe four levels deep, no problem. But if you start to get like seven or eight, you might think rethink your implementation. And then there's another sort of danger. And that is most object-oriented language is allowed you to have something called multiple inheritance. So instead of inheriting from one thing, I could actually inherit it from multiple things. And when you get into that world, you know, the complexity goes up, which means, you know, the debugging gets more difficult as well when you start to run the problem. So you use sparsely, it's, it does all of those things that you said. It makes things easier. You know, you write less code. It's more modular code. It's easier to debug and everything else. But when taking to the extreme, it can cause the opposite effect, particularly in the debug. I guess the analogy for most of the Excel users out there is like not building such a complicated one-cell function, right? But breaking it down over some separate lines and kind of going your work and making a bit more modular rather than having, you know, 80 addition subtraction exponential functions within one cell, right? Just breaking it out a little bit more and then following that logic as you go. Yeah, to some extent, that's true. And it goes back to what I mentioned earlier, that single responsibility principle, right? You should have a whole, your code should really be a collection of a bunch of small functions, each of which have a single responsibility. Like for example, I mean, this, right, this function has a single responsibility, right? And you don't even have to dig into it, really. Just look at the name and tells you what it does. Yeah. Cool. Yeah. So that's object-oriented programming. We will use it in the course project and this course and a couple other places. So you'll see it again. And those examples are complicated enough. You know, perhaps this, my made-up contrived example here with the wells is not complicated enough to show you the real utility. But those examples that I'll, that will, you know, as we move forward in the class, they are complicated enough that hopefully you'll, you'll see the utility. Okay? So there are a couple of exercises here. One on flow control. I guess I didn't mention that when I was talking about four loops and I guess I should do that. I think if you take, if you're taking the Python prerequisites, this should be fairly obvious. But just real quickly, let me just say, you know, when I talk about loops, I didn't really talk about if statements. So four i in range 10, then if you know, if I wanted to basically control the flow somehow, I could say, say, for example, four i greater than five print i. So in this case, it's only going to print the last five things. For i between two and five. What does it like that? There. Sorry about the, the greater than less than but, but nevertheless, there. So here, this is just using, in a loop, using the if statements as flow control. And, and you know, you can have else, if else statements, right? So if i equals seven print. I'll say, oh, if you can say else, print. So you can use these if statements as flow control, right? Again, that's fairly easy syntax in Python. If you have any programming experience, it should be straightforward. So.
 But the declusturing methodology is the one that we can analytically or work with numerically and demonstrate it. And so let's just talk about this data set right here. This is the data set that we're going to do, demonstrate declusturing on for the slide deck. This is the data set that we work with in the code. And what we have here is we have the wells are sampled, irregular spacing. But then if you look in the background, you can see the color is representing a truth model. And so if we could see the entire reservoir, in other words, if we could see the entire population using the terminology we covered yesterday, that's what we would see. We would see that this is worse porosity here, low porosity, 4 to 6 to maybe 8%. And then this area of the reservoir is higher porosity, more in the order of maybe 11 through 16% porosity. And if you look at the wells, what you'll notice is that there's some locations which are missing wells. It's a regular grid. It should be somewhat represented because the regular sampling, but then we're missing samples specifically around the low areas of the reservoir. Now the question is, can we use those wells in order to calculate the average porosity for this entire 1000 by 1000 meter area of the reservoir? And so we need to do something to adjust the statistics to be representative of the volume. We can't just take the average of those numbers because we're pretty sure if we look back, I miss some low values, I would expect myself to be biased high because of that. And so we got to count for that. Okay, and what we're going to do is we'll employ declustering. Now what declustering does is really kind of cool and very simple, and it plugs into our workflows very, very well. Because it takes what we originally had was a data set with numbers at each location, one through end locations. When we perform declustering, we get another column. But other column is going to be the weights, and there's going to be one weight per data value. So we have one column of data, and now we have two columns, the data and their weights. Now the weights are going to be specifically standardized so that they will have all be greater than zero. It doesn't make sense to give zero weight to anything. You know, all data is important. And they're going to sum to end. In other words, if I take all the weights and add them up, their sum is equal to n. In other words, if you had a data set for which every data point had equal weight, they would all have a weight of one. And the sum of that would be the number of data. In fact, when you calculate a statistic without considering weights, you are effectively assuming a weight of one on every data equal weight. Okay. Now that's what declustering does. There's a devising methodology we could spend some time on, which is actually really, really useful and powerful. That's the case, and I mentioned this on Tuesday, that's the case where we are missing part of the distribution. There are cases where we just completely fail to sample part of the distribution. If that's the case, then we really need to use a devising method to impute or to replace or estimate part of the missing distribution. We won't show that numerically today. That's another one of those topics we could cover in more detail if you'd like in another session. Okay. So let's go do devising. We've got our data set. You can see all of the data here. You've got the distribution of the data, and that distribution is right here. That's the histogram of all of that data. This distribution right here is the distribution of all of the truth model. In other words, this is the model parameter. It's the population parameter, the entire histogram of the population. Okay. So this is truth, and this is what we have in the sample set. Now if you look at it, I hope you can see it shifted, that we, the mode is shifted a little bit. It looks like it's sampling the high values more than the low values. There's a problem here. Now I think numbers are more important than just ocular inspection. So if we look at that, what we'll see is the truth mean, the mean of this distribution of prerosities over the truth model is 10% prerosities. The sample mean is 10.5%. The error is 5%. So let me ask you a question. If you overestimate the oil in place by 5%, is that important on a project? Would you all agree? The types of margins we have in many of our projects, I've seen many projects where 5, 10% inflation in reserves or oil in place would have actually been impactful on the decision. And if not the go or no go decision, it could have impacted facilities and planning and contracts and all the other stuff that happens with the field. We do a better job. I know we all like good news, but overestimating and then finding out it's worse is usually worse and even underestimating and finding out it's better is lost opportunity. So it's always better to be more accurate. And I'm preaching to the choir as we say in Texas, I'm sure. So we can all agree that that 5% is probably significant. I've had many examples like this and I'll show some words 5, 10, 15, even 20%. And so 20% overestimation is getting pretty severe. Okay, so these methodology that we're going to employ, I showed this to you guys last time. Actually, does anybody remember it wants to take over and explain this slide for me? I'll take a break. Anybody want to explain how we calculate the weights with cell-based declustering? I'll jump in and help out if you like. Burve data divided by number cells. That was perfect him. Thank you so much. In order to make it so that the weights are not just 1 1 1 3rd, 1 1 1 half based on number data, but that they sum to n and that their weight of 1 would be like a nominal weight where you're neither clustered too many data too close together and you're neither anti-clustered. In other words, you're kind of sparsely sample. Weight of 1 would be nominal weight. In fact, if everything was equally spaced everywhere, everything would have a weight of 1. Yeah, that's exactly it. So we can put the mesh down, count the number of data in each cell and we can get the weight based on this equation right here, 1 weight, just as you said to him. Now you also mentioned this issue about the cell size. Now that's important. Before we go there, let's talk about the location of the mesh. I think just looking at this example, we can see this data value got a weight of 1, but it was a little bit lucky because we could have moved the mesh up just a little bit and it would have been sharing this cell with this other data might have got 1 1 3rd, 1 3rd weight or something. The origin moving around will change the weight if we move that cell around. So what we're going to do is we're using it. We're what 80% engineers will do an engineering solution. You just simply perturb the location of the mesh 10 times, 20 times, calculate the weights every time, average the weights that removes the sensitivity to the origin to the location of this mesh. It won't matter anymore. And now let's, we also have to think about cell size and we did the thought experiment. Do you guys remember if I make the mesh very, very big so all of the data going 1 cell what's the weight? Okay, I saw everybody's doing it. I love it. I love it about being able to see people is there's good nonverbal communication going on here. So a lot of people are like, yeah, one, one, they're all going to have a weight of one. And if you make the cells very small so they all fit in their own cell, all of the data will also once again have a weight of one. So we're going to, we have to get the cell size. Now I will show you a plot later that'll explain how do we pick the right cell size. How do we get that the best we can. But if we apply this methodology a cell based declustering, you can look at the data mapped in space and it's actually quite intuitive to look at or educational to look at the weights assigned to the data locations. So these values are not the actual feature values. They're the data declustering weights. And if you look at it where you have dense data, the data is nice and dense. It's really close together. In those locations, we've weights that are less than one because we have redundancy between the data. It's not representative of as much area or volume of the reservoir. In locations where we have sparser data like here, here, and here, they get greater weight because they're more representative. They're representative kind of a larger area. And they're less redundant. And so they get a weight greater than one. We can look at the distribution of weights. If you employ declustering, I always recommend you look at the distribution of weights. Because the thing you'd be concerned about is you expect the weights to be between like 0.5, 0.2, maybe up to two. But if you've got a distribution where suddenly you've got some weights of 50 or 40 or 60 or something like that, I would be concerned and I would look at the data set. What's likely happening is you probably are mixing multiple data sets together where you have a cluster of data and you have one data way off by itself. And it doesn't, that's not a rational way to treat declustering. You would probably want to do good geologic mapping, segmentation of the problem. Remember everything we do, we never divorce ourselves of the good geoscience and engineering knowledge. We don't want to just use the methods blindly. Okay, less than one, reduce the weight, greater than one, increase the weight. Now what's interesting to make this very clear, what is happening when I change the weight? Let's look at a histogram together. Now just a recall right here, a histogram is a bar chart. You've been the data set with a bunch of intervals. In this case, maybe 5.8 to 5.5 or something like, I mean, sorry, 5.8 to 6.5 or something like that. And then what you do is you, in that interval, you count the number of data. This axis on a histogram is just frequency. If you look really carefully, this is 1, 2, 3, 4 and every one of these bars comes up to an integer amount. Because in each bar, you have a number of data. It's a frequency, right? So I know I have one data in this bin. I have three data, five data, four data, four data, and I could go all the way across and do that, right? Now that's because the data all have equal weight. They all have a weight of one when you calculate a histogram of your regular data. Now when you do declusturing, all of the data have weight. So you saw how this data point here in this bin had a weight of one. Now look at it right now. I can tell you it had a weight of 2.2, probably 2.2 or something like that. You see I'm eyeballing it. But that one data all we did was increase its weight and now the histogram bar goes higher. Now in this location right here on the higher side of the distribution, they got lower weights because we over sampled the highs. And so if you compare from here to here, the bars have fallen. The data in the bars, whereas there was six data, now there's only equivalent of about four, about five data or so because they all had fractional weight probably less than one. Okay. And you can see there's changes here, here, many of the bars have dropped. So that shift up on the low side down on the high side or vice versa, up on the low side down on the high down on the high side has resulted in the entire mode, the entire mean of the distribution to shift lower. Now every statistic will be adjusted by a weight. Now what's really cool is the calculating and average is very simple. We all know how to do that. That's just a summation of the data values and divide by the number of values, right? What's cool is this equation right here is how you calculate a declustered mean. And the way you do it is you just sum the weights multiplied by the data values and you divide by the sum of the weights. And if the sum of the weights is equal to the number of data like we said it would be, it's just divide by n again. Now if you think about it, this weighting, this is a weighted average now. And so the declustering approach is just using a weighting on the statistic. Now I don't show it, but you could very easily calculate a weighted standard deviation. You could do weighted 50% or 90% you could do weighted kurtosis and skew any statistic can be weighted. Okay. So the entire CDF or PDF can be weighted and adjusted. Okay. And that's what we'll do. We'll adjust the distribution. Now when we look at what we did with the declustering, the true mean was 10%. The sample mean was 10.5%. The error was 5% high. Our declustered mean is 10.07. We're only about 0.7% high now. So we have mitigated the problem. We have reduced the error up quite a bit. We have significantly knocked it down. Now we're down to just 1% error. That's not too big of a deal now. We can handle that. Okay. Let's just mention about how we get to cell size. We mentioned if we were to look at the cell saw at the very small cell size, what we'll get back is the actual naive mean. And I apologize. This plot is from a different data set. So the mean is not true mean was 10% in our data set. And the declustered mean was 10.5%. The naive mean we're not we're not looking at the same data set, but this is ill straight up. Okay. The naive mean would be 22% for this data set. As we increase the cell size, we grow it larger and larger. What'll happen is the declustered mean will drop. Now if you were working with a different feature that was clustered in the low values. So it's biased low. This this plot would flip upside down. That as you went ahead and increase the cell size, the declustered mean would go up. Then what will happen is the cell size gets larger and larger and larger. It will slowly start to creep up to the naive mean. So you start naive right here. Let me just change my pointer. You start naive, you go down, you minimize the declustered mean and you come back up. The engineering solution because we know the sample set is biased high is to take conservatively the cell size that minimizes the declustered mean. Now I should say that there are other methods to get the best cell size. And one of them would be if you looked at the data set and you saw a nominal spacing, you could use that nominal spacing to try to figure out the best cell size. And that's an advanced topic that we could talk about or we could try to demonstrate. Okay.
 Now remember when we do Jupiter notebooks in this Jupiter lab, what we can do is we can put lots of documentation. And so you notice we put a list, itemizing lists and so forth. It's all very easy to do. Enumeration, make really nice formatted stuff. And we just explain that we want to work with an unconventional data set. It's not going to be a really simple data set now. It's going to be unconventional multi-variate spatial data set. And we're going to be able to do a prediction of production. Now, spoiler alert, it's the same data set we showed in the slides. So it's going to be something you're already seen. We're going to do instantiation, make the model with the hyperparameter specified. We're going to do fitting of the model, which is fitting in scikit-learn is training. I wish they would have called it train, but they call it fit. And prediction is where we're going to make predictions at testing locations. And the cross-validation is we're going to calculate the error in the predictions at the withheld testing locations. We're going to work with linear regression, multi-linear regression, decision tree regression, K nearest neighbors is there too, to I believe. Okay, we should have that. Let me just check. No, I'm sure it's there. I'll just trust it's there. Okay, so now let's go ahead and we're going to okay, let's start let's move nice and slowly through this and we're going to walk through. Now the first thing we can do, can we go to the kernel and restart the kernel and clear all the outputs? I want to make sure that none of us have something already run or we're already visualizing it. Let's make it like the cooking show where we don't have the pie already cooked. Okay, so restart the kernel and now we're together on the same page. Everybody's done that? Okay. Okay, good. I saw a bunch of nodding thumbs up. Thank you, my call. All right, so now let's import some packages. Remember the world of brilliance is about to come down on us and help us get the job done. OS, if you were working locally and you want to change the working directory and be able to load your data from a certain directory, save it to a different directory, you'd want to use OS to change the directory. We're going to use kind of an advanced methodology called intake to load things in. Intake is actually John Foster. Dr. Foster will talk about this when he does his data science class. If you take that class and it's all about how you do feature engineering, really nice automated feature engineering, the data could come from the cloud, the data could come locally, the data could be any format and you can set that up so that now anybody can use your data without even thinking about that. You can put all of your, you know, kind of your transformations, data cleaning, you can put that in intake. Psychic learn, we have a module called model selection and they have a function called train test split. When we import it, now we can access that directly, train test split, just like we did. Psychic learn, we're going to import SVM, that's the important vector machine. We're going to go ahead and import tree and we're going to import the mean squared error, the r squared values and so forth. Okay, so we can go ahead and build our machines now. Okay, now what we're going to do is we're going to set up a couple of convenience functions. The convenience functions include this function right here to visualize the model. Not a big deal. What we pass into this function is the actual model. The test features, the predictor test features, the response test features, we can, oh, sorry, predictor test feature, number one, predictor test feature, number two for the x and y axes and the response right here in the title of the plot. What this is going to do, it's going to loop over a bunch of combinations on a mesh, make the estimates and give you a nice plot so you can see the response surface effectively of your model. Okay, now we can go ahead and run that block of code and now we have the functions. Now I don't know if I ran that because always forget if I run things. Okay, so we ran that, we're good, we're good to go. We can go ahead and we can run that too and now we have those convenience functions. So really we build those functions so our code can be very simple. Now we just type visualize model and we visualize the model instead of having to have all this code right there in our workflow. We can load the data using intake. Intentake is going to load our unconventional multivariate data set and it'll give us a data frame and we don't even care. There's no nothing we have to take care of because it's all done for us. In fact, it's loading the data from the cloud, which is pretty cool. Now remember, no news is good news. It looked like it worked, but let's always test and check to see if it in fact did load. So the best way to do that is we could preview the data set. Here's our data set right here. We have features, well index, porosity, log transform of permeability to attempt to linearize it, acoustic impedance, brittleness as a percentage or TOC, vitro-night reflectance is a measure of maturity of the rock and production information in MCF PDs. It's a gas rated production. Okay, so we can go ahead and do a little bit of cleanup with the data. The first thing is for everything we do, I don't want well index. I don't want to calculate any statistics on well index. I don't want to use these numbers. They're not helping me. So if I wanted to, I could go ahead and say my data, I lock is a slicing command. Give me all of the samples, but give me only the second because it indexes from zero. So one is the second through eight features. So if I do that, I'm going to extract only these features. I'll remove well index and I overwrite my my data, my data frame. Then I can describe the statistics of that. Let's go ahead and run that code. Do you notice we get a table with all of the statistics and the table, in fact, the table does not have any case of well index. Well index has been removed from it. Okay, so next we're going to do a little feature engineering. We're going to look at the features and we're going to remove any negative values. Why are we doing that? Because if you look, we got negative, brittleness, negative TOC, clearly something went wrong with the data. Now, truncation to remove negative values should probably be should have as a prerequisite, some type of investigation, determination, what went wrong, try to figure it out. I'm just showing basic manipulation here, just an example. When I run this line of code, it's going to go ahead and extract all of the numerical data as a two dimensional numpy and the array. Then I can use numpy functionality like this, which says find all of the values less than zero in that array and set them equal to zero. This is a truncation command command command in numpy, which is really amazing because it's very, very fast, it's very efficient and it's very compact as numpy is incredible. You can do a lot with very short commands. Then we rerun and calculate the summary statistics and you noticed that brittleness and TOC now have minimum values as zero. Okay, any questions or comments guys? Are we good? Are we together so far? Okay, smiling, that's good. I like to see smiling, that's good. Now we're going to go ahead. I want to do something kind of cool and I'll just spoil it for you. We're going to run this workflow by picking two features and we're going to build models. We're going to talk through the whole thing and afterwards you got a job to do because you got to work, right? And your job is you're going to pick your own two features and you're going to beat me. You got to build a better model than I did. Okay, and you're going to set your own hyper parameters and you're going to and we're going to talk about it. Don't worry, but you're going to be on your own a little bit, right? Can't just be me walking through it. So what I did was I made an array of the minimum and maximum values for all of the predictor features. I did that so that we can now run this and visualize for any combination of features and we'll get good color bars, well, good ranges on all of the plots. It'll make sense. So these are just the minimum and maximum values for porosity, for log permeability, for AI, all the way up to production. And by doing that, we can I can now just access those cells, those indices from those lists. These are just lists so that I can then use those to be able to automate everything afterwards. So I just put that in as a list. If you wanted to, you could change these values and you would see a different range of possible acoustic impedances or something. I thought they were reasonable. One of my, so one thing we can do is we can calculate the correlation matrix. Now, anybody here ever calculated correlation matrix when you have a multivariate problem? Anyone? I see that Martin. I saw that nod. That counts. Yes, and Megan, you've done that before. What do you do? If anybody want to provide some suggestions of what we're going to look for, what does this mean? The correlation matrix is going to tell me for each feature, porosity through production, porosity through production, the correlation across each one of them. The diagonal, it's a pairwise correlation. The diagonal is the correlation between porosity and porosity, permeability, log permeability, log permeability, acoustic impedance, acoustic impedance. So they're all one because you're correlating something with itself. Now, when I look through this matrix, I can quickly see that log permeability and porosity are highly correlated. 0.81. You see that? A very strong linear correlation. You can also look across here and you can see the fact that we have very high correlation, pretty high correlation between porosity and production, because this is production down here. Log permeability is pretty good. Negative correlation for acoustic impedance, that makes sense. And we have the fifth predictor feature, which is Toc, total organic carbon. And I'm kidding around, of course, and you're right, I should have jumped to that first, is what I did with my convenience function called plot core. I took the data set and I produced this figure right here, which is color. And the color goes between one is yellow, and deep purple is negative one, negative correlation. The yellow, because I have perfect correlation between the data and themselves. Okay. So let me ask you this, productions down here, what's the best correlated with production? That's important. Prosthetic is number one for sure. What about what would be do we have something that has a good strong negative correlation? Can you see something that's kind of getting me? Yeah, AI has not a bad correlation, right? What about brittleness? How good is the correlation of brittleness? That teal color, where where is that in the color bar here? Pretty close to zero, right? Why is that? Near zero, right, Megan? That's 100% right there. And it's because brittleness is a very non-linear. Negative correlation, anti-correlation. In other words, negative correlation would mean you have a greater likelihood of having a low production when you have a high value for the predictor feature. So an inverse relation, as Sejad said, that's perfect, Megan said anti-correlation. So this idea that if I have high, if I have high acoustic components, I have low production. If I have low acoustic components, I have high production. But guess what? If you have negative point, negative one correlation, that's perfect information. That's a great feature to work with. It's just negatively correlated, which is perfectly okay. That's fine. Okay. So we were doing feature selection. Let's put our feature selection heads hats on right now. Which features would you pick just by looking at correlation between production and the features? If I just focus on this part of the table, which features would you think about working with? Roughly? I'm a zander. I'm right there with you. Proceedies, one of my talks. Irmability. I log permeability. Let's say we want to work with porosity because that's high correlation, but we also want to work with log permeability. What's the degree of correlation between them? It's fascinating because what partial correlation does is it calculates the correlation between porosity and production while trying to remove the influence of all the other features. Isn't that cool? So partial correlation is actually giving you the correlation with production unique to each one of the features. Is that awesome? Now, it's got its limitations. It's super awesome. I have some examples, something we cover in the longer workshop. But this is something really cool, is that that's part of the problem right here. Is that porosity looks really well correlated production. So does permeability, but they share a lot of that. And if we did the partial correlation, they would both drop because they were both sharing. They're kind of a crutch for each other. You got to remove them, separate them from each other. Okay, so these are just some thoughts about features selection. I don't expect expertise, but in a short moment, like in about 15, 20 minutes, I'm going to tell you to pick your own features. Think about the correlation with production. Think about the limitation of correlation. That correlation doesn't do a good job with brittleness, even though brittleness was really good. It's just not seeing it because it's not monotonic and linear in its relationship. Now another thing you can do is anybody ever built a scatter matrix or matrix scatter plot. Anybody ever done that before? These take about 10 seconds to run. Ooh, that was as fast as we're having a really sunny day and the cloud is very sunny today. All right, so this is a matrix scatter plot. Who here has done this before? You got to love Python. This is built directly into pandas. Pandas has pandas plot as a module. And it includes a whole bunch of visualization of your data. And this is just one line of code. I just give it the data. Anybody use Seaborne package before? Seaborne has some incredible plotting for these things. It can do like kernel dance the estimates and really nice contouring and it's really, really nice. But this is simple. So now when you're picking features, you can look at the features as a bunch of two variable at a time, cross plots and see how they're related to each other. Do you see what's going on with log permeability and log and porosity? Log permeability and porosity are super well correlated to each other, right? And that correlation is very linear, right? And these are just every possible combination of two variable at a time scatter plots. Okay, now what's really cool is that the one to one, the processing the prostate would just be a 45 degree line. So instead, what we do is we just show the histogram. That's what you commonly do with these types of plots, right? Now look at porosity to production. Look at that relationship. Is it linear? It's not linear. There's definitely some non-linearity to it. Is it, is it homoschedastic? As porosity changes, the variability of production increases. You see there's very little variability here. Lots of variability here. That's heteroschedastic. That's important. Here's brittleness, totally non-linear. You see that form? That's pretty cool. And all of them have different shapes and forms and behaviors. Okay, so we look at the matrix scatter plots. We're looking for things that are linearly related to each other. We're looking for homoschedasticity. We could even be looking for things like this. It might be a constraint relationship. There might be something that prevents low prosities from having high production. There's a physics constraint in the problem, right? This is all part. Now remember, 90% of the work is data preparation. So our machine learning, that's why I show a little bit of this is because we should be spending a lot more time on this than building the machines, right? So porosity, log perm, AI, brittleness, TOC, vitro-night reflectance. Okay, so right here, if I change this to porosity, I am effectively changing the model to be a model of porosity versus brittleness. If I change this to vitro-night reflectance, I'm now accuse to competing some vitro-night reflectance. Do you see that? And the whole workflow afterwards will automatically switch out the predictor features. I made it automate it. If you, when you do that, you create a list of predictor features that don't change the response feature. And this workflow will update. But let's all do it. Acoustic, impedance, and brittleness. They look like a pretty good idea. Want me to try that out? Okay. What it's going to do is this part right here is going to take the data frame and it's going to extract the predictor features and the response feature into brand new data frames that we can work with. That first step is just giving you a data. I like to do this. If I'm going to remove features, I like to remove them. And that way, don't make the mix up of accidentally using the wrong features. I like to do that. So I do that my workflow. I remove it. Now we're working with acoustic impedance and brittleness and we're predicting production as our response feature. Now the first thing we should do is we should test the statistics. Make sure we got that okay, we got a thousand samples of acoustic impedance, 1000 brittleness. Here's the statistics. And we can check the statistics of production, 1000 productions. Here's the statistics of production MCFPDs. Okay, so those are our response features. Now our first step is we're going to do our train test split. Train test split, 20% random number. We already saw this in the slide. We're going to create four brand new separate objects with the train test, train test, predictor features, response features, the x's and the y's. Let's go ahead and run that. We got 800 data and training, 200 data and testing. Looks pretty good. Everybody, are you, are we all together? Everybody good? Awesome. Thank you very much. And the x's are the inputs. Remember our equation of our machine is y equals f of x. So the inputs are the x's, the inputs are the predictor features. So the x's are the predictor features. And the y's are the response. This is acoustic impedance and brittleness. And this is production. Okay, everyone good? We're good with test train split, train test split, random selection. Okay, we know that. Okay, so now we should check it. Okay, let's check the summary statistics. Here's the summary statistics of train and test. This is a good idea because what would it, what would it mean if the maximum value of brittleness within the test? Let's say it was only 50 or 40. Would that be a fair test? When the training is going all the way up to 93, it would not be a fair test, these were not testing the full combination of possible brittleness. We're missing something in the test. You want to make sure your test is expanded over the entire data set. Now the other thing too is what if your training doesn't include the full range of the values? Well, then we're training the model not including training data from the entire range of the features. So this is what we're checking for right now. Now we'll do the same thing with production with the response. We make sure we got a reasonable range. We're doing pretty good. It looks pretty good. Now a better a better way to do this is to also look at the histograms. Oh, did I forget to run a block? I think I forgot to run a block. I think I forgot to run this. You see that? Make sure everybody runs every block. I made that mistake. Do you see how I got an error saying F min not is does not exist? Fm is not defined. It's because I forgot to run that block. Let me go ahead and run that again. I do that all the time. I get talking. Sorry guys. Okay, let's compare the histograms. Acoustic impedance, training and testing, brittleness, training and testing, production, training and testing. What you want to look for is make sure the distributions are reasonably similar. I don't want to be training with a completely different distribution than I'm testing. That would not be fair. And so you make sure the ranges and the distributions are somewhat similar. We've done pretty good. We have a good train test split. Now this is even a better idea. If you have a low-dimensional problem like we do, predictor feature one predictor feature two responses color, we can visualize the whole thing as a location plot. And so now we got the response as the color coding and the training data and the testing data right here. This is 800 samples, 200 samples, acoustic impedance and brittleness. Okay. How do you feel about that? Is that a fair train test split just thinking about the predictor feature space? It does look pretty good. I would agree we seem to have the same general range of combinations of predictor features covered. Now this is my suggestion. If we were doing this in production, this would be something you could do. You could plot the error in the model after you build the model at the locations of the data. And specifically, I'm concerned here and I'm concerned here because those testing locations are dramatically different than any of the training locations. You see that? That testing location is down here. You don't have any training samples very close to it. So in looking at the performance of my model, it's not a bad idea to consider how challenging of a prediction problem was it. And for this sample and this sample, you don't have training data nearby. Okay. Any comments, questions? We're good. Sorry. A little bit late on the uptake there. Are you concerned then for your testing data that you don't have some of those areas sample like up in the upper right? So this is interesting. Look at this. No testing. Look at this. We have training data. We actually do want to model out there. So Rachel, I would agree. We want to be cognizant of the difficulty and the degree to which we provided sufficient training. So the difficulty in the test and the amount of training available. So we want to look at that. What I like to do is I like to flag. I like to say, okay, in this case, I could actually go in and by hand change the change the train test split and say, no, I need some training up there. I want the model pinned up there, not kind of on its own. You could do things like that. I actually recommend we're thoughtful about train test split. We've just done random selection, random selection here. Okay. Good. Don't make it too easy. Don't make it too hard. That's kind of the main advice. Okay. Let's go ahead. We're going to start easy. Let's build two models, linear regression, one feature at a time and let's just take a look at them. Just to kind of get our feet wet as far as running scikit learn. We import from scikit learn package, the linear regression method, the function for linear regression. We instantiate the linear regression model, linear regression model and linear regression is how we instantiate it. Now, notice how many hyper parameters for linear regression. Thank you very much. There's zero parameters. So we don't put anything in there. And so this is our linear regression model. We're going to fit it. Now, notice what I did. I said zero index on the predictor features. I'm using our first predictor feature only. I'm throwing out the second predictor feature. So it's going to be just univariate. I'm only considering the one predictor feature at a time. And I go ahead and I fit that model. So I instantiate the model, no hyper parameters. I fit it. Fitting in scikit learn is training with training data. So I train with the training data. Notice that this is X train. And this is Y train. Okay. So I'm using just training data, no testing data from my split, right? Then I can go ahead and I'm going to make a bunch of, well, first of all, I'll make a bunch of predictions at the testing data locations. Oh, I'm going to make a bunch of predictions over a wide range of different values so we can visualize the model. I don't have the map. It's just univariate here. And then I can go ahead and make predictions at the no, no, no, wait, wait, I'm not doing the testing yet. So let's just do that. So I plot the training data. I plot the predictions I would make at a range of acoustic competences. And we're just going to plot that up and look at it like a scatter plot. Now the next step will do the testing. Sorry about that. I forgot that I separated the blocks. Okay. Any other questions? Good so far? Good. Okay. Now I broke up the step. Sorry, I forgot that. Now what I'll do is I'll make predictions at the testing locations, the locations for AI, which were in the testing with held testing. And these are going to be the predictions at the testing locations. And then I can calculate the score. The variants explained are squared for testing relative to the truth values. And I can go ahead and plot the model. And I'm going to plot the predictions I make at each locations along with the testing data. I'm going to calculate the error, the residual between my predictions and the actual true values. And I can show the histogram of the error. So let's go ahead and look at that. Okay. How did we do? Various explained 3% are we going to bonus this year? Are we going to be rank 1 this year with a model like that? And then look at this, are these are the predictions made at the testing data locations? And this is the error in the model. The error is really, really high. Variance explained, Skyler is R squared. Yes, they're equal to each other. Okay. R squared is variance explained. Okay. So now we have AI versus production testing data and the predictions we make a simple linear model. So I told you we're going to start out bad. Don't worry, we get better, we get better and you're going to do even better. So this is going to get even better. Let's build the other linear regression model. This one is the linear regression model between brittleness and production. Are things looking up? Is the world getting better? No, no, linear model is not working very well. And let's go ahead and do the testing on the model. Variance explained, negative 3%. Did you guys hear me? I did not miss speak. I'm not drinking too much coffee here. I said negative 3%. Anybody know what a negative R squared means? Did I just end the universe? Is the universe still outside? What does a negative R squared mean? It means you would have a better model if you use the global mean. It means you explain less their ability than if you had all the variability. You've inflated it. That's because this is so non-linear, it's just failing to work. We do better. The model just messed up and the error right here. So clearly, linear regression is not working univirately. When that first you don't succeed, just try the same thing but add dimensionality. Is that what we say? Let's go ahead and do that. Let's do multi-linear regression. Now, from now on, this is the way we're going to do it. We're going to do the exact same thing we did in the course notes. We're going to have all of the steps of one block. We're going to instantiate a linear regression model. We're going to fit or train the model with all of the predictor features AI and brutalness. And we're going to end with the response feature production. We're going to calculate the parameters and visualize the parameters when we can. We'll visualize the model as a map. And we're going to do predictions at the testing locations so that we can test our model. We're doing it by hand. Then we'll calculate the variance explained by taking our predictions and comparing it to the truth values at the withheld testing locations. And we'll plot the error residual. Let's run that block of code. There we go. Boom. One block of code, we built a multi-linear regression model. It's a machine fit training data. And we're giving it all of the data compared to what we did before. What we did before was we took the values. Let me see where it is. We took the values. And we said, give us only the predictor feature number two. Because remember, we zero index, right? So we told it, give us the training data, but give us only the column with the name, brittleness. Because the item one, which is the second item in that array or list, is brittleness. So we said only give us brittleness. Look at what we do down here. We say, just give us all of the training data. We don't say give us just one specific column of the training data. So that's a super cool thing about machine learning and scikit learn. If you provide it one predictor feature, it will build the model with just the one predictor feature. If you provide it two predictor features, it will work with two. If you provide it 100 predictor features, it'll do that. It's expandable to whatever dimensionality of your problem that you give it. Okay. Any questions, any comments? Are we good? Okay, let's interpret the model. We've already interpreted this model. It says, brittleness is meaningless somehow because nonlinearity. It says, acoustic impedance is the most important. And the error is right here. And the variance explained is 3%. Wait, wait, wait, what's going on here? What is this ringing a bell? When did we hear 3% variance explained before? We, brittleness is not helping us. Brittleness is so nonlinear, it works terribly by itself negative variance explained. And when we add it into the model, so let me ask you a question. Is it always better to add more features into your model? We make worse models when we don't carefully select features. Remember that. Anybody who comes to you and says, add everything and the kitchen sink and don't worry machine learning will figure it out for itself. They're wrong. They're wrong. Okay, now there are some machine learning methods that actually try to put feature engineering into them. That's not what we're talking about. We're talking about standard methods of machine learning. Okay. All right. Let's do K nearest neighbor. Now, we can go a little bit faster because we explain the steps and the steps will all just repeat, repeat, repeat three more times. Okay. But I will explain the differences. The first difference step zero. Wait, step zero. I'm starting to sound like Python. My first step is step zero now. Step zero is normalization, min max normalization of the features. I create brand new features, min max normalized. I have to do that because K nearest neighbors works with a distance calculation and acoustic impedance and what was the other one? Birdleness have totally different ranges. And so we got to we got to do a standardization or minimized normalization. This simply forces the values to go between zero and one. They have the same range, the same variance down. We're going to go ahead and instantiate our K nearest neighbor regressor. Isn't this beautiful? Look at how plug and play machine learning is. All I had to do was copy and paste the code and change this line. I got a brand new machine. Okay. Now, what are these right here? What's this? They are the hyper parameters. Thank you very much. So those are the hyper parameters. And so we set hyper parameters. The specifically K nearest neighbors is equal to five. That's my K right there. Now, there's some more complexity. The waiting window can be uniform for an average or it can be distance weighted. The P is the order of the distance calculation. The distance if two means that's Euclidean. If it's one, it's Manhattan distance. We don't we don't need to go into those details right now. Then we're going to go ahead and use the model build the model. It's the same thing we did before. Are you able to run that? Now, what do you guys think? Should we do a little bit of hyper parameter tuning together? Just to kind of get a warm up. Do you guys want to do that? Since you're going to be expected to do this, I got high expectations for this team today. So let's go ahead. Let's do some hyper parameter tuning. Very much explain 40%. Who's happy with that? So let's go ahead. Who's our expert machine learning? Who wants to tell us a good K? Improve that model. Are we underfit or overfit? What do you think? Are we missing important details? This is testing data. So these are the data not used to build the model. And this is how it's performing. Do you think we'll do better with over or should we try to make it more complex or more simple? Okay, I'll tell you what. Let's just take a guess. Somebody give me a number. Do you want to go 10? I'll tell you what, you guys can go a different number. It's fine. This is we're kind of going freeform at this point. I reran the model variance explained 42%. I'm on the right path. I'm on the right path. You see that we're getting more variance explained. Should we try a bigger number? Let's try a bigger number. Let's make it even more simple. Oh boy, 20. You could try something different. Variants explain 46%. You see what's happening here? We are now doing hyper parameter tuning by hand. We have a measure of quality of fit and testing our squared variance explained in testing. And we're changing the complexity of the model, the flexibility of the model to get the best error or reduced error in the model. Okay, so we're doing hyper parameter tuning by hand. Is this cleared everybody? How we do this? Okay, what do you think would happen if we go too far? We've got 200 data. The largest number we could put would be 200. That would be kind of crazy. What if we try something like 100? Do you see the variance explained to start to decrease? We're becoming underfit. Okay, and if you carry on 200 and so forth, you'll see that you're becoming underfit. Okay, any questions, comments? This is a very important concept right here. Okay, good. I can't help myself. Can we do a K equals 1 just for fun? I feel like we have to. I feel like we need to. What happened? We have completely overfit this problem. We have negative 20% variance explained. We're using the nearest neighbor. We're basically the error in training will be what? What's the error in training at the training locations for K equals 1? No error whatsoever. At the training locations, we perfectly fit the data away from the training data. We're doing terrible. We have a completely overfit model to complicate it. Okay, you know, I can't even leave it like that. That model is just like really bothering me. So I can't even move on until I fix that. Okay, let's go ahead and fix it back to something reasonable. All right, and I, my input was K20 in case you're wondering. Okay, good. Let's try a decision tree. We've already talked about every step in the workflow. We don't have to do minmax normalization anymore. A decision tree does not care about the magnitude or variance. It's not doing any distance calculations in the predictor feature space. No minmax normalization, no step zero. We're going to instantiate a decision tree and these are the hyperparameters again, right? Now, what are the hyperparameters for decision tree? How would I find out? Now, if you wanted to really have a really nice control, let me see, you can do, you can do maximum depth. So a decision tree is one split, then you can split each of those regions, then you split each of those regions. Every time I did that, I went down one layer. That's max and depth. One max depth would be one decision, two regions. The next would be four regions. The next would be eight regions. It goes up like that. You can also do the minimum samples per split where you say that I must have at least a minimum number of data within a region to estimate the average. That's a good one to use. Maximum leaf nodes is my favorite. So what I'm going to do right now, if you want to do this too, now what should I, oh, oh, oh, oh, I hit the caps lock. Sorry, but that. My problem I got this big microphone in front of my keyboard, leaf nodes. Okay, equals what, like five. That way, you control the exact number of regions in the model. That's a really nice way to control decision trees. So go ahead if you like. You can change it to maximum leaf nodes. Okay, everybody able to do that? Okay, thank you very much. Thank you for that comment. Okay, so max leaf nodes and go ahead, run that. Wow, that's my decision tree. I have five regions. I said I'm allowed to have five regions, five leaf nodes. That's what they call them because it's a decision tree every time you get to a dead end, it's a leaf. And those leaf nodes are the regions where we make a prediction. This color right here is the average of all of the training data within that region. Okay, so 37% of wow, 37% of variants explain with these five regions. Isn't that incredible? That's amazing. Do you, is anybody bothered by this block right here, this region? There's no data there, right? We're visualizing the testing data. There was training data over there. If you go back, you'll see there actually was training data causing that region. I know it, it actually caused me to think too when I first saw it. Okay, let's go ahead and adjust the hyperparameters. How are we going to improve this decision tree? My machine learning friends? What are we going to do? Is it over fit or under fit? Who wants more complexity? But let's go, let's go seven. Let's go, let's just increase it by two two regions. Oh, you're on the right path. Variants explain when up, right? So we're on the right path. We're going the right direction. Let's go ahead, maximum leaf nodes. Let's try something like 20. Look at that. Variants explain 45%. Look at the shapes getting kind of complicated, today. Oh, so this explains decision trees. You can change the degree of complexity. I recommend changing maximum leaf nodes as a really good way to control complexity on a decision tree. Okay, any questions about decision trees? Are we good? We're good? Okay, so next, anybody here ever use random forest. I would suggest that tree-based approaches with random forest and gradient boosting are competitive with some of the most complicated and most advanced machine learning methods. They can be very, very powerful and the cool thing is they're built on decision trees. They're just an ensemble or group of decision trees. But we'll get into that during the extended course. Would you agree that these are pretty powerful? Okay, support vector machines. Okay, this is more complicated. This is more complicated. And you're going to see I kind of don't, we're going to just show it, but I would not focus on support vector machines. They're very much an expert tool. And so the hyper parameters are the kernel transform to the high-dimensional space. Then you can make choices about a cost function. And in a kernel, a support vector machine, the cost function is controlling the degree of complexity. It's effectively controlling the, what we call a margin, which is the amount of data concerned when we're putting in the boundary that's decision boundaries. What I'll just say is as you increase the cost function, the model will become more complicated. As you decrease the cost function, it'll become less complicated and you can play with that. Let's go ahead and run. This is a radio basis function, a very flexible kernel, pretty high-dimensional stuff going on right here. And we can go ahead and build a support vector machine. Now, what's the first observation? Definitely takes longer to run than a decision tree. In fact, has anybody run it already? Did it run Ben? Did it work? Okay, good to know. Thank you. You are a pathfinder. Thank you. You ran it too. I know what you did. You thought, if I run it first, I'm going to get the cloud resources, not have the share of others. Is that what happened? Okay, so this is radio basis functions, 41% describe. You could play around with the different cost functions. I want to demonstrate one other thing. I don't want to spend too much time on this because we don't really put enough details in. If you do a shift tab, you can look up and you can find out that for kernel, you have linear, polynomial, radio basis function, and so forth. You have other kernels you can work with. It's very flexible. Polynomial, now you can change the degree of the polynomial. This would be a second order polynomial kernel. So it's not that complicated. Just imagine a, it's really a polynomial basis expansion to include the X term and the X square terms. One way to think about it, if anyone's done basis expansion. Okay, so if we go ahead and we run that, look at that. You see it? That's a second order polynomial fit. That's all it is with both of the variables. Okay, so you can see, holy variance explained. That's a, that's a train route. So if anybody was interested, you could try out polynomial, you could try out third order. What I really want to demonstrate is this idea. It can become very unstable. You see that? That's just not working at all. And so these, these, let me fix that. I want to go back to radio basis functions. That was just really terrible. And I'll just rerun that. Okay, and I'll leave it back there. But just to demonstrate a couple. Okay, now, then what I can do is I can come down here when, oh, that's going to take low while the run. When you run this right here, this last block of code, it's going to give you the visualization of all your models, all the different types of models for your features you selected. And the result, as far as the error distribution, you'll get all of that. Okay, let's go back up to the top. I'm going to cut you loose for at least 50 minutes. And this is your job. You're going to come back up here to this block right here. And you're going to pick two new features. Now, you can pick one of the features that I picked. I'm not saying they have to both be different, but you can't pick the same on both. I want you to think about other two features or at least, you know, one new feature and build your own model. To do that, I want you to think about the degree of correlation and the structure between production, you're predicting production in each one of those features and the relationships with each other. And I want you to think about which model you're going to focus on. If you're going to do multi-linear regression and that's your model, think about the linearity of the relationship with production. Okay, that kind of thing. You can use the correlations to to help you out. Remember correlations only capture linear relationship strength. They don't capture the non-linear relation component. Okay, you're going to change these. Then you're going to work through. You're going to focus focus on just one model mainly, the one you want to work on and work with the hyper parameters and try to get the best model fit you can. You have 50 minutes to complete this task.
 So let's go ahead and work with declustering together. We're going to open up a workflow that performs the entire approach of declustering. It's going to use the same data set that we showed in the slide deck. So it'll be familiar to you. And it'll be a chance for us to try something out and take a look at declustering. Okay, anybody who is there anyone here who has never opened or worked with Jupyter Notebook before? We did Jupyter Notebook on Tuesday, right? We did a little bit Jupyter Notebooks. We did some explanation. I'm remembering now. I remember we did throw in some Jupyter Notebook that day. Now, so we did talk about the fact that on these workflows that we have blocks of code and we can run them just to run. We can run them just to recap. We can run them by pushing the error error up here. Now, don't worry. There is a stop button. If you run some code and it just keeps running and running and you want to just stop. You want to go do something else. You can always hit the stop button right here. So you not stuck in your code. The other thing too. And I'm not sure if I showed this just last time. These blocks of code with documentation and equations are in fact written in markdown. And I mentioned that. But if you double click on it. If you double click on the block of code, you'll in fact go to the markdown. And you can see the markdown has a certain syntax to be able to put in bullet points and to put in bold font and to put in subheadings and headings and so forth. So this is how we build our documentation. Now, I mentioned last time this is a very powerful tool. So Jupyter notebook or Jupyter lab. Because you can make these really nice well documented workflows. And you can share them with people. A great way to deploy. You can even host them online and sandbox or in an environment. Okay. So you push the run arrow at the top, the rerun it. When you do that, you get back to the compiled markdown. You see the actual final product. All right. So let's go ahead. Like we did last time, we'll go ahead and we'll step through this together. If anybody has any questions or comments, please go ahead jump in. I'm more unhappy to discuss. And if you have an idea of something else you want to try, I'll we'll try something so, but just let me know. We load geo step pi, which is the package I wrote for spatial data analytics, because that has the declustering code in it. So once we've done that, now we can access it. Now, was everybody able to run that? Did it run this turn solid, but then it finished? I see nodding. Good. Everybody that ran. Let's go ahead. We'll also import numpy. We do that for a raise of data pandas for data frames or tables of data. We also do map plot live so we can do plotting. Great for doing all kinds of different plotting. SciPy for summary statistics. And intake is our feature engineering. Doctor Foster will cover that during his data science sessions shortly. Okay. We can go ahead and run that. And we can use intake our feature engine, our data engineering package to load up a data set called spatial data biased. Okay. Now, every time we run code, I always tell my students, no news is good news. If you ran the code and nothing happened, usually that means no error and you're in good shape. But at the same time, it's always good to confirm. I am somewhat paranoid at times, I guess. I always want to check and make sure every step works. So let's go ahead and we can within when we made the data frame. Well, we got access to all the wonderful pad, pandas data frames, built in functionality. And one of them is the ability to to a preview of the data set. Of course, you can go ahead and say, I want to only see five samples. And then you would just see five samples. You could also say, well, I don't need to see this print out here. I did a print out of a slice of a data frame. At any point, you could put a hashtag and you could go ahead and hide that. And now you just have the data table showing. Like we're doing this, this methodology right here, where we're loading in the data. What I've actually done is just pandas read. And then CSV. Oh, sorry about that. Read CSV. My microphone's in the way a little bit. And then I could put the name my file. So I could do that. I could in fact use the built in functionality from pandas. And I could just read a comma delimited file directly in. And then I could now be my data frame. And then many of my workflows I've done that. Now, what doctor Foster is done by using importing intake. And using the cat command there to load in or I'm sorry using intake and being able to access the data file like an object. And be able to read it in very nicely. Is that he's taken care of all of the data engineering outside of the data science. And he says now he can do that. He can decide on the formats. He can decide on exactly how it gets loaded. And we can be data scientists. We can just take that and just start doing our data science. Now you could imagine you can have a doctor Foster in your company who manages the data. And we could work with that data without having to deal with those details. John, do you want to say something? Yeah, I'm just going to say that using intake gives us a consistent API no matter where the data comes from. So for example, the data is simply in this example. In fact, the data does simply come from a CSV file. But in the energy data science course, we use a number of database calls. So we're actually importing data live from, you know, updated databases. And using intake gives us a consistent API as a how to get the data in. It doesn't matter if the data is stored in a CSV file somewhere on the web locally or making a call into an actual database again, which we do a lot in the energy data science course. Then we can just use it in a consistent way. So that's that's the reason there's no other reason than just to have consistency from all of our data sources. And so all of the data engineering gets taken care of. We can just focus on data science. Hey, John, thank you very much for that. Appreciate that. All right. So we go ahead and we load up our data. Now we can preview the data set. Now one thing I just want to note is that you'll notice if you don't put anything in the head command, it'll just go ahead and show you the first five samples. And that's because if you take a look at the doc strings, do a shift tab with the cursor in there. You'll see immediately that there is a default, the parameter, the only one parameter the head command takes is n and it's set equal to five. So it assumes a default for you. You don't have to worry about that parameter unless you want to look at more data. Now that's good as we work with more complicated functionality to understand that often there are defaults to simplify your call to the function. Now we have the ability with pant with pandas to do a lot of great things. And one of them is the describe command that allows us to be able to do summary statistics of the data. Now we want to do that because we're going to be doing declustering. I want to know what's the original mean, what's the original statistics. So after I do declustering, I can see how it's changed. So we're going to do that. Parosity, we're going to work with the porosity variable 13.5, 13.47 percent porosity is the average. Now you'll notice here we did a transpose right here. Something really cool about Python is when I take this command, df.describe parentheses. In fact, a table of statistics and then the transpose is going to just flip the axes. So you can always chain up your commands. If I remove that, you'll see the only difference now is that we lost that transpose. We reversed the x and y. Now we have the features across the top, the columns, and the rows are the statistics. We put transpose back in. It's going to switch it. So you'll find that many times if you have a command and Python that looks a little complicated. What's actually happening is you have chained up the command. You have one command with an output, then you apply something to that output. So Python can be very compact, but it can be a little complicated because you have all of that old things chained up like that. Okay, so now we're going to work with a spatial feature. So we're going to want to go ahead and and set an x min, a y min, the porosity min and max. So we have color bars set between a min and max value. Not always a good idea to pick the absolute min and max of the data because then you have weird color bars where it's going from 0.0532, you know, and so on. It's nice to get a nice equal interval easier to read color bar. And we can pick a color map to work with something really cool map plot live is that it has access to many good color bars. And what you'll find is that many of the ones were used to using like rainbow have biases in them are very bad for color blind people. They just don't show up very well because the intensity and the tonality don't change together. Well, plasma does. Okay, we'll go ahead and within the geostat pie package, we have programs to do location maps and functions for location maps. If you look at the code, it's not a big deal. It's really just taking map plot live and producing a location map and putting all the labels on for you. If you knew a little bit about map plot live, it'd be easier just to just use map plot live directly. But what I find is that many people. Who first start out working in Python are not used to it. And so this a little bit of a bridge to help people because the location map simply asks what's the data frame the X the Y column the variable column the min max and each of the directions the min max of the variable. It's very intuitive kind of what you'd have in a usual location map program. It's actually formatted directly after the low, low Mac program from G a slide geostatisco library. Okay, so this is the data set to work with. So I guess I forgot we're in fact working with a different data set for this example than the one I showed in the notes. And so this is a good data set. So let me ask you this. By show of hands, who thinks that this data set is clustered. You'll notice here that we have the high values here, lower values here. And do you see how in the high areas. It's much more densely sampled. And the low areas is more sparsely sampled. And this is very typical. In fact, there's something going on here, maybe a pad or something. We're starting to do pad drilling or something. Because we have a lot of really dense sampling in the high area. We identified the good location. Now we're drilling that up. Now we can employ the declustering methodology. Now, one thing I'll just mention is that if you have a function you want to run and you don't know the parameters and you just want to quickly see what the parameters are. You can just type its name. So if I type geostats dot declust and I run that. It'll actually give me the name of the, I'll give me the actual list of parameters that I need for that function. And once again, if you are confused at all, just put the parentheses there and do a shift tab and you'll get to the docks strings again. And this can be very useful because then immediately if the developer has done a good job making the package and putting the docks strings in. It's going to tell you the parameter and what the parameter is a description. And so this should be pretty helpful in getting you started. So you'll see some packages, the standard packages do great job with their documentation. You can just click the get out of that. So now we can go ahead and run the declustering method. Now this is another interesting thing about Python is that when you run a function and Python, you have the parameters that go in. And you have the outputs that come out and you can often with a function have more than one output. In fact, when you look at the docks, you'll see that there will be multiple outputs coming out. Therefore, I need to actually show multiple variables here representing all the different outputs that are coming out. So let me just explain this very quickly. A data frame, the x and y column from the data frame, so you know the spatial location, the porosity value, which is going to be the actual feature that I want to work with. The min max, this is a Boolean zero or one. So what it's doing is if you put a one, you're stating that I believe that I have clustered in the high values, please pick the cell size that minimizes the declustered mean. If I put a zero in there, I'll get the opposite effect. It'll assume that you're clustered in the low values. They'll pick the declustering value that maximizes the declustered mean. But do you all remember that idea moving the grid around and averaging the weights in order to remove the influence of the grid location. That's number of offsets. It's fast. 10 is good enough. You could do 20. It should be fine. Number of cells. This is you remember that chart where we had the multiple cell sizes and we had the declustered means and it went down and it came back up to produce that chart. You have to say how many times you want to repeat the calculation. What's the minimum cell size, what's the maximum cell size. I go back to the data set and what I do is I look at it and I say what's the minimum data spacing. So that if I get a cell size that big, all the data will be in their own cells. So that's how I pick the minimum one. I pick the maximum one such that it's large relative to the area of interest. And so 2000 would be much bigger than the area of interest, 1000 by 1000. So that should be large enough so that I can see the get the full effect of the curve coming back up. Okay, any questions about these parameters for declustering. So now the outputs are going to include the weights. Now you'll see the weights is going to be a one dimensional vector. It's a one dimensional ray, I should say, a numpy array. And it's going to have the same number as the number of data that you were looking at. And it's in the same order. So it's going to be the weights for each one of the data. And in the same order, I can create a brand new column of my data frame called weights. And I can go ahead and run this. I get the weights out. I put it there. The cell size is going to be that cell size from very small to very large discretized from 10 to 2000 meters. Because I said the minimum was 10, the maximum is 2000. And the declustered means are going to be those values that I can plot the actual declustered mean for each run. So these two arrays, one dimensional raise, this allows me to plot that relationship. Okay, so we can go ahead. When we run that code, we've done declustering. If you look at the result, does that run for everybody? If you run that block that's working, it runs pretty quick too, right? It's a pretty quick run. If you look at it, what happens? You get a little bit of summary statistics. We had 290 data, the mean, the mix, men and the max, the standard deviation. And then if you look at the weights, what you have is porosity and the weights for the porosity value, porosity, the weight, porosity. So that porosity value of 11.5% has a weight of 3. So it's going to be really in a sparse location. Now, let me ask you a question. Just given what you've learned so far. If I was to change this from porosity to permeability, do you think the weights would stay the same? So if you look at the data set, the data, the declustering was really focused on the locations of the data. Now, if we had said, I know the cell size to use, and I went back to the cell-based declustering program and I said, this is the cell size I want to use. And you could do that by just setting the C, men, the C max very tight. And it would just produce that one cell size. Then porosity permeability would have exactly the same weights because it wouldn't care about the actual data values. The thing is, we've told it to investigate and find the cell size that minimizes the declustering mean. Because of that, porosity and permeability may not use the same cell size. And the result is the weights could vary a little bit. Okay, so I just kind of throw that out there. Now, you know, if you look at this data set, it might not be too hard to come up with a good cell size. In fact, you look through it and you can kind of see a bit of a nominal spacing and kind of a tighter spacing here. I bet you anything just looking at this, something around 100 meters, the 200 meters would probably be a pretty good cell size. But let's see, let's run it. I'm kind of, I'm forecasting. I forget. I haven't run this for a while. Okay, so we've run it. We get our weights. Now, let's go ahead. If we run this line of code, we're going to display the weights. Now, this is always a good idea because you want to see how the weights are behaving in space. Round here, very low weight, because we have very dense data. In fact, those weights are going down to like maybe point two points, something, right? If you look around here, we got very high weights. These very bright yellows. We had, we already saw a data for weight of three. Now, any questions about these weights? Do these make sense to you? Let me ask you this. And this is an important question. You see the boundary, the area of interest right here. We wrapped it. You know, when you build a model, you always have to pick the boundary. You have to, one of the first things you do as a geoscientist, geophysicist, and an engineer working on a subsurface acetic team, is you decide how big is the reservoir, how big is the study area. If I was to increase the size of this study area, would it change the weights? So this relates to exactly what I'm asking. If I was to change the size of this area of interest, so imagine I was to expand it 500 meters, 500 meters, I expanded out. Would those weights change? What do you think? If you increase that size, it's actually not going to change the weights. And this is something people like about cell-based declustering. The fact that it doesn't really care where the boundary is. The question was, why are the values high on the edge of the data set? And the answer is, because cell-based declustering doesn't see the edge. So let's think about that. It doesn't see the edge. So now, let's pretend we're cell-based declustering, and we're going through this data set. Which data are the most clustered? Well, these are definitely the most clustered, right? But these data right here, they look like they're less clustered, and part of the reason may be because outside the study area, we have no more data. You see that? And so we have to be kind of cognizant. If you think that boundary is really, really important, you think that that boundary was mapped very, very well and is very meaningful. We would switch from cell-based declustering, we would switch to a method called polygonal declustering, which in fact would account for the boundary, and these data points would get much less weight. Now, there's a question. Heather, you asked the question. By expanding the boundary, you pull in more data points when that change your weights, and you're exactly right. My assumption, when I say expand the boundary, was that there's no more data to add in. That it would expand the boundary, and it would just be void. But you're exactly right. If we expand the boundary, and it means I include samples out here that are not here because the fact that I excluded them, that would change the weights. You're exactly right. Thank you for that, Heather. Any other questions? This is all really important. After we run declustering once, I'm going to get you to run it. I'm going to get you to run it on permeability and see what you get. So let's go ahead. We're going to go ahead and produce a bunch of diagnostic plots. And I think these are all really important. Declustering weights. We already did that. We're experts at that. Declustering weight distribution. Always good. See if it's continuous. See if you have outliers. If you have outliers, identify the data that have unusual weights. What's going on? There might be something going on. The naive porosity. I want to introduce this into your vernacular. This idea of a naive distribution, a naive statistic. From now on, every time you work with the raw data and calculate a statistic, I hope in your head, you think this is a naive statistic. Because it's not accounting for spatial context, clustering of the data or anything. It's just taking the data. And this is good because it will remind you the fact that often it's biased. Here's the declustered porosity distribution right here. And so you can see the result right there. And here's the statistics. The porosity naive mean was 13.5%. The declustered mean is 12.1%. The change or correction is 10%. We were over exaggerating or inflating the average porosity by 10%. And if we would have used the naive statistic, we wouldn't inflated oil in place by 10%. This is this is critical. Okay, so now we can produce this plot here. We take the scene, the cell sizes and the declustered means we plot them against each other. I made a kind of a really nice plot here where I take the naive porosity mean I plotted as a line. You can see how we start from almost there at a very small cell size comes down comes back up. And we pick the cell size that minimizes the declustered mean and that cell size was around 200 meters. Okay, any questions or comments up to them. All right. So I want to point something out. And that is the cell size could have been something around 150, 100, probably up to about 250. You notice how the plot comes down and levels out and it comes back up again. That commonly happens. What it tells you is that the cell size is not super sensitive. You can have other data sets where it comes down sharply and back up again. That happens when you have a very like a very strong signal of a regular spacing with nested or infill drilling after that. That can cause kind of a sharper response. Okay, what do you guys think do you guys want to try permeability or do you want to try acoustic impedance. Let's see what we have permeability. I sorry we don't acoustic impedance. Should we try permeability. Did you guys want to try running that? Do you want me to give you five minutes to try to run on your own. This is the advanced course now. This is this is like how Dr. Foster runs his courses. Okay, if you if you take in his classes, you see he does give you a chance to sink or swim a little more often. Okay, go ahead. Go ahead, start working for those of you who want a little extra help. I'll just show you the way we're going to do it is what we're going to do is we're going to have to replace this poor min and max. So let's replace it with a permeability min and max. So let's go from zero and I'm not going to change it from poor min and max because I don't want to have to change the name and the code everywhere. And the max said it to something like around five thousand five hundred. Okay, now when we plot it, it's going to make sense and the thing will have to change down here is just the perm. What is it it's porosity now we change it to just perm. I'm not going to change any of the other names. It would just be too much work. And now it'll use the perm min and max because that variables, same perm is what we do there. So we go ahead and we plot that. Oh, this is definitely a log normal distribution. You see what's happening now is that oh porosity fraction. Oh, I didn't what did I do wrong? I forgot to run that block of code. I've done that many times before. Okay, good. Now those of you who are following me and not working on your own, were you able to get that far? Okay, so we just changed the color. We can change to permeability. Now we can plot permeability. Thank you for bringing it up. I'm sure other people had that same issue. I'm not fixing up the code. I'm not making out. We're just doing this kind of quick. I'm not renaming the plots and so forth. Now we can apply declustering. So we look at it. We're going to go ahead and we'll run declustering right here and we'll change this to permeability. Permability. And we have different cell sizes. And now the weights will come up with are going to be permeability weights. Okay, now if anybody hasn't run this code yet, I hope you can see that these weights have changed. We picked a different cell size. You can see the weights are different now with permeability. Okay, if people are following along with me, were you able to run the declustering? Now what we can do is we can go ahead and plot the weights. Now don't run this. Let me run it and see how the weights have changed. Do you see any difference in the weights? If you look really carefully, it's subtle. It might be some subtle color changes. I don't think the weights change too much. I bet they're very similar. Okay, so now what we'll do here is we'll go ahead and we'll run this. Now the only thing we'll have to change is every time it says, curiosity, change it the permeability. Oh darn guess what I did. I did not I hard coded the X the minimax of it. So you have to just change these values right here to 5500. This is the minimax of the property in the histogram and I apologize. I should have set that as poor minimax. Okay, that should actually work. Let me see and to calculate the mean. There's a lot to change. Okay, we can just change every time we see the porosity feature, we can change it to the porosity permeability feature. And that should be it. Okay. So what happened? The permeability naive mean was 207 milledarsies. The permeability declustered mean is 77 milledarsies. Did anybody else manage to run this? Did you get the same results? Okay, so now we've got a declustered distribution. We should display that in log scale. It would look easier to see in log scale. These it's a log normal distribution. Let's go ahead and this is the last plot will do. Now the good thing is when we did the declustered mean versus cell size it just wrote out that one array. So we should be able to just run this. And this should just I hope this should just work. Okay, so we have to change the axis on for the mean. So we have to change the y axis and we needed to go from zero to about what was it? It was about 200 milledarsies. Let's go 300 and we rerun it. Okay, there we go. There we go. Now we've got some of my font that I put in some my labels and moved around that's fine. But what you'll see is do you see how this shape is different than the shape we had before. How this one comes down more sharply. And then it kind of jumps back up right away. The other thing is I'm pretty sure the cell size was a little bit different. It looks like this cell size is going to be more on the order of 150 maybe something like that. Okay. Now if we had had acoustic impedance data like we I have in many of my other data sets, which way do you think this plot would have gone? If it was acoustic impedance would it come down or would it have gone up? Who hears a geophysicist or works with geophysics data? Anyone? What's the relationship between acoustic impedance and porosity is a negative or positive relationship? So you would expect that for acoustic impedance this plot would jump up and we would pick the cell size that maximizes the declustered mean of acoustic impedance. Okay, just kind of a nice little example. Okay, was this useful for all y'all? Are there any questions?
 What I want to do is I'll cover some salient implement implementation issues and further using machines in our work. And I'll share a vision of success by showing you some really cool examples I think worked well. Okay, limitations opportunities. Have you ever seen this bend diagram? It's a bend diagram, kind of a bend diagram. Anybody seen it? This comes from management, kind of expert management type of programs. Many people make many varieties of this diagram. It's the good, fast, cheap diagram. The deal is this. When it comes to good, fast and cheap, it turns out that every boss wants all three. But that's not possible. Most of the time you cannot get good, fast, cheap. It's not possible. You have to pick two. So when you build your model and you're allocating resources to build your machine, you have to pick good and fast, good and cheap or fast and cheap. But you don't get all three. Okay. So you got to spend the time. If you want it to be good and you want it to be cheap, well, it's going to be slow. And you want it to be cheap and fast. It's going to be low quality. And so all of these things about modeling. We know this from all of the subsurface modeling we do. It takes time to build good models with machine learning. It's the same thing and much of that time is going to be data preparation. We got to account for the resources, the time in the people, the expertise. And I'll tell you what often that will drive you to simpler models. It doesn't make sense. The build models, no one can interpret and people really struggle with and people can't use. I know from working in ETC all those years, if you build tools that no one can use, they don't get deployed and they don't add value. And so we have to do that. Anybody here know Mark Bentley? Any of the Res War modelers ever read Mark Bentley's paper from 2015? I'm not seeing any hands. I'll tell you what Mark Bentley is, he's a friend of mine. He's super cool guy. Love this guy. If you ever meet him, he's super, super chill. Bentley wrote a book with ring rose about Res War modeling in fact. And he wrote a paper to about the same time. What he did was he introduced the idea of modeling for discomfort. Now, I'm not being judgey here, but modeling for comfort is the idea that modeling becomes a tool for verification of decisions already partially or fully made. We model for comfort. We're basically building the model to tell ourselves what we already know or what we think we know. Has anybody ever done that? No show of hands on that. Have you ever seen that? Does that ever happen? It happens in every organization. When I bring this up and I mentioned, I've been to a lot of companies in the last little while they have this happening everywhere. Okay. So this happens in Res War model. We built the model to confirm what we already want to know about the subsurface. Mark Bentley comments that we should basically model for discomfort, which is the opposite. Anybody here like Mythbusters? You have kids and you watch Mythbusters. My kids grew up watching Mythbusters. What do they do on Mythbusters when the thing doesn't happen? They put the pressure really high. If it's TNT and thank goodness it often is, they just put more sticks in. And they basically get to the point where they blow the system up. We should be modeling like that. We should be stress testing our models to see when they really do fail at understanding the sensitivities. Okay. We find in the subsurface we often under-represent uncertainty. So it's not a bad idea to be a little bit like Jamie. Jamie is crazy. He does some crazy stuff on Mythbusters. Okay. Identify remaining upside potential. Secure ourselves against the worst case. We've often been surprised with what happens, right? We need to recognize our biases too, because often we under-represent uncertainty and we often kind of bias things too. Okay. Everything we do in machine learning is statistical learning. I said that yesterday. So we should build on statistics and probability. In my extended course, I force you. Sorry. We will endure a morning of probability and statistics. We have to. Because to do a good job at machine learning, we have to understand probability. In fact, all of the machines we build use probability. Some of them very, very clearly. The line of sight to probability is extremely clear with naive-based classification. naive-based classification uses Bayesian formulations, which are all based on fundamental probability logic and probability assumptions. Now at times, you'll be in a situation where you're trying to visualize your data and understand it. Feature selection is one of those times. Next time when we get into extended topics, we'll do violin plots. We'll look at conditional probabilities. Conditional probabilities are very powerful. Look at the relationships between acoustic impedance and porosity and understand how the distributions change over different ranges of acoustic impedance. Those are conditional probabilities. Very powerful. Go back to probability. It's very helpful. Ferrage test train split. Every time you do random selection, remember 20% of your well randomly selected is literally used to the gray lines. The gray lines. Those are your testing data. Just randomly selected 20%. Now when you're trying to test the model at the testing locations, you're often predicting only half a foot, a foot, or two feet at most away from a known location. That's not a very difficult problem in most geologic settings. Now you could have the opposite and somebody suggested this and I really do appreciate that. Remove the entire well. Well we can do that, but let's be careful because sometimes we can make it too hard. Too hard would be like this. We train here, but we test here. What's the problem? Look at what's happening here. These are different rock and fluids. Things are changing in the reservoir distinctly between this unit and this unit. I may be training on different rock and fluids and testing on something completely different. My model is not going to perform very well. That's too hard. Okay, we would want to do geologic mapping and segmentation of the problem anyway to make sure we have our geo-knowledge engineering knowledge in first. Some warnings. Parsimony. The parsimony principle is start simple. Models must be understandable and interpretable. I'll tell you, I'll share some dirty laundry. I went to a company. I won't mention the company and I was teaching for half a day and it was a great lesson. When we showed up, I said to the team, I said, we're going to spend the morning talking about basic probability, statistics, and fundamental machine learning concepts. You know what they said? They all went like this. They're like, come on, perch. We're the machine learners. We know that stuff. We don't need, well, guess what? We got going and everybody got into it. By the end of the morning, everybody saw they were learning new things. We started with the fundamentals. We didn't jump right to it. I have students who barely understand probability coming to me and wanting to build deep convolutional generative adversarial networks. I think we need to remember it is statistical modeling. We need to start with the fundamentals to do it well. Scientific method is extremely rigorous. Just because we have a fancy machine, let's not abandon that. Let's be very critical. You remember, has anyone heard the problem of geostatistics and patrol that you can make very pretty maps that everyone believes? Have you ever heard that before? You can make a very pretty map. It'll look very believable. It looks heterogeneous. The same problem with machine learning. You can make something that looks very believable. We still have to be very rigorous and test these models. Inspiration from the traits. Anybody have a family member who's the traits person? A machinist. A mechanic. Someone who works with pressure vessels or something like that. Anyone have family? My twin brother is a machinist. My grandfather was a machinist. These people spend decades learning their trade, learning their machines, learning their tools. We should be inspired by the traits. When we pick up a new machine, a new tool, we should be spending time with it to become an expert, to be familiar with it. We should have a toolbox with enough tools that we understand that we can get the job done. We should be inspired by the traits. There's a lot of resources to help you going along on this journey. We have a lot of stuff on my YouTube channel, DataMLMS. We have a lot of example workflows. If you dig through there, you're going to find tons of them. We go through many of them in the extended learning in the next session. Go to the documentation, Anaconda. Everything is great. It's a wonderful time to get started in machine learning. There's so much good resources for you. Coding. Don't be intimidated. You didn't understand everything today. Focus on the minimum requirements. Build up. Play around. I appreciate it. People were playing around. Go ahead and use the type command, figure out what this was, figure out what it does. Use the shift tab. What are the parameters that go into that function? You can figure this out. And remember, the very basic machine is right here. Load the package. Load your data. Instantiate the model. Fit the model. Train the model. Predict at unknown locations. Test the model. That's the most basic workflow for machine learning and Python. It's actually not that bad at all. Okay. And gain experience with a variety of different methods. Try things out for yourself. We have a bunch of well-documented workflows that show you how to do things. Try substituting your own data. The workflow we use today. You could load your own data from a comment-delimited table directly into our workflow with adequate data preparation and claiming. Okay. So you could use these examples. Any questions or comments about these kind of parting words of wisdom? Anybody wondering if my coffee is very cold? I'm just using it to keep my throat. We can dry any comments or questions. Okay. Let's go ahead. Can I show you guys for about 15 minutes? Can I show you cool machines? Are you good with that? Everybody's good. That's a good use of our time. Let me show you cool machines. Okay. The first one is. Machine learning and data analytics provide you some really interesting opportunities to learn from your data. One of the greatest ways we can learn from our big data is to be able to go through the data set and find features of interest. One of my students, Wendy Liu, has been building spatial anomaly detection. Imagine you have an unconventional data set with a bunch of wells with production, with estimated EUR, with initial production, maybe average perocity, whatever it is. You have a map with all of your many, many wells. Now what's really interesting is if you look really carefully, you can find you see this wall right here. It's different than its neighbors. Now I'll quote Morgan Sullivan again, the fellow within stratigraphy for stratigraphy in Chevron. He said we have to look for the difference that makes a difference. In other words, what really matters? And so what Wendy's done is she came up with a really cool model based approach to determine which pairwise relationships have a low probability of occurrence. In other words, they're anomalous. They're unlikely to occur at that distance in orientation. And so what she's done is she can build anomaly maps. Here's all of the data. And this is a measure of likelihood or probability. You see here, here, here, here, and here are locations at which you have wells that are spatially anomalous. Now in this case right here, it's likely being caused by one single well. What can you do? You go to that well and you ask yourself what happened with the completion job. What happened with the local fractures? What is different here? It's a good way to get started looking through your data. Now when you have something like this, maybe you have a region. I know when I was working, I think the Hainesville, they used the term core non-core area. I think Gen V was using this concept of core non-core. Maybe you have a location of anomalously high production. Maybe that's core. Maybe those are the areas we should focus. You see that? This is spatial anomaly detection. It's the form of really, if you think about some machine, you're trying to detect patterns and big data. Like a clustering analysis type of approach. Let me tell you this. This is another example. We're now building our reservoir models directly with machine learning. In fact, what's incredible to me, anybody here worked deep water, Gulf of Mexico, or anything that has compensationally stacked lobes. So these compensational lobes are very important. Actually, Fabian Leisure, do you guys know him? I love Fabian. Fabian and I co-authored something. I think the paper will come out here shortly, in which we did sensitivity analysis across a whole bunch of different parameters for compositional lobes. Anyway, we're now able to build these lobe models up using deep convolutional generative adversarial networks. Anybody have an old picture of maybe grandma and granddad that got damaged? There's a rip. There's a tear. There's a faded spot. There's a stain. And you ever heard of somatic impainting, a methodology by which you can fix tears and rips and holes and pictures. Have you ever heard of that? We're using that to make conditional reservoir models. In other words, you build your reservoir model using machine learning. Then what you do is you cut out the locations around the data. You use the repair method of somatic impainting. And you can now honor precisely the well data while honoring the conceptual information away from the well. So these are brand new reservoir models using machine learning methodologies that were used formally to repair images. Kind of cool. Now, what does this mean? If you drill brand new well, you could also put it in update it with this semantic impainting. And so Hong Kong has done that. Now, Hong Kong Joe, one of my first PhD students has also done something really crazy. And this is something I want to talk about by Don't Have the Slide Jet. When you do deep convolutional generative adversarial networks, you project this heterogeneity model into a lower dimensional space represented by a couple of latent variables, their dials you can turn. Have you ever seen where you take a person's face and another person's face and they turn a dial and they morph the faces, like a male into a female face or any, you know, here goes long and short. You morph in between. Have you seen that? That's exactly how they do it. But we're doing it with reservoir models. And it means you can turn that dial and you can morph across different heterogeneities. And he's using that for history matching. We're actually morphing the models and looking at how they behave and flow and we're history matching by doing that. It's really exciting. We're exploring uncertainty in a low dimensional space. Okay. Now, seismic downscaling my Gulf of Mexico friends or people who worked in West Africa know all about this seismic information doesn't give you what you need to see. You don't see the reservoir architecture. You see this. You see a what do you see? What scale are you observing with seismic in the Gulf of Mexico? Is it complex element complex set? What do you think? Can you see elements? Sub salt. Most of the time, just the complexes or complex sets. You don't see the individual pipes or flow units, right? Okay. So what we're doing is this is what you see, but this is what you want. Has anyone heard of picks to picks? Picks to picks is a locally conditioned deep convolutional generative adversarial network. It's basically able what you can do is you can draw an outline of a cat and it makes a cat. You draw an outline of a person makes a person drawn outline of a car and makes a car with high resolution. That's what picks the picks does. So we're using that technique. You get the outline of the reservoir body and picks the picks will put in the reservoir architecture. Isn't that cool? So we put we train it and we gave it an outline and this model right here is generated by picks the picks. It knew by the Arcuate forms that the abandoned channel must have been right there. It learned that from looking at many, many images. Now you look at this right here, the form of the channel complexes kind of a blob. It doesn't have a lot of description. So picks the picks gets a little confused. I gave you a bad example to demonstrate a limitation. Okay, any question about this idea of using this these types of machine learning for building models in this case seismic down scaling. Kind of cool. I predict in a short period of time many of our subsurface models are going to be machine learning based. They're definitely I know we use a lot of MPS. These techniques are able to get like really realistic architectures and very efficiently. Okay, now I want to kind of like really show off. And so this is an example from my student when pan co supervise with doctor Carlos tours, verdin one of the professors in my department. And these are machine learning models constrained by geologic architecture by seismic and by well data and look at the degree of complexity in these models. This is like you remember how MPS has a kind of a connectivity issue and so forth. These models are really super realistic and we're now building these up with machines. You turn the dial and you can quickly explore the space of uncertainty like milliseconds to build new models really really fascinating. What we're doing is we're training models of like two centimeter by two centimeter, tomography images of the poor structure of these very small models poor scale models. Then what we're doing is we're training them and we're able to get the velocity fields of the fluids moving through the rock. We can get these models with a convolutional neural net within like a tenth of a second. What would have taken they were taking us almost two days on a cluster on tack using a lattice Boltzmann type of solution physics solution. And so what our thinking is is that if we can work out the flow at two centimeters by two centimeters and we can upscale to reservoir units. We can start to now think about the impact of cementation micro permeability like an unconventional on flow behavior at the very smallest scales. And then how it impacts the larger reservoir unit scale. So we're trying to work truly multi scale flow modeling. Any questions about this. What do you think this machine looking machine learning look cool does it look like it's pretty powerful. It's solving some big problems. I'll tell you that. Okay, this one. So this one's going to be the spooky one. I want to end with a spooky one. We use the recurrent neural net. A recurrent neural net is a neural net that actually learns from itself because some of the information is actually passed backwards. So it actually builds up a memory. It starts to learn. And so kind of learn from its history. And so we use the recurrent neural net specific design that's a more complicated design called long short term memory networks. And what we did is we trained with nine injectors in a water flood. And the injectors are interacting with each other. They're close enough that the injection start in the mix. And we have four producers. And here's the blue lines or the blue are the production rates at the producers. And these are the injections at all the injectors, the nine of them. We train for the first 2500 days. So the machine got to learn injection versus production for all four wells. Okay, so far so good. And then we said, okay, now you forecast into the future given the injection tell us what the production is going to be the blue is the truth and the orange line is the long short term memory system. Okay, here's what's really spooky. Do you see injector number four. During the training period, it had constant injection during the testing period. And we did something very different. And this machine still learned well enough about the interactions between injectors and producers to actually do a very accurate job into the future. I think that's just spooky. Now here's what's very interesting. There was no reservoir model. All it was trained on was rates going in and out. We talked about hard data and somebody said the only hard data is production. And so here's the next step. I have a new PhD student working on this problem. The other one graduated. Co supervised by Dr. Lake. And this student's challenge is to try to now figure out what the long short term memory system learned about the subsurface and to try to update subsurface models with what it learned. All right, so let me just wrap up what we just talked about rigor and modeling model building. Remember that you have to do fair test train. And we have to think about these statistical models. We still need geo and engineering input. We can find patterns and big data using spatial normally detection machine learning to save professional time to focus geologists doing more geology engineers doing more engineering. Some of these problems are so big. It's hard to look at the whole thing machines for surrogate models are extremely powerful. I like that. The idea of making fast predictions of production is very insightful. Now we can see immediately the impact of our decisions and machines for building reservoir models. That's coming. I think shortly we're going to see geostatistics become machine learning based geostatistics. That's where we're headed. Okay, that wraps up.
 With that, I have two exercises here, one on flow control. This is an example actually from a reservoir simulation. If you have a grid block or you have a numbering scheme, say you have a grid like this and you want to, basically what we want to do is we want to detect a pattern in this grid, such that I can reproduce this matrix from that pattern. What the matrix represents is in the off diagonals where you see the minus ones. So everywhere there's not an entry in this matrix, it's intended to be zero. We just didn't have a bunch of zeros in there. Where you see the minus ones, these refer to, I guess first I should say, the rows correspond to the grid block numbers. So the first row corresponds to the zero grid block, the second row corresponds to the one grid block, the third row corresponds to two grid block. So the rows correspond to the grid block numbers. The off diagonal entries in these rows, the column number, the index of the column, corresponds to the index of the neighbor of the row grid. So what I mean by that is, let's look at this example. Grid block zero has a neighbor one and it has a neighbor four. So I go over here and in this I have a minus one in the one column and I have a minus one in the four column. When I say one column, four column, I mean the index and starts from zero. So zero, one, two, three, four. Let's look at, for example, two. Two has a neighbor one, it has a neighbor three and it has a neighbor six. So the two row, the two index row is the third row, zero, one, two. And then I'll have a neighbor in the zero one column, that corresponds to the neighbor one. I have a neighbor in the three columns. So zero, one, two, three, that's right there where the minus one is. And then I have one out here in the fifth index column, five index column. So that's where the structure of this matrix, of this grid gets converted into this matrix. The diagonal entries of the matrix are simply the sum, the negative sum of the off diagonal value. So for example, in this row we have two minus ones. So if I sum them up and put a negative sign in front of them, I get two. In this row I have three minus ones. If I sum them up, put a negative sign in front of them, I have three. It also corresponds to the total number of neighbors, right? So if you go back again, grid block zero has neighbors one in four, grid block one has neighbors zero, two and five, et cetera. Okay. I give a pretty detailed explanation of this in the notes there. And basically what then the challenge is is to complete this function called Pente diagonal. And it takes two arguments, the in X and in Y in X and in Y are the number of rows and columns in the grid. So the matrix has in X times in Y entries. Right. So in this case, this is a four by three or there's 12 total grid blocks. Then this is a 12 by 12 matrix. Okay. And so the idea would be to give an any in X and in Y, produce the corresponding matrix that is correct using for loop and flow control. And I have some comments in there to talk about how to do it. I will, we're going to, I'm going to leave this as a true homework assignment. So if you guys want to work on this over the next two days, then at the beginning of our class on Friday, I'll go over the solution to this. If you want to test your solution, you can open a terminal window. Navigate to that directory. The directory is, so if you list, if you list your home directory, you'll see there's introductory course exercises. So if you change directories into that introductory course exercises. And then you change directories into Python flow control. And then once you're in that directory. If you wanted to test if your code is working correctly, you just type Python test.py. And it, of course, you know, it's, it's, it doesn't work correctly because I didn't implement any code. But if you, if you implement it correctly, then you can actually test your code with this. This again is a unit test. These would be the types of things that, you know, I would encourage every function to have. And then ultimately, these would be the things that were, these types of tests in these unit tests would be the ones that are run automatically. Great ye?
 Basically, I gave some comments in the code to say, if I is not on the bottom edge of the grid, well, you know, computationally using, using I as an iterator that will go over the number of rows in the A matrix, computationally speaking, like how could I decide if I is not on the bottom of the grid using in X and NY in general? Is anyone want to try to answer that? Yeah, so because Python is zero indexed, all you have to do is say that it is greater than in X because like for example, in this case, in X is four, but the index is R0123. So if, yeah, I think you said it right, if I is greater than or equal to in X, in the event that it is not on the bottom, right, then it will have a neighbor, you know, so if you look at all the rows not on the bottom, like look at the grid blocks 4567, they all have neighbors below them. Well, how do I index below them? Well, it turns out it's the index minus NX, right? So in this case, NX is four, four minus four is zero, five minus four is one, six minus four is two, those correspond to the columns and the off diagonal, these ones out here, right? So we just stick a minus one there. So you know, there's multiple ways. So you could say, if I is greater than or equal to NX or you could say I plus one greater than in X, they're equivalent statements, right? And so in the event that I plus one is greater than in X, then we will just put a minus one at the i minus NX entry, right? Again, because if it's not on the bottom row, it has a neighbor below it and the neighbor below it is the index minus NX, right? And the same logic, you know, corresponding logic applies to the top, right? So if you're not on the top row, then you have a neighbor above you. And the neighbor above you is i plus NX. So again, four plus four is eight, five plus four is nine, six plus four is ten. And this logic will apply, you know, for it's it's general, right? It works for any number of NX and NY. So that's those two if statements, right? So this loop is looping over i in this range and you know, NX times NY. And so if i plus one is greater than in X, then it has a neighbor below it and the neighbor below it is index is i i minus NX, I put a minus one there. Then likewise, for the top row, i plus one greater than or equal to this guy, then i have a neighbor above me i plus NX and i put a minus one there. So then all it's left is the left and the right. So again, computationally, if i'm on let's say i'm on the if i'm on the right side, how could i computationally have an idea about if i'm on you know, determine how i'm on the right side. Or perhaps it's easier to look at the left side first, in fact, maybe determine if you're on the left side, index i. So if you're not familiar with the modules operator, what it does is give a remainder after the whole division, right? So if i say, if i say, you know, look at index four, right? So if i say four divided by four, that's, you know, one with no remainder, the remainder zero, eight divided by four is two with no remainder, right? So if, basically what you're asking is if the number is evenly divisible by NX, then you're on the left, right? And so the way the way you would say that is yeah. So again, i'm say if i'm not, i'm just going to negate the operation, right? So i'm say i'm going to write the operation to determine if i'm on the left, but then negate it with not equal sign. So if i plus one, modulo nx is not equal to zero, then i have a neighbor to my right, my neighbor to my right is i plus one, put a minus one there. And then likewise, if i'm the right side, then the remainder would be an, you know, in a modulo, the remainder would be identically one. And in that case, i have a neighbor to my left. So in these four if statements will put all of the off diagonal minus ones where they should be. And then i can just take the sum of the row, because everything else is zero, take the sum of the row, sum them up, take the absolute value of that sum, or put a negative sign in front of it, and that's what goes on the diagonal, and then return the a matrix. And so, you know, you can run the test, but we could also just verify visually. So I think what's our example here is a four three. So if you were to look at that and compare it to what i have here, you'd see that they're identical. And this all, you know, it'll work for it, it's general, it'll work for any case. So just demonstrate how you would run the test if you wanted to. So let me delete this, open a terminal window, navigate to the introductory course exercises, Python flow control, and then just run Python test.py, and then to test. Okay. So that's that example.
 Okay. Quantifying uncertainty. Now, there's many things we can do to quantify uncertainty. What I cover here is just one method accounting for kind of one major source of uncertainty. Later this morning, we're going to spend an entire lecture talking about uncertainty and the sample statistic. Now, we talked about already there's an issue around bias because we cluster the samples in a certain location. But even beyond the bias, we still have uncertainty. Let me ask you a question. What volume or proportion of this 1000 by 1000 have we sampled with this data? What proportion? What proportion? If I eyeball it, what do you think? 1,100? Do you think 1,1000 maybe? Or is it even less? Let me ask you a question. Are those dots drawn to scale? If we drew those dots for the wells to scale, how big would they be? Could we even see them? In fact, we sample how far when we do a well log, how far do we penetrate into the rock, into the formation? And it depends on the log. I get it. Some logs have some logs have very, very shallow penetration indeed, you know, like imaging logs and so forth. They don't see very far at all. Right. So clearly, we, if we were to draw these to scale, they would not even fit in the plot as a pixel. That would be too big. Okay, so now we get back to realizing we've only sampled 1 trillionth or 100 billionth of the reservoir, maybe an unconventional. And so let me ask you this, how well when you sample 1 trillionth of a volume, do you know the sample mean? We nailed it. We got it. There's a good chance we know very little about that we probably have uncertainty about it. That's for sure. Let's admit there's uncertainty. I don't want to be too pessimistic. How about these standard deviation, the skew, the P50, the P90? We don't, we have uncertainty. We need to count for those uncertainties. Okay, so that's what this is all about. It would be very useful if we could calculate a measure of uncertainty in the statistic due to limited sampling due to the fact that I only have 200 or whatever number of data that I have here. The impact of that uncertainty could be significant 20% porosity plus or minus 2% porosity or 5% porosity or whatever it is would be very meaningful for us to be able to use to model up to making a decision. It would be very important to have that. Who here has done the bootstrap before? Who's done statistical bootstrap? Now remember when I say bootstrap, it's not a dance move. It is a statistical exercise, but it should be a, should be a dance move. It sounds like it. Bootstrap is a method to assess the uncertainty in a sample statistic by repeated random sampling with replacement. That's the statistical definition. Let me give you the practical definition, a statistical methodology to get uncertainty in any statistic and next week, any machine. You can use bootstrap to get uncertainty in a machine for machine learning, which is awesome. So you can, uncertainty in your model too, right? Okay, bootstrap. The assumption is that you have sufficient representative sampling. Is anybody having deja vu? We talked about representative sampling just this morning, just now, you know, in the last hour, and we calculated representative statistics. What does it mean? Your declustering is a prerequisite for your uncertainty modeling with the bootstrap. So everything you're going to do is going to start with first doing a declustering to get representative statistics. Your uncertainty model is wrong if it's already biased, right? Because uncertainty is giving you the range, the dispersion on the thing, but it could be shifted, and that would be, you know, that's still a problem. We've got to take care of that. Anybody ever read the fine print on a brand new product that you bought or a brand new service, there's always lots of fine print. There's limitations and there's significant limitations and assumptions with every statistical method. When I teach my students machine learning the very first machine I teach them is linear regression. And I show them that there's an entire slide of six important assumptions to linear regression that very few people understand. And so we have to give the assumptions. We assume the samples are representative. That's good. We already said that. We're going to assume stationarity in a very short time in about half an hour an hour. We'll talk about stationarity only counts for uncertainty due to two few samples. There's a lot of sources of uncertainty. Some of them are uncertainty due to estimating away from data. Uncertainty due to the fact that you had to make some type of model assumptions. Uncertainty due to the fact that you had to make a decision about where to put the area of interest you move you you made choices. Does not account for the area of interest assumes the samples are independent. All of this is saying that we do not account for spatial location. Now there is an advanced lecture where we could talk about methodologies for bootstrapped that account for spatial location and redundancy. But to get into that we have the first talk about the barogram and we'll talk about that shortly. Okay. The bootstrapped methodology. It's very straightforward. Imagine that I had 10 wells. So I have 10 wells on every well I measure a single metric. So my feature is going to be the subsurface variable. It could be average porosity. It could be production initial production over the first three months. It could be anything like that. So what I have is 10 samples of that feature. Now what I do is I can take those 10 samples and I can turn them into a CDF. This is not too bad. All we have to do is just rank or to them. Put them all together and assign a waiting. Now the waiting we're going to apply for the data is going to be the declusturing weights. So we would get a CDF of all those values. Okay. Accumative distribution function. Okay. Now what we do next is we employ Monte Carlo simulation. Who here has done Monte Carlo simulation before? Monte Carlo Monte Carlo. A powerful, powerful methodology used widely to solve many problems, including, you know, problems such as that you'd use with Markov chain Monte Carlo simulation type methods that we use from Bayesian. And purchase all the time and very complicated systems that very powerful methods. But Monte Carlo simulation at the very heart is very simple. It's just simply drawing a random value between zero and one. And then going the negative direction or the inverse of the cumulative distribution function to get a value. It's effectively drawing a random value from a distribution. This is a cumulative probability from zero to one. If I draw a value randomly from a uniform distribution with a value 0.543 something. I can then look up the associated value from the CDF that's drawing a random value from the distribution. Now if I do that enough times, I'll get the distribution back. I hope you can see that if I do 1000 Monte Carlo drawings, I would get back the exact same distribution. Okay, because it's an unbiased drawing and I have enough samples. But if I do, if I have 10 data and I do 10 Monte Carlo simulations, guess what I get right here. I get a brand new distribution from the data. This is the concept of sampling repeated sampling with replacement from the data set. This is a single realization of the distribution. Now, why is it realization? Why is it different? It's different because we had 10 data. But here, I'm going to draw some of the data twice. Some of the data are going to really luck out and be drawn thrice. And some of the data are going to be unlucky and not drawn at all. In fact, due to theory, we can say in expectation about one third of the data will not be drawn in expectation. Okay, so the result is I get a new distribution. If I take that distribution, I calculate the metric of interest, say the average of porosity for the well, that would be one realization of that metric. Okay, what am I going to do? I said repeat it sampling with replacement. So what I'm going to do is I'm going to repeat. This is realization number two. I get a brand new distribution by bootstrap. I get all of those resembles 10 times. I'm sampling with replacement Monte Carlo style. And then I get a brand new distribution. You see it's different. And then I calculate the average of that. That's a second realization of the average repeat repeat repeat. And if I do it enough times L times where L is a very big number, it will converge on the correct distribution. I will get a distribution right here. And that distribution is the uncertainty in the average due to two few samples. Ah, what is the minimum number samples to do the bootstrap. And so this is a very good point. And so let me just let's do a thought experiment. If I had only one sample, can I do the bootstrap? What would happen is that if I had one sample when I do Monte Carlo simulation, I would just draw from a step function with only one sample. I would get the sample back. And there would be actually be no uncertainty every single time this distribution would just be one spike, one spike, the average, you can't really do an average of one anyway, it'd be weird. The rule of thumb is this. Have you ever heard the law of the kind of the smallest sample size you can use in statistics is seven. Have you ever heard people say that you can't really do anything less than seven data. Now you'll run into people who are a little bit more rigorous about that, and they'll say 30. And that's probably associated with this idea between like small sample sets used for T tests and so forth. In general, what I do is I say, well, you need several data. You can't even calculate an average when you have just one or two data. And so you really do need several data to be able to do this. Okay, now let me ask you a question while we're talking about number of samples. We all admit it that if there was 10 data that these distributions could change quite a bit. And the average between each one of them, the realization averages could change quite a bit and there'd be quite a bit of spread on this distribution. The uncertainty in the average can be quite wide with only 10 data. What would happen to the uncertainty in the average if I had 100 data. It would get narrower and narrower. In fact, if I had 1000 data or 1 million data, each one of these distributions would be exactly the original CDF. You have enough samples. It would be to the third decimal point. It would look basically the same. And the averages would all be the same when you pull them together, it would just be a spike. And there would literally be no uncertainty at that point. Does that make sense, everybody? Okay, good. I see a bunch of nodding. So this is the boot trap methodology. Now, here's the workflow, Efron, the original publication on it, 1982 statistical resampling replacement, calculate uncertainty in the statistic itself. Now, has anybody ever done confidence interval calculation like back in stats 101? You guys had that. I know I can almost see the look of fear. Thank you very much, JP for that. I can say look a fear when people are like immediately having a flashback to stats 101. I know it was not a good memory. I know that because my wife is in nursing right now and she's taking her stats course right now, which is really funny because some of her study partners has suggested that I should join their study team, which I felt was unfair because I technically teach this at a university. Anyway, so the point is that Shucks, I just blanked. I did just blank. What was I talking about? Oh, confidence intervals. Thank you very much. Okay, sorry about that. So confident. I had a stats flashback. That was my problem. I had my own personal painful memories come back. Okay, confidence intervals. All of the confidence intervals. Do you remember those terrible formulas? And you had to choose the normal score, the Gaussian distribution or the student T distribution blah, blah. It was a lot of complexity, right? It turns out that you can do the bootstrap to numerically, experimentally, just calculate the confidence intervals. In fact, that uncertainty in the mean. If you were to actually calculate, just take the variance of that and you compare it to standard error in the mean, which is calculated as the sample variance divided by the number of samples, it works. So all of this bootstrap, you could use it to do confidence intervals and hypothesis testing without knowing any of the theory. So you can actually do it experimentally, just in case that's something you guys want to do, you could do that. Okay, the cool thing about bootstrap. You could not work out the uncertainty in many statistics that we work with without doing the bootstrap. In fact, the bootstrap is completely general. You could get uncertainty in any measure. We don't have the theory to do it for all possible distribution shapes and everything, just with confidence intervals. So bootstrap can be done. You can use it to do it on anything. Okay. The caveats, of course, if this is the first time you've been exposed to bootstrap, well, congratulations, like and welcome because this is one of the most powerful statistical methods developed in the last century, like this is really powerful stuff. But it does have caveats. You assume representative sampling will take care of that through using declustering. And it, what's interesting is if you want to get away from that, if you want to count for the dependency or the spatial continuity of the problem, you can do that. And the, and the fine professor, Dr. Jernel, Dr. Andre Jernel from Stanford, came up with a methodology called spatial bootstrap that I'm more happy to discuss. Okay, let's go ahead and load up a data set. Let's do some of this bootstrap. So we have a bootstrap demo. So I'm going to go ahead, switch my screens to the other side. Just doing that really quickly here, shops, stop sharing, share the other screen. Okay, and sure. Can you guys see my screen now? You can see the workflow from the one we just covered. Let's go back to the table of contents. And we got an exercise called the bootstrap. Okay, let's go and load that up, see what that's all about. Now, one thing I noticed was that these already run, and I do like having the sense of mystery and suspense of wondering if my workflow is going to work. Rather than having it like a cooking show where we're kind of saying, here's how you make a pie and here's a pie out of the oven. So let's go ahead. If you go up to the top, if you want to share the excitement, go restart kernel and clear all outputs. Okay, so then your worksheet is no longer pre run and we're running it for the first time. Okay, as everybody been able to do that. Now, I'll tell you what, there's many times when you're coding where you're going to do multiple intervals with your Jupyter notebook, and it's going to get kind of weird because you declared a variable, but then you change the code and it's going to become a jumbled mess. Please feel free to do this, restart the kernel and clear all outputs in order to be able to start fresh. I recommend if you build a workflow, please do that to do the final test on your workflow. Because I've done this before I made a workflow at ran perfectly well. I handed it off to someone else in a broke. And the reason it broke was because I had code there previously where I declared something and then I remove the code probably by accident. And then I didn't see because it was in memory. It looked like it worked. Okay, bootstrap. Every time I give you guys an exercise or a demonstration, I like to put some documentation in it. So it's a little bit of a standalone product. So all of the stuff I just discussed is summarize, summarize, tear for you, even with the figure right here. Okay, so we got bootstrap. The first thing we're going to do is we're going to load up just that pie. We use it for some visualization, some simple stuff, not a big deal. We're going to do more using kind of the scientific Python library. Okay, but locate import that. You notice the circle here goes dark. It's processing. You can also look down here and I'll tell you what's happening. It's running or idle. You'll see that if you if you want to check. You can go ahead. We'll load up numpy for arrays of data. Pandas for data frames when we load our data using the the great feature engineering that Dr. Foster put together for us with intake. You'll look, you'll enjoy learning about that when you teach us more about that. Thank you, John. Matt plot live. Seaborn for some really cool plotting. We'll do some kind of cool plotting. We got to show off a little bit sometimes. That's one of the cool things to about working in Python really cool plotting in statistics and data analytics. Sci-pi to do some summary statistics. Sci-pi signal. We're going to use that for a kernel smoother. You'll see why and we import random. We need to do some random stuff. Bootstrap is a random operation. Really. Okay, let's do our feature engineering. Boom. One line of code done. And as John said, could come from anywhere in the world. You didn't even have to see it was all standardized for you. And thank you for bringing that up. Now what we're going to do now is I have to admit I like to show results that are interesting. And my data set had too many data. The uncertainty was kind of low. So let me introduce you to this concept here. Data frame has built into a random sampling program. And I can tell it sample 0.25 random state and that number. Now anybody here ever worked with random number generators work with random number seeds. Why did I include a seed? Why do you think I put that number there if anybody ever worked with random number generator by doing that we will get the same result repeatability. So remember random number generators or pseudo random number generators. You give it a seed at starts as Tyler said using that in order to get the same sequence of random numbers. Truly is pseudo random numbers just mean they have the right statistical behavior or random. There's really no random within you can't tell a computer draw random number really. Okay, it's just an algorithm tries to mimic it right the right stats. But it's repeatable. Now you'll notice by doing the sample fract 25.25 I said take 25% of the data at random leave everything else out. That gets us down to 72 samples. Now I'm going to have you guys repeat this exercise with different numbers of data so you can observe. So it gives us a chance to observe the impact of number data on our uncertainty model. Okay, we do this we do this all the time we're becoming experts we preview the data set. This is what we loaded. This is the same data we worked with before I think it might be okay. So let's go ahead describe the data we've done this before you'll see as we go through there are time together. I'm going to start speeding up when we're repeating. But I put that in the workflow because you should always look at the data right so I don't want to overly simplify things. Okay, we got porosity. Let's focus on porosity for our uncertainty model will use plasma because it's a good color scale will do a location map again you see I'm just flying around now. Okay, 72 data at random everybody has the same data right. If you change the random number seed you would in fact have a different value. Okay, so that's our data we're going to work with now I told you we have to do the biasing declustering first because our uncertainty model will be just biased. It's not good to have an uncertainty model and have it biased and bootstrapped does not take care of bias we have to do that for ourselves. Now because of the fact that we've already covered this I do it all in one cell so just run this and what's happened is we got declustering weights and the declustering weights have been added to our data frame. Now this is kind of fun is I wrote a program and what the program does it calculates the declustered average and standard deviation that way when you look at the average and the standard deviation of the original naive data the naive average and standard deviation you can compare it to the declustered mean and standard deviation you can see it's changed. So if you're curious about how to calculate a weighted standard deviation you can go ahead and check out that methodology it's right here in which which all I've done is I've used numpy has a built in functionality for accounting for weights and an average and if you think about a variance is simply the average of the squares. Okay, all right, so the square difference between the values and their average and I use the declustered average so that's there for you. Okay, so now let's go ahead and let's do this approach called bootstrap now what I want to show you is that you remember how we had to do one through L little L through one through big L realizations. So what we'll do is we'll say number of realizations the big L will be 1500 now we could do many more and many less and you could try that out and we could see. Then we're what we're going to do is we're going to loop over those realizations every time we're going to draw a number of samples the number of samples is going to be equal to K equal to the length of the data frame in other words we had 72 data and the data frame we're going to draw 72 times. From that data frame and we use the random choice function to do that and when we put data frame porosity dot values in what we're doing is extracting a one dimensional N D array numpy array from that data frame that we can then draw from and you see what's really cool is we can actually tell it to use weights in a random sampling. And we put weights in the random sampling it's like we're doing Monte Carlo simulation from the CDF but the CDF is corrected for the waiting. Okay, so we'll get back will remove the bias that we had because it clustered sampling so you might ask why is that that random choices allows for weights it's because we're not the only people in the world who realize that there's there's a bias issue when it comes to sampling in space. The hydrologist have been doing waiting all the time when it comes to figuring out rain gauging figuring out stream flow gauging I should say and they mount a precipitation on the ground because they're they're precipitation measuring locations are always going to be clustered they always have kind of sparse and dense sampling of rain. Okay, so many scientific fields have been doing this is this thing okay, so let's go ahead will run multiple realizations. Of the distribution now up here you put eight and what it'll do is show the declustered porosity this is realization number one realization number two three four five all the way through eight so you can now see eight realizations of the distribution when I did 72 resamples with replacement from the distribution let me ask you a question can you see the difference. The declustered versus this one they declustered versus this one DC. They change quite a bit a yeah it is so you can really start to see that now every one of them will have a different average okay so let's go ahead let's try something fancy now i'm going to run this without any warning because i'm trying to get the jump on everyone else on the server because this one's a little bit of a heavy calculation I think it let's see how long it takes the run. Now and go ahead run it to okay so what this is done and i'm actually kind of proud of this code I think this is a really cool idea I ran 1500 distributions right and they all had histograms then what I did was I took and I calculated from the histogram a PDF a PDF is a probability density function it's a smooth representation of the histogram and we got it by using a kernel density estimator it's a really simple. You basically replace every data set with a tiny Gaussian distribution it's really what's going on there then what then what I do is I take and I plot every one of those continuous lines with a certain degree of transparency 1500 of them so what are you visualizing right now the uncertainty in the distribution shape you see that okay so now let's step back and look at that distribution what is the uncertainty in the distribution. What is the uncertainty in this distribution okay what part of the distribution is the most uncertain. Is it really uncertain on the tail the min and max value right here you see the most amount of uncertainty that's the part that's changing the most okay good thank you very much let me ask you this is there any any uncertainty that this distribution is by modal. What does by modal mean what's the uncertainty that this has two separate peaks. What do you think do you see any realizations of the distribution for which this peak disappears no so is that it is not cool we're actually testing the uncertainty in the fact that this is multi modal and if you look it really doesn't it really doesn't ever level out it really does always there's a few realizations that are kind of low here but the vast majority of time it really is two peaks so I think that's really interesting okay let's go ahead and calculate the uncertainty in our statistics what will do now is if we run that block of code what it's doing it's running 1500 bootstraps calculating the mean calculate the standard deviation and then giving you the uncertainty distribution in the mean and the standard deviation. That's it right there that's 1500 bootstraps means 1500 bootstraps standard deviations pretty cool a.
 The Python object oriented example. So in this example, you have a class that converts field units to SI units with all these conversion factors and then all these member functions that would convert a viscosity value given in field units to SI units. And they're all pretty straightforward because they're just multiples of the conversion factor except for temperature. So then we can write another class that goes the other way. So from SI to field and all we have to do is basically just reset all of the conversion factors to one over the value that they had. So when we define this init statement, remember this gets run upon class instantiation, then we can to inherit data that the attributes from field to SI, we use this super class, this calls the init statement of field to SI, which defines all of these conversion factors. And so then we just take those conversion factors and redefine them to one over what they were in field to SI. And then we don't have to write any of those member functions that will all work except for temperature because again, temperature is not just a simple multiplication between the two. And you know, this would work either way. So you can run the test on this if you'd like. If you want to see the solution for any of the problems that are labeled exercises, they are in a solution branch. You know, it's all a get repository. And so if you were to navigate there in the terminal, you just say get checkout solution, then you would see the solution. In fact, that's what I did here. I'm looking at the solution right now. So for example, if I type get branch, you'll see I'm on the solution branch. So if I were to type get checkout master, it's going to take me back to the master branch. However, in this case, it's going to say you made some local changes to the files that will be overwritten by the checkout. So please commit them or stash them. So I made some, you know, when demonstrating flow control exercise, I made some changes to that file. And one thing I can do to get rid of those changes, of course, I could commit them. But I don't, you know, I want to keep our solution branch clean. So I'm not going to do that. I can just say get stash, which just stashes those changes away. And then I can go get checkout master. And now if I close these, for example, if I close those and then go back to and reopen them. You'll see that they're back. There's no solution there now, right? So there's no solution. Now you might be mad at me because I made you spend all that time on the terminal yesterday learning get and get hub. And then last night, I installed a plug into our system here that allows you to actually do it via GUI. So what you can also do now is over here on the left hand side, you'll see a branch that says, I mean, a little icon that says get if you were to click on that. And then you can actually just switch to the solution branch right there. It tells you the on the solution branch. You do have to close the file and then reopen it. And there we have the solution again. So you can switch between branches from this tab over here now. Again, if you make changes, then you'll have to commit those changes or stash them. And from the GUI, there's no stash option. So you actually would have to make a commit by filling in a commit message and hitting this button right down there. There also will be some more advanced features later that will not later when we start to build out the dashboard. If necessary, I have some intermediate branches in there, intermediate tags to commits such that it will allow people who, if you cannot get caught up on your own, you'll be able to immediately jump to the current state of where everyone is. And you won't be able to do that from the GUI. So you still do need to know some of the command line stuff. It's also still very useful to run these.
 And so what's the motivation? Well, the concept of stationarity, the concept of spatial calculations, spatial continuity, predictions in space, they're central to all of our business. And everything we do, even when we do just basic data analytics, even when I do basic, like machine learning, I want to always be thinking about the spatial context. And so we want to give you the best information about how we account for the spatial context. Okay, so let me just go ahead, move a couple of things around. I want to be able to see people's faces because this is really good feedback. I really appreciate that. I'm putting them. All you guys are looking at me right beside the slide deck. So I can see when people look confused or anything like that or a few of questions. Okay, so let's talk about stationarity. Now we talked Tuesday morning about this as a preview. And I told you everything was going to come back again. There's no escaping it. And so stationarity, any statistic that we want to calculate requires replicates. You can't calculate an average from one sample. You can't calculate at P 50 or P 90 from one sample. And so we need multiple measures in many scientific fields. They have access to multiple measures at the same location. Many original statistics came from beer. I gousin all those folks, you know, they were actually doing statistics on a plant making beer. And they would go and they could open up a pipe and beer would flow out and they'd take a beer sample. And they would measure the, what are they measure in beer? Clarity, color, odor, something, hopsyness, alcohol content. There were things there to measure in them. And then they'd come back later that afternoon. They'd do another sample, another sample. And they could see the, how was their process behaving over time? And they'd see the distributions and so forth and make decisions about how to optimize it. We are different because we don't get to turn a tap. And another sample comes out. We go to the subsurface and we cut a sample out like a core. And we go back and there's no additional samples available to us. There's only one sample available to us at every location. So instead of time coming back and multiple times to get replicates, we pool samples over space to calculate our statistics. Now, if you think about that, that's a big choice. And that results in a lot of considerations. And that's what we want to cover. Most of all, I can summarize it as this. I have to determine that space over which I'm going to sample and pull things together must be all the same stuff. Now, I apologize. I know that's kind of getting very statistical and esoteric in my language, but it's all the same stuff. It has to be the same thing. Okay, the decision of stationarity is the decision of a domain that you want to work with. And it's an expert choice. It's not a hypothesis that cannot be tested. It is, in fact, one of the biggest subjective parts of what we do in the subsurface. And guess what? Any type of mission to try to remove all subjectivity from subsurface modeling or subsurface work tends to simply be using kind of gross assumptions to sweep things under the rug. Often we're fooling ourselves if we think we can take away all subjectivity. Now, I know as engineers, that's kind of hard to hear. In fact, what's fairly funny is I'm half an engineer and half a geoscientist. So as I say this, I'm actually simultaneously disturbed and encouraged at the same time. Because I'm talking about the subjectivity. Now, without this decision, we're stuck in the hole, we're stuck at the data locations. We can't calculate statistic. We can't do anything. In fact, I have to decide on two things. There's two components to my stationarity decision. The first decision is an import license, the choice to pool specific samples to evaluate a statistic. I've got one through seven core samples in this area of interest. When I decide to pool them together to calculate a histogram, that's the import license. That's the decision of what I can pool together to calculate that histogram. The export license is different. That's the decision of where can I use that histogram as a predictive model? Because it is. It is a predictive model. I have a P10, a P90. I've got it. I can calculate a P10. I can calculate a P90 or a noble might reverse those. I think you do. Let use the X on P10. So P90, P10. And you could calculate an average, which would be a very good estimate. That minimizes the squared error that you could take the average. Very cool. And so this is a predictive model. The choice of where to use that predictive model is the export license. So you could decide that this dashed line area, you can use those statistics as a predictive model. You could go all the way out here. You could actually use this distribution in an analog field, like use it as an analog for a field on a different part of the world, in fact. And that's a very strong decision of stationarity. You're saying that what you sampled here in the earth is going to be the same or same enough as what you sample over there. Now, the geologic definition, often when I teach, I do have a lot of geoscientists. I think we're a little bit low on geoscientists, but a geologic definition would be something like the rock over a stationary domain is source the positive preserved post-depositionally altered in a similar manner. In other words, they were talk about the processes by which the rock formed and was preserved. And they'd say it's similar. And it's mapable and we can therefore use it all of it to pull together to get statistics for us to make predictions. So S4 unit and the Tress Pazzo's formation here in southern tip of Chile, which is beautiful rock. If you have a chance to go, I had a wonderful month or so down there. This S4 interval could be deemed to be stationary. It's all stationary. It's all the same type of rock. The statistical definition of stationarity goes like this. The mean, the expectation of the feature of interest over locations U, U is a location vector, is equal to a constant mean for all locations. So in other words, the statistical definition is you can remove location. The statistic is invariant under translation would be the fancy way to say that. It does not change. So now notice in the statistical definition, you have a metric. You decide what the metric is and you decide over what region is it invariant. So you got two components there at a stationarity. The distribution, F capital is a CDF. In statistics textbooks, that's a CDF. And it's the CDF for the Z variable or feature. It's dependent on location. And then you say it's not dependent on location. It's constant. The CDF, the entire histogram is the same everywhere for all locations. And the verigram that we're about to talk about can also be stationary. Stationarity, what metric or statistic over what volume. Okay. And you can extend this to many different methods, metrics. It can get very complicated.
 What I want to show you now, take a minute to show you now, is our course project. So in learning, in learning all of these components, these individual modules and the exercises that are associated with them, we'll begin to put together building blocks of pieces of what will become and we will build by the last day this dashboarding application, what I call the AutoType dashboard. And so this basically allows you to select a state and an operator and then an API number and it will bring up the production history for that API number. And so right now by default it plots VPs wells in New Mexico, but for example, if I change this to Noble, then it will update. I messed with this earlier and it seems like it really competes with band. So I'm actually making live database calls. I'm calling, we have a database that's sort of equivalent to drilling info. In the sense that it has all the, it's compiled all the public production histories for all the data, I mean for all the public data that is available in the United States and we have it in a database available to us and accessible from our learning management system. So I did was playing around with it earlier while I was on Zoom and it seems to compete with bandwidth for Zoom while I'm doing this Zoom video quite a bit and so it's fairly slow. But you did see when I changed from New Mexico to, I mean, BP to Noble, it did update the well locations and then I can basically go over here and I can select either in the drop-down list over here, which is an updated list of API numbers that four nobles wells in New Mexico. I can select them here or I can select them on the map as well. So for example, if we wanted to maybe zoom in on this group of wells right here, I could select one of them and that wells, it will sink. It takes a moment again because of the bandwidth issue. So there it goes. It highlights the API number over here and the drop-down selector. So that's the API number associated with that well and this is the production history for that well, I guess before it was shut in. And then if we choose to, for some, you know, it's not a robust tool. It wasn't meant to be. I mean, you know, this was built or intended to be something that we could build in a few days together. So it's not the most robust tool in the world. But for data that's, you know, sort of well behaved, we can try to fit TypeCurve to it. And in this case, I don't, yeah, and this data is not a good data to try to do that. I think I, let's select those down there, see what happens. Again, when we build this, yeah, it's kind of slow, but there you see some TypeCurves where, so that in that case, there's, what appears to be one well is actually three wells really closely spaced to each other. So right next to each other there, it's three separate wells, but when I was zoomed out on them and I selected it, I selected all three wells. So it brought up all of their production histories and then fit TypeCurves to all of them, you know, as they are there. So again, when we build this within our learning management system, it'll be a little more speedy interaction than what you see here. It won't take as long as I've shown the update because again, I'm actually running this on my local laptop and I'm competing with, you know, every time I select one of these, I have to make a live database call into our database, get the data, fit the TypeCurve, update the plots, all while competing with bandwidth with Zoom, which is a bandwidth hog. So that's why it's a little bit slow on the update, but the one we build in class in our learning management system should be a little more interactive.
 And what I've cursed you with is the next time you sit in the meeting and somebody shows a histogram or a PDF and it will happen all the time it happens, you're now going to look at that and you're going to think naive, right? You're going to think naive, that's the raw data that wasn't declustered. And you're also going to think, huh, was that stationary? Where was it collected over over which area can I use that distribution? Okay. So let's talk about spatial continuity. We talk about spatial continuity because it's the way we quantify the spatial relationships and everything we do in our business is spatial or temporal. It really is. Like if you look at most of our data sets, they've a spatial, even production data at a single well, it's got a temporal behavior and it's got relationships over time. And you could use the same type of calculations. In fact, the diagram I'm about to explain in time series analysis is known as autocorrelation. And it's the same thing. It really is the same thing. Okay. So let's go ahead and convince ourselves what we already know. And that is that spatial continuity matters. We'll give ourselves an injector for producers. We'll use the same distribution of porosity, permeability, but I will build different spatial arrangements of permeability. And I'll just set the per the porosity to be a simple model. It'll be related to permeability. Okay. So we only have to visualize permeability. So let's get let's begin the left hand side and the right hand side are permeability from one to 100 milledarsies. They all have dramatically different spatial continuity. This is spatial continuity of this model. Does it have good spatial continuity? Does it have good spatial correlation over distances? This model is random. In fact, at every single location that I did Monte Carlo simulation from the permeability distribution just random. Now kind of cool. It looks random. It is random. Okay. It has kind of a mid-range spatial continuity. At this location right here, the well saw high permeability and its high permeability to this distance right here. High permeability, high permeability, low permeability, low permeability. There's a correlation over distance. I will explain that shortly. Okay. Let's go ahead and run flow simulation. I was lazy. I didn't run full flow simulation. I ran a fast marching methodology, which is a good, what is it called? A permeability weighted distance calculation of earliest arrival. It's a very simple. It's very, very fast too. It's a good measure of heterogeneity. The time of flight is the outcome from that run. We've got very short time, longer time, longer time. You see what we're seeing? We're seeing the substitution of the in-suchu fluid with the injected fluid. We can see the sweep pattern in the reservoir. This, this. Which one of these has better recovery? Look at that, eh? You got nice, equal sweep, nice piston-like displacement. That's beautiful. I wish, I wish that upon you that all your fields produced like that. Just nice sweep. I wish that upon all of us. But of course, in fact, we have heterogeneity like this, which does beg the question. This had no spatial continuity. This had really pretty good spatial continuity, which was better for production. No spatial continuity, isn't that funny? Now, this is a unique case. This is what I would call homogeneously heterogeneous. You see that? I'm telling you, part of my goal of this time together is to give you cool phrases you can use at dinner parties. I really hope somebody says, homogeneously, I'm sorry. That was probably kind of a bad thing to say right now. I know we're all isolating. Shucks, when you have dinner parties at some point, I hope you will celebrate it by talking about homogeneously heterogeneous phenomenon. Okay. Okay. Now, how about this? This is even better spatial continuity. In fact, this is beautiful spatial continuity. What's going on now? Which one has better production? Yeah, I'm seeing it. Now, which one's better? Middle, mid-range spatial continuity or very long spatial continuity? Which one's better production? Is this a Worshara test? Is this like, I'm like, hey, what do you see in the ink blobs? What do you think? Is it better to have long continuity or mid-range continuity? I personally think mid-range continuity was not a good thing. You see, we have early breakthrough to one of our producers. We have almost no sweep to these producers over here. These are not looking great for us. I apologize. The sun is coming through my window right now. This is just the time of day where it's getting me. I'm getting lit up now. Okay. And now, we could also have spatial continuity with anastropia. Now I'll tell you, geology is anastropic. Like it commonly has preferred directionality and now look what's happening. We got no sweep to these wells right here. We're only sweeping. We got early breakthrough to this wall again. And so clearly heterogeneity matters. Spatial continuity matters when it comes to our fields. And so we want to quantify spatial continuity. Spatial continuity from very short or no spatial continuity, random, medium, long, and anastropic. All of them had an impact on the flow behavior of the reservoir. And so we really need to be able to quantify to measure spatial continuity. And we need to be able to then impose it in reservoir models. And even more than that, we need to impose it on our data analytics, all of our data and analytics. In fact, when we do bootstrap, spatial bootstrap uses a measure of spatial continuity to account for that. A topic that I've introduced through some of our office hours is even this idea of number of independent data. And all of these concepts are based on incorporating spatial information. Okay. Now that I've said spatial continuity like 10 times, I should define it. I should give you a more rigorous definition. Spatial continuity, correlation between values over distance. This value correlated to this value means spatial continuity. No spatial continuity, no correlation over distance. It would be like random phenomenon would have no spatial continuity. Homogeneous phenomenon for which all the values are the same. That's perfect spatial continuity. So that does kind of raise an interesting idea. Spatial continuity tells us about the level of predictability away from a well. If I have good spatial continuity or homogeneous phenomenon that are perfectly continuous, I can resolve the entire reservoir with one well because I know all of the values away from it because it's homogeneous. Okay. So this relates the idea of spatial continuity, provides you a definition. Who here, by show of hands on the participant window, who here has done or calculated a verogram before? Anyone? Okay. We'll lower the bar a little bit. Who here understands what a verogram is? Who here has, no, I think that covers it. Okay. So today's surely a great day. I get to describe verograms to you guys. So this is what a verogram does. And if you think about it, it's actually very simplistic. What it does is I already said spatial continuity is correlation over distance. This value is correlated to this value. That's spatial continuity. So to calculate spatial continuity, let's just go through our data set and identify all data this far apart from each other. So we'll form a vector. That vector is known as the H lag vector. And it's going to be have an orientation like a vector does and distance like a vector does, right? And so what we'll do is we'll scan through our data set and we'll find every time we have data this far apart from each other. Then what we do is if you look at this equation right here, we calculate one half the average square difference between those values. Okay. Here's a pair of values here and here. Let the square difference do it again, do it again, do it again, take all of those pairs and sum the square differences and then calculate the average of that one over n of h. Why is that n of h? It's because you only have so many pairs and the number of pairs will vary based on the each vector. If the h vector is very close together, you might have more pairs, more far apart, you're going to have less pairs. Okay. And two, to make the math work, we do one half. We're engineers so I get to share this with you. Who here is calculated a moment of inertia recently? You guys know what I'm talking about, right? Everybody knows what I mean by moments of inertia. Remember how that was kind of some of that early statics and dynamics and we learned about all that integration with it and that was really cool, right? That's exactly what this is by doing one half, this turns this calculation into a moment to inertia of the points around a 45 degree line of the scatter plot of the tails versus the heads. And that also makes the statistical math work. So in case anybody wanted a flashback to statistics and to fundamental statics and moments of inertia from engineering, we had both in a day. So that's a great day. Okay. So this is simply the way I think about is one half the average square difference between the head and tail values. Okay. Now this plot right here is the plot of the tail values to the head values of all of those data that we identified. And that's known as an H scatter plot. And that's what we get the moment of inertia of the moment of inertia would be just the point moment of inertia of these points around that 45 degree line. And it makes sense that if the points are scattered further away from the line, this value gets larger like a moment of inertia would. How would we minimize the moment of inertia of points around the 45 degree line if we're assuming that we're rotating around that over what configuration of points would we minimize the moment of inertia? What would happen if what would happen if all of the points were on the 45 degree line? Then we would minimize the moment of inertia and the verogram calculation, all of the head and tail values are equal to each other. This goes to zero. We sum up bunch of zeros. We take the average of zeros, the verogram value would go to zero. So in other words, the homogeneous phenomenon or perfect correlation would result in a verogram value of zero. All of the head and tail values are the same. The verogram is zero. Okay. Now let me define the verogram because I've already shown you the equation I described how we calculated. The verogram is a measure of dissimilarity versus distance. It's calculated as one half the average square difference of values separated by a lag vector of H. And so that's how we would say it with words. The precise term for verogram is semi-verogram. Not a big deal. The reason being is because we do this one half, they call it semi-verogram. Some people in statistics literature will calculate a verogram without the one half. We in geostatistics, we don't get worried about that. We just call it the verogram. It seems kind of a little esoteric. We're just showing off now. But the reason we put the one half there is not so just so it's a moment of inertia, which was cool. But so that this mathematics right here works. The covariance function, which is a measure of similarity over space, is equal to the variance of the problem, which we will call the sill minus the verogram. What we've done by putting the one half in there, we make that math work so that the verogram is just the opposite of the covariance function. And the covariance function is a measure of the similarity. Now there's also a Corellogram. You may hear about that. It's just a covariance function standardized by the variance. If the variance is equal to one, the covariance function are both the same. Now let me go ahead. I'm going to make five observations of a verogram and we'll circle back and we'll make sure that we kind of nailed this down. The first observation of a verogram. Now to make these observations, what I've done is I gave myself a data set. Everybody see this data set right here. It is an exhaustive data set on a grid on a mesh with a whole bunch of porosity values. High porosity is a hot color, a red color. Low porosity are cold colors, blues and purples. And so you can see we have kind of heterogeneity here in our field. Okay. Now, put your fingers together like this. Pick a spacing and put it on the screen and move around on the screen and compare values to each other. If your fingers are close together, do you see that that verogram would see lows and lows, mediums and mediums, highs and highs compared to each other? Do you see that if your fingers are very close together, you're always comparing values with similar, they're similar to each other. The average square difference would be very small. Okay. Now put your fingers further apart and go through the data set and compare what you see at your two fingers. Now I see a low and a high, low and a high and a low, a medium and a low. I see a lot of contrast now. What does that mean? As the distance H lag increases, we expect the experimental verogram values to generally become larger. Do you see that? The one half square difference is generally getting bigger. And so as we plot the experimental points of lag distance H, the distance our fingers were apart versus the semi-verogram value, we expect monotonic increasing on average. In general, that should happen most of the time. Any questions about that? Does that make sense? Okay. So it's a good measure of spatial continuity. We're characterizing spatial continuity across different scales. That's pretty epic. That's pretty cool stuff. Let me talk about something else. When we calculate the verogram, we are greedy. We use every possible pair in the data set. I've taught verogram calculation many, many times. And my students still think that we go and we just identify one or two pairs and we're done. Nope. I look for every possible data that are that far apart. I take all of them. I want all of those samples. I'm greedy about that because I get the best statistic. In other words, for this set right here, my fingers are apart about 100 meters. I go here on this data set with the grid. I'm going to calculate the first value with the value about 10 cells over to the left. To the right. Then I'm going to move over one repeat, repeat, repeat, repeat. And then I'm going to move down one row, repeat, repeat, repeat. I'm going to calculate all of the possible pairs. I'm going to include all of them. And so for this exhaustive data set, I'm in fact working with probably getting to be about 9,000 pairs or so. Quite a few pairs, if this was 100 by 100 or so. So there's a lot of pairs. We'll take all possible pairs. Observation number three. So third observation. In order to interpret a verogram, you must plot the sill. The sill is going to be a line you put on the verogram plot that is going to be equal to the variance of the problem. Now this is really cool. The sill is the maximum amount of variability. In fact, if you think about it, if I asked you what's the degree of correlation between two points, and your answer was, well, the level of dissimilarity is equal to the global variance, I'm basically telling you that there's no correlation. It's the maximum amount of uncertainty over that lag distance. It's basically draw any value from the entire distribution, and that's what you should do. Okay, so the sill is the variance. And remember the math, the covariance function, the measure of similarity over distance, is equal to the variance, the sill minus the verogram. In other words, when I plot the sill, I can think in my head that this distance from here to here, at that distance of 150 meters, that's the degree of correlation. So what I can see is that very short distance is I have large correlation, at very long distances I have very small correlation. Okay, so this is critical to us interpreting, because if I put the sill way up here, then all of these distances have very good correlation. But if I put the sill right here, no correlation passed about 400 meters. Okay, any questions about plotting the sill? Interpreting a verogram. Okay, these are very important. Now we can go a step further, and I can talk about how we interpret individual points on the verogram. If I have a point down here, at point four, verogram value, and the sill is equal to one, then I would expect that the corellogram would actually have a value of point six, this distance right here, because the sill is equal to one, it's standardized. And that would tell me the correlation coefficient of the HH scatter plot between the head and the tail values at that distance right here of about one or two or about three meters, maybe. Now, if I look right here, the point, the experimental point is on the sill. If I plot the scatter plot between the tail and the head locations at that distance of offset, what I'll find out is that the correlation coefficient is the difference of the distance between the sill to that point, the correlation coefficient is zero. It looks like a shock on blast. There's no relationship. At that distance of about 12 or 13 meters, there's no correlation. And if I go above the sill, look at these experimental points right here, the correlation coefficient I apologize, that should be negative point four seven. Okay, we have negative correlation when the experimental points are above the sill. Okay, any questions about this? Good, good. All right. So now we know how to interpret a verogram based on the sill. We can interpret what's known as the range. So the distance at which the experimental verogram reaches the sill is known as the range. That, the way I think about it practically is this. I imagine I drilled a well. And now I imagine away from the well at a certain distance, I have no information. Because I'm past the range, there's no correlation. I have no information. So from now on, if you've calculated a verogram and your boss comes in your office and says, we drilled the well right here, what's going to go on over here? You now have my permission to say we know nothing. Well, you could have trend information and other type of secondary information, but just data driven information from spatial continuity, you have no information. There's no correlation. Okay. So the range is a critical thing to interpret to understand spatial continuity. In fact, the bootstrap was assuming that all of the data are outside of the range from each other, i.e. they have no correlation with each other. That's what bootstrap assumed. So that's good. Now you can check that assumption because you know about the range. Okay. Let's talk about the nugget effect. Sometimes there is a discontinuity in the verogram. That distance is less than the minimum data spacing. This is known as the nugget effect. So you see this experimental verogram point, point, point, point. There is an offset right here, the shortest distance. It doesn't seem to be extrapolating to zero. Now the verogram value at an h equals to zero is equal to, which of the verogram value be an h equals to zero? The whole lag offset, one half the average squared difference. At a zero lag, you're literally comparing data with themselves. I'm saying, oh, what's the difference between data? Well number one, well number one, well zero every time, right? And so you're exactly right. Thank you, Scott. So we know it should be zero right here, but you can still have a discontinuity that occurs at very short distances. This is known as nugget effect. It comes from the mining industry in which a gold nugget can naturally form in a diamond core sample, a diamond drilled core sample. And what that can do is over a very short distance, it can cause a large amount of discontinuity or change and it shows up as nugget effect. Okay. Now, let me ask you this, do we have nugget effect in our reservoirs? They have it in gold mines. Do we have it in our reservoirs? Is there any physical reason that we would have very short scale discontinuity or change? Unconventional, unconventional production. If I was doing a spatial continuity measure of unconventional, would I expect to see short scale discontinuity? IE, great changes between wells that are right beside each other. What causes that? Okay. What causes it? What do you think? Is that kind of the secret sauce, eh? I think personally it probably has something to do with maybe natural fracture systems that we drill into or we intersect when we do our artificial or hydraulic fracturing. I think there's also variability in hydraulic fracturing. I agree, faults. Now, what's very interesting is that also random error would cause nugget effect. So I want people to be careful about nugget effect. If you're a measurement tool and that got talked about during the break, thank you very much Dan. We talked about measurement error. The measurement error will actually be directly shown as nugget effect. Now why do I bring that up? If you're trying to understand the variability of your problem and you're trying to figure out what's random noise or error in the machine or the tool versus what's actual geologic features, you can use the verogram as a diagnostic tool to detect that. And I think that's very, very powerful. In fact, I suggest people do verogram calculations in order to understand your data set, kind of a default thing we should really be doing. Even if we're not going to build reservoir models. OK, let's look at three different maps. These three different maps all have different verograms. What I want you to do, who here has seen the Matrix movie? That's classic sci-fi, right? OK, many of us have seen the Matrix movie, right? I want you to see the Matrix. Basically, get to the point where you see the Matrix. In other words, you look at the data set, exhaustive map. I want you to see the verogram in it because the verogram is a good quantitative way to see spatial continuity. It's like seeing the Matrix. OK, so you see this phenomenon, this phenomenon, this phenomenon. And then these are the verograms by which they were made. So these are the verograms. Now, I want you to put your hand up on the screen. Don't touch the screen if you're one of those people who hates fingerprints on your screen. But go ahead, my students always touch my screen when they come and explain things. Go ahead and put your hand up there and block this part. Just show this part right here. OK, put your fingers up there and now compare each one of these verograms. Now just what you're doing right now is you're focused on the short scale continuity or discontinuity, because the verogram is discontinuity, right? Now look at the data sets and you look at the short scale. Now the spherical has pretty high discontinuity. The exponential, we could argue, has even a little bit more discontinuity in the short scale. And the Gaussian houses its short scale continuity. Beautiful, beautiful. It's very continuous. Now look at the image. Like stand back and look at short scale continuity. Do you see it? Do you see how the image in the bottom is extremely smooth? It's very short scale continuous. OK, everybody, you see that? OK, now we're going to do a trick. Take your hand and block the short scale and just focus right here, right at the very long scales. OK, so you put your hand up to the screen. I hope what I can convince you is that those three functions are more similar in the long range. They're becoming very similar to each other. Now this is the trick. Look at the three images and pay attention to long range continuity. Ignore the smoothness. See beyond, see the matrix. Look at the long range. In other words, notice how this is high. This is high. This is high right here. This is high. This is high. Notice the spacing of the highs and the lows. Can I convince you that they have very similar long scale continuity? You see that? Isn't that cool? Now seeing spatial frequencies. OK, all of our many of our retis wars are multi-scale and many of them have spatial frequencies. And by calculating the verogram, we're quantifying spatial continuity across different distance scales. And we can see that. And that's very, very powerful. OK, now I'm not going to teach this to you, but I really wish I had time to do it because this is really valuable. We in fact build models of spatial continuity with nested verograms, each of them describing the different spatial frequencies. And when you do that, you're actually decomposing the problem into each one of the frequencies. And what's really powerful is sometimes they can be explained through geological, geophysical and engineering phenomenon. We're actually understanding the heterogeneity and taking it apart. And I'll give you a very simple example. In an unconventional setting, we might find production has short scale variability due to the fractures and so forth. But we can still see in unconventional transitions or large scale shifts in production due to overall change in matrix and geologic setting, thickness, bitumen content. I should have said TOC or something like that. I went to oil sands. I'm a Canadian. I sometimes go back to Albert. But you see what I'm saying is there's long scale features you can decompose and explain. And here's where it gets a little more powerful. I like to give gold nuggets, speaking of nugget effect. All of those spatial frequencies describe different components of the variance. And the variance is the uncertainty. Therefore, we can know where we can focus to resolve uncertainties. This is really kind of some powerful kind of like next level stuff. Okay. Verograms are very, very powerful. I do think they're really cool to work with. Any questions about the verogram? Verogram calculation? I want to just comment a little bit on calculating verograms. I don't think today we're going to take the time and calculate verograms by hand. I do get people to do it. Sometimes I think it's good. If you do have time after this, consider doing a by hand verogram calculation. It's always nice to do. If you wanted to do that, you could in fact just take this data set. I believe you could copy it from the slide deck. I think from the PDF, you can still just copy it. You could just put it directly into Excel. You could calculate the verogram on it. Not a big deal. It'd be super easy to do, spoiler alert. The way you would do it is you could either make a cell here in Excel in which you would just take the difference between this cell and this cell and square it. That would be the one-lag verogram calculation. Then I would just repeat, repeat, repeat. Once I have that in one cell, I copy it all the way down. I have n number of data. I would have n minus 1 cells here because the very last one will go off the edge of the data. Then I could go ahead and take the average of that, take one half of that, and I would in fact calculate the verogram for the first lag. You could also do this. This is kind of fun. Take the data set, you copy it, and then you just slide it. You offset it by one. That's the same thing as what I just did. You just do the comparison between these data, square that, take to some of that, take one half the average of that. This is all the square differences right here. Take the average divided by two, you get that value right there. Another thing to fully explore and to explain this, we could go ahead and plot this value versus that value, that value versus that value. That would be the H scatter plot that I talked about before, where you're comparing the tail and the head values. I'm basically taking my two fingers, scanning through the data set with the spacing of one data offset, and I'm just comparing the head and tail, head and tail, head and tail. If I calculate the correlation coefficient of that, I'll get 0.77. The verogram value was 0.23. Do you see what's happening there? Like I told you, the variable has a variance of 1. Therefore, the correlation coefficient of the H scatter plot is going to be equal to 1 minus the verogram value. You see the 2 sum to 1. This explains the overall calculation of the verogram. I repeat it for a lag. This is the lag of 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. That's the lag of 12 data offset. If that's measured in quarter meter, that's going to be 3 meter offset, or sometimes we'll just say it's the 12th lag. I can go ahead and calculate that. I want to point something out. What happens when I pick 14, 15, 16, 20 of lag? How many pairs do I have? Does it go up or down? As it go to larger lags? It's definitely going down. Do you see that? How we're getting fewer and fewer data to compare? The other thing I want to point out is as I go to a very long distance, I'm comparing only the edges of the data with each other. I'm only comparing the ends. I don't use the middle data anymore. It can become unreliable. The rule of thumb is don't calculate the verogram beyond one half the data extent. If you wanted to, you could go ahead and take the data set, calculate the verogram. If you did that for all of the possible lags, the first lag, one quarter meter, second lag, one half meter, this lag is one meter, and so forth. This is the verogram you would get. Now that you understand the verogram and the interpretation principles, may I ask a following questions. First of all, what is the range of spatial continuity for this phenomenon? What is the distance over which I can make a prediction? What is the distance beyond which I know nothing? What do you think? Okay, about, I love it and Cody, thank you very much for using the approximate mathematical symbol. These I agree with you. There's some interpretation here. About one meter, you get picky about it and say, well, I think it's more like 1.25 or 1.5. But clearly, you could even go a little bit further. 1.4, I would agree with all of that. It's really, you know, the data is a little bit noisy, but you could say somewhere around that ballpark is the range. Thank you very much for that, everybody. Okay, let me ask you this. What's going on at three meters? If your boss said, make a prediction at three meters, would you make a prediction? Do you know anything? Trick question. Okay, it is information. Now when I say beyond the range, you know nothing, that's assuming that the verogram tracks along the sill. It often does. If the verogram rises above the sill, it usually indicates you have a trend in the data. And negative correlation is information. This is getting to be a correlation coefficient of about negative 0.5. That's getting to be some pretty good information right there. So yeah, we want to include that. Now let me ask you this, the total data extent was 10 meters. What's going on here? Do you see how the verogram is just kind of getting kind of crazy? What do you think? Remember when we looked at the verogram calculation, when we get very far, we're comparing very few data, we have very few pairs. We're only looking at the edges of the data set. When we get beyond about half the distance of the data set, it's becoming unreliable. This one you calculate a verogram. If you do calculate beyond half, just in your mind, remember that it's unreliable. Be a little bit careful about how you use it. Yeah, here's the verogram and the covariance function. I provide the equations. I explain them. The covariance function, how to calculate it. I've already explained practically speaking how we relate it to the verogram. And the way we do that is if this is the verogram, monotonically increasing from the nugget effect up to the cell and exceeding the cell in this case, the covariance function is going to start at the nugget effect subtracted from the cell and it's going to go lower and lower and lower. The covariance function is a measure of similarity over distance. So it's the upside down. It's the opposite of the verogram value. And the definition of the covariance function is shown right there. The main principles I want you to retain is the nugget effect, the short scale discontinuity, remember, it could be measurement error. In fact, in spatial phenomenon for which you don't expect nugget effect, use the nugget effect to work out what proportion of the variability is random noise. That's very useful. Cill is the sample variance. You plot it to be able to interpret and the range is the distance at which you know nothing. At the point at which beyond that, you have no correlation any longer. Okay. Here's a bunch of things we would do if we were doing a full course on spatial continuity. We would talk about exactly how to calculate a verogram with a regularly spaced data. We would talk about valid spatial models. That's where we put the multiple spatial structures together into a verogram model. And we talk about how to do that in 3D. We use a concept of geometric anastropie. That's the same as Walter's law. If someone has geoscience background, it's the idea that the variability I observe in this direction is the same as the variability I observe in the other directions just with different scale length scales. And so we use Walter's law or geometric anastropie mathematically. Okay. But we won't go into that. I will mention the fact that if we're calculating a directional verogram though, we do need to work with some type of a tolerance window. So when we calculate verograms, we're going to have to make some interpretations. We're going to pick the lag distance over which we want to calculate the verogram. And we're going to pick a lag tolerance. The reason we do that, if I'm calculating a verogram in the 45 degree direction, this direction right here, what is the probability that the data is exactly on that vector 100 meters apart from each other? Have you ever been in a field where you've seen exactly the data, the wells are always 100 meters apart, 100.00? They're usually not. There's going to be some variability in the location. So what we do is we say, I'll say 100 meters in that direction plus or minus 50 meters. So I'll accept everything from 50 to 150 meters. And for the second lag or third lag, I'll say everything from 150 to 250 and so forth. And so we get this type of a template. Now we also want an angle tolerance because what's the chance of two data or two wells being exactly 045 asmaps separated from each other, not going to happen. Most of the time it's going to be like 4153, whatever. So you're going to put like an angle tolerance. You're going to say 45 degree plus or minus so many degrees. Okay, and we can do that. We can calculate our verogram very nicely. We can make sure that we get a nice reliable verogram calculation because we're pulling the data. Now, let me ask you this. What would happen if I made the angle tolerance 90 degrees? At that point, I'm going in this direction, but I'm going full 90 degrees each direction. What would that do to my verogram calculation? If I went like this and I went in this direction or went in this direction, would it matter anymore? Would direction even matter anymore? If you think about it, 90 degree plus or minus would make direction no longer matter. All of your pairs that you'll be collecting would always be just plus or minus a distance. That's an isotropic verogram calculation. 90 degree tolerance gives you isotropic verograms. Okay, now just to kind of further illustrate this, what we're going to do is we take and we're going to form this template. We're going to say I want every data point that's plus or minus this distance right here. I can take that template and I can anchor it on one of my data. I can identify all of the pairs of data that have that distance and direction tolerance. Then I'll put them in the verogram calculation and then I'll take it and I'll move to a different location and I'll do it again. Now remember, I told you we're very greedy. We're going to collect all the possible pairs. We move to the next node. Move to the next node and we do that through all of the data that are available to us and over that entire calculation, we calculate one distance, one point on our verogram. We repeat that for the different lag bins. This bin, this bin, this bin and so on. That's what gives us the entire experimental verogram. That's how we're going to calculate it. Now choosing directions, how do we do this? I think I'll show an example of this is we got to pick a major direction of spatial continuity and many many many reservoirs have a primary direction of spatial continuity. Let me ask you this. What would cause directionality in porosity in a reservoir? Why would we have better continuity in one direction versus another? You could imagine just look at a modern day delta and you'll see the river channels are going from the on top of the slope down into the basin. That causes if deposition is being controlled by those river channels in the channel or outside the channel, you would expect that to cause alignment, primary direction of continuity. Thank you, Tyler. There's many causes for directionality in our phenomenon. Some of them are very complicated. I think about carbonaceous type settings and they got those whole environment of a reef platform type setting where you have large trends due to the fact that you have high and low energy, high and low nutrients, deep and shallow water. It can cause huge impact of directionality that can locally change. What we'll do is measure the directionality very simple and 2D, we just pick an aspect. Why direction is 0, 0, 0, 9, x direction is 0, 9, 0? We'll just measure an asmr between. 360 is coming all the way around back to 0. Not a big deal. If we do omnidirectional verogram calculation, we don't care about the direction. We're just saying that we just want to understand the behavior of a continuity irrespective of a direction. Some reservoirs are isotropic. Some don't really have strong directionality. Some like, I know some coastal type settings and what can become very confused and don't have really great directionality. Okay, so guidance for a verogram calculation, how we can do it? Like separation distance, you want to pick this fundamental lag distance as being the minimum data spacing. If you go smaller than that, you won't be able to calculate a verogram. There won't be any pairs available to you. The lag separation distance based on the minimum data spacing. The lag tolerance, one half of that, that means that this template has no gaps. We're going to this distance right here, plus or minus, this distance right here, plus or minus, there's no hole in our template. We don't miss any data. The verogram is only valid for distance of about one half, the field size. So calculate a verogram up to about one half. If you've got a thousand meter by thousand meter go to about, you know, if it's isotropic 700 meters because in the 45 degree direction, that's 1400 meters. You know that kind of thing. Put that together.
 Next thing we're going to talk about is intake. Okay. So if you'd like to click on that link, it'll open up the lectured notes. I'll be serving them from my slide view here. Okay. What does intake? Intake is a package, a Python package that is intended to take the pain out of data access and distribution. It's a package for loading, investigating, and organizing data. And we'll use it from the perspective of a data analyst. So you could also think about this as a data engineering tool. And aside from the little bit of data engineering that I'm going to show you in this presentation, we won't be doing any data engineering. I was the data engineer. I set this all up for us so that we could just use it. The benefit of this, and like I said, we're going to be in all of the exercises going forward, we're going to be using real data, whether it's, you know, petrofysical data or, you know, production data. And the in the cases of the production data, that data comes from, you know, it's a live database call. It's actually a post post secret SQL database. For this class that we didn't want you to have to know SQL and know how to write these sometimes complicated queries. So we've hidden all that from you and intake is the kind of tool that allows us to do that efficiently. And not just for SQL, but for basically anything, the data could be stored in CSV files, in several other types of data structures, you know, that are useful for storing data. And we'll just use intake to, you know, basically call the data in from a data source. And I'll show you how to do that. And then once we do that, the data will be stored in containers that we'll learn about, right? So we try to teach you things in order. This is the only time that I kind of have to get, we have to get a little bit ahead of ourselves in the sense that I know you don't, you may not know what a numpy array is or a pandas or a task data frame. We're going to learn that, you know, later today, what those are. But when we pull the data in from intake, it will already automatically be loaded into a convenient data structure for us that'll be one of those things, a numpy array or a pandas or a task data frame. Like I said, it reduces boilerplate code. I have an example of that in the slides here, facilitates reusable workflows. One really nice thing is that we can install data sets as Python packages. Which allows us to actually version control our data in a way, which is really neat. And then you can have these so-called self describing data sources. I'm sure that you've had people send you data before and then you try to look at it and you can't make any sense of what's in there. There's no description. There's not a proper description of the thing. So when you do the data engineering on this, in this package, then it allows you to set all that up automatically. You can also do some quick plotting, which I'll demonstrate. So the way we'll primarily interact with intake is just like this. So you'll always have to import basically the standard catalog. So we'll say from intake import cat, that's going to bring in a catalog of data sources and if you want to see what's in that catalog, you just list cat. So if you type list cat is going to show you this list of data sources. And all of these data sources, I mean, they're most of them are intended to be named in a way that is somewhat meaningful. I think most of you could probably figure out what production by state means. In other words, this data source takes an argument state like New Mexico, for example. And it will bring in the production history of every well that we have in the public database for every operator in the state of New Mexico. You can look up the production history of a single well by its API number. This is a more general thing that allows us production columns. This allows it to basically tell it exactly what we want to bring in from our database. You almost have to have some knowledge of the database to be able to use this one, but most of the other ones, that's not the case. So APIs with production data by state, tickers by state, well location by ticker and state, well location by API. I think most of you can probably figure out what well location by API is. If I give it an API number, it's going to return the well location, the surface whole latitude and longitude for that well. These other ones are certain. If you took Dr. Purchase class the last two weeks, you saw it, you used some of these. Those are some of the petrophysical data he uses in his class and we'll actually use a little bit in this one as well. So John, can I interrupt real quick? I'm getting a couple questions about something and I know I know you like to talk about this one as well is question coming up about the difference of view that people have in their classroom versus your PowerPoint as well. I think people are thinking that it's that it's a little different. I know we're on the first packet and just want to make sure you just tell the difference on that. I think I mentioned that the other day, right? If when I was discussing Jupyter notebooks, so if you open this, I know I did because I went on this rant about PowerPoint reporting, right? And ever tough. So if you open this, you see a plain Jupyter notebook which parts of have will be identical. Like for example, this part right here is identical to what I was presenting in my slide, but then there's a lot of explanatory text. It turns out that this is actually the same exact notebook. In fact, I can go here and exit out of this view and there you see that this is exactly the same notebook that you're looking at, but you can't launch it in the slide mode. And the purpose of that and I, you know, again, I went into this rant the other day is because PowerPoint is the worst tool in the world for reporting. You cannot take, you know, this is my philosophical rant here, but I'm not the only one who feels this way. And you know, you cannot, if you try to make a report in PowerPoint, then it's a horrible presentation. You never, you never see a TED talk where the slides have a whole bunch of words all over them explaining everything like the people like you do when you see a PowerPoint that's intended to be some type of reporting. So if you try to use PowerPoint to do reporting, it makes for a horrible presentation. And you know, otherwise, then you can't have enough information in there. So my slides that I'm showing you are just extracting a subset of what you're viewing. So when I launch this in the slide mode, then you just see a subset in which I talk about the differences. And that's why I try to design these slides to be good presentation slides, but I try to also leave the notebooks to be read as documents or reports. Right? It's a real beautiful thing about Jupyter notebooks. And then I can, with one, you know, with one thing, I can give a presentation that's, you know, a good presentation, not, I mean, I hope it's a good presentation. It omits a lot of that text that makes for poor presentations. And then otherwise, I can leave you with something that if I'm not presenting and talking and you need to go back and use it as a reference, then there's a real document that you can read as a reference. So is that, does that answer the question? Aside from my, I think that was, I know I think that was perfect because I think I I've started appreciating the beauty of Jupyter as well. And I wanted that explanation because I didn't think it would do justice responding in an email. I really wanted you to showcase that because I think as people use Jupyter more and more as well, it's a, you know, you start discovering cool things about it. So I just wanted you to showcase that. I know you like to. Yeah. And I think I did talk about this the other day because I also showed, I also showed you this book that I've been working on. So the data book, which is not, not ready. But these are also Jupyter notebooks, but they've been, in fact, there's the one on intake right here. However, it's, it's, this one's not complete, but, and, and, you know, it's got some errors and stuff. And that's why it's not quite ready for prime time. But this is exactly the same Jupyter notebook. But now I've published it as a web page. And it has this, you know, much nicer formatting for, for reading as a book, right? Just one document, one document gives me this, you know, the ability to, uh, present it as a slide, the ability to, uh, send it to you as a computable Jupyter notebook and the ability to serve it as a book on the web that is very nicely informatted for reading. So again, I mentioned, you know, what I need to actually, I think I need to, I think okay. So, returning to here. We list the catalog. We're then able to basically say I want to, here I'm demonstrating how we actually pull in data, say for production by API. So I just give it an API number as an argument for this data source, call the read member function on it and it reads it into a pandas data frame. Now I know you don't know yet what a pandas data frame is. We'll learn that later today. But then if I can print out the first part of the data frame and it shows me that I have a volume oil, volume gas, volume water in barrels, MCF and barrels. And I'm only printing out the first five lines here, but that's all the monthly production history. Because this is a pandas data frame, I could say change this to plot and we'll get a plot. It's not a very good looking plot. I'm curious I have the scrolling turned off, but there you have a, you can see the top part of the plot there. And yeah, I just executed code in my PowerPoint too, by the way. Not only did I execute code, but I executed code that made a live database query and produced a plot from my PowerPoint, my PowerPoint slide. Obviously, I'm being facetious. This is not PowerPoint. You can't do that in PowerPoint. So let's talk about how we can use intake to reduce boilerplate code. In this case, I've written out all the SQL that it would take to basically perform the same operation. So we have a username and password. We have our datam server or a post-secual server. Here's how we format the query and then we can use the pandas read SQL command. Again, I'm basically just demonstrating or just saying that it takes this much code to reproduce this one line of code here. I think I apologize, but in the current state of my slides, I can't scroll down, but you should be able to see the output in the slide, in the notebook that you're looking at. Reusable workflows. I just, I have this function that is, I called my fancy plotting workflow that just takes the catalog and an API number and produces this plot. It's the same plot I showed you just a second ago. It's just been reformatted to plot the gas on the right axis and rescaled so that because it doesn't make sense to plot them both on the left axis because they're different units, right? The oil is in barrels and the gas is in MCF. So I just have this function that does that for me that takes in a catalog and an API number and then produces this plot. Now for the moment, we're going to, just for demonstration purposes, I'm going to play data engineer just for a second. In this case, what I'm going to do is I'm going to use Cat to read in a couple of APIs and then I'm just going to write them to local CSV files. That's what these two commands do. I read in from our SQL database to production, you know, production histories from two APIs and then I write them to a CSV file and those are stored, you know, in these files, production that are scored in the API number. And we can verify that they're written there by using the Linux, the Unix command LS. So if I LS the directory.files, you see these two CSV files are there. And in fact, if we want to look at one of the CSV files and see what's in them, then you can just use the Unix head command and to look at the, say, first five lines of the file. So I'm actually looking at the first five lines and what was written there. This is what that file looks like, right? So it has some headers, has dates and num and integer numbers corresponding to the monthly productions for those guys. So I, I'm just did this so that we could do some data engineering. I'm kind of showing you what's one of the pieces that's behind the scenes there with intake. So now what I'm going to do is I'm going to use intake to open that CSV file. So I'm using this intake open CSV that will, and with some arguments about parsing dates. And ultimately then I, that automatically creates this sort of dictionary, this set of information that defines the data source. And I can access that by just printing out this dot yaml. So that yaml is a markup language. And so that it produces this dot yaml. I take that dot yaml and I write it to a file. Right. So I just copied what was on that last slide. And I'm writing it to a file called indie production dot yaml. Then I just changed a couple things. I changed production by API. I changed what used to say CSV to production by API. I replaced the dot files directory with something called catalog directory. And then I made this where that number was hard coded, the actual API number. I gave, I made it a parameter with this special syntax. And I wrote that file to north Dakota production dot yaml. So then I have a new catalog, right. So not our default cat, but a new cat that we call CSV cat. Its information is held in this north Dakota production dot yaml. And if I list it now, there's only one data source in it, production by API, the one that we just wrote. And so then to demonstrate my fancy plotting workflow, again, I had a function that took in a catalog and an API number and produced a plot. And before we were using the default catalog and the query was done by making a SQL query, in this case, I have a new catalog that we just created and the query is actually reading from a CSV file. But the benefit of this, because we did the data engineering carefully, is that my fancy plotting workflow doesn't care where the data come from. It doesn't care that it, in one case, it's making a SQL query. And in this case, it's just reading from a CSV file. All of the details of where the data comes from are hidden from the data analysts, right. And the data analysts, again, it's a clear separation of roles in this case. And the data analysts just wants to get the data plotted, see what's going on. They don't really need to know all the details of exactly what the SQL query was or whatever where the data come from. So that's just one demonstration of doing that. As I mentioned, you can install data as Python packages. In fact, that's what we do in this. So I've done the data engineering. I store that in a package called data. And then I use basically a package installer called Konda that will install this from our source, our channel called data. And literally, I just issue this command on a command. If I go to a new computer that doesn't have this data on it, I just run this one command. And all of a sudden, I have the data available to me. That's it. I mentioned self-describing data. So I can basically on any data source, like production by state, I can just call describe on it. And that will output this long description. In this case, it tells me that this is a SQL query. It gives me the parameters that you can input. For example, if you had your own username and password, if you had a different subdomain that you wanted to go to a different database, all of that could be the minimum you need for production by state is just state. You can just put in the state, New Mexico, and it'll pull in all the production for the whole state in New Mexico. There's also even four state. There's actually, again, I can't scroll right now, sorry, but you see Ohio Michigan. There's all the allowed states right here. And that's useful. So if you did get an error, like if you put in New York or something, where there's no production history for it any wells, maybe there is for New York, but some state that doesn't have any production, then it would actually tell you what the valid options are. That's something that even a SQL response would not do. It would just error. There's even a built-in GUI for intake, so you can just type intake GUI. And then this is something you can interact with. The descriptions are over here. The data sets are here. And you can look at them and see what the descriptions are. You can actually add and take away data catalogs from the GUI if you like. There's also built-in quick look plotting. So for example, if you pull in the data source, PuroPerm, I can just simply say plot scatter, and it will plot this data. And of course, this is not a very good plot because it's just plotting against the index. But if I say that I want to plot the porosity on the x-axis and the permeability on the y-axis, then I get a much better, more meaningful plot. These are all built-in. I didn't have to do anything. It automatically, I just call plot on it and it does its best job to plot it. Now again, it didn't do a good job, but then I can add some additional arguments there. We can persist data, which means in the case where we have data that I'm doing an actual SQL query, if I call this a time-it function on it, it will actually run this a whole bunch of times and do some statistics on it. It says that this query takes 2.34 seconds. So if I then pull in the production history from this API number, then it will, if I pull in the production history, it's issuing a SQL query and it will bring in the data and it takes 2.34 seconds. However, I can call persist on it on the data source and it'll bring that data in once and store a local copy in the most efficient format available to me. And then if I call it again, and in this first case, I actually said persist never to make sure it doesn't pull in or use the locally persisted data. But in this case, if I just get now that I've called this persist on it one time, you can see that this has been reduced from 2.34 seconds to 95 milliseconds. So that's the persisting in the data. I guess that actually, I thought I had one more slide, but I guess that ends the discussion of intake.
 your home place is kind of like your workplace, right? You have a similar type of setup. I hope. Okay. Okay. So now we can go ahead and we can look through this. Every workflow. I like to put a bunch of documentation, explain the context. You have that as a standalone product that you can look through to remind yourself of the high points in the lecture. We talked about all of these principles already. So I won't spend any time on it. Geostat Pi, we need it for sure now because we're doing real geostats spatial data analytics now. Okay. So we run that import geostat Pi. We're going to import these are our friends now numpy pandas map plot line every workflow. Basically I do this in every time I build a workflow automatically import those. Now we're going to go ahead. Thank you, Dr. Foster took care of all of our feature engineering for us. And we go ahead and load up a data frame. This one's called spatial data multi-variate biased. Okay. So this is specifically a very biased data set, but very honest bias data set. And I appreciate honesty. Okay. So now we're going to we've done all this. So you see I'm going to go very quickly now. We're going to look at summary statistics for all of our features. We go ahead. Now we're going to transform the feature that we want to do. The feature that we want to work with to standard Gaussian Gaussian shape with a variance of one and a mean of zero. We're going to do that because we assume the verigram is going to be used in a Gaussian simulation method. That's not in scope for this course. But let me just leave it to say that we need to do the Gaussian transform for a simulation method. If we were doing that. And it does give us better behaved verigrams. The Gaussian transform actually does deal with outliers and gives us a nice smoother well behaved distribution to work with. And that's good. That's people often do Gaussian transform just to get better quality verigrams. Okay. We'll do the normal score transform of both porosity permeability. We've created standard Gaussian features and poor. Oh, let me. Oh, I forgot the reset. It doesn't matter. We'll keep going. N poor N perm and you notice the values. If you did summary statistics, they go between negative three and three now. They're standard Gaussian distributions is now is everybody together with me. Anyone having any issues or. Okay. Now let's whenever I do any mathematical statistical operation and workflow. I check it. And the way to check a Gaussian transform is to look at the CDF, the original porosity CDF was like that. The permeability CDF was like that. And the Gaussian transform. Those are some fine looking Gaussian CDS, right. That's exactly what you think Gaussian CDF. That's that skewed S shape, right. So what was the permeability distribution before we transformed it? What was it shape? Are you able to go from CDF to PDF or histogram? What do you think? Is it skewed? Is it symmetric? Yeah, skewed definitely skewed positively skewed. In fact, this distribution is pretty darn close to being log normal, I believe. So it is one of those distributions that comes up and it's got a long positive tail on it. Okay, and you can see that because the CDF is showing you the at each interval, you can see the density directly as the slope of the plot. Okay, good, good, awesome guys. Your intuition serves you well. Okay, now let's go ahead and look at the data. When you do verigram calculations, please always look at the data. Why do we do that? Why do we need to see the data to calculate a verigram? What are the, when I calculate a verigram, what do I need to decide? The most basic decision I need to make is the lag distance that smallest spacing that I want to consider. Now, if you look at this data set, 1000 by 1000, 1000 porosity normal scored permeability and the cross plot between the two, you'll see you have quite a bit of data that's probably 30 meters apart, maybe 20 meters apart just eyeball it, it doesn't have to be perfect. Maybe a hundred meters for many of them, so you make a choice based on that. The other thing too is I plot the scatter plot between porosity permeability because if they're highly correlated with each other, I expect the spatial continuity to be very similar between them. Okay, so I'm checking to see what's going on between them. Okay, so now we're going to go ahead and we run the verigram calculation. The verigram calculation we're going to use, let me see where I do it. I'm just looking for my parameters. Okay, oh, sorry about that. I actually, this right here is the program map calculation. I put it in the workflow. I got a little confused because I forgot I put it in the workflow. This is the verigram map calculation. Now, let me show you this. I want to show you how cool this is. Run this block of code to get a verigram map. Now, I think this should excite people because you saw the data. Now, please, could you look at the map, stay here on the screen after you run it did it run okay for you. You got it good. Stay there and now let me keep the data here in front of you. So I'm going to have the data. You have the map. Okay, everybody's doing that. So you got my map by data. You got the map. Now look there and see what the map was able to see. Now, the map, I'll go there just for a second. I'll come right back. This location right here is zero zero offset X and Y direction zero zero offset that location right there is the verigram of the zero lag. So this is the verigram at a 200 meter in the Y direction zero meter in the X direction offset. This is the verigram at a negative 200 meter in the X direction zero zero meter in the Y direction you see that. So we've actually calculate the verigram for all distances and directions all at once. Now you keep looking at the map and now look at the data at the same time. Let me ask you a question. What is the primary direction of continuity? 0,000 in the in the Y direction 090 in the X positive direction. What do you think? Can you see the major direction of continuity? Oh, I got the chat window blocked. No, no, I don't. Okay, good. Anyone go ahead. Just put in the chat window. What do you think it could be? Can you see in this data set? How it's kind of high all in this direction low low high again. Now if we go back to the verigram map. Look at what the verigram map is saying. It says the verigram value is low. Low colors are the purple colors and it keeps going. It stays low for great distances going in this direction. The verigram values are rising up very quickly in that direction. The verigram is the measure of discontinuity. It's the measure of dissimilarity over distance. The verigram value staying very small in this direction indicates and it's a very nice 45 degree angle. It's the 045 asmeth. And you see that so you can see right from the verigram map, the directionality. You can also eyeball the range because if you look right here, the normal score verigram, the orange color right there is a 1.0. That's the sill. So if I drew a line around here, it would go somewhere right there. You see that. That would be the sill. And so in this direction, I reached the sill at a distance of about 200 and something meters in this direction, I reached the sill when I don't I never see the sill. I have long range continuity even beyond the distance over which I calculate the verigram. I have good predictability in that direction. So this is very powerful. You could calculate the verigram map in an unconventional restaurant for any of your features of interest. And it would tell you about directionality kind of like having a, you know, like it's like a rose plot, but data driven directly from the individual samples in space. And I admire the interpretation on the predictability. I love communicating this as a degree of predictability, but I also like it from the degree of anastropia. What's the directionality locally. And I think this is very useful because you start to think about I have a PhD student who uses this measure directionality to figure out for long horizontal wells. What's the overall heterogeneity observed by one well. And what's really interesting is we've actually recently related that to production and we found a correlation. Now the power of what we do in data analytics is we get something and we can know how much we're explaining what proportion of the variance that we're capturing with the model. And when we use spatial approaches like this, we know how that will actually decrease over distance. And I think that's very powerful having a multi-variant predictive model and knowing spatially how it gets worse the further you go away. You see what I'm saying. All right, great discussion. Thank you very much. Let's go ahead. I think it's always a good idea to calculate experimental verigram. Let's go ahead and calculate some experimental verigrams right now. And so what we're going to do is we're going to set the lag distance to 100, the lag tolerance to 100. Now let's not do that. Let's change the lag distance to 50 because I want to follow the rules I gave you. So can we go ahead and set the lag distance is 50. And the lag tolerance is 100. Now we looked at the location map and there were plenty of data that far apart from each other. So we should do OK, I think we'll go ahead and set the number of lags as seven. Nope, we want more because we got a thousand by a thousand area. We want to calculate a verigram up to about one half the extent of the data said that's about 1400 meters across the diagonal. We could go to 700 meters. I need 14 lags at 50 meters each to calculate this verigram up to about 700 meters. Okay, everybody good with that. We'll do an asthma tolerance of 22 and we'll calculate a bunch of different asmons. Let's go ahead and we'll set those parameters. And now what we're going to do is we're going to loop over. And we're going to calculate a bunch of verigrams. Did everybody did this run for you all all y'all. Did you get the verigrams experimental verigrams. Did you see how easy that was now we got these nice verigrams that we can go ahead and interpret. What was our primary direction of continuity. What did we decide it was. We said it was let's go let's take a look at 45 degree. Here's the 45 asthma experimental verigram. Look at that. Wow. Okay, so that is exactly what we expected. We got it rising up slowly and then it levels off. We call this a pseudo-sill. We note the fact that we in fact have never reached the cell in that direction. We have preserved predictability beyond distances for which we've actually calculated the statistic. You see what happened. Now, so let's look at the other direction. So what's the range of spatial continuity in that direction. It's kind of a trick question, right, because it's really got undefined range. You mathematically speaking, you could say very large infinity or something like that. It's really a large, large range. Okay, now what's the minor direction. What would be other directions that we could consider? Well, zero, zero. The range is about 300. So five, the range is about 500. 67.5, about 550. 90, about 350. 111, 112.5, about 280. Now look at 135. 135. We had the primary direction like this. The other direction 90 degrees from it is that 135. Look, its range is only 250. Do you remember when we were looking at the vergon map. We suggested that the distance from here to here was about 250. So it's all consistent. The vergrams that were calculate direction ill in a direction are consistent with the vergon map. And what we'll generally do is build a vergram map to identify the key directions and then we'll use good carefully calculated experimental vergrams to get a sense of the ranges and the structure and the shape. Okay, so we calculate experimental vergrams. Now, let's try something. Let's try something different. What's the minimum lag distance you think we can calculate a vergram at? If we look at the data. What do you think is the shortest, the smallest sample distance? Anyone? A little bit difficult to see. These are 200 meter intervals right here. Some of these pairs are pretty close to each other. Do you think we have any data 10 meters apart? You know what they say? Go small or go home, right? So let's go ahead and let's try. Let's do a 10 meter lag and let's repeat this vergram calculation and then we'll be done with this. Okay, so 10 meter lag distance. Okay, what should we put for the lag tolerance now? If I have a 10 meter lag distance, what should the tolerance be? Well, the rule of thumb was to set it about one half. So we'll do about five meters. How many lags do I need now so I can cover half the extent of my space? What do you think? We want to go about 700 meters. Each lag is about 10 meters. We can go 70 lags and we'll cover the space. Now let's go ahead and run that. Does everybody with me? All right, good. Let's go ahead and do that. Look at that. Did you guys get something like that? What happened? Oh, geez, that's really pretty wild. I'm going to make a T-shirt out of that. Looks so good. What do you think, guys? What's the one word you would use to describe these results? Do they look noisy? Would you agree that we're starting to get noisy results? What happens when you calculate a histogram and use a very small bin for your histogram? Have you ever tried that? Anybody calculate a histogram with a very small bin? Have you seen that the histogram kind of jumps up and down. It has very noisy. The bins change a lot from bin to bin. This decision of the size of a bin is a decision in all of statistics. It's all over the place. You know, the lumpers and the splitters, right? There's people with lumpers and split. Those people who lump and people who split. If you pick too small of a bin size, the number of pairs in the bins is very small and you get a very noisy result. But what happens if we pick a very large bin size? So I'll just do this really quick. What if I pick a lag distance of 200 meters, lag tolerance of 100 meters, and now I calculate to 70? I mean, oh, so 200. I need to go about, sorry, about four. I can only calculate about four bins, right? Because that'll get me to 800 meters. I don't want to go much further in that. And the 100. Let's go ahead and I'll run that. Do you see what I get now? What happens if I pick too few lags with very large bins? I get a very smooth result, but I lose resolution on the problem. When you pick lag size and lag tolerance, it's always a bouncing act like the binning of a histogram, too big of a bin, smooth, but you lose information. Too small of a bin. You get more information, but it starts to become unstable and noisy. What's the right choice? You pick the bin size, the smallest bin size you can use, that's not too noisy. And that'll give you the most amount of information. And that's the best I can do to help you out with that. Okay. Any questions finishing up with diagrams. We're good.
 So the solution for this one is pretty simple. When the first time you load intake, you might get some little warning message. You can just ignore that. If you run it, if you execute to sell again, it will go away. But then from there, you can test this locally in the notebook. You need an API number to do that. If you wanted to go grab one from the notes. And there we go. So in this case, you just need to uncomment that line, add this, you know, well location by API. And you know, it takes a keyword argument, the name of the keyword is API. And then we're passing this argument down. So perhaps, you know, it might make a little more sense to do something like this. Just so you're not confused by saying API equals API, the idea here is that you're taking whatever, whatever is here and you're passing it to the keyword, to be the value of the keyword argument there and then call read on it. And then I gave the code, these are basically, it reads it into a pandas data frame by default. And of course, you guys don't know what pandas data frames are yet. We'll talk about those later. And so I go ahead and pulled off our latitude and log to from those.
 So this is an interactive workflow for you to get to have experiential learning with creaking. I want to find a new way my students could experience creaking right first hand. So what I do is like always I'm a very wordy person I like to talk to much. I put all the explanations behind creaking. I put that in there and then I also explain a little bit about spatial continuity because this exercise will allow you to play with spatial continuity and creaking. Okay. So I describe spatial continuity. Let's go ahead and import my package to your step pie. We need that because we actually need it for verigram calculations and creaking that go on under the hood. Then what we need to import will be a bunch of stuff. Anybody ever seen I pie widgets? Okay. I'm telling you right now who we here has ever done by show of hands who here has ever done mat plot live plotting. So a few of you have done that. Would you like your plots to come to life? I pie widgets can take a standard mat plot live plot and put a dial, put a slider, put a selection bar and you can make your plot interactive. That's exactly what we're going to do here. Very awesome. Very epic. Okay. So let's go ahead and import them. Okay. You notice it imports I pie widgets. I believe it's just part of standard and a condes. So that's cool too. So let's go ahead. I made you see this simple, simple creak. I made a short program to do creaking for three points to estimate one location or multiple locations. I made a very simple creaking program. If there's a coder here who's kind of been sitting there going like, how would I do creaking and how would I do it as simple as possible? This is the shortest code I could write in one morning last Saturday, I think last Saturday, to do simple creaking. So if you want to know nuts and bolts of creaking, that's it right there. Loading in data, locations, the values and making estimates at these locations, that's creaking with the verogram and a mean supplied by the user. Okay. So I had to do that. So I had a fast creaking method. Okay. Let's build the interactive display. This part, I'm not even going to say anything. I'm going to say nothing because this is all of the plotting the interactive part. If I taught you this, we're now teaching you how to build interactive plots. But please, it's a good example. If you want to get a chance, I can't help myself. We go ahead and we're able to basically set up a bunch of sliders here and down here, we do information from the sliders and then down here, all you had to do was connect it, tell it to connect up. It's done. It's connected. So we run this right here. This is where the magic happens. Okay. Now, let me change my screen size. Okay. How's that? Can everybody still see this? Is everybody have this interactive display on your screens? Okay. Let's try a couple of things together. Okay. I'm going to explain a couple of things. The first thing is the one everyone to notice that this is the unknown location here. Do you see the one, two, and three? Those are right beside the data point locations. They don't have circles right now because the circle is going to be scaled by the weight. Okay. I'm going to scale the circles by the weight. This value right here is the amount of weight. This is the data value. So data point number one has a value of one. The global average is two. It's three points. One, two, three is my data. Very simple circumstance right here. Do you see this circle right here? That is the very gram range. Now you notice how it's an oval. It's an oval because you see 1,000 meters, 1,000 meters. I have anastropia because my plotting is not perfect. I wanted to make it bigger. You know, I could have made it better. Okay. So, now what I want you to do is go ahead and click here and push up, up, up, push the up arrow, up, up. Do you see what's happening? The very gram range in the major direction is increasing to 255 meters. From here to here is 225 meters. Okay. Now the estimate at the unknown location is still 2.0. The global mean, nothing has correlated yet. Okay. We're all good. We're together. This is creaking. We're doing live creaking right now. Creaking fans. Creaking is happening. Okay. Keep going. Up, up, keep going. What happened? Do you see this point appeared? And a weight of 0.01, the weight on the global mean is 0.99. The sum of the weights is all summing to 1. Okay. The sum of the weights is 1. And most of the weight is still going on the global mean. Okay. Let's keep going up, up, up. Look what's happening. You see the circle get bigger? Data point number two now has a weight of 0.18. And when we're at a range of 4.75, and the weight on the mean is 0.82. Okay. So it sums to 1 still. Okay. Now the estimate at the unknown location is still 2. Because it's a combination of weight on the global mean of 2. And the data point which is 2. Okay. Are we good so far? It's working. Okay. Let's spin it. And click the up arrow. Did it spin? And you see what happened? We lost all of the weight to that data point because it's no longer correlated with that direction. Allady added. Okay. Let's go again. Okay. Let's increase the range of the major a little bit. Let's go up, up, up. Let's get that next data point. Data point number one. So be more interesting. Let's go take it up to about 700. That's fine or even 800 is fine. Take it up 800. That's cool. Okay. So 800 by 100 geometric anastropi in that direction. Click on the asmuth button and now hit the up arrow. And we can spin this around. Okay. Do you see what happened? We got a little bit of weight from data point number three. And if we spin it up. To we get here again, look at that. Oh, wow. Almost half the weight on data point number two. And half the weight on the mean. And and now what I want to point your attention to is this distribution right here is a Gaussian distribution with the mean of the creaking estimate and a variance of the creaking variance. So this is your uncertainty distribution at the unknown location. Okay. So let's try a couple of things out. One thing we could do is we could increase the minor range. And you'll see the ellipse slide is expanding. You see when we do directionality, we have an ellipse. When we have the major and the minor, we can increase that to like 300 or something like that. Now, another thing, let's demonstrate a couple of concepts of creaking. You see these sliders right here? I can move the data. So if you grab the slider and you move up, watch what happens. Data point number one moved up. You see that? Let's move it. Try to get as close as you can to 500. Now if you click on the dot and you push the arrow, oh, it's not stepping. That's too bad. Well, we get to this close. You can get into about 500. Well, I nailed it. I got it. Okay. Now you can slide across here and get it pretty close to 500 here. What happened? You see that? The weight on data point number one went huge. And the estimate at data point number one, look at the estimates now 1.15. The variance is strong quite a bit. You could actually try. See if you can get it right on 500. That would be pretty cool. See if you can get it right to 500. It's hard to do. I got it pretty close. If you can get it right on 500, it should be an exact estimator with an estimate of one and creating variance of zero and there'll be a spike on the plot. Okay. What do you think? So far so good. Okay. Anybody here play hockey? Hockey? There is. I'm going to ask one of the hockey people. Tell me what a screenshot is in hockey. I'll tell you what if you are trusting the person on the point to do a slap shot right under your armpit, that's some that's a dream. Because I'll tell you those shots are coming really fast and that puck is heavy and hard, right? Okay. So the whole idea is some one of your forwards or at least, you know, at least one of them is a really big person who probably doesn't even know how to skate. That's the way that's the way we used to play hockey back in the days away in Gritsky, right? You'd have that one person who's just, and they would go. Right in front of the goal tenders hitting them with the stick. Well, you know, pushing at them, trying to get them out of their goal crease because they're blocking the shot. Now right then you drop the puck back to the point. You drop it back a distance to somebody who's got a lot of space to shoot and they do a slap shot and they aim it to just barely miss their forward and to put it past the goal tender goal tender does them a chance. Can't even see the shot. Okay. That's a screenshot. Screen has screening too. What would screening look like in Krieging? Let's go ahead and try it. What I want you to try to do is screen the shot, screen the estimate, take one of the data and block one of the other data. So I just did it right now. Do you see how data point number one is blocking data point number two from making the estimate at the unknown location? Do you see that? That's a screened estimate. Okay. Kind of like a screenshot and hockey. Okay. Now what is the weight on the on the this is my forward right here blocking the goal tender. What's its weight? 0.79. It's got really good weight. The size of the circle is huge. What's the weight on the value that's screened? Do you guys get negative weight? Negative weight. What does negative weight mean? That is the screening effect in Krieging. And what it means is that Krieging is capable of extrapolation. If all you use is positive weights that go between zero and one and they sum to one, your estimate must be between the range of the minimum and the maximum of the data. You cannot extrapolate outside the range. If you're using residuals with negative values and positive values and you allow for negative weights, you can in fact extrapolate. Krieging as an estimator will in fact extrapolate and can go outside the range of the data. That's a very fascinating phenomenon. And I'll tell you what it can get very philosophical thinking about the fact that that's the best linear unbiased estimator and that actually works and it does work. It's really, really cool. Negative weight screening. Okay. Any questions about this demonstration? Did you like the interactive demonstration of Krieging? Is that useful to see it? Anything else you want to try? So that's good feedback. I could switch the sliders with a slider with a number where you can enter the value. Adam asked the question, could is there a way you can get to 500? And I met, so I have a new version. I just fixed it. What it does is when you click on the distance, when you click up and down, it skips by one meter and that helps you. But I could also do a slider with a number that you can enter in. In fact, you can have all kinds of different widgets in your dashboard and they could include ones where you just enter the number. And admittedly, maybe I want to replace this with you just enter the number. Thank you so much. Let's try making the range very big. So what I would recommend is let's put X back in its place. I mean, a data point one, let's put data point one roughly in its place. I would bring in data point number three, bring it kind of closer, getting within the range of continuity. Get everything so it's all got some weight. Okay. So we got something reasonable now. So we got weights on everything. We've got an estimate. Data point number three has the greatest amount of weight. Our estimate is 2.55. We're getting towards data point three. If you move data point number three, just a little bit over and even further, it gets more and more weight. We could put data point number two over a little bit. It's going to lose some weight. The distribution is going to move side to side. That's super cool, really cool to see. Now let's go ahead and increase the range. Now I say just make the range really big. What happens to our system as we make the range very big? Let's make it really big. Now one thing I should have pointed out, I do have the major minor and multiple directions of the barogram shown here so you can visualize the barogram while doing all. But if I make it very, very big, what happens to my system? What are you seeing right now? Make it even bigger. Much changing. Does anybody notice the variance on the estimate right here? What happened to it? With wonderful spatial continuity, everything is continuous. Everything is related to each other. The uncertainty in the creeping estimate goes way down. You see that? The simple creeping estimation variance is only 0.03. It's very little uncertainty. That's the first thing we'll notice. The weight to the mean goes to 0 and some of these weights all go to 1. In other words, the data is completely informative. Even if the mean, the global mean was very different, it has no role to play anymore. They're driven locally, completely driven by the data now. What do you think of that? All right. Now, let me just ask any questions about creeping? Any questions about the way it performs, the way it behaves?
 So what is NumPy? Or I guess before we talk about what NumPy is, you might have heard this ubiquitous statement. Python is easy to code in, but it's too slow. It's too slow to do any real work. And if you use pure Python to try to do numerical computations, it is slow. I mean, it's slow in a sense that, you know, MATLAB is slow in my opinion. So, you know, I think most people that are saying those kinds of things are used to programming in C or C++ or 4TAN or some compiled language. And Python is an interpreted language. But in addition to that, it's an interpreted language that has something called duck typing. So duck typing is just a phrase, you know, it means that if it looks like a duck, then it's a duck. And otherwise known in computer science world is dynamic typing. That means when we specify an argument to a function, or we define a variable, we don't have to declare its type. If you ever coded in a compiled language like C or C++ or 4TAN, if I declare a variable X, then I have to tell the compiler what type of thing that that variable is going to store. Often this is an integer, a floating point number, something like that. So, we don't have to do that, which means we can write code a lot quicker because we don't we don't have to declare every single variable and every single function argument before we before we write it. And it's actually the interpreter itself that figures out what you mean. And that is this duck typing, right? So in other words, if I have a variable X equals to one, then the interpreter is going to interpret that as an integer. And it's going to carry out operations on it with integer math. So if I add two things to integers or whatever. It also has some sort of fallbacks in the sense that if I declare it to be an integer, and then later on in the code somewhere, I try to add a floating point number to it. So, if I have an integer, let me, let me, probably this is best just demonstrated. Let me open up a blank jubber notebook. And if I have a variable X equal to one, and then later on, I were to say X plus 1.1, 1.1 is a floating point number. When I, when I originally declared X equal to one, the interpreter interpreted as an integer, right, because there's no decimal place associated with it. However, here when I went to add a floating point number to it, it on the fly converted the integer to a floating point number so that I could do this type of math. This is something you would not be able to do in a language like C, right? So in C, you'd have to declare int X equals one. And then if you can't change its type, you can't add a floating point number to it later. You know, without, without it complaining. And Python does all that for you. That, that is the nature of this duck typing. Sorry, every time I have to switch browsers or stuff, I have a, there's a lot of stuff open on my screen with a chick to zoom and everything. So I have to move things out of the way. Anyway, duck typing is what makes Python slow because the interpreter has to do a lot of figuring out with respect to every variable, what type of type it is and what type of enter, what type of math I'm going to do on those type of operations. Okay. So the solution to that is to use numpy data structures. So numpy data structures are Python objects that contain a raise of data of a single type. So the numpy data structure, unlike a Python list, a Python list can carry, as we demonstrated yesterday, you can carry integers and floating point numbers and strings and you can iterate over all of them and it, you know, it doesn't care. Numpy data structures all have to be of the same type. So you can have an array of integers, but they all have to be integer. You can have a ray of floating point numbers, but they all have to be floating point numbers. And you can even control, you know, the, the precision of the floating point number, you can have a 32 bit floating point number or 64 bit floating floating point number. And then you store your data in there and then these data structures will hold that data and then we can operate on that data and the operations are actually implemented in compiled C code. So it turns out to be very, very fast when we do this. And essentially it's, it's, it's really close to as fast as C, which nothing's faster than C. I mean, C is the, is the fastest language there is. So, so let's look at how slow Python is. Compared to numpy central. So we're going to use this built in time it function. And we're going to use a something called a list comprehension in Python. I don't think I discussed those yesterday. Real quick, I can demonstrate what a list comprehension does. So if I want to build a list on the fly. So for example, if I have a list, I'm going to set it equal to an empty list. I did demonstrate this type of thing where I can say, four I in range 10. And then I can append a number to the list. So if I run this. And look at what a list is, it built a list for me in that for loop. Okay. A list comprehension does the same thing, but it's actually much faster. So if I, what I can do is I can say, I for I in range 10. And that will automatically build the same list in a shorter syntax. And it turns out it's actually quite quite a bit faster. If I run that versus this. In this case, the list is so small, it may not may not be noticeable. But we'll see. Yeah, so it's it's a little, you know, the list is very small here. So it's a little bit faster. To do it, what's called a list comprehension. And again, they produce the same result. Right. So if I make the, the bigger the list is the more speed gain you're going to get from using a list comprehension. So you see this is takes about, you know, to build a list with 100 numbers, it's about half, you know, half the time as it takes to do it in this for loop. Okay. Now let's compare that to what numpy would do. And something similar. So in this case, instead of just building a list, we're going to not just build a list, we're going to take whatever I is and add one to it. So we're going to do a mathematical operation. Okay. So if we do it in Python with a list comprehension, add one to a million numbers. It takes 461 microseconds on my computer. If I do it with numpy, it takes two and a half, I'm sorry, I said, I said microseconds, I should have said milliseconds. If I do it in numpy, then it takes two and a half milliseconds. And in fact, I think it should run faster than that because we imported the library. So if I remove the import library statement, I don't know, that's, that's okay. It should work because this is correct because the time at line is only this one. So these two things are doing the same things. It's just in one case, I'm using a pure Python list here, I'm creating a numpy array with a with a million numbers in it. And then I'm adding one to that number. And the fact that I can just simply broadcast this integer, what appears to be like an integer addition across the whole array. This is something we call a numpy we call broadcasting and I'll discuss that further in a second, but you can see it's, it's, you know, multiple's faster. Okay. And in fact, the more comp, the bigger the array and the more complicated the arithmetic, then the faster that you're going to see numpy behave. So there's also so called universal functions that are associated with numpy. So yesterday, we looked at there's a math library in Python and we use it to use it to pull in the value pi, but it also has trig functions to find it like sign. So in this case, what I'm going to do is similar things, except instead of just simply adding one to a number, I'm going to take the sign of the number and square it. And I'm going to do this for a million numbers. And if I do that, it takes 677 microse, millisand milliseconds. Then if I do it in numpy, in this case, again, I'm using these broadcasted operations. So this, this first thing builds a list with a million numbers in it. And then I can feed that as an argument to the numpy sign command. And that will take the sign of every number in that list. And then, so then I have the sign of every number from one from zero to zero to 999,999. And then I can square all of those numbers. And you can see again, this operation takes about one seventh of the time that the pure Python version did. There's many ways that we can build numpy arrays. We, when we import the numpy library, it's idiomatic to import it, import numpy as MP. So this is so ubiquitous that even when you, even when you go to Google help, if you just Google something like MP sign or something like that, you're going to get, you're going to get the right. You know, results that you want because, you know, everyone does this. It's very rare that anyone would ever just import numpy and use the whole name. So at this point, it's kind of standard practice. Or as we speak, we say in programming, it's idiomatic to import numpy as MP. And then we can build a numpy array as a with a Python list. Of course, the list has to be of the same type. So we couldn't, we couldn't build, it wouldn't know what to do if we fed it a list. That is a mix of say integers and strings. In this case, since we didn't use any decimal places on the numbers, it's going to interpret this to be integers automatically. So this will have a data type of integer. You can have other types, you know, so in this case, I have floating point numbers and then I even have a complex number. So this will be of a complex number type. And then we can build numpy arrays from Python lists. There's also some built in commands to build arrays. So a range builds up a list. In this case, the argument is the beginning, the end and the steps. And so this is going to build a list from minus 10 to 10 in steps of two. So in this case, I even specify the data type. Now, because I didn't use decimals over here, if I didn't specify the data type, what this would produce is a list of integers. But because I explicitly said I want floating point numbers. In this case, the keyword float means 32 bit floating point number versus 64 bit floating floating point number. So the precision is of a double 64 bit floating point number is called a double in numpy. So it's double precision. Nowadays, almost everyone just, I think that, you know, tends to use double precision for everything. But if you don't really need the precision, your code will actually run twice as fast. By just using floats instead of doubles. And also create numpy arrays in a linear space. So this, this would be from number one to four. And because I used periods afterwards that implies floating point numbers. So build up an array of floating point numbers that go from one to four in and there should be six numbers in total. So there'll be five increments in between. Right. And four will be included in and in and there'll be four numbers in between five five segments. And of course, there are then you can, there are ways to load it from a file to, although we'll discuss, when we talk about pandas, we'll talk about how to load data from a file in that in a way that's more efficient in my opinion. What we can do here is, we'll create an array that from an A range nine, if you give it a single argument, it's going to start at zero. So it's going to go from zero to eight zero one. There'll be nine numbers total. And we can reshape the view. So if I, if I don't, if I don't have this here, this is a one dimensional array. However, I can reshape it to be what appears to be a two dimensional array. And I say appears to be because in memory, it's still contiguous. You know, in memory, it still sits in, and it's really just a one dimensional array in memory. And if we reshape it like this, we can interact with it as if it's a 2D array, meaning we can index into it in separate dimensions. So in this case, we're going to index into it. The colon means everything. So in this case, it's, you know, the first one is rose and the second one is columns. So take all of the rows and the zero column. That should be zero, three, six. And that's, in fact, what we get. These are, you know, I said at the very beginning, numpy arrays are objects. So they have attributes. One of the attributes of every array is its shape. And so in this case, you know, it'll tell us the shape attribute will tell us its shape. It's a three by three array. Use a similar slicing and syntax as what we used in Python lists. In this case, what we're saying is go from, if you, if you, this is like saying go from zero to minus one. If you remember, zero is the first entry minus one is the last entry. So this is, this is saying go from zero to the last entry in steps of two. However, if we omit them, it's the, it's the same thing. The default would be zero. The default would be minus one and the steps of two. So if we go back up here and look at this, we're going to go from in both the rows and the columns. So we're going to step in increments of two. Right. So we should get zero, two, six, and eight. So we're skipping two along the first row, skipping, you know, along the columns and skipping two along the rows as well. In this case, I assigned. I signed why a new variable why to be equal to X and we output why. Now, just like we did in Python with Python lists, we can reassign a part of it. Right. So in this case, I'm going to reassign the zero zero entry to be a hundred. In other words, that entry right there, I'm going to change to be a hundred. And if I print that out, you'll see what you expect, right, that changed to a hundred. Now what you might not expect is that X also changed. And that's because that why is just a view into X, right. So again, this is that pass by reference thing. So why is just a view into X. It doesn't create a copy of these four components of X. It just contains the information that points in memory to where it's actually going to be. And therefore, if I change why I also change the original data X again, this, this is something you just have to be aware of. And if you're not used to, if you're coming from MATLAB or something like that, MATLAB wouldn't do this MATLAB would create a copy. If you set one variable equal to a part of an array of another variable, it would create a copy in memory. And that copy can be expensive, especially if you have big data, the copy can be very expensive. So this is a much more efficient way to do it. You just have to be aware that if I change why I'm also changing X because of this pass by reference semantics. If I don't want to do that, I can explicitly put this keyword, you know, member function copy on it. And that will create what we call a deep copy, like, or, you know, it's just a copy. So in this case, if I reassign why to be a copy of X and then change the zero zero entry of Y, then I get, you know, what we'd expect, right. And then I just left unchanged from before and why has been changed. This is, let me see if I can hook up my whiteboard real quick. This is one that takes a while for people to understand typically. So I think I'll do a little whiteboarding here. Does everybody see my whiteboard now? Okay. So, if you might remember that, you know, the definition of a derivative, the mathematical definition of a derivative is this. That's how we all learned it in cap one, right. So, in a computer, you know, obviously, if we want to approximate a derivative, one way to do that is, you know, in computer, obviously the derivatives more accurate is as H goes to zero. But, you know, we can't actually send it to zero in a computer, the computers do discrete mathematics, right. So we can, we can make it small, but we can't make it exactly zero. So, we can make an approximation to the derivative with something like this all over. You might, you might call what's in the numerator delta F. So basically delta F over delta X is approximation to a derivative. So, what we have then, what we have then here is I have an array. This is just an array that goes from zero to 40 in steps of two. So, you know, what X looks like. I'm not printing it out, but it's zero, two, four, six. Right. And Y is zero for sixteen. And so, what I can do, I want to compute delta Y over delta X. Well, delta Y would be, you know, they're, they're, they're, they're, they're, they're, they're, they're delta Y would be, you know, this minus that, right. So, that's, that's, you know, four, this minus that 12, right. So, and so what I'm doing here is this, in this final difference, the first, why that goes from, I'm, one to the end. So, I'm using indexing to say that why goes from one to the end. That is, it goes from, it goes from this entry to the end. Right. And then the second term, here goes from the first entry, zero to one from the end. So, it goes from here to one from the end. So, it omits the last entry. Right. And so, I have two separate arrays. This is an array, and that's an array, and they're staggered by one. So, if I subtract the two, I get delta Y. Right. Same thing goes, the exact same thing applies for delta X. So, in one, you know, even though this is an array that's got 20 entries in it, in one shot, in one line of code, I can compute the finite difference approximation for the entire array. Right. So, this is the, this is the derivative of the entire curve. Right. And of course, we know that the derivative of this, I mean, it's simple, right. So, the derivative of this is just two X. So, I, you know, here I'm just plotting the details, right, the plotting it, right. So, the black line is Y versus X. Why is a function of X? And the red line is dy dx as a function of X. And it is, you know, if you were to actually compute the slope of this thing, the slope of it is two. So, that is the derivative of that curve. And, you know, in this case, in this case, it was, you know, it's a simple function, you know, that just produces a straight line, but, you know, you could use this technique to compute the derivative of essentially any smooth function. And it's a little more complicated, but you can also do second order, you know, approximations, the second order derivatives, if you wish. Second derivatives. You know, we mentioned broadcasting earlier. So, broadcasting is this idea that we can, you know, multiply entire arrays at once. Right. And so, we don't, we don't have to loop over the array. Like, if I have an array X and an array Y, and I'm going to multiply the two together, I don't need to write a for loop to do that. In fact, this is something I always say in, you know, in all my classes, if you're doing numerical computations in Python, and you write a for loop, you're probably doing it wrong. I'm not saying it can always be avoided, but when you find yourself writing a for loop to do numerical computations in Python, stop yourself and think about how you could use a numpy data structures and broadcast the operations. And if you do that, your code will be immensely faster orders of magnitude faster. And there are sophisticated broadcast rules in, in, in this, in the sense that here, I have a, so far we've only looked at two dimensional arrays, but we can have an indimensional arrays. They, they're not limited to two dimensions. And in fact, what we're about to create is a three dimensional array. So, the, the first thing we do is just create three arrays that are a bunch of random numbers. There's, it's a two dimensional array that has 800 rows and 600 columns. Okay. And then what, what goes in those number entries is just a bunch of random numbers that I create with the random number generator. And I'm just going to call those red, blue and green. And then I'm going to use those three two dimensional arrays to combine, create another array, which is going to add a third dimension. So I'm going to create a new array with numpy array. It's going to take as arguments red, blue, and green. And if we look at the shape of that, then we have a three by 800 by 600. Okay. If I take that array and I transpose it. So the T on the end, so I have this array RGB, the T transposes that array. So then, then I have a 600 by 800 by three. So three becomes the outer dimension. Then I can multiply that by a three by one. So as long as the, as long as the outer dimension of the first array aligns with the, what I'm trying to say is, as long as the last dimension of the first array aligns with the first dimension of the second array, I can multiply them together. So in this case, I'm multiplying effectively what I'm multiplying is red by a half, blue by a half, and green by a quarter. Okay. And it does all of that. So this is a big computation, right? Because it's, it's three times, I mean, how many numbers are in this array? It's three times 800 by 600. And so then I'm, I'm multiplying that many numbers by, you know, by a number. So it's a, it's a big computation, but this, this, you know, it does, it doesn't, you know, blink of an eye, right? It doesn't take any time at all. And so visually, I have a visualization that kind of helped us see what we did there. And it, you know, so you can, you can imagine those, the red, blue and green correspond to RGB pixel coloration values in an image. That's, that's us, you know, 800 by 600. And so this was the original image. And then we multiplied it, we scaled the colors, right? And then so you get this. So this is just a visualization to show that we actually did something because we can't, you know, just a bunch of random numbers being multiplied by a half and a quarter is a little difficult to go and visually inspect what occurred. But, you know, that was a fairly large operation that if you did it in a pure Python for loop would take, oh gosh, I don't know minutes at least, if not, if not an hour to do. And then you use numpy raise and it happens and, you know, split second. So numpy also has something we call fancy indexing and we'll use this some later when we go to build the dashboard. But so if we have an, if we, if we have an array, we'll just, we just create a array with a range function. It's got nine numbers in it there. We can, just like we did with the Python list, we can take one of the values, right? So if we want the, the three index at 0123, that's the number three, that's what's returned, right? That's no different than what we do in a Python list. What is different is something called fancy indexing. So you can't do this with a Python list. So the index, what I'm taking is actually a list in it of itself, right? So in this case, I'm going to take not just a single value or a slice, a contiguous slice, but I'm going to take 0, 3 and minus 1. And so that the minus 1, of course, is the last entry, right? So I'm going to take 0, 3 and the last entry. So that's called fancy indexing where we use multiple, we pass in the list as an index. Just as a demonstration, you cannot do this in Python. So here, I create a pure Python list, right? This is not an umpire array. This is a Python list. I sign it to XX. And if I try to use that fancy indexing syntax, I get an error. Right? So it tells you it has to be an integer or a slice, not a list. It works in two dimension as well. So here I just reshape my array to be 3 by 3. And then, you know, we can use fancy indexing in the two dimensions. So if I pass in just a single list, then it's going to, you know, with 1, 2, it's going to give me the 1 and 2 rows, right? So I'm going to take the second and third rows, the 1 and 2 index rows. Likewise, I can pass in, you know, a more complicated. So I'm going to take the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row, the first row 1,1 is 4, and 2,0 is 6, the 2 index row, the 0th column. So this is just using fancy indexing and two dimensions. Another really powerful thing, and this is where we kind of start to start thinking about things we might do in data analytics and data analysis and data cleaning. So what we can do is use something called Boolean Arrays with fancy indexing. So here I just created a random set of integers that go from minus 10 to 10 and there's 10 of them total. Then what I can do is I can just say, sign that to a variable X and then I just say where X is greater than zero and this Boolean operation is broadcast against the array X and so it basically will automatically test every one of these to see if it's greater than zero and you'll see that they line up, right? So where it's greater than zero, you get a true and where it's less than zero, you get a false. Okay. Then I can, so here I'm assigning that to something called index and so this is a Boolean array. It's just an array of truths and falses and right past that as an index into X, it's only gonna return the values that are true. So if we run this, you can see there's no negative numbers here. Right? So this is called, you know, this is also called fancy indexing but we're using a Boolean array to do the fancy indexing. It'll only return the values that are true. So, you know, when you have large arrays of data, this is a very efficient way to sort of extract parts of them. This is very common right? You might have a bunch of data, but there's nois in the data. You know, there's not supposed to be negative numbers and there are. So perhaps you want to ignore them or perhaps you want to change them in some way. You can use this kind of Boolean indexing to do that. In fact, I don't know if I have another example, but in fact, you can actually even do reassignment with this. So for example, if I use the array and I say I wanna set, I can negate the array with that tilde, it's a negation operator, that changes all the truths to falses and falses to truths, right? So basically what I'm saying here is where there's false, I wanna set it equal to zero. And I can, and then I can print out X and you'll see that that happened, right? So where the original array had negative numbers, I just truncated them to zero, right? You can also use the numpyware command to do something similar. So this basically does the same thing. The first argument to where is some type of Boolean operation or index array and the second argument to where is what to do if true and the third argument is what to do if false. So basically this is saying where index is true, just return the value of X, otherwise return zero. And so if we do that, you get that. So these two operations do the same thing in this case. So you can use the fancy indexing along with reassignment. And, and you know, if there was a mill, if this array had a million numbers in it, it would, it would be just as efficient, right? So, so now we're starting to see tools that can help us efficiently analyze lots of data. So that's the end of, the numpy lecture, I have a couple of exercises.
 All right, so before we can jump into a variety of machine learning methods and we will, we will just like you saw day two was a lot of hands on running models, doing things. You're going to see that this will be the same way, but before we do that, we do have to have one lecture where we recover some fundamental concepts in order to move forward. So let's give an overview of machine learning. Now first of all, the thing that I want to demystify about machine learning is that no matter what people tell you about machine learning at the end of the day, this is the model right here. It is simply going to be a set of input features and predicting an output feature. It's all about building a model so you can go from one to the other, or we're going to talk about more inferential methods where we try to understand all of these inputs. But the basic building blocks are going to be predictor features. Now back in the old days, if you were doing basic statistical modeling, you would have called these the independent variables. It's kind of the modern terminology now is this column, a predictor feature or predictor features. Now you'll note that with every model we build, we're going to have this random error term. In fact, that's good. We have that because if you didn't, if there was no random error at all in your model perfectly fit your data, it means you're in a very dangerous territory. You're more likely than not overfit. Okay, so you're more likely in a situation where you're convincing yourself, you know more than you actually do. The response is going to be the output. So the predictor features are the inputs, the response features or back in the good old days, we would call these the dependent variables are the outputs, the Y. Now I'll always in the models we build, don't just be one Y feature, one output, but don't worry, there's many models in which we'll predict more than one thing at the same time. We can multiple channels coming out of our model. That's possible too. Statistical machine learning is all about estimating this function to go from inputs to outputs for really two purposes, inference and prediction, inference and prediction. Now inference is all about learning about the system. Now in classical statistics, we would call that the population. We're trying to understand that natural system. We're trying to build some type of the relationships between all of these predictor features. We're trying to understand how they interact with each other. Are there sweet spots? Are there redundancies between the features? What are the ranges of values? All of the stuff we did with multivariate statistics and looking at matrix scatter plots, we're all trying to learn about the system. Now if you get a little confused about inferential statistics, here's a couple examples to help you out. First of all, the general description would be given a sample from the population, describe the population. That's inference. Given a limited set of observations, try to understand what the system does. Now to break it down to a very simple example, given seven heads out of ten flips of a coin, what's the probability that the coin is fair or that the coin is weighted or biased? You think I'm kind of cheating with my coin? You're using an experiment or set of samples to try to figure that out. That's inference. Now inference is actually kind of difficult. It's more difficult in prediction. And it's not necessarily as simple as seven heads out of ten flips. We could be talking about seven successful wells out of ten in an exploration program. In other words, we can use these statistics for or inferential approaches for our business. And in that case, we would be inferring what's the probability of successful well in our res world. Now, I actually have some Bayesian examples of working that out. I'll see if maybe I can squeeze that in. I know it's one of obvious favorites. So I'll see if I can squeeze that in when we start to talk more about machines and Bayesian approaches. Now prediction is moving the other way. Inferences like going backwards from the sample to the population. Prediction is about taking the knowledge about the population and predicting the next sample from the population. In other words, I know something about the reservoir. Now tell me how many successful wells I should have in my exploration program. Now when we go back to machine learning and our model, our F, and we'll show it as an F hat because our function of going from inputs to outputs is really an estimate of the natural system. F hat is an estimate of that function. It's all about the purpose of predicting the most accurate output from the model, the Y hat. And so prediction we're focused on getting the most accurate estimate. Now I taught my students in my undergrad class yesterday, and we were talking about machine learning for the first time. So I'm having a little bit of deja vu, but what's really cool about it was one of the students was asking about how important is it to understand the system. Why don't we just focus on accuracy of our predictions, you know, just purely data driven. And I've been to tech meetups where people say in prediction, I don't care about the system, I just want accuracy in my predictions of the output. I don't think we should ever go there. I think we should always have an inference and prediction coupled together. Just to learn about the system and prediction to try to make the best estimates of what the system will do. We want to predict the samples from the population. This is really straightforward. This is, I tell you, I have a fair coin. Now you flip it 10 times and you tell me if you flip it 10 times, how many heads will you get or what's the probability of seven heads? In other words, many companies will have an exploration probability of success. Use that model to figure out if I drill a package of 10 wells, all of them exploration wells, true exploration, what should I expect for an outcome? How many successes should I expect? And if we do that, we know when it's time to say, oh, something's wrong for our exploration program, it's too risky. Our probability of success is not correct because it's, we would likely have had more success than we actually observed using statistics protects us from jumping to conclusions. We understand what we should have seen. Now when we're building machines, inferential or predictive workflows, we always have to make a choice about which machine to use. And no one method performs well on all data sets. Now you might have heard about deep convolutional generative adversarial networks or even just plain Jane artificial neural networks. It turns out they're great in a wide variety of problems, but they're always tuned towards the problem. We specifically convolutional neural nets are excellent with images. Artificial neural nets would not do as well if you were trying to work with image data. Recurrent neural nets are work well with temporal data and you'll find that random force works well on a wide variety of multiberry data sets in which we have low model variance, but all of this will come up as we go. There's many choices about what machines to use and we need to be able to assess the quality or performance of our machine. Now also we want to do this based on experience and understanding of the data and the limitations of the methods. It was interesting. We were in a PhD defense yesterday, a proposal defense with Dr. Foster. And it was really cool what he said because the student was jumping into a very complicated form of machine learning as a solution to a longstanding problem in oil and gas. And Dr. Foster reminded us just the level of complexity and the challenge in training those models. There's definitely some models are just very hard to use and take a long time to train and build. That's for sure. Okay. Now we want to measure the quality of the model. And so for a regression model where we're trying to predict a continuous variable or output response feature, the most common measurement to use is mean squared error. We're going to use it all the time. The only time we'll debute is when we talk about decision trees, but they will just use the residual sum of squares, which is just not taking the average of it, but it's the same measure. And so what we'll have is we'll have n observations. And what we'll do is we'll take, if you look here, one over n times the sum over n is an average. And this is the squared difference between the truth value at the eye location and the estimate from your model at the eye location given all of the inputs. So this is the mean squared error. And it's a good measurement of the performance of your model. Now the problem we're going to face is the fact that this is over the one through n data you use to train or build the model to fit the model. That's not actually what we want to know. I don't, I do care about how well your model performed when you were building it with the training data that it could see, but a much better test is how does your model perform with data unseen and unused to build it. And so this right here is the expectation, the performance of the model statistically averaged over observations, why not, which were not actually used to build the model. And these are the estimates at those locations when we did not train with that data. So we want to be able to test our model over a wide range of unsampled or unused data. That's the best way to know how it performs. And we'll talk a little bit more about how we do that fairly. Now if you take a look at that equation, the expected, the expected a square difference between the truth and the prediction at data that's used for testing that they'd not, that's not used to build the model. You can actually take that expression, it's a quadratic, and you can expand it. And now the derivation is available in hasty and all's book on machine learning. And I did reference that on the very first day. And if you do, if you expand it, it will simplify down to these three components right here. You have model variance, model bias, and irreducible error right here. Now what's really interesting is that they're all very intuitive and they're additive. And that's a powerful thing. When we talked about trend modeling and estimation in space, I think I mentioned the additivity of variance, which is very powerful concept. The variance is a measure of uncertainty or error within our model. And it is additive here with the three components. So let's break them down. Model variance. Model variance is the error due to sensitivity to the actual training data. It's a sensitivity of your model to the actual training data. Now if you think about it, a linear regression model is a pretty simple model. And you can move, you could change the training data a little bit. And that model would just move just a bit, the slope and the intercept would just change a little bit. It wouldn't be a big deal. If you took a ninth order polynomial fit, and you can all imagine what that would look like, lots of bends and loops in it, right? Ninth order. And if you change just a couple of training data, your ninth order model will whip around like Indiana Jones whip, like just whipping, it would just fly around. Because that model is much more complicated, it's going to be much more sensitive to the data. So what do we see? We see a function like this model complexity, low complexity, high complexity, and model variance jumps up. The more complicated the model, the more sensitive it is to the data. And that is a source of error in your model. Model bias is kind of like the opposite. It's the error due to the fact that your model is a simplification. It's not exactly the natural system. It's too abstracted or simplified. Okay, so if you use a linear regression model, imagine a nice straight line to fit from the predictor to the response features, but the natural phenomenon is complicated. Clearly you're going to have error in your predictions because of the fact that your model is too simple. Okay, so model bias. As the model complexity goes up, model bias is going to fall off. Your model becomes more complicated. It's more flexible. It can fit the natural phenomenon. So model variance goes up as the model complexity goes up. Model bias goes down and they're additive. Now there's one more component. And that's a reducible error. This is error due to the fact of your data. You did not sample everything. You did not sample all of the ranges of the features you know. You maybe you missed the low prerosities, the high permeabilities. There's something you missed. Maybe you missed the combinations. Maybe there's in the high dimensional space of prerosity, acoustic impedance, saturations, faces, whatever it is. Maybe you didn't sample all of that space. You're missing. There's a hole in it. Or maybe you didn't even sample an important feature. This happens all the time. If we'd had a measurement, horse on a well is a great example. We often lack the data we want and horse on a well. So we don't have the full log suite along the horse on a well. Or maybe we don't know the production by frack stage. If we knew that and we often don't run those logs, the production logs there. So we don't know how to partition it. That's reducing the ability of our model. In other words, that's causing error. Now what's interesting is irreducible error doesn't change. It doesn't matter how complicated your model is. Your model can't fix it. I can't use the most advanced machine learning to get rid of this error. In fact, you couldn't replace me with Andrew Ning from Stanford University. Or one of the forefathers or one of the developers of machine learning, the greatest experts in the world couldn't get rid of this irreducible error. With the best tool in the world, the best practitioner in the world, we're stuck with that. It's because of our data. Now if you add them all together because they are additive, what you get is a curve like this. And so that's the total test error, the test mean squared error. Okay, in this equation right here. Now what's interesting is if you wanted to pick the very best machine to use, this is instructive. Let me ask you, is the most complicated machine the best one? Very, very interesting is that there is a sentiment among the practitioners that if we use more and more complicated machines, we're going to do a better and better job. But because of model variants, we're actually driven to often use a simpler machine. Okay, so this will come up over and over again as we explain different machines. But the reason I cover this and spend some time on it is because when we talk about like ensemble methods like random forest and treebacking. It's all going to be about attacking or attacking the model variants, trying to reduce that level of variability due to the data, that sensitivity to the data. With every machine, we're going to refer back to these three concepts of error when we talk about the goodness of the machine. Now I've mentioned a lot of terms here and it's about time to define them. The first one would be model parameters. Now, let me just provide a very simple example of a polynomial model. And so we have a polynomial model like this. The output, the response feature is equal to the predictor feature cubed squared to a one power and the constant. And you've got a B3, B2, B1, and C. Now if we had our training data available to us and we're trying to fit a third order polynomial, you could imagine that I could change B3, B2, B1, and C. And when I change them, I'd move the function up and down. I would change the slope globally. I would change the inflection points. I could change the shape very, very well. I'd have a pretty good flexibility. What I'll do during the training stage is I will set these parameters such that I minimize the error with the training data. I try to get the very best fit. These are the model parameters and they are trained with the training data. Now the hyperparameters are totally different. If the third order polynomial had these as the parameters, the choice to use a third order, a first order, a fifth order, a seventh order, that's the hyperparameter. Hyperparameters control the degree of complexity of the model. Okay, so in this case, it's the choice of the order of the polynomial. Now for each one of these polynomials, the first, the third, the fifth, and the seventh, I can do the very best job I can to fit the training data. This is my best fit line for the third order, the fifth order, and the seventh order right here. Now what's very interesting is that as I make my model more complicated, I tend to make it more flexible and a more flexible model is better able to fit the training data. Now here's the reason that we get into tuning hyperparameters. Because if I was to go ahead and just pick the model that has the very best fit of the training data, I would always pick the most complicated model. You see that, that model, the most complicated model would always best fit my training data. But at the end of the day, I really want a model that's going to perform best in testing and just intuitively here, you could imagine if I withheld one of these data and refit the model, clearly that seventh order polynomial model would do a terrible job fitting any withheld data. In fact, I don't even know where this model is. Like it's gone way off the screen at that point. Excuse me. So what do we have? We have a workflow that requires two steps always. Now you'll see people who do a three step workflow. We're going to keep it at two steps just for simplicity. We could discuss alternatives if you like. We'll have feature or spatial coordinate. We have a space we're working with, another feature or spatial coordinate. And we're going to have all of our data. We could imagine a scatter plot, a location map, whatever it is. We're looking at the data in that space. We're going to take the data and we're going to separate. We're going to withhold a certain portion of the data. We're going to call it testing data. And then we're going to take the remaining data, the training data. We can go ahead with that data. And we're going to train the model parameters to maximize accuracy and training. While we do that, we will vary the hyper parameters. So we're actually training a bunch of different levels of complexity, a simple model, a complicated model, and so forth. And with all of our machines, we're going to hyper parameters where we can determine the level of complexity flexibility of the model. Then what we do is we're going to take the model trained on the training data. And we're going to check each one of those levels of complexity against the testing with held data. And we'll take the model that performs the best in testing with the with held data. This whole approach is to avoid the overfit problem. To avoid that problem of being driven to the most complicated model, because we get the best fit with the training data. Okay. Now, up to there, any questions about the concepts of predictor response features, parameters, hyper parameters, and the training and testing workflow. And it's probably spurred by the fact that we have this relationship right here, a mean squared error. Now if you go back to just fundamental modeling, you recognize that this is an L2 norm. And in fact, we're using the squared error makes it an L2 norm. And notoriously, L2 norms are extremely sensitive to outliers. In fact, if you look at a variety of things we've looked at already, like means and variances, while a mean and average actually minimizes an L2 norm too. And that's very interesting. And the result of that, we all kind of have had experience with data like that, where you have a distribution and you look at the mean, and the mean is kind of way off the mode, the most likely value. It's kind of pulled to the positive or the negative direction. And when you look at the data set, you find out that there's outliers. There's a very extreme value that one really high value pulls the average up. Okay. And so the same thing happens that happens in basic statistics because of this use of an L2 norm happens exactly with machines. In fact, we use L2 norms in basic linear regression. And it does cause pretty strong sensitivity to outliers. And there have been a variety of different methodologies like the last soul methodology that tries to do regularization on a linear regression fit to try to reduce that sensitivity to the outliers and the data. Okay. What do we say in summary? Our machines because of this kind of ubiquitous use of an L2 norm are all going to be very sensitive to outliers. So as part of feature engineering upfront, we definitely want to detect and remove outliers. Now here's the problem. The problem is that outliers are not just in univariate and we talked about univariate, I believe, on day one or day, day two actually, univariate with regard to one feature. It's not always something where you just plot a histogram and you can see an outlier. Often the outliers will occur in a higher dimensional space. So that's very challenging because now it's hard to see them. They may not look like an outlier when we plot them on a histogram of all the features individually, but in the combination jointly, it's an outlier. The other comment I'll make is that today we will cover PCA, principle component analysis. It's very interesting about that approach is it actually does an eigenvector calculation off of a covariance matrix. And when we talked about correlation coefficients, do you remember us talking about how sensitive they are to outliers? That you could take a bunch of data that have no correlation, put one data point up here and suddenly you have 0.95 correlation coefficient, one outlier. And so outliers will have a huge impact on principle component analysis, these is dependent on these correlation coefficients. Does that answer your question? I actually do recommend as part of feature engineering look at potential transformations. Now the concern I have is this that depending on the methodology we're going to apply. Now for instance, I explained partial correlation coefficients. That methodology actually requires bivariate Gaussian distributions. And so I'm cool with or I'm in favor of a Gaussian transform as part of your strategy if you're going to employ methodologies like that. And even when we get into other types of machines, you'll find that a certain distribution might be expected. Okay. So I do like transformations to treat outliers only in the case where it matches up or doesn't cause some type of issue with your modeling afterwards. Because technically if you wanted to really test your models performance in real world, you cannot peak. What I mean by that is technically that testing data should really be with help. And the way you should think about that is imagine you as the modeling expert working with a group of geoscientists and engineers and other folks and they don't let you come in the room. They send you the training data and you don't actually get to see it. The ultimate validation of the model should be performed on data that was not used at all to assist in the model construction. But that is very challenging. What you often find is that there's many decisions that get made in the model, not necessarily the exact model construction, but other decisions that get made that are going to in fact use all of the data. We could we could get into a really interesting philosophical discussion about what a real test of the model would look like. So that's the first comment is often there is peaking. In other words, we don't really get real world testing. We really are we may not use the data and training, but we use concepts from the data. We use distributions from the data. We use the data to help us figure out outliers and so forth, right? The other comment on make is that in our industry, we rarely do fair test train. The tech companies who work with exhaustive data sets, they often do just random withholding of the testing data. And in fact, we'll do that in our workflows. If we wanted to do this really well, we would go into your data set and we would look at it and we'd see how we could chunk out, remove blocks of data or maybe even entire wells so that we could recreate something as challenging as the real world use of your model. That's what we should be doing. If you randomly withhold 10% or 20% of your data for testing and then you go ahead and try to estimate that your worst estimate creation problem is what? Six inches of foot vertically along the well lock, right? Now you try to use that real world model for what to predict at a brand new well location. That's a kilometer, that's half a kilometer distance away from any data you have. Two totally different problems. So anytime you do this with test train, we should be thinking about simulating the process of using the model. Same difficulty. Any other questions? Was that helpful? Okay. Testing and training. Now we've talked about model complexity, so let me define what I mean by complexity or flexibility of the model. There's a variety of concepts we could use to say a model is complicated. The number of features. This gets at the idea of the dimensionality of the model. We have a very high dimensional model. There tend to be more flexible, more complicated. If we were to spend more time and we totally could spend a lot of time, we could spend entire morning on feature engineering, we would talk about how to pick the very best features to put into the model and to preserve the most amount of information. The number of terms are parameters. The first order, the second order, the fifth order, the thirteenth order polynomial. They all have different numbers of model parameters. Their complexity is tied to number of parameters. We're going to build a neural net on Thursday. When we do that, we're going to use, when we check it, I think it's going to actually be thousands of parameters. It can be very large number of parameters, small number of parameters. That also impacts complexity and flexibility. The expression of the model. When the model be expressed with a polynomial, like a very short equation, very easy to use, you make the model, you hand it to your friend, you could send it in an email in the text. It's a very simple model. Or is it going to be something more complicated? Its decision tree is not that bad. It's a bunch of nested conditional if statements. Or is it going to be like a neural net? One of my students built a convolutional neural net during their internship in La Salle and the most national laboratory. What was amazing, they came back, I think the model was in fact, I think it was gigabytes in size. It was so large, it was amazing. There was different degrees of complexity we could work with, flexibility versus accuracy. Well clearly, the more complicated or flexible model is, is generally going to decrease the mean squared error in training. This is the example from James and all in their book and they took this data set right here and they fit it for a very simple problem. And they increased the flexibility of the model and as they did that, what you can see is this is what typically is going to happen. The error mean squared error in training is going to take a nose dive. It's going to drop. And technically, you could probably get to a point of complexity where you can get that error to zero. It's generally not a good idea to select the method to only minimize the training data MSC or mean squared error. In fact, if you take and you withhold some of the data and you check its performance in testing, what you'll typically find is mean squared error often will actually drop a little bit. It should drop and then it's going to start rising up right here, the testing mean squared error. And so what we'll find is once we get past the boat here where we're starting to rise up the testing mean squared error, that's where we call ourselves over fit. We're using too much complexity that's not really supported by the data. Now I like going back to classical statistics because I really do at the end of the day, I believe machine learning is really statistical learning. And so let's just take a very simple example. If we take a third order polynomial and we fit it to this data right here, this is the third order best fit model. Now what I like about classical statistics is you can calculate confidence intervals. Now of course this is a frequent approach and we can argue about whether or not we should use Bayesian intervals and so forth, but it is providing us some measure of the how much we believe we can rely on this model given some assumptions. Now if you look at the confidence intervals right here, this is the P10, the P90 or reverse depending on what is P10 or P90 here. But what you'll see in general is that the form, the variability of the model is really within the confidence. In other words, this is meaningful. This, what happens here, if you go from the P10 to P90 is significantly different than what happens here. Now if we go to a fifth order polynomial, we have more complexity on the model, but the confidence intervals start to expand. In other words, we're reaching a point right here where we're basically saying most of the features or forms within this model are not really reliable. It's the confidence intervals large relative to the features or form. Now if you look over here, this is the distribution of the residuals. The residual is the error from the data to the model, data to the model. So imagine vertical lines, those would be the error with the guard to making the prediction of fine sir. Now as we increase the fifth order polynomial, this distribution variance is going down on the residuals. We have a better fit to the data, but the confidence interval is expanding. We have a better fit with a model that's much less reliable. So now take it to an eighth order polynomial. And an eighth order polynomial, we're just one order away from perfectly fitting the data at every single location. Have you ever heard of anyone doing machine learning and they said we got an R squared of 0.99. We perfectly predicted. This is probably what they're doing using a very complicated model and they're fitting the data almost perfectly. Now the distribution of the residuals is almost zero. The variance is almost zero. There's very little variability. We have very little error at all locations. The variance explained by the model R squared will be extremely high. But look at the confidence intervals. The confidence intervals have gone away. They've disappeared. They're off the plot. In fact, if you were to change the scales, you'd find out their way up here, way down here. In other words, we have a model that's perfectly accurate in training, but the model has absolutely no confidence whatsoever. It's not justified by the data. And that's why I like going back to just basic fundamental statistics and confidence intervals. Now with machine learning, this is what we're avoiding. We're avoiding fooling ourselves by overfitting to training data and creating a model like this that doesn't have justified complexity from the data. Now I like going to Wikipedia. And Wikipedia has a really good article on overfit. And the way they explain it is they say overly complicated model that explains idiosyncrasies of the data. In other words, you're capturing noise in the data. More parameters than can be justified by the data. You just don't have enough data to support that degree of flexibility to it. You think about it like this. Imagine if you have a linear model and you're trying to fit it with just two data or three data. You really can't say you know two points do not make a linear fit because we know there's always noise in the data and so forth. But the results in very low residual variance, the R squared will be very high. The model looks very good. It's very accurate at the training data. You claim more than you actually know. Now I really like this figure right here from the Wikipedia article. Imagine a classification problem where we're trying to come up with a decision boundary to separate between red and blue categories. Now you could come up with this black model right here and you would have to live with some misclassification. And we'll talk about how we score that when we do a little bit of categorical classification. And so you could live with a little bit of misclassification or you could build this green model right here. The green model is perfectly accurate in the training data. But look at what it did. It did this in order to be accurate. That's where clearly in a position where we're fitting the noise or the error in the data. This model would not be useful with withheld data in fact. Okay. Any questions about overfit comments? All right. Okay. Now with all of that we begin machine learning. We have the basic concepts, predictor, features, response features, parameters, hyperparameters, training, testing. We're in pretty good shape to get started. These methods will allow you to perform inference and prediction which are both important operations. We'll work with complicated data sets. You can work with big data. You can detect patterns in your data. Very, very powerful. Now remember in our business to win, it's really about having the best data. And I remember when I first started in the industry about 16, 17 years ago, I remember hearing that from the managers. Those with the best data win. Now it's those with the best data and do the best with the data who win. And there's a lot more tools to help us do that. We are at the beginning of the fourth paradigm of scientific discovery. There's a lot more every day that we're doing with data driven discovery, data driven approaches. We have a lot of fields where we clearly have big data, smart fields, 40 seismic surveys, computational resources are expanding to meet all these challenges. It's wild. There's so many computational resources now available out on the cloud and they're all being used by many of the energy companies. The result is with all of this great data and computational resources, much more machine learnings being done. We're going to start with unsupervised dimensionality reduction approaches like principle components analysis. It's an inferential approach.
 So I pull off only the oil and it's stored in this monthly production. So I drew it as a curve here but of course these are discrete points. Now they've already been accumulated every month so we could if we really wanted the cumulative production we could just sum up all you have to do is just call numpy sum on monthly production and that's going to give us the total production but that's not a very interesting problem. So let's make it more interesting and let's assume this is a rate. So let's assume instead of just being barrels that it's say barrels per month. So this is a rate as if we took these data points by going out and looking at the wellhead and looking at the flow rate on the wellhead and gave us instantaneous barrels per month. Obviously that doesn't really happen but let's assume that that's the case. And now this problem becomes just slightly more challenging. Instead of just summing up the numbers we have to integrate the curve. So we have to take the integral of the curve. We have to find the area under the curve. And does anybody remember the numerical technique for doing that or the simplest numerical technique for integration? Everybody remember the trapezoid rule? So what we do to integrate this is we assume that the space between every data point can be the area between two data points can be approximated by trapezoid. So this is a point A and this is a point B. The area that is A plus B over two, the area of a trapezoid. A plus B over two times delta X, B minus A. Now in our data it's always space on a month. So B minus A, they're incrementing space by month. So B minus A is always one. You can even ignore that part. And really this is all you have to do. But you need to do it for all data points. So the idea is we compute the area of a trapezoid for this guy and we compute the area of a trapezoid for this guy and we compute the area of the trapezoid for this guy. And then we sum all those areas up and that gives us the cumulative. That's the trapezoid rule for integration. So again, if you didn't know better, you might be tempted to loop over all the data points. Compute, you know, X at i minus X at i minus one, all that divided by two store this value and then sum that to the next one. But using numpy data structures in a way that's analogous to the finite difference in operation I gave earlier, showed earlier, we can do this all in just one line of code. We can compute the area of every trapezoid and sum them all up in one short line of code. Go for it. We're also, while you guys, once you finish or whatever, go ahead and take your coffee break, go to the bathroom. We'll build our break into this work period. And let's see, it's three o'clock now. I think it's a work on it and take a break, maybe 20 minutes is appropriate. So we'll reconvene at about 320 and I'll go over the solution to this. Okay. Just go ahead and go through the solution to this one again. So the monthly production is stored in this array. This is a numpy array. And we want to compute just like we just like in the finite difference operation. We want to compute from one to the end this time instead of minus plus. The beginning to one from the end. And then of course all of that divided by two. So that right there will compute. And again, you'd have to, this is like multiplying by one because every increment is one, right? But that will compute the trapezoid area for every segment, right? Of the entire array. And then we just have to sum all of those up. So we can use the numpy sum command for that. And if you wanted to test this locally. Then use the API number that I supplied. And that's it. You can test it via the unit tests on the command line if you want. But I'm pretty sure that's the right answer. You may have any questions, understand it, okay?
 I want to introduce one very simple machine. So let's start with a single machine. Now, remember, just to remind ourselves from day one, we define machine learning as a mathematical statistical model that learns from data, support it from expert knowledge. We put that in there, not everybody agrees with us, but we know we need expert knowledge. Not explicitly told how to predict its general, the machine can learn on a variety of problems and be able to solve them. So that's once again machine learning. Now, when I give that definition, when I teach my graduate level machine learning course, I introduce this machine every time, and I ask them, is this machine learning and I challenge them. So as I introduce this machine to you, you think about whether or not this is machine learning by that definition. Okay, density, porosity, we want to be able to do this. Of course, if we're trying to do well log analysis, we detect density, we go to porosity and so forth, we need a relationship like this. I'm going to build them, I'm going to build my machine. Okay, so one line of code, and I'm going to build a linear regression model. Okay, now Python code to do that is just simply from sci pi, there's a functionality called linear regress. I put in the predictor feature density, and I give it the for the training data, and the porosity, which is the response feature. And if I run this line of code, it will go ahead and then print the parameters for my model, the slope and the intercept. And I ran it and I get this model. This is my model. It's a line porosity, B1 density, plus the constant term. Okay, that's my machine. Now let me ask you a question, is this machine learning? Did I meet the bar? Does it bother you? Is there anybody here who looks at linear regression and says that can't be machine learning? I know how to do that. I've been doing that the whole time. If I change the data, will my machine learn from the data? Will it do its best to fit the data? If I if I was to change some of the data around the slope would move the intercept would move, this model is able to learn from the training data. Yeah, now there's nothing intimidating about linear regression. Would you guys all agree that a linear regression model is super simple? Do you feel like I'm trying to trap you? A little bit, a little bit. Okay, okay. So let's look at linear regression. Now I'm going to do this. I don't like suspense. Why am I doing this? I want to show you that with machine learning every methodology has limitations assumptions under the hood. And I just want to show that with linear regression so you'll be able to see that. Okay, so how does linear regression work? You're going to form model porosity is going to be a constant plus a slope term time density. We're going to minimize a loss function. You remember how I mentioned about L2 norms. There it is again. The mean squared error or this is the sum of squared error. That's fine. You can do that too. You don't have to take the average. The sum of squared error, the sum over all of the data, one through five data here, of the difference between the data value and the prediction from the machine square it. So the sum of squared error is just going to simply be the square of all of these right here. Okay, now there's a derivation of how we minimize the sum of squared error. And if you do work through the derivation, which is actually really fun, what you'll find out is that we can actually have an analytical form for linear regression. Now enjoy that because with many our machines, there's too complicated for an analytical form like this. In fact, many of them we have to use some type of iterative type of a solution scheme to get to it and optimization. Okay, so we can go ahead and calculate the slope and the intercept term for a linear regression model. Okay, now what do we point out? It's an L2 norm. It had a technically a loss function, these squared error at all of the data locations. Okay, now now if you dive into linear regression and you say what are the assumptions of linear regression. I don't think many people knew that there's so many of them error free. The predictor variables are error free, not random variables. Okay, so let me show you something. You see right here that we accept that there's going to be a degree of error with regard to the response feature. Our model has in precision in the response feature, this feature right here, these lines right here are the error, but you notice how have no lines going this way. The assumption of linear regression is there's no error in the actual measurements that go into the model. This value in density is known perfectly well. You see that that's an important assumption. Assumption number two, this is a no brainer linearity. You're assuming that the combination of features can be described by a linear function. Of course, of course, with a parametric model, you assume that the parametric form fits the natural setting. That's not a big deal. Now, this is going to turn this is going to cheer me up right now because every day I like to be able to say the term homoscedasticity. And so this is the first time I got to say today, you see I'm smiling. This is just makes me so happy. It's like literally my favorite words, heteroscedasticity, homoscedasticity. Okay, what is it? Constant variance error in response is constant over the predictor values. Okay, now let's go back here. You see these error terms. There is an assumption that those error terms on average stay the same constant that they don't change. We don't have higher error in the low values of X low error in the high values of X. That would be a model with heteroscedasticity. Now, as anybody out there ever linearized the variable to do linear regression like doing a log transform of permeability. Everybody does that everybody does and I've done it even my course notes and so forth. Now here's part of the problem when you do the log transform. It's a nonlinear transform. It actually causes the errors to be heteroscedastic. Okay, and so this is really an issue when it comes to trying to do transforms and I brought that question came up before. We do the log transform to try to linearize the model so that we can then do linear regression, but we invalidate the model because we violate the assumption of homoscedasticity. Okay, well, what am I saying? I'm saying I'm not saying stop doing that. I'm just saying check the model afterwards. The best way to do it is after you build the model, back transform it with the exponentiation back to original permeability and see how your model fits there. Because it may look good in the log transform always back transform and check. Okay, constant variance. I'm in a good mood now independence of the error. Okay, this is interesting. These errors right here have to be uncorrelated with each other. It can't be this is high error. Therefore, this will also be high error kind of a situation. You can imagine spatial correlation. In other words, think about calculating the verogram over the feature space and you could do it. You would not you would expect nugget effect. Hey, are you guys having some neurons fire? Just just for my insanity, my cheerfulness today remind me what the nugget effect is so I can have a quick drink of my this is spinach in case you're wondering why it's green. Now get effect anyway, we know the verogram is zero at zero offset because it's data with themselves. But there's an offset. Jad very short distance below the minimum data spacing. And that offset is caused by. What causes that offset? There's two possible causes one is natural and one is an issue with the data. One of the causes is ran mayor if you put ran mayor on the data will cause nugget effect. Okay. The other the other cause could be there actually is nugget effect and I admit that carbonaceous reservoirs. Oh man, those are dogs practice sometimes right those digenetic overprints can actually cause cavernous porosity with bit drops. Right that can happen and can also completely cement up the reservoir in some locations right. And so that variability where suddenly you have cavernous porosity and sometimes you have like cemented up completely would be nugget effect to. Okay. Causes the kind of short scale verabilities that are extreme. All right, the other assumption is no multi-colonarity. Now, this is the idea that one of the features can be described as a combination of other features. If you think about it, if you're into matrix math, we'd be talking about the rank of the matrix is is lower than the number of rows and columns. In other words, it's a machine or a system, I should say, for which you have redundancy between the features. You don't have as much information as you would suspect you have multiple inarity. You can have that. It would cause sensitivity. We're going to don't worry when we do horror stories with purchase shortly. We'll talk about multiple inarity. And we'll talk about it as far as the curse of the mentionality. Okay. The model can be tested for significance. Now, this is wild. It turns out that when you calculate the model, you can calculate the variance explained by the model and you can proportion variance variance is additive again. So what's really beautiful is you can calculate the proportion of variance described by the model captured by the model and the proportion of the variance unexplained by the model. And that's the R squared value that people use. Now, if you're doing linear regression, it turns out that the R squared value can be calculated prior to building the linear regression model just as the squared of the Pearson product moment correlation coefficient. I'm showing off now the correlation coefficient that we commonly use, right? So we can measure the goodness of the model. We always want to do that. But we can also calculate confidence intervals of the model. In other words, I can calculate the confidence interval or my uncertainty in the model parameter. For instance, the slope. Could it be like this or this or the intercept could the whole thing move up and down. And there's equations analytical forms for it. Now, what's interesting is that we only have analytical forms for kind of more of the simple cases of models, not for the general machines that we work with. And only under certain assumptions of distributions of the parameters or the features going in, I should say. Now, we can work out these uncertainties in the parameters using bootstrap and other methods. And we'll actually do that when we do these calculations like the tree bagging approach. We're going to be working out kind of an uncertainty model in a way. We'll use it to reduce model variance. And we also have prediction intervals. And so we can use the model to predict the uncertainty in the next sample or observation. And so with the parameters with this model, I can calculate the uncertainty plus or minus for porosity given density with an uncertainty model, not just the estimate, but the uncertainty too. And so with our machines, we do have methods to try to get at the uncertainties too. Okay. So would this be a fair model to work with? I think given that number of data, it looks pretty good. It looks like it's not a bad model given the scatter. I can't imagine how a more flexible model would improve because it looks like this is mostly kind of noise, right? Would it be safe to extrapolate with this model? There's a famous example of longhorn Texas linebackers in which they took the weight, the average weight of linebackers year on year for like a decade. And it basically was just going like this, like you know linebackers, you're just getting heavier and nutrition, focus on more strength on the field and maybe less speed. And what was interesting if you extrapolate that often to the future, you'd be predicting like weights of elephants. And if you extrapolate into the past, you get negative weight, you know. And so clearly we got to be concerned about when we extrapolate, when we build a machine, let's be careful. I'm comfortable using this machine between this range of densities. I would not use this machine if I was dealing with lead or something with a totally different density. Okay. So what did we learn from our simple machine? It's flexible. It fit the data. It was a parametric model. I agree it's not as flexible as a non parametric model. Minimizes the error with the training data, important assumptions. There was like five assumptions that many people don't think about when they build that model. And they invalidate with common workflows like using a log transform of permeability. There is the model can be tested for significance proportion of variance explain means squared error and so forth includes uncertainty in the model. Often try to get at the uncertainty in the model given the data predictions based on new data with uncertainty are very useful to always think about uncertainty and issues with overfitting extrapolation. Actually, Dr. Foster, we were in a faculty meeting. And I think we were all getting fatigued with the fact that people over and over again just kept bringing up machine learning machine learning and doctor foster, I keep using doctor foster quotes. This is kind of fun. And the faculty meeting he just stopped her when he said every time upon for me to yeah, you don't mind the john and hey, you let me know if I miss quote you. All right, go ahead. Hey, we're friends, right? We're still good. Okay, okay. So every time you hear machine learning, this is what Dr. Foster said in the faculty meeting, just think of a balanced linear regression. Great, John, is that what you said? Yeah, something like that optimization. Yeah, fancy linear regression sounds right. Yeah, right there, right? And so sorry, I should actually put foster 2019 personal communication or something there, right?
 I'm going to assume that you perhaps tried this or at least read the assignment description. Now I know we solved this in sci-fi and in sci-fi it was actually much easier because we just built, we used the built-in curve fit method. However, we could solve this in numpy too by basically implementing the underlying functions in curve fit particularly for this so-called B squares problem. And so the model is the same. We want to basically find the best fit for a coefficient in them. That fits this, that's proportional to this function. And then additionally we have an offset, a Y intercept to cap a 0 is what we called it in the sci-fi problem. But you can actually solve this with a set of linear algebra operations. Essentially you need a matrix A that looks like this and a vector B that looks like this where A is literally just a column of 1s and a column of the functional evaluation for the different values of porosity that we have in the data set. And then these capas are the different values of permeability that we have in the data set. So, first thing I did was define a function that we call kc model which just does nothing more than evaluate that function just evaluates that function. Then if you skip down to fit you'll see that the first thing I do is evaluate the function by calling the kc model. So now model contains just the functional evaluations. It's the column that I need here. It's this column. Then we create an A matrix that has, and we're using the built-in numpy commands. We create an A matrix that has a column of 1s. And well the way we actually created is by creating a row of 1s that has the same number of entries as model. And then a second row is model. And then we just transpose that so that we have a column of 1s and a column of the model evaluation. That becomes our A matrix. Then our B vector as defined above is just the permeability. And then you see I just call this least squares a comma b. The least squares problem is the fine here. It's the solution to this equation right here. So what we're doing is taking the matrix product of a transpose a. And then so that's the left hand side matrix of the equation. And then the right hand side vector is a transpose dot b. And so what this returns is, will be two numbers that will correspond to the slope and the intercept. And so in this solution I actually go through and plot it as well. So you can see here this is just the definition of the solution of that class. Then here is where I actually call it. So I instantiate the class by calling kc equals cos any carbon. And then I call kc fit that returns the slope and an intercept. And then I'd need to remain commands or just plotting it. And you know, we'll actually learn how to do this today with Matplot Live. And then I think I showed you last time you could also do this and see born with just using the built-in regression plot and you know, they're identical. So then there's also the fit through zero. And the way we do that is we just mirror the data. So it's kind of a little trick right. If we want the if we want the line to pass through zero, if we take all the black dots and we mirror them into the lower quadrature down here and then fit a fit of line through all of them, then it'll have to pass through zero. And so that's what we're doing there. And we're just using built-in numpy commands to do that. So I'll let you take a closer look at that on your own.
 What I want to do is I want to provide some examples of machine learning. Now you have an option now. We could dive into simple methodologies and kind of move forward or would you like me to kind of give you a vision of some more complicated methods and then go to simpler methods and start building up. You guys want me to start with some kind of like a little G whizz kind of inspire inspirational stuff. Be said, I guess in a way, I have to make up for all that linear regression talk. Machine learning, when it fails, you often don't understand it's failing and it doesn't fail gracefully. There's a great quote from a good talk on that. Okay, examples of machine learning. The next couple lectures today and the next day, we'll get into a bunch of different machines. So don't worry, there won't be this boring Michael talking the whole time. It's no, no, no, we're not going to do that. It's going to be you guys doing things. Let's talk about support vector machines. We won't cover. I don't think we'll cover support vector machines, but we'll see if there's if you guys want here more about it, we could support vector machines are really, really powerful when it comes to getting a decision boundary between categories in feature space. Okay, porosity, acoustic impedance. Well, maybe you want faceys one and two, two faces for which you have a, you're actually accounting for geophysical and petrophysical relationships. And that can be very useful. I have often seen faces develop petrophysically that are invisible on seismic. They don't show up very well on the seismic. And I've seen vice versa. I've seen people build seismic faces that just gives me night terrors because like that's scary because then we can't interpret them at the wells very well. And so forth. And they may not be tied to production either. Okay. So what support vector machines does is fascinating. It says, I want to fit a model that can tell me where the boundary is between red and blue circles or categories in porosity and acoustic impedance. In other words, I decided this is going to be faceys one and faceys two, but I want you to come up with a decision boundary that I can use. Support vector machines was designed for this problem right here when the data are overlapping. In other words, you have to classify in a situation in which the data is noisy. The blue cases and the red cases are on top of each other. What support vector machines does is it uses all of the data close to the proposed boundary to determine where the boundary should be. And those are what are called support vectors. Now the other so in other words, support vector machines will make this same boundary even if you had a million samples of blue right here. You see that? If I had a million samples of blue right there, it would not change the boundary location. Because support vector says only the data that are in contention near the boundary should matter. And that's powerful because many methodologies for classification would move the boundary based on a whole bunch of samples that are well within the boundary. You shouldn't care about them. You know, you know. Okay. Now the other thing that's really cool about support vector machines, would you believe that this model is a linear fit in a higher dimensional space? Isn't that cool? You project the problem to a higher dimensional space where you perform linear fitting, a linear hyperplane. And then you bring it back and when you back transform it to the original space, it's now going to have features like this. Look at this. Would you believe this is a linear hyperplane in a high dimensional space projected back to your low dimensional space? Isn't that cool? Okay. So support vector is extremely powerful. But guess what? You can have a good model. You can have an okay model and you can overfit and look at the degree of overfit with this model right here. That would be terrible at predicting away from the training data. Okay. Let me show an example. What one of my students did, which I thought was pretty cool, was they took a bunch of different common architectures and what they wanted to do, it was a warm up project, is they wanted to build a methodology where they could train an artificial neural net and it could go into a data set like seismic or like some type of interpreted model or even a synthetic model like a reservoir model and could locally classify the reservoir plumbing. Do you have channels? Is it low bait? Is it kind of a isotropic kind of a circular type of features? What's going on in the model? And what was fascinating is that we built a whole bunch of models and we're able to train this artificial neural net to be able to detect different features. In other words, it could see these were lobes, these were channels. It was doing a great job of detecting. Now there was a level of accuracy where it leveled off because artificial neural nets are really good at where you have high dimensional problems, but they don't actually account for image data. An artificial neural net is working pixel by pixel. It's not actually looking at the entire image holistically all at once. And so what we did was we then took that methodology. Oh, I don't have it here. We then took that methodology and we applied it to a convolutional neural net and we were able to actually have the accuracy improve even higher. So we used the convolutional neural net next and we're able to improve the accuracy of this method. Now the student was so inspired by these convolutional neural nets that they decided from then on to start building reservoir models with convolutional neural nets. And now this is really exciting to me. Let me ask you, noble energy. Do you have any reservoirs that you would characterize the architecture as being low bait, compensationally stacked lobes, that type of thing? I know they're in the deep water, sometimes shallow water too. In West Africa, offshore, they have these types of compensational lobes. In fact, they're them and deep water channels are kind of the common architecture. What Hong Kong Joe did, one of my PhD students, he's trained a convolutional neural net to learn compositional lobes. And what was really cool about it is we could then get it to build a whole bunch of compositional lobe models and then we could do a lot of really fascinating things with it. One of the things we could do was we could take these models and we could condition them to well data and I'll show that I believe. But we were also able to generate a single realization of the reservoir like this, another realization of the reservoir like this. And then we were able to actually turn a dial. Now have you ever seen where they take a picture of someone's face and a totally different person and they turn a dial and the face morphs from the one person to the other person. The eyes move, the eyebrows, broadened, the chin gets bigger, even male to female, anything they just change, right? This is exactly what we're doing with the reservoir model. We're morphing from one realization to another from here, across section to here. Here's a plan section right here to here. We can actually morph from one to the other. Now what the students doing right now is they're using this to do history matching because that gives you one dial you can turn to change the model between one realization to another and you can turn that dial and see how you can maximize the fit with production data. Any questions about this idea of building reservoir models with a convolutional neural net? All right. So another thing that a student's been doing recently is there's a methodology known as semantic in painting. And what you do with semantic in painting is really, really cool. You'll have conceptual and perceptual information. And so the conceptual information, the information around the boundary, the perceptual information, the information kind of overall across the image, we can use both of those sets of information. We could take a picture of this woman here, block out part of her face and have the machine rebuild using what it sees around the edges so it's consistent and using what it's seen in other pictures of human faces to put something in. Now what's really interesting is I think this case, this guy has blue eyes, brown eyes here, you know, it doesn't know the eye color from the face. But it does a pretty good job of getting a reasonable fit of features and forms. Here's a case where we restored number, a nine where part of the nine was missing. Okay. Now what the students doing with this is they're taking convolutional neural nets, they're building up reservoir models and then they're using semantic in painting to perfectly condition the models to well data. Now the conditioning challenge of getting precise well match at the well locations at the high resolution of the wells is a long standing problem in many reservoir modeling methodologies. Even when it comes to object based methods, even some of the convolutional neural nets that we build, we're not precisely getting a match. But what we can do the semantic in painting is we can actually force the model. We remove part of the model around the wells. We put the well data in the interpretations from the well log precisely right. And then we go ahead and we restore using this methodology to get a perfect match between the well data and what was going on around the wells. This is a methodology of precise conditioning. It could be used for a variety of different conditioning challenges in the subsurface. This is an example right here that I find almost spooky. This is an example of a long short term memory network. It's a form of recurrent neural net. Neural nets most of the time when we explain them to on Thursday, they feed forward. The artificial neural net takes information from the inputs to the outputs. A recurrent neural net allows it to loop back. The looping back, the backward connections create memory and it makes these methodologies recurrent neural nets very good at dealing with time series data. Because if you think about it in time series data, you're trying to learn from what happened in the past and impose that in the future. You're trying to make a forecast forward. What one of my students did who did graduate just a year or so ago with a PhD is they took this data set right here with nine injectors, one through nine injectors right here with different water rates, water injection rates for water flood. When there was injection data, we trained with the first 2500 days up until here. We had the production rate at Wells, producers 1, 2, 3 and 4. The blue is the actual production rate given the injection in the water flood at these locations right here. We trained with the first 2500 days and then we forecasted forward for the next 1000 days. It's fascinating if you look at the long short term memory system, it did a great job of forecasting the orange line, the true production. By the time you get maybe about 500 days, 800 days, it starts to diverge and that makes sense. You're getting into a really long forecast. You're going pretty far forward. Now what's really, really cool about this model, it knew nothing about the subsurface. All it had for inputs was the injection and it had the production. In fact, it didn't even know where the wells were. We never told that the locations of the wells, it just learned the interconnectivity. It learned the interactions between injectors and producers. This is the part I said it was spooky. The spooky part was this. Injector number four, look at its injection for the first 2500 days. Injection, low rate and then dropping. And then look at what we did with it. We spiked it up. And it's still, the model still performed well. In other words, it wasn't just learning the very simple patterns of relationships between injector and producer. It learned something about higher order relationships and interconnectivity between all of these wells because it was able to actually extrapolate beyond what it saw in the training data set. So that's pretty exciting stuff. Many questions about this example of production forecasting with a recurrent neural net because that whole tradeoff that we talked about day one between complexity and interpretability comes in here. And so this was my concern. I looked at the PhD student. I said this worked really well. We ran a variety of tests. We felt pretty comfortable. We had good performance. But we never fully understood why or what it was doing. We never could see the, as you said, the MX plus B, the equation in it. You could look at the neural net. You could see all the weights. But you have to understand, you know, we're at a point where we have thousands and thousands of parameters now. It's a very, very complicated system at that point. Yeah. So I have another PhD student and part of their challenge is to look at this model. And you remember the wolves and the dogs? Which pixels made it look like a wolf? I want to do that with this model. I want to find out which pixels or what time periods over what predictor features the injections told it to make this go down or make this go up or make this have error. You see that I want to understand what it's performing. In other words, I want to understand what it learned about the subsurface. Yeah. Any other questions or comments about this?
 So, Sypai is the scientific tool kit. That is, it's the collection of algorithms that we commonly use in scientific computing. In fact, in most cases, Sypai are just thin wrappers on much older libraries. Libraries that have been around since people have been writing Fortran code and developing libraries for numerical computation from the 60s and 70s. These are like very robust algorithms for doing, you know, numerical linear algebra, you know, solving linear systems of equations, all these kinds of things. And we'll, going to some of the packages here in a second, but it's just a collection of packages. And in many cases, these are just much older, really robust, well-established packages for doing numerical computations. Yeah, so if you're coming from MATLAB, it's the combination of NumPy, Sypai, you know, so NumPy is the data structures. And all of Sypai works with NumPy data structures. So, you know, if you're looking to integrate data, if you're looking to solve differential equations, whatever, they'll all work with NumPy arrays as input. So, it's the combination of NumPy, the algorithms in Sypai, and then the plotting library, MATLAB which we'll talk about on Monday, it's the combination of those three things that really reproduce all of the functionality of MATLAB with an open source package. So, there's many, many submodules like, and they're organized like this. So, when you import them, you actually have to import the submodule two. So, instead of just saying import Sypai, you actually have to say import Sypai special or import Sypai integrate. So, Sypai special, these are all your kind of special mathematical functions, vessel functions, integrals of vessel functions, shavishab functions. There's hundreds of kind of special functions, error functions, all of those, you know, ganna functions. That's what's in Sypai special. And then kind of also just things that don't fit anywhere else. Sypai integrate, that is your numerical quadrature. So, quadrature is a word for integration, numerical integration. So, these are your quadrature rules, you know, both for integrating functions, you know, mathematical expressions, and for integrating data, like the trapezoid rule. So, we just wrote our own implementation of the trapezoid rule, and it's very simple, right? It's not hard, but there's also an implementation in Sypai. And there are better integration rules in some cases than the trapezoid rule, like perhaps you might remember Simpson's rule. So, whereas trapezoid rule just connects two points with a straight line, Simpson's rule connects three points with a parabola, and then computes the area under the parabola, right? So, Simpson's rule, those kind of things, plus, you know, your fixed quadrature rules for integrating functions. Sypai optimize, these are your minimization routines, both single-variant and multivariant minimization algorithms, root solving, you know, that's a common thing that we do interpolation. Sypai interpolation is your interpolation routines, polynomial interpolation, interpolation, Laguna interpolation, Bessier functions, B splines, all of those kinds of things. FFT pack is your fast Fourier transform, and again, you know, most of these are just wrapping older libraries, and there's been FFT libraries around forever, you know, FFT-Lib. Single processing, these are your filters, digital filters, other things like that. Linear out-libris, some of the linear out-libris is also implemented or implemented in NUMPI. So, you can do some linear out-libris with NUMPI without needing to install Sypai at all. You can solve linear systems of equations in NUMPI without Sypai, but with Sypai linear out-libris, so apparently there's some overlap, but with Sypai linear out-libris can also do is have a sparse linear out-libris. So, often in scientific computing, we encounter major sees where most of the entries are zero. This is the case in numerical solution of most partial differential equations. Most of the entries in the matrix will be zero, and we often don't store all those zeros because they're just a large memory footprint. We just store the non-zero values in a special data structure. That's not just some big matrix. We just store the non-zero values, assume everything else is zero, and then Sypai linear out-libris gives us the ability to do, you know, solve linear systems of sparse equations very efficiently and other things like that. LU decomposition, QR decomposition, eigenvalue problems, all of that would be in linear out-libris. Again, sparse eigenvalue problems in that ARPAC statistics. So, Sypai stats gives you a lot of common distributions. You know, your standard Gaussian distribution, of course, but other viable distributions, gamut distribution, lots of distributions, as well as some other stats functions. Image processing, file input output for special types of like random axis databases that you're often useful for storing scientific data, HDF5, other things like that, and there's several more. I should probably take weave off of theirs, not really used anymore. Weave was a way that you could actually write for Trancode and have it compiled and used by Sypai, but we don't do that anymore. So, I just have a couple of examples here. I mean, I can't go through all of Sypai. As I mentioned at the beginning of this class, to be a good coder, you have to be a good Googler, and that's definitely going to be the case when you start to encounter some of these algorithms in Sypai, because it's just thousands. In fact, I do in UT, I teach introduction to numerical methods where we go through, not even there, I can't cover them all in a whole semester, but in that class, I cover maybe a hundred, and it takes a whole semester, I don't know, maybe not even 150 of these algorithms, and it takes a whole semester to do that. So, there's no way I can cover all of Sypai, but I'll just give you some examples of the types of things you can do. So, here we have a function that is the integral, I mean, well, it's an integral, right? So, it's an integral from 0 to 4 of x squared, and I chose this because it's simple enough we can compute it analytically, and in fact, we can compute it analytically in our heads, right? The integral of this is 1 third, in closed form, it's 1 third x to the third evaluated at 4, right? Or as I've written out there in closed form. So, this is the true integral, right? 21 and a third, that's the analytic integral. We can also use numerical quadrature here. Now, of course, we would never actually use numerical quadrature when we can compute the actual integral that easily, but in most cases, you know, the functions are much more difficult and they can't be integrated analytically. This is an example of using a lambda function as an argument to a function, right? I showed you that yesterday, but here's an example in sci-fi where we need to do that. So, we have this sci-fi integrate quad, quadrature is another word for numerical integration, and in this case, we're numerically integrating a function, right? So, we put in the function as a lambda function, right, x squared, and we put in the bounds of the function, 0 to 4, and in this case, it returns two things, answer and an error estimate, and I'm just printing out the answer there, right? So, you can see that the numerical integral is equivalent to the analytical integral to essentially machine precision, okay? There are also, you know, double quad, triple quad, fixed quadrature, and then of course, like we mentioned, the trapezoid rule and Simpson's rule, okay? Before doing, you know, for integrating data. Yeah, so here's an example of computing a matrix inverse. This one we could actually do in numpy, so that, you know, like I said, there's some overlap numpy lin-alge. You could compute an inverse. So, I just create a matrix and random numbers and compute its inverse. I also compute the eigenvalues of that matrix, which are somehow magically real, which is surprising given that there's just a bunch of random numbers. Here's just an example of interpolation. So, here we have the function is e to the minus x over 3. And what we do is we interpolate. So, I create a numpy array that's just an a-range that goes from minus 1 to 12. And I just do that so it classes through 0. And I just as a smooth curve. And then we interpolate that function. And we interpolate that data and create a function. So, this f is a function. And so, then we can integrate that function. I don't know why the appears to be cut off a little bit. But then just to demonstrate that, you know, it is in fact a function. So, we integrate the function on the on the interval from 0 to 10. And it gives us this number. And then I just to show you that, you know, another way to integrate the same data without creating a function, right? So, up here this is data. This is discrete data that we interpolate a function to and then use a quadrature rule to integrate. And here we just integrate the data without, we skip the step of the interpolating function and just integrate the data. And in this case, you know, there is some difference in the two integrals, but they're fairly close to one. And we're using Simpson's rule to do that. Again, integrating on the domain 0 to 10. So, that just gives you a little bit of the, you know, flavor of the ideas. It's all I have for slides on Cypod, but, you know, basically, this is just think of it, you know, any kind of standard numerical algorithm that you've ever wanted. That's where you're going to find them, okay?
 Let's talk about, I love this because this part is going to be horror stories with Dr. Perch. I think it's important for me to kind of scare people. I think it's kind of part of my job. We'll talk about dimensionality reduction and the curse of dimensionality. There's many challenges to working in multivariate problems. They're really big. There's so many features to work with. I've worked on reservoir problems where we've had, you know, 50 or more features that people were interested in. They thought we should explore. Now we want to be able to learn from our data, simplify our problems and build robust models. I'm going to show you, in fact, that we do a better job if we do a great, that we pick the best features or we project to a lower dimensional space, we're going to get better models and more interpretable models. To motivate this, let's talk about the curse of dimensionality. Now one of the definitions, and I said this on day one, of big data is variety. That you have a lot of features to work with. The, some people in our business in reservoir modeling called this massively multivariate data sets. I like that. It sounds really serious. Traditional reservoir modeling workflows were really bivariate. Two variables at a time. Facies, porosity, together, porosity, permeability, together. So you're modeling two variables at a time in a sequential manner. And then saturations by facies, maybe counting for permeability, but usually by facies or location in the reservoir or something, right? Simulation of permeability, counting for joints, simulated porosity is all very two variable at a time. Uncomventional and whole earth models have inspired or motivated or forced us, maybe kicking and screaming, to include more features or variables into our models. I remember being challenged to model not just the reservoir, but above the reservoir, so we could figure out about different risks and seal performance and so forth, right? So we require more variables. Unconventionals are driving us to consider all kinds of things we've never put in our models before. T-O-C, maturity measures like vitro-night reflectance. It's weird. It's like we're trying to get a reservoir on the source, I guess we are. You know, geophysical properties, geomechanical properties, geophysical, so challenged in uncommentions, porosity and different facies we would never considered before. Mud, more mud. When working with multivariates, more challenging to visualize and detect the relationships between the features. So we're going to be working with more features and it's going to be harder to do. And here is the cursor dimensionality. When working with more features or variables, it is harder. It's more difficult to visualize and understand what's going on with the data. There's more data required to infer joint probabilities. All I mean is we need more data to know what's going on, to be able to observe what's going on with enough resolution. We have less coverage. In fact, we don't actually cover the full space of the, what we'd call the predictor features space, the full common tutorial of possible features with each other. We don't actually see it. More difficult to interrogate, check the model. More likely we have redundant features. In other words, multi-colonarity. It's more likely that we have features that are combinations of each other. Happens all of the time and it actually, it blows up your model. It really does result in less robust models. More complicated, more likely overfit. So that's the cursor dimensionality. Let's just talk about it really quickly. Visualization. I will show you and I promise to show you throughout the entire sequence of these courses, simple problems with like two predictor features at most and one response feature. And the reason I do that is so you can see the machine. If I start working with seven predictors and two response features, you can't visualize it very well. With one predictor, one response, you can see the entire model. And I explain the concept of overfit, complexity versus flexibility and so forth. And I talk about the performance and testing. Look at this. Clearly the third order model performs the best in the testing. Look at how my seventh order model performs at the withheld testing locations. You can see it. You can just see how the model is performing. That's really great. You can do that with that situation. You can also start to talk about the range of applicability. Where are you extrapolating? In a low dimensional space, you can just see it. Boy, in high dimensional space, it's really hard to know when you're extrapolating. And are you overfit? You know, can you defend this model? If you showed this model from your boss, would this be a model you'd be proud of? This would not be a model you'd put on a t-shirt. That's for sure. It's not a great model. Okay, two predictor features of one response feature. Now we have predictor one, predictor two, and the response is coming out of the screen. It's the color map. Now what's really cool about this is we're mapping across and I will call this over and over again, the predictor feature space. Okay, so predictor feature one, two, this is the predictor feature space. You can observe it. And you can immediately see that a decision tree is very much a discrete, stepped prediction model. It uses regions. And you can also see that it does hierarchical binary segmentation. You can visualize all of that. Very, very powerful. So two predictors, one response, we can see a lot about range of applicability over fit as a defendable we can see. Now as soon as we move any further, it gets hard. For predictor features, one response, I'm not even showing the response anymore. This is a matrix scatter plot, facies, porosity, permeability, acoustic impedance. The relationships you see are getting complicated. Ah, for extra credit, what is this right here? Sorry. But yeah, that's exactly it. That's heteroscanasticity. The variability of permeability varies by the degree of porosity. Very cool, right? And you can see that you have other types of things. Look at this. A nice, homoscanastic linear relationship, almost bivari Gaussian looking. That's really nice. And this right here, a unique constraint relationship between permeability and acoustic impedance. Very cool. Okay. But remember, when we're doing that interpretation, and I'll tell you extra credit, you get that extra credit for saying heteroscanasticity, you're only interpreting two variables at a time. You're not interpreting. Would you believe you're missing a lot of information if you could see three or four variables at a time and really see this space? It's all projected in fland. You're missing a dimension or two. Okay. So let's talk about the curse of dimensionality. The first one is visualizations going to be harder. The second one is sampling. Now let's to explain this. Let's take one feature, porosity. And I want to understand the distribution of porosity, univariate distribution, the histogram. Now what I need is I need to break it up into bins. And for each bin, I need to infer or calculate what is the probability to be in that bin. Now so I need a probability in each bin. I need multiple samples or replicates in each bin to get that probability. In other words, normally speaking, I need multiple data in each bin in order to come up with these probabilities, the bin probabilities, right? I can't do it with just one data. And so I could imagine whatever that nominal number of data I need, maybe 10 data per bin and I've got eight bins, I need about 80 data for one variable or one feature to be able to get this distribution. Now what happens if I have two variables, two features? I have eight bins, eight bins. Over the entire predictor feature space, I have 64 bins to work with. My bins go up to the power of the dimensionality, number of bins to the power right here. And then the number of data I need per bin is the number of data I need. In the one-dimensional problem, I need 80 data, the two-dimensional problem, I need 640 data. You see what's going on here? That's going really exponential. That's going to go, I'm going to need so much data. Now you put into that equation 10-dimensional data set. And I'm telling you right now, you don't have enough data in the world to be able to figure out that. In other words, it gets harder and harder to sample and calculate the actual densities, probability densities in this space. Because ideally what we really want to understand is one understand the probability shape. This shape right here tells me the full relationship between those two and I won't have access to it in a high-dimensional space. Now gets worse than that. If it was just sampling, I wouldn't be so concerned. Coverage is what keeps me up at night. This is horror stories with Dr. Perch again, right? Coverage is the biggest part of the curse of dimensionality. It turns out that when you've sampled one trillionth of the reservoir, and we said that a few times now, what's the chance that you've sampled the ends of the distribution? What's the chance you've sampled the very highest and lowest prerosities per abilities? It's astronomically low. In fact, I think we're pretty overconfident in some cases. In many cases, if we're going to say we even have 80% coverage of the range of values. And in fact, what can happen is we might even be missing some of the intervals between the minimax. We might be missing stuff elsewhere. I don't think we have perfect coverage of this. Now imagine in one dimension, 80% coverage is not going to bother me. I'm okay with that. Look at what happens in two dimensions. In two dimensions, I have 80% coverage of acoustic impedance, 80% coverage of porosity. In two dimensions, I now have 64% coverage. Do you see the trick? In three dimensions, four dimensions, my coverage is the univariate coverage to the dimensionality of the problem. I hope everyone can see that by the time I get to a seventh or eighth dimensional problem, I've got very little coverage of that problem. Isn't that crazy? Now, if anybody wants kind of a little bit of a chill as far as kind of like, what does this really mean? It actually means in high dimensional space that all of the space starts to become corners or edges. That's a really interesting thought. And you can work it out mathematically by taking the ratio of the volume of a cube to the volume of a hypersphere within that hypercube. And if you do calculate that ratio, it'll diminish to zero over a high dimensional space. In other words, the whole space becomes corners and edges. Anyway. In a dimensional space, we have very poor coverage of it. Okay. And multi-colonarity is also a major issue. Now what does this mean? It means that a multi-colonarity suggests that there's a high degree of correlation between supposedly in the pen and variables. Now what I think, a good way to describe it would be this. If I had feature one going in the left right direction, feature two going in the up-down direction, what you could imagine is if I had my data set like this, it's a two-predictor feature data set, but can you see it all falls on a line? It's all correlated strongly with each other. Now what happens is if I try to fit a plane, because the plane is the prediction model coming out of the screen at me. The response feature is coming out of the screen. If I try to fit a plane through that model, if I change one or two of the data, that plane will swing. It's very unstable as a model because of the fact that the two predictor features are highly correlated with each other, almost multi-colonarity. Okay. Now if they were perfectly multi-colonarity, in other words, they'd be collinarity, really, because there's two of them. In that case, all of the data falls on a line. You can't fit a plane to a line. The model has actually no prediction accuracy anymore whatsoever. This is multi-colonarity, and the more features you have, the more likely that there's going to be features there that are combinations of each other. And I promise you, it happens all the time. You don't expect it, and it's very hard to see, because often there'll be multiple features combined to make another feature. You can't just see it. It won't be a simple linear plot on a scatter plot. Okay. Any questions about the curse of dimensionality? I should have brought a lantern, turned the lights off while I talked about it, it would have been kind of cool. Okay. Curse of dimensionality. So because of the curse of dimensionality, we'd get better models with fewer informative features. In fact, I disagree quite strongly with the philosophy many people use, and I think it's misused. I don't miss class by data mining. But the people in data mining who say just through all the data and will figure out the pattern, I get really worried about that. Various correlations are going to happen all the time, and this problem happens. Throwing everything in the kitchen sink into the model is going to make the model less stable. The model is going to perform poor. Fewer features for a model that are simpler, faster, and easier to visualize, and less likely overfit is better. Okay. Dimensionality reduction. So we have two choices when it comes to dimensionality reduction. We can either do feature selection, in which case we just say, well, we've got M features, I'll take P, reduce set of features to work with. And so you find what many companies would call the big hitters. What are the features that have the most amount of information? The ones that are most important. The other option we have is feature projection. We transform the data to a higher, from a higher dimensional to a lower dimensional space, feature projection. Okay, feature selection. There's multiple methods. Any questions, sorry. Any questions about cursor dimensionality? Makes sense? All right. Thank you very much. Thank you, Tim. Okay, consider the feature selection approach that I would recommend is consider a wide array approach to assess variable importance. I don't suggest just using one metric. I'd suggest using multiple metrics, look at the data, look at the metrics and figure out by that what's going on with the data in order to make an informed decision. A variety of metrics you can use, visual inspection of the data, the distributions and the scatter plots. Not a bad idea. In fact, if you find the data is unreliable, it's summary statistics are wonky. There's all kinds of things going on. It doesn't look, you know, there's lots of outlier problems. There's missing parts of the distribution, you know, bias. Then that may motivate you to either work with the data more feature engineering, data prep, or you may decide to remove that feature. It's unreliable. Statistical summaries are not a bad idea too. Just look at the summary statistics, see how they are, see learn about the distributions, learn if they're reasonable. Model based approaches. The model based approaches are really interesting. What we actually do is we use the machine learning model to tell us which features are important. A random forest actually provides a measure of feature importance as part of its output. It can tell you while it built the model, which were the features that were seen to help the model the most. That is really, really cool. Recursive feature elimination is another methodology we can employ. It's a trial and error approach. And it's very logical. If I've got two features and I want to know which is most important, build a model with one feature, build a model with the other feature, then build a model of both features together and see which one of them performed and see what the differences are. And that's a measure of, you know, which one should you use in the model, which one's performed best? Okay. Feature projections, another methodology we can employ. Now the thing with feature projection, what we do, and I should mention now, we will go straight to feature projection. Now I'm going to see how we do a feature projection. If we do, if there's kind of more time, you know, depending how we get through it, I may decide to go back and we do, may do some feature selection together. Now right now you can mute me and let me know that you think we should rather do feature selection instead of doing feature projection. I would do that. Just in case. But we'll start with feature projection and we'll see. I'm not seeing anybody speaking up about it. Okay. So feature projection, the dimensionality reduced by projecting the features through a transformation to a lower dimensional space. Given the original one through M features and always use M for the number of predictor features, the inputs, we would, we would be able to project that to a lower dimensional space and we'll call that P. Now if you just think about it, if we have M features, we have M choose two possible scatter plots to consider. So even if you were just thinking about the two variable at a time relationships, like those matrix scatter plots, we already have this many scatter plots to consider. That dimensional space is really hard to work with. If you have 10 features, that's a lot of plots to look at. So once we have four or more features, understanding the data sets can get really hard. So we'll go ahead and try to do a projection. Okay. To find a good lower dimensional P space for the original M high dimensional space. Now the benefits are going to be direct. The first benefit is computational time and data storage. If you project your data from 10 dimensions into two dimensions, you're using often 20% the storage for the data. Now that often means like 20% of time to load the data and depending on the operational load or the computational complexity, I should say of your machine learning, it can often be a savings of exceeding a reduction by 80%. Because often you'll find that these operators are more like an O2 operator, O3 operator. In other words, as you increase the number, it goes up to the square of the cube. So if we can reduce the number of features, boy, it can really speed up all our calculations. Now when we model with the reduced dimensional space, it's actually going to take care of automatically the issue of multicolinarity, which is really cool. We do feature projection. If you have multicolinarity, it's going to smash it. It'll just take it right out and we'll remove it. And so you'll see we can take her at that. The limitation, more difficult to understand the model. When you project into the P, good, lower dimensions, we lose the physical interpretation. Because what's going to happen is these P features are combinations of the other features. They're not porosity and permeability. They're going to be porosity permeability weighted and combined linearly, with maybe a low acoustic impedance and other things mixed in for good measure. A little bit, it'll be more difficult to understand the model. The new features are combinations of the original features. They do lose their fiscal meaning. There's a variety of methods available. We're only going to cover principal component analysis.
 Not that the previous numply implementation wasn't also simple, but now we can just, you know, I even left the import statement for sci-fi integrate. We can use the function trap z. And if you recall, if you remember, if you recall, I mentioned before when you're using a Jupyter Notebook, you can put your cursor on the function and if you hit shift tab, it will bring up the documentation. So we could also make sure what happened there. Anyway, we can also look it up online, of course, if we just Googled sci-fi trap z or sci-fi trapezoid rule, we would get what we needed. But you know, since we have it there, I know what the function is, but perhaps I don't know what the function signature is, well, it tells us like it takes as the first one positional argument, why, why is a function of x? And x is optional. You can see it's a keyword argument where the default value is none. And it tells you the sample points are soon to be evenly spaced dx apart if x is none. Well, then you look to dx and you said spacing between sample points when x is none, the default is one. Well, this is our case, right? Along the x axis, there's an increment every month, right? Every one month. So x is just equally spaced dx is one. Then all we have to do is put in the y value, which is simply monthly production. So that's it. Then we can test it if we want with this API. John, are you able to zoom a tad bit for us? It's not bad, but maybe just a little bit. How's that? Is it better? Or did that not change anything? It didn't change anything. Okay. Is that okay? Did that zoom in on me? I think it did, but I think we can read it. Okay. And must have had something to do with the way the screen sharing of the browser is working. So that was really simple. Got the same result we got before. So let's go to something slightly more challenging, and that is this, because any carbon problem. So if you recall, because any carbon is a relationship that relates, a porosity to permeability, right? And it's through this functional relationship here. So where where fee is the porosity, and there's some parameter that you have to fit in. And possibly some offset there. And so what we're going to do is we need to first write a lambda function that replicates this model. So I'll just call it KC model. And it's going to be equal to a lambda function. Remember, I said to other use case for lambda functions. Or whenever you need to define a function inside of another function. So I guess before we do that, let's take a look real quick at the sci-fi curve fit. I'm going to look up the online documentation. Ultimately, we're going to use sci-fi, optimize curve fit. We're going to. And so we need to look at the argument structure of this, right? This is, you know, the first argument is a function that is a callable, right? The model function, F, where the first variable is the, the first argument is the independent variable. And the parameters to fit as the remaining arguments. So our function has to have this structure, the independent variable first, and the parameters to fit. Second, okay? So returning to Jupyter lab, everybody did that update Jupyter lab? Okay. So the independent variable of this model is fee. The parameters to fit our capa out capa zero and in. Okay? So we can type. I think another thing I didn't, maybe not haven't shown you guys, but you can use actually Unicode characters in as variables. And so if I remember when I was showing you guys markdown, I showed you a little bit of a text syntax for certain types of math, fonts and variables and stuff. In Jupyter notebooks, well, you can actually type let's act syntax like fee and then hit tab and it'll convert it to the Unicode symbol. So I can actually have fee is an actual symbol or variable in my code. And, you know, I could also have a capa. I don't think it'll do the zero. Capa, right? You can just say capa zero. And, um, M. So my functions, the function of those three variables. And again, the function itself is capa plus M times. Fe cubed over one minus. Fe squared. So that is my model function. And so then I just need to call sci-fi optimize curve fit. Now, I want to also point out when you're looking at the documentation of curve fit down here where it says what it returns, it returns two things. And then we'll have optimal values of the parameters. And the covariance matrix. Okay. We're not going to we don't care about the covariance matrix in this case. So we're just going to ignore that part of it. But nevertheless, we need to at least have a placeholder for it. We're going to use that unpacking syntax. So basically what's going to this is going to be sci-fi. And then we'll just use the function of the first argument to the first argument to the first argument to the function. The second argument is. X data. Right. That's the porosity in this case. And the third argument is Y data. That's permeability. And there's stored as class attributes up here. So I'll say self. Phorosity. Self permeability. So this is going to fit the curve to that. Return the optimal parameters that is capa alpha and in in P off and return the covariance matrix. And I'm just going to throw it away. Sign it to the underscore there. And I want to return P off. And so there. That's the fit function. And we should be able to test it. By instantiating the class. And then just calling kc fit calling the function. And so there's the. There's the parameters. And then of course, the. And I'm going to show you. I'll show you a plot of what we're doing in just a second. And then we're going to make sure it makes more sense. But. You know, there's also this. Offset, right? I mean, the permeability. This capa alpha is zero, right? Where in reality, if the porosity zero, the permeability should be zero. The data is not saying that the best fit through this data. And the process zero, the permeability is four milledars. Which is obviously not physically correct, but there's noise in the data. So if you wanted to change that, I mean, I'll always have to do. For fit through zero function is just. Just get rid of the capa zero. So we just modify our function to get rid of that. And everything else we can remain the same. And then we could also call. Fit through zero. So. Let's take a look. I'm going to use a, you know, we haven't really talked about plotting and. Empython right now, but I'm going to use a special function to help. To plot this really quickly. So. And on the x axis is going to be kc. Corrosity. Cubes. I think you guys let me make a mistake. So the mistake right, the function is one minus p, the quantity squared. And I accidentally put it. And then I put it in the square. In the wrong place. So. I can update that. I was a little skeptical of the numbers I was seeing. Now those look right. So now. Fix that here. I realized that when I was typing that. That's what we're doing. So the raw data. And I'm going to put labels on the axis. But what's on the x axis here is that function. This function. And what's on the right axis is permeability. So this is this is feed cubed over one minus feet squared, the quantity squared. And then. This is permeability. And that's the raw data, the data points of the raw data. And the line. Is the line that's fit, but with these parameters, right? So the intercept is cap up the intercept is 10. In this case. So it doesn't go through zero. It intercepts at 10. And the slope of the line is like 2300 or so. So we fit a line. We've done a linear regression that we fit a line. This is our first machine learning that we've done in this course. We've done some simple linear regression here. And that's what we implemented, right? So this C-born is a statistical visualization. Package within Python. And it actually has a single function and reg plot that will just do that for us. But this is the actual what it's doing behind the scenes is what we implemented up here. Everybody okay with that? The reg plot even takes a step further and puts the air bounds on the lines well. So we'll talk more about plotting on Monday. Everybody okay with that? Any questions?
 So pandas is the package that's going to replace Excel for you. Whereas numpy and pandas is actually built on top of numpy, we'll see that at some extent here in a second. Whereas numpy gives you indimensional data structures. So we looked at mostly 2D, but we did look at at least one 3D example. But you could have 17 dimensions in those numpy data structures if you want. There's no limit. In pandas, we're primarily talking about tabular data. Think of Excel spreadsheets. We call them data frames. We'll see one in a second. Basically, you have labels at the top and you have some type of row numbering for indexing along the rows. These are the types of data structures that pandas gives you. These are called data frames. And so just to start off, in a similar way that we import numpy as, you know, MP, we almost always import pandas as PD. So again, this is idiomatic. When you go to do Google searches and other things, you'll notice this is everywhere. It's everywhere on stack overflow. It's in the pandas documentation this way, everything like that. So just so you know, if you really wanted to do something cruel to somebody, send them a Jupyter notebook. And at the top, you know, put like import pandas as MP and then use MP and your code everywhere. You'll really confuse them. So, you know, throughout this course, we're going to use intake to load data and deal with it. But pandas is so good at loading data from CSV files that I can't not talk about and show you. I mean, even if you don't want to use the functionality of pandas, it's so good at parsing data from CSV files that I would still recommend it as the way to load data. So there's a function in pandas called read CSV. I've provided, you know, a CSV file in a folder called data set. And so we're reading that in and then we're printing off the first five lines so that command head. So we're reading it in into a data frame. DF is a variable that contains the data frame. The data frame again is this tabular data structure. Again, labels across the top and indexes across the bottom. In this case, it looks like probably I accidentally when I, when I output 200 wells, I accidentally put the index itself in there, which is not really necessary. So typically the index over here on the left side is the row number. It's just monotonically increasing integers. However, they don't have to be the index could be anything you set it to. And often when we're dealing with time series data, the index would be the time stamp. So like we had a data frame that had monthly production in it, we could set the index to the month. And that would allow us a nice capability later on, particularly with respect to plotting. So we're storing DF is a is a data frame object. And if we then you know an object that's implemented as a class, right? So it has it has attributes and it has functions that operate on one of those functions is head. And that just prints off it's just a way to print off the first five rows of the data frame. So our data frame has X, Y, some faces, conventional, porosity permeability, because it's beatings. Okay. Another function that's defined for all data frames is describe. And so the nice thing you just call it and automatically computes some summary statistics for you. So you just call the scribe on the data frame. It will tell you the total number of samples. The mean of every column, the standard deviation of every column and the court, the min max and quartiles of every column. So automatically computes that for you. So it's a nice way to get a one shop view summary statistics of what's in your data frame. It's a good way to just make sure everything checks out. It has a number of samples you expect. You know the other things like that. So you can select parts of a data frame. You can get it by their labels. And this is what I really like about pandas is because I'm a huge advocate for code readability. It's so important that your code is readable for it to be maintainable. It's read far more by humans than by computers. That's why I was telling you other day you should, you know, you're variables should be self explanatory. You shouldn't need comments in your code to explain what variables are. So you can just refer to the data frame. And you can just refer to the data frame. Well, in this case, we have data frames that are, you know, have labels on them. And so we can just refer to portions of the data frame via their labels, right. In Excel, you have like alphabetical labels across the top. Do you refer to columns of the data? Is it possible to rename those? I don't even know. I'm never tried. Can you can you rename? Is there like an Excel expert can answer that question? The alphabetical numbers on the top of the. No one knows. Okay. Feel like it should be. But anyway, so you know, when you go to write an Excel formula, you have to. Refer to the column, right? The column by its alphabetical name. And so you have to do some kind of decoding, right? If you go back and you look at those formulas, I mean, never mind the fact that you have to write. Formule in a cell and it can be really complex and ugly. Never mind that. And even if you get past that annoyance, you still have to decode what the. What the column names are, right? You have to go back and look. But here it's obvious, right? If I'm trying to pull off the porosity column, it's called porosity. I know what it is. It's self-evident. So again, I'm just pulling off the porosity column, printing out the first five lines. I can pull out multiple columns. Okay, Jason says that you can, but your data has to be placed in the table. Okay. So evidently, there's some functionality to be able to do it. I thought there would be. But. So. Here's an example of just pulling off one column. I can also pull off multiple columns by passing in a list. So in this case, if I want the porosity and permeability, then I would just pull off those two. And again, in most cases, you'll see the head function on there because I don't want to print out the entire data for you. I just want to, you know, just view it. Because in this case, there's 200 samples, right? I don't want to look at all of them. And look at the first five for demonstration. So that was an example of using the label. However, if you really want to. Grab things by their, well, rather. With the local function, you can grab things parts of the data frame. Be of their index, right? So in this case, the first, just like in a numpy array, you have like the first axis, the second axis. And a pandas data frame, it really just always two axis. The rows and the columns, right? Or the index in the label. Is the correct terminology. In this case, we can use the, the local function. The, the first axis is the rows. In this case, we'll just pull off rows one to two. It supports the numpy slicing types and tax. And the columns. In this case, the list, porosity and permeability. We can also use numpy slicing to select a sequence of labels. So in this case, again, just pulling off the first and second row. But now instead of just the porosity and permeability, I'm saying. From porosity to acoustic impedance, right? Be of the, the colon operator saying get, you know, get all the labels. From, from porosity, moving to the right to acoustic impedance. And so you get, get that. So that's a look. There's also ILOC, which allows you, if you really want to, to index purely by the integer. Location, right? So in this case, the first is the, the rows, right? So I'm saying from one to three. And this is the column. So from the third index column to the fifth. And this supports the Python style indexing in the sense that. It will not take the actual last value, right? So it goes one, two, but it'll miss three. And. You know, same thing in the columns. It's taken the third and fourth column, but omitting the fifth. So that's ILOC. It allows you to actually select rows and columns by the integer location index location. So it's like index locator. I mentioned that pandas is built on top of numpy. And we can actually get the underlying numpy data structure from, you know, the pandas series or data frame. So I've been calling everything a data frame. What a panda series is, is like a data frame with just one column and an index. Or you can think of, you can think of a data frame as a collection of series. So series is just a column of data and an index that's associated with that column. And again, if it's just monetetically increasing integer from zero, not that useful over a numpy array. And index is something more meaningful, like a timestamp or something like that. Then, then, then it's, you know, that's a different scenario. And it's more meaningful. So if we want to get the, just the underlying numbers. So that we can use, you know, do numpy style operations on something. Then all we have to do is just take the column we want or the columns we want or whatever. Or the entire data frame, even. And just put dot values on the end of it. Now we're returning to numpy array with the values, right? So now there's no index, right? So if I, if we look at that. We have the index and the, and the value. So, you know what I, I commented out the, if you don't know what I did there, I commented out the attributes. So now, this is just like calling data frame porosity. Or I could even. Put the head on there so we can see it like we've been seeing it. So there's the first five rows of the data frame. But if I just want the values, I do that. We can rename the labels, right? So, you know, I don't like the label faces underscore threshold underscore zero point three. I just want to call that faces. So I can rename it with using this rename utility. This rename keyword argument columns. And then it takes a dictionary. So by default. Pandas data frames will usually return a copy. Or a modified data frame. If you don't want that, if you want it to work on the data frame in place, you use the in place command. So in place equal true. And this actually modifies the data frame in place. So if you look at data frame head, you can see that, you know, I renamed. That the face is permeability to perm an acoustic impedance to AI. The labels have been changed. Right. I can add a new column to a data frame. So. This doesn't answer your complete question because you're asking about a derivative. But if you need to add, I mean, if you need to add a column to a data frame, it's just that simple. It's just like adding an entry to a dictionary. So in this case, we're going to add a new column. The column is going to be called zeros. And in this case, we're just going to fill it in with num, you know, a numpy array of zeros. However, you know, if this was your derivative computation. This could include. You know. Numbers from taking from the other columns in order to add them together. For example, I guess, you know, what I'm giving example of what I mean by that. So we could take the porosity. And divide it by the permeability. For example, this could be some type of derivative computation. Like you could actually do something like this would be the feed the capa. I don't know why that's useful, but. You could do something like this. Well, there's multiple ways to do it. You could use loke. And then you could say. Just like we did before from one to the end. And from. The end to minus one. There. And then you could say, you know, you could say something similar for permeability. Right. And that would work. Now, you would have to. You know, the derivative because you're taking increments, there's going to be one less number. In this, then there are in the current data frame. So you have to pad it on the begin on the front of the back with a zero or something to be consistent with the number of samples. In other words, in this case, there's 200 samples. The result of this computation would be. So you can see the data samples. This is the nature of the derivative computation because you're taking delta, you know, delta something over delta something. So it's always going to be one less. Yeah, so you want to remove a column. We just added a column zero. If you want to drop it, then you. You can drop it with the drop command. So in this case, we'll drop it by the label. And then the axis one just means to drop the column. And again, I'm doing most of the operations in this slide deck in place. If you want to move rows, you could just do it like that again. You don't need to specify the axis command. The default is zero. So if I wanted to take out the first row by the index, then I would do that this way. And you see that it's actually. The one is missing. Right. It's gone. You can sort. So in this case, just like you would an Excel, right? You can sort based on a column and it'll sort the entire data frame. Right. So you'll notice that all the indexes are jumbled up now. They're all jumbled up. Again, that's. So I'm sorting the data frame based on porosity. And this time, I printed out 13 lives just so you can see a larger selection of how things were sorted. And in fact, you know, the porosity is sorted from largest to smallest, right. Going down. And the indexes are all jumbled up now. So if we wanted to reset them, then we just use this reset index. And that will reset them all. Right. So if you want to do, you know, when we use a lot of times, you know, we're talking about machine learning. With respect to pandas, the labels are what we call features. Right. The rows are called samples. The labels are called features. And so if you want to do some feature engineering and, you know, a data science role or machine learning role, you can do that by creating new features. Now, in this case, the features aren't that interesting. But for example, if I wanted to change my porosity from a decimal to a percentage, I can do it this way. It'll create a new column in the data frame with the porosity converted to a percentage. If I wanted to, for whatever reason, have a porosity permeability porosity ratio, I can do it this way. Again, just adding a new column to the data frame, follow with the mathematical operation. In most cases, you can, you can do, if you, especially if you're just operating on the entire column of the data frame, you can just. It will imply broadcasting style operations. So multiplication division is this will divide. It element wise division element wise multiplication. Just like an unpy broadcasting wheel. And you don't need to, you don't need to pull out the values right. You don't need to actually pull out the numpy array. You can just operate on the whole series. And again, the operation will be done on the data. The index, the index is not, not bothered by this division. The signing conditional category. So this is more feature engineering, I guess. The reason the numpy where command, if you remember, I demonstrated that where now we're just saying that, you know, where data frame porosity is greater than point two. So if you sign a value high, otherwise, I want to sign a value low. And if I then print out the part of the data frame where there's highs, you can see that, you know, zero, one, two. Index have high. And, you know, if I wait, if I say porosity low, then, you know, I have to go to like 165, 166, 167. Remember, we sorted the data frame on porosity. So if I would have only just printed out the head, all you would have saw was high, high, high, high. Right. I have to go to the 165 row before I get below that threshold of point one, too. Because again, I sorted the porosity earlier. I sorted based on porosity earlier. So this is just a display that we see this new feature over here, right, high or low porosity that we created. We did it in one line, right, one line of code. The other ones are just printing out the results. More kind of feature engineering again, using the numpyware command. So in this case, and you know, it's kind of contrived example, but, never less. In this case, if the porosity is greater than point two, we just want to pass the permeability through. However, if it's less than point one, two, we want to set it to point zero, zero, zero, zero, one. Okay. So again, if we look at this, you know, if you print off the first three things, the permeability cut off, these are all. If you look at the perm, they're all above, I'm sorry, if you look at the porosity. Right there, the first three are all above point one, two. So this perm cutoff is unchanged from the original perm, right, that is same. Perm here, perm cutoffs the same, because the porosities above point one, two. However, if we go down further into the data frame, where the porosities below point one, two, then, you know, the perm. So this is the original perm, and we've cut it off point zero, zero, one. So just doing more feature engineering. Now, this is kind of where you start to get into like really thinking about replacing Excel, right, because if you're just adding two columns and data together or something like that, well, that's not that hard to do in Excel. So let's say we have some more complicated function. And in fact, in this case, I write the, I define the function in Python, right. And in fact, what it is, is it's that cost any carbon function, right. So I have a, cause any carbon with a threshold. Again, I don't know why you would do this, but it's just an example, right. So if you have a function, could be as complicated as you can imagine, it could be 2000 lines of code. You write the function, and you don't have to write it in a little equals, you know, all in one line at the top. You write the function in Python, like you write any function. And then you can apply that function to the data frame with the apply command, right. So in this case, I'm going to create a new column called because any carbon apply to the porosity, right. This guy takes two arguments, but the second argument has a default value. And in fact, I change it here, but nevertheless, I apply. Cos any carbon with threshold. And it operates on the porosity column, creating a new column. Right. So this is my, you know, the last column of yours, my new column, which is the cos any carbon model with a threshold. So for very low, for very low porosities, it ignores the, you know, whatever the function evaluates and just sets it to equal to 0.001. Just that's written in code, right. That maybe like this slide might be the, you know, the most important one in the group. It could mean hopefully everybody understands like, you can write a function in Python and have it operate on a column of data. And you don't have to write in one line this long crazy function that you would write in itself. I mean, the other thing is this function is much easily tested, right. I mean, I can stick in a, I can stick in an individual value and test it on my calculator, seems working correctly. So I can test this function. And then I can apply it to the whole data, the whole column. So here we're going to, I'm going to intentionally, you know, all we have, we have real data. A lot of times are, we have missing data, right. For whatever reason is data missing. And so in this case, I'm going to, I'm going to intentionally create missing data. So I'm going to set at position one of the porosity column. I'm going to set it equal to none. And you'll see that that's automatically converted into a pandas, not a number in the data frame. Right. So if I print off the first three values, you see there's not a number right there. Okay. So I can use a couple of functions to find those missing data points. Like so, for example, I can call is null. This function inside the index here, right. This produces a Boolean array, true and false. We've seen how to do that. And then we combine that with fancy indexing essentially. So this produces a Boolean array of true, false, true, where is null, right. True where one of the columns has a null value. Right. And then I just use that to index into it to print off that column. And we see it's one is right where we stuck it in at. Like so, and you know, in this case, we know exactly where we stuck the man, but in most cases, real cases with real data, you know, you know, you know, 10,000 lines and their nans in there, you don't know where they are. This is a way to find them. There's a nice one liner. If you just, I'm not saying you should just drop your nans because it may be that there's just a man in one column, but all the other columns have values, right. And so in that case, you probably wouldn't want to drop the whole row because the other columns have some meaningful information in it. So in that case, you probably want to replace the man with, you know, some average or, you know, something that's called data imputation in the data science. But in this case, just to demonstrate, we can just drop the whole row using this drop in a command. So we just one liner. And in this case, just one row, but if there were 100 rows with nans, they would all be gone. And we're, you know, again, we're doing it in place in the data. So in the, all the examples up to now, we read in the data from a CSV file that was loaded into a data frame and we've been manipulating. What if we want to start from scratch? Okay. So in this case, we create a bunch of random numbers. That are in the range of zero to point two. Okay. That's this first line. And then we use the cousin, Carmen model to, you know, create a permeability array. So these are two or numpy arrays. And now we want to create a data frame. Well, the first thing we do is create a Python dictionary. Remember our dictionaries, keyword value, keyword value. So in this case, the keyword is porosity, the label, the value is the numpy array. So we create this data frame dictionary. And then we can create an actual pandas data frame that we can use all those functions that we've already seen and operate on. By just calling PD data frame and passing in the dictionary. So to create a dictionary to create a data frame from scratch, you create a dictionary first and then just pass it in where again, keywords are the labels and the values that would be the columns of data. Then we can, you know, if you want, then like I said, once you have it in a data frame, you can operate on it with all those functions that we've already discussed, sort values, whatever. There's also the functions to merge data frames. So in this case, I'm taking off a piece. I'm sorry, let me silence my slack messages here. In this example, I'm taking off a piece of data frame, calling a data frame one. And a piece of the data frame, calling a data frame to, in this case, they're copies. I'm not doing that in place. These are copies. So these are two new data frames that are just subsets of our original data frame. And then we can use this concat function to concatenate them together. And you see the indices are retained from the original data frames. It is possible to ignore those indices and or reset them so that they just monotonically increase from zero. Well, that's coming it out. So if we just, perhaps it wasn't run. We can call plot on a data frame. But if we just do that, if we don't do anything else, we just call plot on a data frame, well, it automatically plots the features against their index. And those indexes are meaningless. They're just monotonically increasing numbers, right. In this case, their meetings. And so, you know, this is not a useful plot, obviously. So it'll create a plot, but it's, but it's not useful. So a more useful plot would be to set the index of the data frame to be, in this case, porosity. And so I want to plot porosity versus permeability. In this case, I take the data frame, I set the index to porosity and then call plot on it and you get something more meaningful. Now, if you don't want to, if you don't want to set the index, you could pass arguments to the plot command. In this case, I'm actually taking it a step further to not just call the plot. The default would be a line plot. In this case, I'm calling it plot dot scatter. And I'm actually telling it what the X and Y value should be. And I get a more meaningful plot. So, you know, this is another really nice thing about pandas data friends. You can just quickly, you know, a lot of data analysis, just looking at data, you know, and coming up with clever visualizations for high dimensional data sets. And so, you know, with pandas, you, the fact we can like create these really quick look plots without having to, you know, maybe they're not the most customized and pretty plot that you would put in a paper. Or, you know, even show your boss, but as an analyst where you're doing exploratory stuff, just trying to look at data figure out what's going on, the ability to take data from a data frame and just quickly visualize it, right and kind of built in that that's really useful. So, we can also write the data frame to a file. So, in this case, you know, I'm taking our data frame, our original data frame and writing it. So, original was 200 wells. Now, just create a new one called 200 wells out. And that writes that to the file. And then I'm just using the Linux head, the Unix head command actually show you that this did create that file. And this is what this is what the first five lines of it look like. So, again, labels, common separate rated labels in the first line. And then just a bunch of numbers below it. I can't remember. I don't think I mentioned when I talked about read CSV. There's also a function called read Excel. So you can actually read in Excel notebook directly. You don't have to convert it to a CSV file. And there's a bunch of options for those that allow you to to, you know, if you want only one of the, you know, if it has data and multiple sheets, you can extract the sheets, the columns, you know, there's, there's probably to both read CSV and read Excel. There's probably 50 keyword options. You know, a lot of them have the default values. I mean, all, essentially, all but one of them would have default values. However, you have a lot of control over how it parses that data. And again, that's why I say that pandas is really good at reading in data from files from text file.
 Okay, principal components. Who here has done principal component analysis before? Now, I don't go into a lot of details on PCA. There's a lot we could talk about eigenvectors, eigenmath and so forth. We'll just keep it very simple here, high level kind of description. We're going to project the data down to a p good lower dimensional space, p number of projected features. Now, the available number of p, funny enough, could be either m, the number of original features. In other words, if I project to a lower dimensional space, I do have the choice to go anywhere between one and m dimensions. In other words, I could project and then choose to use all of the possible dimensions. And I'd be still at the same dimensionality of the original problem. Or I'm limited by the minimum of n minus the number of data. Now, this is really simple. What it means is I can't describe a two dimensional space. I mean, I can't describe a three dimensional space if I only have two data. I need to have in fact three data to describe that space. I to describe a line. I need, oh, sorry, I got that wrong to describe a line. I need two data to describe a plane. I need three data describe the other dimension. I need one plus. In other words, I can't describe a dimensionality space greater than n minus the number of data I have. Now, if you're running into this constraint, I would say stop and rethink what you're doing at work that day. Because if you're trying to model things in 3D and you don't even have four data or something like this is just scary time. Okay, so I would never, I would just always try to have much more data than that. Don't run into that threshold. Components are ordered. The first principal component is going to describe the largest possible variance. We're going to get a projection or a direction that describes the most amount of variability possible. The second component describes the largest possible remaining part of the variance or orthogonal to the first component. And you'll carry on, on, on. The last principal component is fully determined because of the orthogonality constraint. Okay, so just to show you visually what that looks like, if I have data like this, why is a feature x as a feature? If I was to go ahead and calculate the eigenvectors and eigenvalues, the first eigenvector would go like this. In fact, the first eigenvector would be the direction. If I was to project the data onto that, I would have the most amount of variability along that projection. Okay, so in other words, you could also think of it as loosely like a best fit line. Okay, because you wouldn't in fact be minimizing the error or orthogonal to that line. So it's like a best fit line. The second eigenvector is orthogonal to the first eigenvector. In a two-dimensional problem, you calculate the first and you're done. The second one is just constrained by orthogonality. And the eigenvalues will be the amount of variance described in each one of these directions. Again, you can see here, most of the variabilities on this direction, very little variability on this direction. Now, let me just draw it a little bit better for you. I take from the hasty and all book right here. And what you can see is if I do principal components analysis on this two-dimensional data set, the first principal component, if I was to go ahead and calculate and project the data onto it, each one of the data projected are known as principal components scores for the first principal component. So this data point projects here, this data point projects here. And this line right here is the first principal component. Okay, now the amount of variability is the amount of variance you would observe of this predicted data along that line. Then the second principal component and the first principal component together would describe the most amount of variance you could with those first two dimensions. So in this problem, first principal component, second principal component. And so the result is a plane. That's the best fit plane. It would minimize the amount of error orthogonal to the plane. The most amount of variability would be described in the plane. Okay, so this is the idea of principal components. Now here's a nice example right here that explains the variance components. If I have this data set right here, feature one is y, feature two is x. If I look at the original data and I just projected on y and I projected on x, this is what you would get. The projection of the data on x would have a variance of one. So all I did was take the data and collapse them onto x. You see that? And that's how we got this right here. If I take all the data and collapse it the other direction onto y, this is what we get right here. And it also has a variance of one. Clearly, we've standardized the variables to have a variance of one each of them individually. But then if I calculate the u principal component, I project the data onto this line just like this. I would get this and the amount of variance is 1.78, almost 1.8. In other words, 90% of the variability could be described with a one-dimensional model. If I projected on you, I could throw v away and this would be my data set describing 90% of its variance. And what we'd say is 90% of the structure is now captured by that model. We have to accept that variance as the measure of structure. Okay, and the second principal component of the orthogonal to it, we would project the data. So if I project the data like this, like this, like this, I project it all, I would get this scatter of data. The variance on that projection is 0.2. So 10% of the variance is described. You see the conservation of variance? The total variance of the problem was 1 and 1, 2. And now we've distributed more of the variance. And if you look at this, all we did was a rotation. Principal components is a multi-dimensional rotation. That's all it is. Any questions about this? Principal components? The dimensionality reduction happens when you throw v out. You throw v out. You say, now I don't need you v. And now you only work with you. Now you have a one-dimensional problem. There's always, there's no free lunch, though. We lost 10% of the variability that we won't be describing right now. Okay, so when we get dimensionality reduction, we're going to just simply throw one of them and we're going to represent everything by a reduced in this case. We had M equals 2 originally, but we're going to move forward with P equals 1, 1 dimension. That's our dimensionality reduction. We lost 10% of the system variance. The assumptions, we're going to make a bunch of assumptions every machine we make an assumption, large enough sample set for reliable correlation calculations. This eigenvalue eigenvector calculation, as I mentioned earlier today, I'm talking about outliers, is based on a covariance matrix or a correlation coefficient matrix. It's fine if it's standardized. And then you have to do the eigenvector eigenvalue calculation on that matrix. Well, you don't get reliable correlation coefficients if you don't have enough data. So you want the number of data to be at least equal to five times the number of dimensions. I would suggest you probably need more than that, but you need to lease that. And in total, like for any case, you never want to do PCA if you got less than 150. These are rules of thumb that people use to determine. There has to be correlation between the features. If there's zero correlation, what would it look like? What would a zero correlation look like as far as this data set right here? And you imagine if it was like that, you can't rotate the system and describe more of the variability or less of the variability. It's all going to be the same in every direction. Isotropic, really. Yeah. Their outliers must be addressed. The covariance is a correlation coefficients are super sensitive to outliers. This method really falls down if you have outliers. Your create synthetic structure, artificial structure. Okay. Non-linear patterns. Look at this. Woo, you cannot do PCA with that. That would be you rotate it. You're going to describe the same amount of variance for every direction. Non-orthogonal patterns. That's interesting. This data set does have two important structures to describe the majority of variability. But they're not orthogonal to each other. And the orthogonality strength is required with PCA. You have to, you have, so really you can't deal with this situation. Obscure patterns. Oh my gosh. This is crazy. If you tried to do a projection with this data set right here, it's just going to, you're going to get weird compromise. It's not going to make any sense. It's really not a good idea. So you don't want to work with any of these types of patterns in your data. Graphically representing what it looks like. You have the original data or the white circles, the white filled circles with the black outlines. You do the first principal component. You do the projection of the data. The data on the principal component are now called the first principal component score. It's like a rotated coordinate. Okay. Then that is what we're going to move forward with. If we do dimensionality reduction, we now have the purple data. Now, anybody wondering about how is this different than linear regression? With linear regression, remember, we were in the business and let me show, let me show this example right here. With linear regression, we were in the business of minimizing the squared error with respect to the response feature. In other words, this direction right here. Now, what's very interesting about linear regression and people don't realize this is that the model is different if you regress porosity on density versus if you regress density on porosity. It matters which direction you go. And the way to think about is this. Minimizing the error in this direction is different than minimizing the error in this direction. You see that? Now, it's really cool about PCA. It minimizes the error orthogonal to the line or the plane. And think about a hyperplane if you're got more than one dimension being considered. So hyperplane. So it minimizes this. What does that mean? It means principal components don't matter the order. There in fact is just one answer. You see that? It is like thinking about a best fit line to this data set, but there's no choice of going from here or her first. Okay. So in a way, I like principal components for that too. I think that's pretty cool too. Okay. Principal components. Limitations. Now, the general workflow that you're going to deploy goes like this. This access right here is the number of features that you're working with. You have originally M features. The original data is big. It has a lot of features to deal with. Okay. Variety is big. Then what you do is you go ahead and you're going to do principal components analysis and you're going to throw out the M minus P features. You're going to say, I don't need those features. I get rid of those features. Now what you can do, the very first thing you could do is you could look at the component loadings. Remember, the principal components is in fact a linear weighting of the original features. It's a rotation. And so it in fact, you can look at the numbers that were used to get the rotation. And it'll tell you about the contribution of each feature to each one of the principal components. Principal component one, maybe it was prostate and permeability. Principal component two, maybe it was completion information. That often happens. And you have neat groupings to look at. The other thing is you can look at the principal component scores. The principal component scores are simply looking at the data in the low dimensional space, the purple circles. Those are principal component scores right there. Then you can do your statistical modeling and inference on principal component scores in the reduced dimensional P space. And then at the very end, you always come back home. You always got a back transform. The cool thing about principal components is that you were able to get this original projection into the principal component score. You were able to do this rotation. You can do the back transform. And the mathematics are so easy. It turns out the back transform is exactly the same as the forward transform in matrix map. It's really, really cool. So we can go ahead and we can just do the forward and back transformation very easily. And we can go back and forth. Okay. So we'll go ahead. We come back to the original space. So we built the statistical model and the low dimensional space. Then we use it in the original feature space. We have to do that. Okay. Let me show you a quick example. We'll get fingers the keys. We need to do some coding principal component analysis. We're going to have a bunch of data. You know what? Why don't we just go ahead and go straight into our code right now?
 Instead of trying to just write this in the function, I'm gonna kind of do this one line at a time. I don't know why I'm struggling with my screen real estate today and having trouble keeping your heads when in a place where I can do everything else I want to do and still see you. So, you know, I appreciate any kind of head nodding or responses because sometimes I'm only looking at one of you and I feel like I'm talking to no one. Anyway, okay, so what I'm gonna do is I'm opening a new cell and eventually we'll put all this in the function for testing but what I'm gonna do right now is just write one line at a time. And this is to kind of demonstrate the issue. And if, as I mentioned in the project description or problem description, we have a data source that is wall location by ticker and state. And we can look at that by putting in EOG and New Mexico, I'm sorry, North Dakota, and calling read on it. And that's gonna read in the data frame. And you'll see it, you know, it takes a second. If you wanna look at the data frame, you can then I assign it to a variable that I called EOG APIs in North Dakota. So this is a data frame and if we look at it, you can see what it looks like. So basically just has three columns, API, latitude and longitude of the service hole. And this comes from a SQL query into our database. So these are all of EOG's wells in North Dakota. So we have all of the API numbers here. Of course we have this other information but it's not really relevant to this problem. But we don't have in this is the production, right? So we have to go to another data source to get the production data. So I'm gonna call that North Dakota production and that is stored in production by state. And so we just call this with state equals North Dakota. Now this might take a minute because this is literally pulling in. I said it isn't take a minute, it didn't take very long at all. This is literally pulling in. I didn't call read on it. Sorry, if you call read, it'll take a second. Because it's pulling in the data for every single well, the production data for every single well in North Dakota for all of history, right? And so this is 1.29 million rows, which all of those correspond to a month of production for a well. Now in this we do it, we have API numbers. But the problem is we don't know which of those API numbers belong to EOG. That's all the wells for all operators. And so what we need to do is pull out only the wells that are owned by EOG. And the way we can do that is there's a pandas function called is in. And what that allows us to do is, in fact, we'll only do this for API. So we'll pull off the API column, right? So remember, if I just do that, it's just given us the API numbers from the data frame. And then I'm going to cross reference to that with this is in command that's going to say is in EOG APIs in North Dakota. And so what that results in is this Boolean array, true and false. Now all we see there, because it only prints out the first five in the last five, evidently those don't belong to EOG. And so all we see is false is, however, there are true in there for sure. And if you remember, we have these Boolean arrays, we can do something called fancy indexing, right? Which basically we can index into an array and it will return only the values where we have true. Okay. So I can use that to index back into sorry. I can use that to index back into the North Dakota production data frame. The way I do that is just with the square brackets, right? So remember, even though it's kind of long, everything inside those square brackets just returns this Boolean array of true and false. And then we're going to use that fancy indexing to get the truth, right? And if we do that, we get that. And now you see there's, you know, now we've reduced it where there was 1.29 million rows. Now there's 61,000 rows. So these, and you see that the indexes, you know, they don't even start until 208,000. Right. So the first 208,688 belong to somebody else. Right. So these wells belong to EOG. Now they are kind of, it's kind of hard to read this and we're not going to leave this. But if you'd like to kind of group this a little bit easier, one thing you could do is you could call set index on this. And set the, you can use something called a multi index. And so if we set the index, and I might have to, I don't know if I should do API or date first, but let's see how it looks. Yeah, I think this is, this is the way I wanted it. So now, now this is actually pretty readable, right? So I have for this API, these months of production, and those are the values, right? And then it goes on the next API and it's months of production. And we're not going to leave it like this, but just for visualization purposes, I wanted to show you that you could call that set index on the data frame to get this kind of result. Okay. So I'm going to remove that part of it now. So this is. These are E O G's wells in North Dakota, right? What I, what I then want to do is. I want to group those wells. By API. Whenever you use group by you, you always follow it with a function. Remember, from my notes, there's this thing called apply, which allows us to apply a generic function to any. Call them of the data frame. In this case, we don't really need a generic function. We just need the song, right? But I think, let's see, the question asked. Did I not say oil, the five highest producing wells. Okay, well, I should, I should have said oil, right? But I want, because I want the five highest, we're talking about, oh, in this case, I want the oil, not the, not the gas or water, right? So what I'm going to do is I'm going to. Use this apply basically anytime you use group by. It has to be followed with some function, right? In this case, the function is a function I'm going to create and apply it to only the oil column of the data frame. And so in this case, I'm going to write a lambda function. So lambda f or f takes the volume of oil, formation barrels, right? So this label. And sums them up. I just have a typo north, North Dakota production. Okay. So what that did was it grouped. So up to here, we had all of EOG wells in North Dakota. We grouped them by API and then we summed up the oil columns. And that gives us the cumulative oil per, per API, right? So then we can sort. Now we have them in order from highest to lowest. And I think I asked for the first five. So I could just print out the head. That's the first five. And that's the, that's the answer. John, I think what you do a really good job, especially in that one line of code showcasing is that sometimes we look at like a line of code like that. And we just get really intimidated. But I think what, let's key in what you did is just breaking it up into, you know, individually doing each part of what task you want to complete. And in fact, as I, what I'm going to do is, I'm going to copy these lines back into the function. And then when I do that, I'm going to sort of reformat this long one. You'll see what I'm going to do. So I'll call this. Highest production. Sorted. And I'm going to paste that long line of code. But one thing nice about Python is that I can, if I just. Surround a statement in parentheses. So I'm going to surround the whole code, the whole line in parentheses. As long as I group it by in parentheses like that, I can break it up into multiple lines. So now I can actually put a carriage return anywhere in here. And I can break it up into multiple lines. And if I do it like this. It's a lot easier to read. And then I just want to return this. So then, you know, just to verify that we can add that this works all in one step. So when you run it all together, it takes a minute, right? Because it's doing those SQL queries. To pull in those two separate data frames and then doing all the merging and sorting. Right. So again, when you break it up into multiple lines, it's a little bit easier to read, right? So the first line gives us all of E O G's wells in North Dakota by comparing those two data frames. The second line groups them by API and then applies a function over the volume oil formation barrels column and sums, sums it up. So this function basically just grabs one column and sums it up. And then, so that gives me the up to that point, I have the total production for every well by API. So then I just sort them highest to lowest and print out the first five. I would love to see someone do that in Excel. I mean, this data frame in and of itself is 1.29 million rows. So you're going to have trouble just with that. Any questions?
 Now, you know me, I like to give you an educational product. So I put some of this, you know, fancy markdown text. So we have a nice little explanation. You can embed a little math in there too. That's pretty cool. If I run that again, you'll see that I actually made some mathematical relationships and so forth explaining principal components analysis. Okay, let's go ahead. When we do principal component analysis, we're going to start working with Scikit Learn. This is the standard library. We're going to use it all the way up to neural nets. And it does, well, we'll use TensorFlow when we get to neural nets. Scikit Learn does a lot of machine learning decomposition import PCA. That gives us principal components analysis. We're also use the standard scalar. We have to do standardization of our features when we do principal component analysis. Principal component analysis trying to describe the components of variance. If you change porosity from percentage to fraction, you change the variance by four orders of magnitude. Variance will go up to the square of the change. If you do divide by 100, that's a change of 10,000 invariants. That would make the porosity become dominant or weak when you consider variance analysis like this. We got standardized everything that we variance one and mean of zero. Okay, let's import that. Too wordy. Okay, let's go ahead. We'll count this right here as a function I wrote in order to create a correlation matrix. Okay, let's just run that. Now plot core is going to be available to us in the workflow later on. Okay, let's go ahead and Dr. Foster set up our feature engineering. We're able to load in the feature here from the cloud and we have an unconventional multivariate data set. You run that. Now if you're new to Python and you just ran that and you're kind of feeling like I don't know what happens, no news is good news. Remember that. There's no errors, that loaded. If you ever doubt, you can go ahead and just visualize it. In fact, a good way to visualize is just take a slice of it and view it. This slice right here was simply saying, give me the very first seven samples index from zero to six zero to six samples and show them to me. Okay, so the samples are well indexed, prostate, log perm, aha, I did a log perm transform there to be careful about home scanasticity there if I try to do linear regression, right? Acoustic impedance, bird on this TLC, vitro night reflectance and production in MCF PDs. Okay, gas production. Okay, now what we can do next is we can do summary statistics. This is something I really love about Python working in Python, although the statistics calculations are really easy to do. In fact, Python pandas, which is how we build our data frames, has built in describe function. Now give us a bunch of summary statistics, one stop shop. So we can see porosity through production, counts means the count is interesting. It will remove null values. So you can look through the counts. If I see 900, 1900, 800 something, I know which features are missing more values. I can see that it's accounting for that. It knows about missing values. Okay, let's go ahead. I have some negative bird on this here. I gave myself some negative values, some anomalous outliers just to kind of demonstrate an outlier treatment. Carefully, you can do a truncation. This is only when you know, and it can explain it as being a tool error, and you know that those negative values are erroneous, you could set it as zero if you think that's rational. At any moment, you can take a data frame and extract the two-dimensional array of all of the data values and then apply num pies, functionality for being able to do really great things with data frames. I mean, sorry, with two dimensional arrays or three-dimensional arrays, they call them NDE arrays, multiple-dimensional arrays. And so we can do this operation right here. It filters through and finds every case with a negative value, and it sets it to be equal to zero. Now, Dr. Foster will spend time next week explaining to you all how to work with num pie and NDE arrays, and pandas, and data frames. I'm just showing you some basics. My focus is doing machine learning. Next week, he'll be more focused on teaching you how to do like good data science and be able to work with your data more. Okay, let's go ahead and run that. We've now removed all of the negative values. You can see right here minimum values of zero, zero. We had negative values before. Okay, so we have our features to work with. We want to do dimensionality reduction my PCA. What was the very first assumption? I should have reset the kernel, but I didn't. Okay, the very first assumption is that there's correlation between the features. So let's do a correlation matrix. If you run that, it produces this correlation matrix right here. And if you look through the data very carefully, you'll see in most cases, there's a pretty good degree of correlation. Now, there's some cases right here where it's kind of very close to zero. Now, that right there, I believe that would be vitranite reflectance versus perosity, because that's the second last feature compared to the first feature. Perosity with vitranite reflectance have a very poor relationship. It's very close to zero. But in general, we seem to have pretty good relationships. I like visualizing the actual correlation coefficients as colors. Oh, cancel. Oh, I did that back. Sorry. So what we can do is we can run this plot and it gives us by color the correlation coefficient zero is going to be this teal color dark purple is going to be negative one. Yellow is positive one. Okay, how do I know that this is a correlation coefficient matrix? What gives me what gives me confidence? That's what I'm looking at. Okay, the other thing too is you notice that range from negative one to positive one. And the other aspect is look, it's symmetric. Coralation coefficient between acoustic impedance and log permeability is the same as the correlation coefficient between log permeability and acoustic impedance. It's symmetric. So you see, it's a mirror image over the diagonal. Okay, good. Coralations. I like looking at matrix scatter plots. I'll tell you what I have workflows where we do multi-variant analysis. There are so many cool matrix scatter plots that you can produce using seaborne. This one right here, I believe this one came directly from Matplot Live. Matplot Live right here, but you can do really cool seaborne plots where what you get is oh, that's actually from pandas. Pandas creates them too. Pandas plots. Okay, what's really cool is seaborne will do things with like kernel density estimators and all kinds of combination plots that are really, really neat. If you wanted to display multi-variant data, it can be very powerful. Okay, so why did we produce this plot before I did principle component analysis? What am I looking for? Principle component analysis is really going to be trying to do a rotation and project onto the rotation and it's projecting onto a line, a hyperplane, and so forth. I'm really going to be concerned if I see strongly non-linear features. That could be an issue. If that's the case, we may consider doing a non-linear version of PCA. Right here, this feature right here, this feature right here, there's some non-linear features in here that we might be concerned about as far as doing PCA, but we'll just take note of them and we'll move forward right now. If we wanted to, we might try out something more complicated. Let's go ahead and we want to pick, we're going to start with two features so we can simply view everything. At any moment, I can take my data frame, I can extract out certain data. I'm going to extract out the first 100 samples of feature one, feature two, porosity, log permability. Okay, I'm going to plot them versus each other and I can look at the histograms and see how they're behaving. Another thing I want to do is I want to produce a scatter plot between the two, porosity, log permability right here. Okay, the next thing I can do is I can now go ahead and run a standardization on the features. Porosity ranges between 10 and 20, log permability between 0.5 and 2.5. The degree of variability will be dominated by porosity. The way to think about this is it's like I'm trying to find the best fit line, but imagine if I plot at this with the axis being equal to each other. This log permability only goes between about two units. While this goes through about 10 units, the shape of the data would be rectangular like that. Okay, in that case, you would see the model would just think that porosity describes the most variability. Okay, we'll do a standardization. We'll put everything on equal footing. We do the standardization. It's complete now and we can go ahead and plot the result. Zero mean variance of one ranging from about negative two points something to positive two points something. Both of them have the same spread, the same variance now. We can repeat the scatter plot and look at that negative two, the positive two, negative two, to positive two or so. They're about we got the same level of variability between the two features. We're ready for principle component analysis. Now, here's the power of Python and using scikit learn. All I have to do is I import PCA. It was part of that decomposition module within scikit learn. I can do principle component analysis, the number of components equal to two and I can tell it to fit the data. The data was the porosity and permeability log permability data. If I run that command, I can go ahead right here run it. I've already done I've built my PCA model. I now have a principle component model. Now, anybody noticed the fact that I can actually make my markdown look like code. If you ever want to do that, if you want to explain them the documentation, what you're doing in the code, you want to make it look like it's code, you use this command right here and you can actually make it look like Python code. When you run it, it has that look so it's really good for documentation to explain this is what I'm going to do with the code. Okay, so we ran that and what we get is we can actually access the principle components and say give us the components. I want to actually see the components. Now, this part right here, I'm just rounding the answer. I don't want to see it to the eighth or ninth or tenth decimal place. I want just the first three decimal places. That's the round command right there. NumPyNP is able to round an entire array of output. So it rounds it all at the same time. Okay, so I get .707.707 and so forth. This is the first principle component right here, second principle component and this right here would be porosity. This would be permeability. To convert porosity into the first into the to get the first principle component, we take .7 times porosity plus .7 times permeability and that's how we get the first principle component. This is what we do to get the second principle component to porosity and permeability. If you look carefully and if you're knowledgeable about matrix math, you'll recognize that that's an orthogonal matrix. It's a rotation matrix and it does not cause distortion in the data. Okay, now if we were doing the more complicated example, we would look at the principle component scores and we'd make some observations. I'll show an example of that later. Okay, let's go ahead. We can run and see what's the variance explained ratio from each one of the principal components. And here it is. The first principle component describes 90% of the variance, about 8.9. The second principle component describes the remaining 10%. By design, they will sum to one. Okay, so now what we can do is we can plot and see what we actually did. This is the standardized log permeability standardized porosity. This is principal component 2, principal component 1, right here. And here's returned the reverse PCA. We back transformed. Now, I want to apologize for something. There's something I should change here. You notice the axis here goes between about point almost, almost negative 3 to positive 3. And this axis goes between negative 1 to 1. If I was to make it so the axis had the same extent, it would become apparent that this data set is aligned in the first principal component. You would actually see that this is in fact a rotation of that. And I apologize. It's kind of spread out because of the way I plotted it. But it's this rotated. So the first principal component actually just is in this direction right here. And then we can back transform it. So we can do our math here. We could collapse all of our data to the first principal component. Do our math here. Let's demonstrate how we do that. Let's go ahead and run this again. But we're going to in fact do dimensionality reduction. This is the original data. This is the standardized log permilities standardized porosity right here. This is the principal component score 1 versus 2. We're in that transform space. This is principal component number 2 set to 0. That means that we've taken principal component scoring number 2 and we threw it away. We said we'll set the principal component scores to 0. We're not using principal component number 2 any longer. This is the reverse transformation now describing our data set with one feature. And this is de standardized. So we're back in the original units of porosity units and log permability transform here. So we now have a model in original space which you can turn one dial principal component score 1. And we can scan through all of the data with one dimension. And we describe 90% of the variability. Okay, any questions about that. So let's go ahead. What we'll do now is you don't ever want to look at all those steps. I did that in order to just demonstrate really what you'd want to look at as original data and the transformed from the standpoint of reduced dimensionality data. So the original data and the reduced dimensionality data. This is where this is what we can fit in the lower dimensional space from that original data. Okay, so let's you know that was interesting but I think it was a toy problem just to demonstrate. Let's go ahead and let's go back to six features. What I'm doing here is I'm saying okay, let's extract from the original data set the first 500 samples and let's see one through six features. So we run that it slices it's known as slicing a data frame. We get a data from slice. And we're going to go ahead and describe the summary statistics of that and we do that. We get these statistics right here. Prossey through vitro night reflectance. Now we're doing some real dimensionality reduction. We got a pretty big dimensional problem in fact. We already did these plots where we checked the correlation coefficients between them all. We know they all have pretty good correlations. There's a couple cases here where it's a little bit of lower correlations. Okay, now let's go ahead and we'll do the standardization. We have to standardize. Every time you do PCA, please standardize because it really makes everything so sensitive to the units and it shouldn't be sensitive to the units. Okay, so we did standardization. And so we check we make sure that the mean of the transform and the variance of the transforms are all zero one. We can we can check that. Okay, then what we do is we're going to produce histograms always a good idea. If you're going to do multivariate analysis like PCA first do univariate analysis. Check the distribution. See that they all make sense. Don't have outliers and so forth. Okay, let's go ahead and principal components analysis. Scikit learn. You got to love it. You just do PCA number of components six components feed in the data. A way you go. Now let's go ahead that one line of code and here we go. This is now the component scores the component loadings I should say first principal component, second principal component, third, fourth, sixth. Okay, this is the porosity log perm all the way up to the very last one which was Vittor Knight reflectance. And so these are the weights that we're going to use to get the first principal components. So the first principal component that describes the most amount of the variability is dominated by porosity and the negative of TOC. Isn't that cool? So the majority of the variability of the system can be described by the combination positive porosity negative of TOC. The second principal component that's orthogonal to the first and describes the most amount of remaining variance is dominated by brittleness and a bit of TOC and porosity. You see so you can step through this and interpret and you'll see groupings. If you do that you're now one of those finance people. That's what they love doing this stuff. They do it a lot. Okay, so we can apply those principal component loadings in order to apply our rotation. This is an orthogonal matrix. It'll rotate our data. Okay, so let's go ahead. Let's go ahead and calculate what's the variance explained. Okay, so first principal component describes 46% of the variance. A sixth dimensional problem you collapse the principal component scores number two through six to zero you remove them and you would still describe with a line 50% of the variability of this sixth dimensional problem. Isn't that cool? So one dimensional model would still do 50% of the variance. The second principal component describes another 25% or so. So we've got pretty darn close to about 70 something getting closer to 75% variability can be described to about 72% can be described by using the first two principal components add the next one in now we're dealing with like 85% and you know so forth and so on. We could remove the last principal component only and all we would miss would be about 1% of the variability and the last two and all we'd be missing would be on the order of about 3.4% 3.3% of the problem. Okay, so let's go ahead and visualize this. This right here is the relationship between porosity and log permeability originally. This is the relationship if you only use one principal component to describe the entire system. This is after two three four five and six six is the original. If you retain six principal components and you originally had six dimensions or six features what if you gained? All you've done is put your features into a non physical combination of the other features if you retain the same number features you haven't gotten any feature any type of dimensionality reduction. So really it wouldn't be a good idea to stay with the original numbers. Okay, so and what here we do here is we just calculate the variance that's observed in porosity and permeability with the first principal component so forth. So we can look at the two variable relationships given a different number of principal components used to describe the six variable system. Okay, now what'd be interesting too now is why don't we go ahead and we can run all of the different cases of possible numbers of features retained and let's do scatter plots of that and that's really kind of cool because now we'll do the matrix scatter plots of what what it would look like if you retained all the one principal component two principal components and so forth. So we'll calculate all of the possible cases of retaining different numbers of principal components and we'll go ahead and produce the matrix scatter plots. Now is any, okay good they're working right now. This is the original data set right here. This is the matrix scatter plots between all of the features in each other. This is the relationship between all of the features in each other, the matrix scatter plot when you retain one principal component. Does this make sense to everybody? With only one principal component you can only describe linear relationships. That's the main point here. If you've only got one dial, one axis you can only, everything is one is only going to be a line. That's all we can determine with one feature. Two principal components, look at this. This is fascinating. What happens now is we're starting to describe this whole thing in a two-dimensional space and look at how much of the variability of different feature combinations we're already starting to capture. This is really, really fascinating. In fact, is we're starting to see a lot of different relationships between things. We're starting to see things pretty well. Okay, three principal components. We're starting to see even more four, five, and six. Six principal components this should look exactly like the original data. Any questions about this use of principal components? You could go back to the, now first and foremost, the main methodology, the standard methodology would be you do the projection, you get the calculation rate here of 50% variability in the first principal component. Then you decide, I can live with 50%, and you remove everything else. You say I need 70%, you keep the first two. That's the standard methodology. Now at the same time, when you investigate the first principal component and you see that it's dominated by prostate and TOC, clearly if you retain only the first principal component, you have a model that's dominated by prostate and TOC. See that? It has to some degree brittleness is also coming into play here, but really the rest are all very small. So you could think about it like that. Now the banking people, the finance people, will kind of see groupings and they'll say, well, this amount of variability is related to this feature. Now what's really cool is all the principal components are independent of each other. So what first principal component sees is independent was second principal components. So if one of them is seeing things related to completions, another one may see things related to reservoir rock. And if they are truly independent in nature, there's not a degree of dependency, you'll pick that up, you'll see it here.
 That pot live is the kind of default plotting library for Python. It's one of the oldest. There's a lot more modern plotting libraries and we'll discuss at least one of those and I'll mention a few of the other ones at the end of this as well. But some people complain that matplot lives a little bit verbose, meaning it takes a lot of commands to style the plot in a way that you would normally think would be a nice, presentable plot. And there is some truth to that for sure, but the other side of that is I believe that any two-dimensional visualization you've ever seen can be visualized on that pot live. In other words, it has all of the features. It's the most complete kind of plotting library and you have the ability to fine tune every detail of the plot to make it look exactly like you want. And this is not seen that picture before. You might know what that is. This is the first photo if you will. Photo is actually the result of analysis, but it's the first image of a black hole, right? Which was highly publicized last year by the event Horizon Telescope collaboration. And this plot, of course, the reason I'm showing it to you within the matplot live discussion is because it was created in matplot live. So they used matplot live to do the visualization for that. So we're first talk about 2D plots. So we're just talking about normal kind of scatter plots, bar plots, line plots, the things we normally sort of think to make in 2D dimensions. And every matplot live plot contains 2 things. When you instantiate the figure, when you sort of set out to create the figure, it will return one or more axes and the figure, right? And I know a lot of times this is probably what's confusing to a lot of people. But you can think of the axes as kind of the individual lines on the plot, you know, the data, the end, you know, the things that however you're visualizing the individual data set. So it could be lines, points, you know, whatever marker you choose. But this is the data, the individual data that's plotted on a plot. We call those axes in the terminology of matplot live. And then the figure is kind of the final image that could contain one or more axes. So the figure is, you know, what controls the overall canvas, the layout, the size of the image you're going to see, the title, if you had multiple sort of subplots on a figure, we'll see that later. And, you know, all of that would be controlled by commands to the figure object. Okay. So here's kind of a reference that comes from the matplot live frequently asked questions that talks about the kind of anatomy of a figure and give some different labels to, you know, major and minor tick axes and other things like that. Okay. So in this case, in my example, you know, the blue line would be one axis. The red line would be one axis. The data set, you know, the dots out there would be another axis according to that kind of terminology. I feel like my, I feel like I got out of order. Okay. So here's a basic example where I'm just plotting some numbers. And in this case, I, I almost always use this subplots to instantiate a plot. So that returns fig and axes. And then in the event that, you know, you don't give the subplots any arguments, it just returns sort of one subplot or figure. But it's, it's nice to use this because later if you want, we'll see, but when you have multiple plots, then you just give the subplots some arguments. So it kind of just one thing to remember, even if you're going to create only one plot, you instantiate it with this subplots command, it'll return a figure and an axis. And the axis is where you would set some value. So in this case, I set the x label to some numbers. And then I'd plot just 0123. And then all the default values, the default gives you a blue straight line. But we can add some additional styling to the plot. So for example, in this case, I just add a grid by just saying axes dot grid. So not that interesting, but you can add some additional styling in this way. You can also use numpy. So map all I was fully compatible with numpy and numpy data structure. So before I just put in a Python list of numbers, 1, 2, 3, 4. But here, now I have a numpy array. So I create a numpy array that goes from 0 to 5 in increments of 0.2. And then I plot, so that's just a straight, you know, that's just my basically x data. And then I plot t versus t, which gives me the straight line right here. In this case, I'm also controlling the color. So I specifically say the color is blue. And then I gave it, give it a label as well, linear. Again, for I create another axis, right? So axis plot, t, t squared, k, that k means black. So b is already taken for blue. You can also write out the whole word if you want it. You can say blue, you can say black, that would work. k is the shorthand for black. So that produces this line. And I give it a label. And likewise, and I take the cubic. It also accepts lettac styling. So in this case, I set the x label. And I put it inside a dollar sign. So if you remember, lettac from our discussion of Jupyter notebooks, how we can typeset mathematical fonts within Jupyter notebooks, we can do the same thing with our styling and the labels. And one really nice thing about this is if you're intending to put these figures into papers, particularly papers that you write with a text, then there's a way to basically put it in there in such a way that when the document is typeset, that the fonts in the figure will match the fonts in the text exactly. Like there will be identical fonts. So that's particularly useful if you have like mathematical equations or something that you're trying to use for labels or something in your plot, and you wanted to match what's in your text. There's a straight forward way to do that in MATPLOT LIVE. And in my, in that I know there's not another plotting library that allows you to do that in such a straightforward way, for sure. So in this case, then I go ahead and add a grid and a legend. So the point here is that, you know, MATPLOT LIVE plots are fully compatible with numpy data structures. So there are quite a few built-in styles. So, you know, I mentioned earlier that one complaint about MATPLOT LIVE is it takes a lot of, a lot of commands to kind of style the plot in a way that you might like. There's a lot of built-in styles. So if any of you ever read the website, 538, Nate Silver's website, Nate Silver is a, you know, kind of got famous for, he's a pollster and kind of got famous for predicting the outcome of several elections. And he has a website called 538 and they do a bunch of statistical analysis, typically with respect to politics and sports. And if you ever go to that website, he has like a very particular style. All his figures always have a certain style to them, particularly this gray background grid lines and certain line weight and other things. And so you can actually use, there's many styles built-in and you can just use them out of the box if you like these kind of styles. If you like the way the plots look. So you can say, you know, MATPLOT LIVE style used 538 and you'll also automatically get these plots that look like Nate Silver plots. Of course, my favorite style is XKCD. So I don't know if you're familiar with the webcomic XKCD. But he has these hand drawn comics and he's real popular in coding circles because he has a lot of his comics, reference, you know, nerdy jokes. You know, like that only sort of coders or scientists and engineers would get. And funny enough, these developers MATPLOT LIVE actually, the real guy's name is Randall Monroe. He had, they had Randall write a sampling of his handwriting because the XKCD comics are actually Randall's handwriting. And so the MATPLOT LIVE developers actually had Randall write some, you know, sample of handwriting and then they used that to create a font that matched it exactly. So this is exactly, you know, matching Randall's handwriting. And when I first saw this, I thought it was a gimmick or joke. It was like, but then I later have actually found it useful, particularly when trying to just draw a curve that's representative of a trend. Perhaps you don't, you don't want to miss anyone to mistake your figure for something for true data. Well, you can use this kind of comic styling to just illustrate that, you know, well, we expect the trend to be in some, you know, in some fashion. But then you can actually parameterize it with real mathematics. And then you get this nice kind of comic styling on. But I'm just amused by it. That's why I showed here. Here's all the built-in styles. Again, you have to just try them out to see how they look, find, you know, which one that you like the most. And there are other ones that you can install from third party folks or whatever. And of course, you could even create your own style. If you have, you know, a certain group of settings that create plots in a very particular way, well, you just save that as your own, your own style. And then you don't have to use all those commands again as you go forward. Another way to kind of get around some of the verbosity that it takes to create map plot live plots is to use it with pandas. And so we already be saw in the pandas lecture that any data frame, you can just call plot on it and it'll do something, right? And so in this case, this is that data set we looked at in the pandas lecture, the 200 wells data set that has the porosity permeability, coos, the pédons, facies, that kind of thing. And there, anyway, we can of course, just like in this example, we're using pandas directly. But under the hood, map plot lot use, I mean, pandas uses map plot lines. So there's a lot of default settings that kind of, you know, quickly get you a plot that might seem reasonable, but you can also have full control over it by instantiating a map plot live plot. So if you notice the first line is just like I have in the other ones, but then what I do is when I call data frame plot, I pass the axes in, right? So when I instantiate a map plot live plot, I pass the axes into the pandas data frame. Pandas will then plot the data on that plot in a scatter plot. But then I can go ahead and add some additional details. So in this case, I set the x label and the y label. And again, like I meant, showed you before, I'm using this latex font, right? Latex styling, right? So, so that I can have these symbols fee and capa as my labels. So, and then I just call it grid on it as well. So this is kind of the best approach in my opinion, especially if you already have your data in a data frame, which a lot of times we do, particularly if you recall, I mentioned that pandas has some great ability to parse text files. So to pull data in from a CSV file or even to read and excel spreadsheet, pandas is really the best tool for that. So your data is already in a data frame. Just do this, right? Just create a plot, pass the axes into the data frame plot command, and then you have full control over it by just modifying that the access commands. Here's just another example of creating a histogram, again, doing some more thing, right? So, in this case, I create a true subplot, right? So I pass in two arguments. I say I want one row and two columns. And when I do that, axes is actually a list of the axes for the plot, right? So later, you'll see down here, I'm actually pulling, you know, applying some settings to the zero, and you know, this is the first entry in the axis list, and one is the second entry in the axis list. So again, I instantiate the object, pass the axes through to the data frame histogram command, right? And then I have the full control to style it in a way I want, right? So I remove the title, set the x label to permeability, and the y label to number of currencies, and then just set the y limits in both cases, right? So there's a slightly more complicated example, doing the same thing, and using the histogram plot from pandas. So this is a reference that I found. I had to give credit. It's like practicalbusinesspython.com. So where I found this, in fact, one of our datum students that took, of course, last summer, passed this along through our datum community Slack channel, and I liked it so much, I decided to go ahead and put it in the notes, because I think it's very, you know, if you go to the website where he creates this, he creates this figure, you know, and basically one line at a time talks about all these modifications, but then in the end you have this kind of nice summary. This is the figure that's created, and these are all the commands that you would use to create that figure, by doing all these kind of adjustments. In this case, it's just bar plot, and you know, it's not really relevant to, you know, it's just stock, you know, information about companies or what not, but nevertheless, I really like this kind of summary figure. So credit to these guys. So again, mostly what we do is 2D plots on that plot live, but there are some 3D plotting features, and in this case, the 2D contour plot, to plot the, in this case, the depth of a reservoir. So this is a real reservoir, this nitulate field in Alaska, I think, and here on just these are the boundaries of the field. The way you get a contour plot to sort of plot this white space out here is just in the Z value, you have NANDs. So if you see, if it sees not a number, then it will, you know, basically plot the white space like this. So these are the boundaries of the field, and then this is the depth, so you can see the deepest part of the reservoir is in this kind of street right here, and I'm just using this contour plot to create that. And also, there's a similar plot, same data set, but I'm plotting it in a surface plot. And in this case, initially, I have just this kind of plot, but if you're working in a Jupyter Notebook and you would uncomment this map plot live notebook and execute it, and I don't know, it affects my scrolling when I'm in slide mode, but you can actually see, I can then, just within the notebook, I can grab and rotate this plot, allowing me for some, you know, interactivity, you know, right within the notebook to kind of see the various features of the reservoir. And I'm sure I could tweak this sum to, you know, probably get the aspect ratio, or it's not, you know, because in this case, the X and Y are on thousands, and this, and the depth is, you know, 100 and something at the deepest. So aspect ratio of the plots a little wonky, but still pretty neat that you can interact with it like that. So there are other plotting libraries for Python that are popular, and there's a far more than what I listed here, but bouquet, we're going to talk about next. So I think I won't say anything else about it. Other than just say, as we'll see, it's very useful for creating kind of interactive web, you know, plots that really are intended to be displayed on web pages. And we'll see, we'll see some of that in the next talk. Hollywood is something that's kind of an interesting tool, and that it actually allows you with one set of plotting commands, you can then choose the back end that you want to actually create the plot, and the choices come from either bouquet or map plot live. So again, map plot live is really great at making really high quality figures that would go into, you know, really intended for print publication, right? So things that would go into articles, you know, books, things like that is really what map plot live shines at because you have that full control over all the tiny details. bouquet is really better at making interactive web base plots. Well, what if you want both? You could actually use Hollywood with one set of commands, and then just choose the back end. In one case, you could choose to create a bouquet figure, and the other one you could choose to create a map plot live figure, and you could kind of get both that way. Also, the API, the application programming interface to Hollywood is very similar to bouquet. It's far more similar to how you create plots in bouquet than that plot live. So with typically with fewer commands, it takes fewer commands in Hollywood and bouquet to create a nice looking plot. Plotly is another tool some people like, and it has some dashboarding features. Also, you know, really intended for web based type plotting and other things. All Terra is the newest of the group. It's based on a grammar of graphics, sort of a declarative language of graphics called Vega. It's called Vega. And it really comes from the JavaScript world. So all of these are capable of producing JavaScript, like again, web based, you know, figures. So they actually produce these libraries actually write the JavaScript code that then gets displayed and allows for interactivity in a webpage. And all Terra, you know, again, because it's borrowing from Vega, which is declarative language in JavaScript to create figures, produces JavaScript. But all Terra is a Python library that produces that JavaScript to produce plots. And it's closely related. If any of you have ever programmed an R, and you know how an R you would create figures with GG plot or something called the grammar of graphics, all Terra is more similar to that grammar of graphics approach than any of the other tools. Of course, Matplot Live, which we're learning here, is much more similar to Matlab in its sort of application programming interface for creating plots. In fact, that's where the name comes from. Matplot Live. It was kind of a rip off of an initially just a clone or copy of the plotting capabilities in Matlab.
 Let me I think that there's a really nice example of principal components that can be shown that can really kind of helps to take this home. And I think, well, first of all, let me just because I mentioned it a couple times, this is an example taken directly of taking the first 15 principal components of all of the Dow Jones companies, their performance over time. And so imagine what they did. They took like how many companies are in the Dow Jones. I'm not a finance person, hundreds, right? So imagine hundreds of companies and they looked at the correlations between all of them over time, their stock performance. Then what they did is they did the first 15 principal components. And then they looked at the oh, first principal component describes most amount of variability. And it looks specifically at the sense, like what is it negative or positive loadings. But they also looked at the magnitudes. And so you can look across here and see which companies are negative or positive. And this is a big data set where only we're truncating it, right? And then they look at the second principal component. And it describes an independent aspect. So imagine the stock market. Imagine that, what would be something that influences the stock market? And I bet you, I bet you anything, I bet you anything, if you ran this analysis right now, you would find principal component one would be massive amount of variability. And you would probably find pretty consistent performance among many different types of stocks and maybe bonds or other types of, you know, what do they call them? You oh, Shucks asks it's what you'd find out instruments. You'd find out that they behave differently. And you would probably look at and say principal component number one is the corona principal component. You would seriously see that. And then you see principal component number two could be seasonality. Maybe there's something in the stock market that's related to people getting tax returns or people, you know, people having more cost of heating or cooling their homes in the south or something like that, people are probably more disposable income in the winter, that kind of thing. Okay, so that's how you could do it. Now, this is a really nice example right here. Those of you who have been through this before, have seen it before, we take optical character recognition up until convolutional neural nets became popular. People were using PC8 as kind of one of the tools to kind of help get this done. Now, if you look at this database right here, and there's a standard database of hand drawn letters, 16 by 16 cells grid of gray scale values going from zero to one in gray scale, or maybe zero to 255, I'm not sure how they did it. And so you code up the pixels by pixels. And now if I take all of those threes and I try to describe them, what is the dimensionality of that data set? What's the original dimensionality? 16 by 16 gray scale values between zero and 255 or something like that. That's a 256 dimensional problem. Now, okay, everybody sit here and use your mind that, you know, your brain that developed in sub-Sahara Africa on basically nothing eaten by a lion, and now try to imagine 256 dimensions. Okay, who's doing it? I can't imagine 256 dimensions. That's really impossible to imagine, right? Okay, so now, look at this data set. Do you think you need 256 dials to describe all of the variability between them? Isn't there a high degree of commonality between threes? They, after all of the all threes, I don't see anybody here who made a mistake and thought they were supposed to fill in the square. And I don't see anybody who like just didn't put anything in. Clearly, there's a lower degree of variability. Okay, so what do we do? We take the 256 dimensional problem. We calculate the covariance matrix between all of those pixels. And then we calculate the eigenvectors, eigenvalues. And when we do that, this is what we get. We get the very first principle component. We get the specific principle component scores, or right here, and here. And what it's going to be is we can in fact take and project the data through these filters. They become like filters in 2D. It's pretty cool. These are the loadings, the principle component loadings, they become like these filters. And so we apply them. What we can actually do is we can now calculate for any one of those threes. We can now calculate the principle component scores. We can project them into this rotated space. First principle component, second principle component, and these green dots are all of the threes. Okay, so they've all been rotated into that space. We've done dimensionality, reduction, we're only concerned in the first two principle components. Now what we do is we take the green samples or green examples that are closest to these axes. So in other words, it's a nice regular sampling of that lower dimensional space. And then we plot them up. We look at them. Now this is fascinating. The first principle component that describes the most amount of variability. If you look going from here to here, from here to here, those are the samples and how they line up on that first principle component. The difference between here and here is when you pick up your pen. The most amount of variability in threes is due to when you pick the pen up. Some people do with three where they kind of stop early. And some people do with three where they continue and loop around pretty high. That's the most amount of variability. That's the biggest dial. The second principle component is how hard you push when you write the threes. How thick are the lines? And so you look from here to here, the difference is just the thickness of the threes. Okay, so if you look through this data set, the first two principle components are when you pick up the pen and how hard you push when you write your threes, that describes the most amount of variability with regard to this data set. Now you could calculate principle component number three and four and you may be able to interpret it and see that there's other features that are being picked up. Maybe something about loopiness or something like that. Okay, any questions or comments? Not a 256 dimensional problem indeed. Probably could be described with much fewer principle components. And the other thing I want to point out is you notice how we had to project it into the actual feature space, the lower dimensional feature space and look at it in order to interpret it. Principle component analysis doesn't come back and say it's the length of the tail, it's the thickness of the line. This was an interpretation. You see that? So remember with principle components, it can give you dimensionality reduction, but it's up to you to figure out what those principle components are and to understand them. So there is that step of interpretation. People do this in our industry. Principle components are used in seismic interpretation. Here's an example right here, trooper and others where they took six seismic attributes. And when they looked at the first three seismic attributes, they found they captured 97% of the variability. And here's the first two, one and two principle components score one principle component score two. And what you can see is really fascinating because the first principle component that describes the most amount of variability is seeing the within channel heterogeneity. The second principle component score is actually picking up more kind of the gross geometry of the channel system. So they both seem different parts of the image. The fascinating thing is the second and last and the last principle components scores only captured noise. Like they're literally just noise from the seismic acquisition and you could throw that out. So it's actually a form of denoising. Okay, any questions or comments about principle components? Okay, let's I'll tell you what, let's go ahead and go to clustering. We'll do a quick explanation of clustering. We'll try clustering out. Are you guys interested in clustering right now? Another option is we could go to feature selection, but I'll just leave it. Okay, who votes for doing clustering right now? Put your hand up and I'll assume yes means clustering, no means. Well, if you want to use the yes or no buttons, yes means clustering and no means you want to go to feature selection. Okay, okay, the voice of the people has spoken. Okay, democracy, and action. I love it. Okay, thank you very much guys. I appreciate that. Okay, sorry, I had to iterate on that a little bit. Okay, let's go ahead. Oh, the course notes. Do you see the clustering there in the notes? If you go back to table contents, yep, there it is. Okay, and the exercise for clustering, it's right up here. Okay, good, we have it. Okay, we're good to go. All right, so I'll go ahead and open up the notes here locally. Okay, let me explain clustering pretty quickly and we'll go straight to code. This is a pretty simple methodology. Okay, the first thing, it's a powerful methodology to discover natural groupings within your data set. Now, if you're trying to come up with a decision rule and you're trying to do predictions, we got to use other methodologies and I'll explain how we can use this methodology for prediction, but this is inferential. This is you have unlabeled data, you don't know the why, you're trying to let it find the groups for you. Okay, prototype method. A prototype method is interesting. What you do is you look at the data in feature space. Now, feature space simply means we got feature one, feature two, feature three. We're looking at it relative to the features like the cross plot scatter plot, right? Then what we do is we identify for each group a representative type, the example, the one that represents the rest of the group. Now, in this case, it's going to be the centroid of the members that belong to that group at the centroid. And literally, I mean the centroid. Calculate the average in each dimension. It's that one right there. Okay, now a prototype doesn't have to actually be a sample. A prototype, in fact, just it's a imaginary sample. Pyrosity of this amount and permeability of this and much was never observed in the data set. The prototype is just representative of the group. Okay, now we're going to use the prototype method as a tool to be able to do clustering. K means clustering. We're going to assign group membership to a data set such that all training data in the feature space are going to be signed to a group. So the groups are exhaustive. If you take group one, two, three, four, up to K groups, the probability of belonging in one of those groups is going to be equal to 100 percent. You have to belong to a group and the groups are mutually exclusive. You don't get to belong to more than one group at a time. You have to belong to a only one group. Okay, segmentation of the training data, we're going to basically segment it up into group one, group two, mutually exclusive and exhaustive. Okay, now every time we build a machine, we're going to have a loss function underneath the hood. And this loss function looks like this. It really is simple. This double sum right here is the pairwise comparison of all data within a group, pair to pair to pair all the common to world data to each other. It's the square difference between them. And for the question we had before, this is a measure of distance in the feature space. We got to do standardization. We're going to get really messed up here. Okay, so we do standardization. We calculate a distance between all the pairs inside a group. This part right here is simply saying we're going to do that over all groups. And we're going to do an averaging. So we're accounting for the fact that sometimes we have more samples within one group versus another. We don't want to wait one group more than the other. So this is a little bit of a waiting that goes on right here to make sure we wait by number samples in the group. Okay, we're going to do that over all of our groups. And then because we're machine learning experts, we're going to call it inertia. I don't know why. I don't know why they like to grab names like that. You know, epochs for neural nets and inertia. I somehow feel like the computer scientists are coming up with this stuff just want it really cool sounding names. So anyway, so they call this inertia, but it's really the square difference between the samples within a group. Okay, we're going to minimize that inertia. Now graphically speaking, what does it look like? This is the difference between samples within a group. I was lazy. I only showed the nearest neighbors. I didn't show the full commonatorial. That calculation is going to take this data versus all the other data in the same group. And I want to minimize that. Now what's fascinating if you minimize that, you actually maximize the difference between the samples between groups. There's a conservation of variance effect there too. Okay. So there's a large solution space. If you think about it, the commonatorial of group assignments is massive. It's a very big space. But what's really crazy? The problem is practically solved iteratively and it works really fast. It actually works really well. I think we'll demonstrate it solves in like four iterations for our problem. It may converge on a local minimum. What do we do practically speaking to avoid that problem? In other words, when we're iterating by adding and subtracting examples from each data set or by looping or by moving our data from group to group, what can happen is we can get stuck in a local minimum. In other words, we may see like we're in the best solution, but if we were to do a lot more perturbation, we'd find a better solution for the grouping. Now to avoid that problem, what we'll do is we'll just start with a random seeding of centroites, solve the problem and we'll repeat, repeat, repeat. And we'll make sure that there's not too much difference between the solutions. So that's one way to do it is we can just check random starts and make sure we converge the same answer. Okay, the methodology clustering, we're going to train data and training data in the future space. We're going to identify which are similar to each other. It's unsupervised learning. There's no labels. There's no Ys. It's all Xs and we're just comparing them to each other. It's inference. It's all about the inputs and learning the system. We're not making any predictions here. Prototype method because we use those prototypes as a tool and it's iterative solution. We need to do normalization because it uses Euclidean distance. It's going to be sensitive to the variability of each one of the features. I'll show you the result of that if you don't do standardization. Feature waiting could be done though. You could say that one feature is more important and you can put that into the methodology. And there's a supervised learning variant, which I'll show you at the very end. Okay, here's an example right here. I don't want to do it like this. Let's go do code. Do you guys want to open up some code?
 Okay, so I got some documentation I explained clustering right here. Now what we'll do is we'll go ahead and we'll import just that pie. We use it for some visualization, not a big deal. And we're going to port NumPy's MatPotlib. You'll notice no psychic learn. The reason being is it seemed like it was a better idea with clustering to code it up my hands so we could watch the iteration. It's so simple I coded up. Okay, so let's see, I'm going to show you how clustering works while running it. We're going to walk through it together. Okay, we got a methodologies here that we're going to include an assignment and an update function. We're going to load in our data. We got our data. Now I do this operation here. Random sampling from the data of 30%. I'm removing 70% of the data just because it was so much data it was kind of dense to look at hard to interpret just for visualization for our problem. We'll describe the statistics. We've done this before. Now this right here is something new. We're going to go ahead and take the original data, porosity, acoustic impedance. We're going to find the, we're going to normalize it by finding the maximum and minimum values. And this calculation, take a porosity value, subtract the minimum divide by the range is what's known as a min max normalization. It forces the values to go between zero and one. It's the same as standardization, but it's called normalization and it gives us really nice looking plots. But most importantly, the same variance for each one of the features now. That's important. We don't want the features to have totally different variances and one will dominate this calculation. Okay, it'll look like it's more distant. Let's go ahead. We take the data. We visualize the statistics on it. We've already looked at this data. I'm not going to spend any time on it. And we're going to go ahead. We can skip through here. This is just setting the ranges, the min and the max. That for our visualizing our color bars and our axes on our plots. We'll set a random number seed. We do that so that everything from now on is going to be the same between us. We don't get dramatically different results. And we'll say that we'll have K different. We can have up to seven different categories. Okay, this is our original data. Original data right here. Normalize data right here. Zero and one normalization. Min max normalization. Okay, are we all together guys? Are we good? Okay, I see nodding. Good. Okay, so we're going to work in this space, but we're going to project it back to this space so we can see the original space the whole time. What do we do? The iterative solution proceeds as follows. The first thing we're going to do is we're going to randomly assign prototypes. Now how many prototypes do we want to use? Let me ask you this. How many categories do you think we should get from this data set? What would be a reasonable number? Am I squinting too much if I think two or three is a good number? But we could do seven. We can make a decision right now. K rate here is the number of categories we're about to assign. Anybody got any opinions? Let's do three. Okay, so change K equals three. Now we're going to have categories. The first one's red. The second one's green. The next one's blue. Okay, we're going to have three categories. Okay, now we go back. We plot that again. The first thing we're going to do is we're going to initialize random centroid locations. Okay, did you guys all get the same result as I have? You have the first blue centroid or prototype here, prototype for red, prototype type for green. If I stopped right now, do you think I have good prototypes? Those are lousy. Those are terrible prototypes. In fact, and it's okay, they can be terrible. You'll see what will happen next. The first step, let's take all of the neighboring samples, the sample data, the training data, that are closest to a prototype and assign it to a membership to that prototype. Okay, so this code right here, the assignment code that we declared up above is manually going to do that. All of this is just plotting really, really simple stuff. So we run that. Look at what we got. What do you think? Do you see it? That's what we got. So this is how we solve for K-means clustering. We sign random prototype locations iteratively. We're going to go ahead and assign the nearest values to the membership. Now, what can we do? If I look at those prototypes and I look at the members, do you think the prototypes are representative of their groups? It's outside of its examples. So what do we do next? We're going to update the prototypes. What would be a rational place to put our prototypes? Now remember guys, still free to use the chat window too. I do monitor that. Nobody's been chatting today. We could assign them in the centroids or the middles. Okay, so let's go ahead and do that. So this next code is just going to assign, take the prototype, move it to the center. Take the prototype, move it to the center. Take the prototype, move it to the center. Does that make sense? Okay, so good. Are we done? Yes. You know, we're getting there. We're getting there. What's the problem? There's just one problem remaining. The prototypes look good, but what about the group memberships? Are they correct? You can see how these samples right here are not assigned to the closest prototype any longer. So we'll rerun the assignment process and we get this. Now we have the new assignments. Now let me ask you a question. Are the prototypes still good? Is this blue prototype here good? It's not in the center of its group any longer. So what do we have to do? We have to sign all of the nearest data. We have to move the centroid or the prototype. Then what we have to do is re-assign the data, move the prototype, re-assign, and we do that. That's the iteration. And what's amazing is this works. It's crazy because this is a high combinatorial problem if you think about it, but this iteration actually works and doesn't take many iterations. Okay, if you run this code, this code iterated enough times to solve the problem. Now in the previous class, I, in fact, put in a counter there and it was four iterations to get to that solution. How do I know to stop iterating? What do you think I did to check? All I had to do was wait until the centroid or the prototype stopped moving around. And then it's frozen. It's converged on a solution. And I could do a random restart. I didn't do that here, but if I could do a random restart with a different random location of prototypes and they should come to the same locations, then I know I found the global minimum. Okay, any questions about K means clustering? And sometimes maybe you get a unique cluster, not because there's a physical meaning maybe because you have missing data. And that's good. Then you can interpret that. You can look at that and say, no, these clusters are identified here because maybe there should be more samples here and this should be a supercluster, that kind of thing. The other thing too is PCA is very useful for that too. You take your data and you project it into a lower dimensional space. You could see holes in that space. You may not see the holes in the multi in the high dimensional space because it's so hard to visualize. You're always flattening it, projecting it and you're losing a lot of the variability when you do that. Yeah, there's, there's different ways. Now if I can, there's one more type of methodology. It's called DB scan. There's many ways the cluster now DB scan works differently. What it does, it assumes that every data point is unassigned to a group. There is no initial assignment. Then what it does, it looks around and sees if there's a large enough sample set within a small distance. In other words, does it look like that could be a group? If so, they all get assigned to a group. Then it iteratively visits every location, finds if it can see the new new group. Or if two groups or individual sample are too close to another group, it assigns it. So it's a bottom, it's a bottom up methodology where you grow the groups from the data. Now what's fascinating about that, it will account for connectivity. This methodology right here doesn't account for the connectivity. If it did, it would have assigned this all as one group. You see that? Because they are highly connected. This other methodology will actually account for that. It's really powerful. Those methodologies will detect outliers too. There's a lot you can do with clustering.
 Yeah, okay, I'm just making sure I have switched back to the master branch. I'm not on the solution. So this is a plotting class, right? So you notice I put this in a class and it's really overkill for creating a just a simple plot, but we'll see why we need it later when we begin to because we'll reuse this when we start to build out the dashboard. Basically, they're just two commands, right? So basically what we want to do, this is how you would use the class. So you're just going to basically have to write two lines. One right below here that adds a Y label. So whatever you, whenever you said, say, set up date plot, you'll feed in an argument and whatever that argument will become the Y label of the plot. So you just need to add a Y label to the attribute self.ax. So that's just one line there. And then here is where you'll add the plot command where you have this data frame that's coming in as an argument. And so you'll want to just call data frame plot passing in self.ax to the axis, as well as these, you know, read my notes there. So you'll want to pass pass in the API that comes in as a function argument as the label pass in the default line style as the line style. And the attribute self dot color cycle, basically this thing, copy it, and set it equal to the color in the pandas plot command. And the reason for that is, you know, again, we'll reuse this class later when we build out the dashboard. And I want to control the color of the plots, the color of the lines on the plot such that when I plot, when I fit a model to it, I end up getting the same color plot, the same color line. So if I have data that's plotted in, you know, the monthly production is plotted as points, when I fit a model to it, I want it to appear at a line of the same color. And that's what this color cycle thing controls. So again, just two lines of code. And it should be able to uncomment these lines right here to see the plot that you produce. Okay, so yeah, like I said, you just have to add two lines to this, to make this code work. The first is just literally setting the label. So we have this set up date plot, which takes an argument, Y label. And we just want to pass that to the axis. So we have this class attribute self dot axes. We want to set the label to be the Y label that comes through. And so that we just call the set label set Y label, remember function on self dot axe. And that's all we have to do there. And you can see yourself, this setup date plot is called when the class is instantiated. So it passes in the argument Y label at class instantiation. It passes this in there to set up. So this first function sets up just sort of the some of the details of the label and other things. And then we add the plot and the second one. The reason for that is later, we're going to use this class in that dashboard to create plots, you know, basically this one class, we can then create plots for oil, gas and water just by instantiating them differently with oil, gas and water as labels. That'll save us a lot of well, save us some code when we do that. So then add plot again, we're just going to data frame is comes in as an argument. So we're going to use the pandas built in plotting command, data frame plot passing in as an at passing in self dot axe to axe. And then just passing in these other arguments from this function to set up the default line style, the default marker and again, the color as well as finally the the label, which gives us API number. So if we run this, I didn't run this first line, first cell. So if we run this, we get this kind of plot, right? So it's labeled, the correct label on on this side. And you know plots plots the data like this, basically the data points are plotted as points, and they're connected with a dashed align. So later when we fit a model to this thing, we'll plot a solid line through it. And you know that color cycle stuff that was here is what will force that line to be the same color as the line that you see here, the orange color or whatever. Any questions?
 So bouquet is another plotting library and the reason that you know I feel like it's as I mentioned last time in the map in the map plot. The reason I feel like it's worthwhile talking about two separate plotting libraries is because I really think they're distinctly good at two different things. You know map plot live is really good at making you know publication quality visualizations where you want full control over every tiny detail. bouquet is good at creating decent looking plots with much fewer commands and then really intended to be presented in web browsers either in jubber notebooks like we use in the class or as standalone HTML web browser or HTML documents and these can be as I'll show you they can sort of have embedded data or they can have callbacks to a server where computations are done in order to update the view. They're also good they have some kind of really nice features for large dynamic or streaming data. So if you had the data that was you know constantly being updated you could easily you know create a bouquet app that would automatically update upon you know the data being updated. So create a simple bouquet plot just like we did in map plot live we'll just kind of create the simplest thing. We need a figure that like instantiate the figure and that again controls the kind of canvas. So the titles as we'll see the figure size will as we'll see there's some interactive tools and toolbar. We can control which tools are available the location of the toolbar. It needs a data source and usually the most natural thing to use there is a pandas data frame and even if it means you need to create one really quickly but as you learn when we talk about pandas to create a data frame from scratch it's a simple really is just putting it into a Python dictionary with you know label in the data. And then the you know once we have instantiated the figure and we have defined a data source and we just pass that data source into some type of glyph. Glyph is the terminology here. It's kind of an analogy with the axis in map plot live that is you know what we're actually going to plot on the plot lines dots you know whatever. So that will use glyphs for that in bouquet. So we're going to read in our pandas data frame just like we did before. This is the same one we we we've used several times and I'm printing out the first three lines as you can see it. And then we simply call bouquet plotting figure to instantiate the figure. Then we call bouquet models column data source. What this allows us to do is to transform our pandas data frame into a data source that bouquet wants right and so essentially all you have to do is pass your if you have your if you have a data frame all you have to do is pass that data frame into this bouquet models column data source that defines the data source and then you pass that onto the glyph right in this case we're just going to plot circles right so just we have the figure object is P. We're going to assign have this member function circle. Then we define the x and y labels porosine permeability these come or they are identical to the labels in our data frame. So that's real nice that we don't have to use any indexing or anything else we can you also makes the code very readable in the sense that we're just plotting using the labels based on that come from the data frame again passing in the data source to this keyword argument source for the glyph and then we just show the plot and it looks like this. Now this is an interactive plot so we can choose one of these tools on the side we could zoom in on the data we can reset the view we could actually save this to a PNG or something like that we can pan around. So this is really good you know if you have really big data and you know there's certain features of the data you want to zoom in on you can look at it like that whatever. So here's just some additional styling for a plot so in this case we read in some production data from a CSV file and then in this case I'm controlling this just issues some of the additional figure commands so I'm controlling explicitly the width and the height of the plot. Now I have time series data on the x axis so I actually specify the x axis type to be a date time and then of course set the x axis label y axis label and here I'm specifically I'm restricting the tools to be just pan and box zoom so these are the only only tools that that are available in this in this data set pan and box zoom so can't even reset the window without basically re executing the code. Oh I guess there we go. So yeah this is a little more complicated example where I'm doing some explicit styling here I'm using a line plot instead of a circle glyph plot. It's having some trouble this seems to have broken recently but it's not broken in your view so I'm going to go back to the Jupyter hub quickly. I think I'll just look at it as you're looking at it here. So bokeh makes really nice geographic plots so it's a way to create plots on a map so in this case plotting all of pioneers wells in the Permian Basin. So a couple of the Google maps API has its own set of functions so in almost all cases you're just going to use bokeh plotting figure to instantiate a plot. But in the case of the Google maps if you want to use those then you actually have to use this special function bokeh plotting gmap and it requires an API key which you have to get from Google and I think the fact that I have this is really what broke my slide over here because I restricted the API key in the Google Cloud services to only work from certain domains one of them being classroom.datum.org which means I restricted it from me being able to use it at my house and wasn't able before class today to get that updated. So that's why this works here because it allows the Google API allows calls from classroom.datum.org but it doesn't allow calls from the IP address at my house. The reason for that is because the Google API calls are not entirely free. They're free. I think you get like a thousand a month for free and then anything over that is like number one tenth of one penny per API call. So anyway, that would just be the case if you had something on the open web and people were using your tool a lot, there's some penalty for kind of overusing the API. Anyway, if you want to use this Google map you have to use the special function gmap to create the figure and you have to have an API key which you can get from Google and I'm not discussing how you do that here but there's information on the web. There's also some options that you can pass to the gmap where you want to center the map, the zoom level, what type of map you want to use. In this case I'm just using the terrain that you can use the street value, the whole full street view, whatever you want. There, and you pass those options into this gmap as well. You can get all the tools, those things are the same and then the rest of it is really the same once you get past that. So we're passing in a data frame as our into this bokeh models column data data source and then plotting circles on the map where the wells are. In this case, you know, I've got the zoom level set such you can kind of see the whole state of text but you can zoom in. Oh, I'm sorry, you have to use the wheel zoom. This is a box select tool. You have to use the wheel zoom, this command right there and then you should be able to zoom in on the Permian and you can see, you zoom in far enough, you'll see the structure like you can see most of the five spot. So Pioneer has a lot of wells in the Permian obviously. And you could select these wells, in this case they don't give you a lot of information but if you do select them what you see is that some of those other settings like the fill color and fill alpha. So fill alpha is the transparency. So these are plotted, you know, quite large with respect to the actual area, the circles themselves. But so it's kind of nice. In some way, if you have this transparency feature, if you're zoomed out to some extent you can see where the higher density of wells are because of this because you know, the more that they're overlapping there's a higher density of wells in those regions and this transparency kind of allows that to highlight that a little bit. So that's that using the Google and using the Google Maps API. In addition to that we can, here's another example of this, this is a fairly complicated plot I made with bokeh and it has fully embedded data. So this is a full standalone web page. It's not running with a bokeh server or anything in the background. This is just an HTML web page and it's fully interactive in the sense that if you hang on. It seems to be. Yeah, so all I'm doing is I'm moving my mouse over the well. So I guess first of all, the wells themselves are colored. These are EOG's wells in North Dakota. They're colored via their total production in terms of oil, right? You can see in this kind of region close to where my mouse is, is where the highest producing wells are, sort of the sweet spot in the formation. Not only that, when you mouse over the wells, you get multiple information. So you get the tool tip pops up that tells you the API and also tells you the total cumulative oil and gas that the well I'm hovering over has produced in its lifetime. And then down below you see the production histories for oil and gas. And the two plots are linked such that as I'm mouse, as I move my mouse around, you see the ones on the bottom, you know, immediately updating as I move my mouse around. And in the case where my mouse is hovering over multiple wells because they're, you know, at the scale I'm doing it on top of one another, then I get both production's history. You know, I get the production history of multiple wells. If I wanted to freeze, so as I move around, the bottom plot is continuing to update, right? If I wanted to inspect the plot on the bottom, what I can do is I can click the plot on the top, click it, that freezes the view on the bottom. So now I can go down here and inspect, oh, okay, this is that API well, whatever, this is this one. And then, you know, so if you notice now, as I move my mouse around, the highlighted wells, now it's not changing the wells on the bottom view. If I go back and I click on the anywhere away from a well, so out here, again, then it returns to like the fully updating mode. So as I move my mouse around, everything is updating. And again, you know, I have all these tools I can zoom in, you know, I can select particular well, and then, you know, view it down here. I can select a group of wells, and it will bring up all of their histories. And again, as long as they're selected on the top plot, I can come down here and inspect them. And then again, if I wanted to go back to fully interactive mode, I just do that. So I, this is just, I'm not showing the code here. This is just demonstrative, you know, this is a demonstration of a fairly complicated thing that you can do in Bokeh. And you see how fast it updates. I mean, it's just, it's lighting fast. It's all, this is all written in JavaScript and produces a standalone HTML document, which I've just hung on my own webpage so that we could easily reference it for the class. There are some additional, so in the plot that I just showed you, all the interactivity and everything was provided through, you know, internal JavaScript. However, you can also have interactivity with callbacks into a Python kernel, meaning I can click, so the figure you see is written in JavaScript. And as I interact with that JavaScript figure, I can have certain callbacks that will certain interactions will cause an actual computation to be done in Python. And then that, that the result of that computation will update my plot. So that's called a callback. And there are, you can do it with, with Bokeh alone. However, what we're going to learn next is a package called panel, which makes some of it a little bit easier. So panel is just written on top of Bokeh and allows you to do some things without quite as much verbosity. So I would recommend, you know, if you want widgets and callbacks to Python kernel and stuff to use panel. And that's what we'll use here when we work on the auto type dashboard shortly. And then like I said, you can serve interactive standalone applications with Bokeh serve. And that would be a case where you actually needed Python computations to be done and interact with the JavaScript. So basically anywhere you can serve a web page, you can serve one of these Bokeh applications and have interactivity with the plot such that, you know, it's calling back and having computations done in Python and the results updated on the plot. We've already talked about the other plotting libraries, of course. So I won't mention those again since we just discussed them. Are there any questions? So you know, a lot of data analytics is just coming up with clever ways to visualize high dimensional data sets, right? Look for patterns because as good as computers are at finding patterns, they're actually not as good as humans. We're still better in most cases. And so, you know, a lot of what we do is data analysts is just figure out clever ways to look at visualized high dimensional data sets. And I think, you know, hopefully, you know, something like this just gives you an idea of the type of things we can do in Bokeh, right? Because again, I have these multiple kind of hover, like if I hover, it's, it's giving me, of course, the API number, but also the computation has been done such as it rolls up the total cumulative oil and gas for that well. And then you can interact with it like, like so. So I think those of you that have used Spotify are probably used to see in some type of similar visualization in Spotify. But when you hear in the, in the time it would take you to write the Spotify template to see something like this, you could write the Bokeh code and then you have your own free version. Now, you're not, you're not paying Spotify. All the tools are freely available. What does they Spotify? I think it's Spotify, Spotify. I guess I'm thinking about music today.
 All right, let's get started again. So I think this would be kind of a nice thing to do. It's a little bit out of order, but Dr. Foster actually produced a principal component demonstration using interactive Python, plotly or some type of methodology where you can automate plotting and charts. And so if you click right here, if you go to week two and you look at dimensionality reduction, PCI demonstration, you can open this up. Now I already opened it up. It's available right here. Now what you'll notice is that at the very top of this screen, I think if I let me see if I open it up again. No, let me don't do that. At the very top of the screen, you should see three dots. Do you see three dots at the top of your work full? Click on that and it will expand all the code. Click on the code and hit run. And you should get this display right here. Have you guys been able to do that? Takes a second to run, right? Give me a thumbs up if it worked. Okay, awesome. You got the plot. So now here is really cool. Is you have the original data set right there. You see the blue points. This right here is the distribution of the error. So that's now you could imagine if you were PCA, what you would be doing is you would be rotating that line, that first principal component. Okay, and your job would be to minimize the error. Now the error, all of these orthogonal lines right here. And I hope I can show you this that as I rotate like this, can you see how this distribution is all starting to expand? It's starting to become higher error. As I get, as I get right here, I reach a point where the error is quite maximum. In fact, if you were to calculate the average, but just look into the distribution, you can see the values are shifting quite high. And if so, this would be like the second principal component for this two-dimensional case. It's the principle component, which would have the highest orthogonal error. In other words, it would have the lowest amount of variance described on this line. And if you rotate back, you can see how you've reduced. So like this, the maximal and like this to minimize the error. So I think that's a nice little visualization of principal components and what it's doing. Now, any questions? Was it useful to see something like this? Do you guys like that? Kind of interactivity around principal components? Good.
 So this time, instead of using Google Maps, we're just going to use one of the other vendors, there's open street maps. And in that case, one advantage of Google Maps is it has its own projection from latitude and longitude. You don't have to do anything. So remember, as you all know, latitude and longitude is defined on a sphere. So if we're going to take those degrees on a sphere and plot them on a two-dimensional map, we need some type of projection to do that. And there are lots of different projections. The one we're going to use is this so-called Mercator projection. And I've already written the code that you need to do that. So basically, what you have to do is pass in latitude and longitude values from our database, transform them through this Mercator projection. And this function returns that transformation. So if you look at this create-well location map, the first thing I do is go ahead and do the transformation for you. So once you read in the wells, again, you might ask why I always say, why I always use EOG in order to code. It's because that's the best data there is. I mean, first of all, just in general, the public data, North Dakota has the best data. It's all much newer. There's not a lot of missing in messy data. And then for whatever reason, EOG seems to, you know, just all the data is nice. It's very nice. You won't run into any messiness or problems with that data. So that's one of the reasons I always use it. So yeah, you'll have to... There's some comments on how to do it. You have to add the figure command. You have to basically just uncomment this tile provider. And you'll want to make sure that you write your glyph code. Right? So if you want to plot them as circles, you'll want to make sure to write it below the tile provider. Because the order does matter. If you plot the glyphs above the tile provider, when you put the tile provider, we cover the glyphs up and all you would see is the map. So make sure to put your glyph code below this. Right? So basically you need to instantiate a figure, set the X and Y axis type to Mercator projection, load the latitude and longitude into a column data source or a data frame, and then plot a glyph here and completing this function create well location map. So in this case, we're going to just cheat a little bit to speed things up. I'm going to go over to the notes and grab a few things. Like for instance, I'm going to grab this figure command there from this plot, this production plot. So and that's what I'll use as a the basis to start my plot here. Now you'll notice up here, I've imported, see how I've imported from bokeh plotting import figure. And the notes I didn't didn't do that shorthand import because only because I just want you to see the full scope of where the things are coming from. But since I imported it as fun bokeh plotting import figure, I don't need to put the bokeh plotting on the front of it here. So then I want the X axis type to be Mercator and also the Y axis type to be Mercator. And then I don't really need. Tools I want to have a pan box zoom box select reset. And let's see, let's make it 800 by 600. So all of that you could, the main thing on this line is you have to have the X and Y axis type set to Mercator. Everything else is up to you. You know, you're your preference. Okay. We have to uncomment that, the tile provider. And then the next thing I'm going to go back over here and steal these two lines of code here. So in this case, we had a data frame, but we don't have a data frame here. We're passing in latitude and longitude. They're being transformed. And so we need to either create a data frame or we can just actually just create a Python dictionary where we want the lat. And I'm just going to use shorthand names, but lat equal to lat and long equal to long. So I know I use the same names, but this is a string lat that's going to be equal to this lat here is what's returned from the transformation. So it's that. And then I have a string long that's equal to this thing here that's returned from the transformation long. And then I can just use the strings here, lat and long as my F reference. So long lat and then everything else I think can stay the same. So let's see if that worked. I think I'm just going on here. Well, I can do it this way. So so this second function just define it's a function. You still have to call it. So to call the function. Let's see. Okay, so I did the same thing. I copied that code, but I don't need bokeh models because I've already imported it up here as just column data source. So I need to delete that. And there we go. EOG's wells in North Dakota. And can zoom in on them if I want.
 Okay, multi-dimensional scaling. What is it? Well, once again, our motivation, we work in high-dimensional datasets, highly multivariate, massive multivari datasets. We want to project to lower dimension, to improve interpretability. But it's weird. Principal components is a rotation projection. I think it's cool, like it's useful, but there's other ways we could project. And one projection is multi-dimensional scaling, which is a projection that preserves the dissimilarity between the samples. Okay, imagine that. That's it's gold in life, is to keep the dissimilarity between the samples. Okay, let's look at multi-dimensional scaling. And I will first explain it to you using a simple example, then I'll show you of real data. Okay, so imagine if I had a list of cities, Boston, Chicago, DC, Denver, and so forth. And all I had was their differences. I just had a measure of dissimilarity between all of the cities. In other words, this 963 is the miles distance between Boston and Chicago, DC and Boston and so forth. This is the matrix of pairwise distances between all of the cities. My question to you is if I had that matrix, could I restore or calculate the cities in some type of two-dimensional space? Do you think it'd be possible? This is exactly what MDS does. You take your massive multivariate data set because this right here is 1, 2, 3, 4, 5, 6, 7, 8, 9, features. And we just took the measures of their dissimilarity versus each other. And then we used that to project into a lower-dimensional space. Okay, so we're going to project into a two-dimensional space. Here's multi-dimensional scaling. I got Boston, San Francisco, LA, Denver, Chicago, 100%. I'll tell you what, this example honors precisely the pairwise distances between the cities. Am I correct? This model is correct with regard to that, but the distances, the pairwise distances told me nothing about the orientation. And so this methodology is invariant under rotations and translations. I don't look at my axis. Like, what is this? I don't know, 0, 0, like, it doesn't make much sense. Okay, but the point is I could do this. Okay, now this is pretty cool. I didn't have information about translation and rotation. Now let's take this and imagine if I had original features, and I can calculate a distance matrix, I should be able to calculate this projected space where I minimize the distortion in the dissimilarity pairwise between all of the data. That's what I want to do. Okay, so let's do that. This is going to be a non-linear transformation. That PCA stuff is an orthogonal transformation. It's honoring the, like, everything, nothing is skewed, like it's just rotated, right? This is not, no, no, this is going to skew. This is going to be non-linear dissimilarity-based dimensionality reduction. Dimensionality reductions perform such that the error in the sample pairwise distance of memory minimized, sorry about that. So we'll formulate it as an optimization problem. This is the true pairwise distance. This is the pairwise distance after my transformation. Let's minimize the distortion. Let's minimize the square difference between those two. That's exactly what MDS is doing. It's using this as my optimization problem. Okay, I jump a couple of steps, or I just, let's just look at what it looks like in real world. So this is an amazing paper. My good friend, Professor Karris at Stanford, works in spatial data analytics, really cool guy, and he had a student. And what a student did was fascinating. They took a bunch of different types of reservoir models. Okay, so a bunch of you know about reservoir modeling. And so you know, there's a bunch of different methods. Virogram based used the Virograms from yesterday to build models, which is really cool. Object based uses object templates, and MPS uses multiple templates. This is a new technology, just available in patrol in the last couple of years. And you know, there's a bunch of people who developed that. Okay, what they did was they took all those models and they calculated a bunch of metrics. The metrics that they used for distance, a high dimensional space was to take all of the models and calculate the global distributions of porosity, permeability, the local cluster locations. In other words, where do you have high permeability, high porosity? Where is that in the space in the model? The flow behavior. So they had flow response. They did a flow simulation. They saw which models flowed with a earlier water break through what was the water cut at a certain time and that kind of thing like a water flood. And then pixel-to-pixel comparison. They actually compared the models. Locally to sorry about that. Locally to see if they're different from each other. Okay, now if you think about it, these metrics is a high dimensional problem because they had multiple flow, multiple spatial heterogeneity metrics. It was like 50 by 50 features to work with. A lot of features. And they calculated the pairwise differences using all of those metrics and then they reduced it to this two dimensions. This is basically collapsing the problem to a two dimensional problem. And what's really amazing is that of course you retain the labels on all of the samples. So the verogram cases are red. The object-based models are green and the MPS models were green. The object-based models were blue. If you look at the result when you project it into this lower-dimensional space, what does it tell us about the degree of similarity and difference of the different reservoir models with regard to flow and heterogeneity? Are the verogram-based models similar to the MPS models? To the green models? Are the object-based models more similar to the MPS models? I see people nodding. Thank you very much. Yeah, yeah. So isn't that cool? In fact, when it comes to heterogeneity and flow performance, some of the blue object-based models overlap with the MPS models. You have a model up here, object-based, object-based, object-based. They overlap with each other. They have similar types of flow and heterogeneity. The verogram-based models behave distinctly different than all the other models. The other thing that's fascinating is look at the size of the space explored. The verogram-based models explore this size of space. The object-based models, this MPS, this size, you might argue that the verogram-based models explore a larger space of uncertainty. They see more. They have more differences between them. Okay, so what could we do with this? You could do this with your models. You could also imagine if you had wells and each of your wells had many features. The average velocity, TOC, completion information, number of facts, you know, you could have completion and petrophysics and geophysical information all together. Then what you could do is you could run multi-dimensional scaling, project this into a two-dimensional space and you could visualize how all your wells clustered up. Now, if your wells have a natural label, you could color code the map with that. Like maybe one of them comes from Formation 1, Formation 2, Formation, you know what I'm saying? You could look at and see how they overlap. You could be thoughtful, you could say, okay, I'm going to use a bunch of production metrics here and now I can compare wells and see if they have differences when it comes to production. Any questions about multi-dimensional scaling?
 Now, I have to admit, I don't think I have an excellent example of multi-dimensional scaling on my own. I think I've demonstrated it. I would love to work with anyone who wanted to try to test it on a more interesting data set. I think that'd be interesting. Oh, I promised some fine print. Let me make one comment. With principal component analysis, you could project into the lower dimensional space and you could go back. So you could do your modeling in principal component space and you can go back. With multi-dimensional scaling, there's no back transform. There's no way to go back. Now what does that mean? It means if I do MDS, I can visualize the data in that space, but I can't pick a point that's not a sample and say, what would that be in the original space? You can't do that. You can't go backwards. So it means you can't really use it to support a predictive model in that space, but we'll use it to do inferential learning. Okay. So multi-dimensional scaling, I put a bunch of information here about what we're doing with it and let's go ahead. We'll do MDS. The MDS method, we can use the random projection approach to be able to do it. So scikit-learn has random projection. We're able to do MDS with these approaches. So let's go ahead and we'll import all of the standard C-born and numpy impandas for plotting and the sci-py and so forth. Let's go ahead and load our data up. We've got our multi-dimensional data set we've been working with before. Ah, can we do the kernel reset and clear outputs? I forgot about that. My bad. So we've got our head come back up here and import the packages. Okay. Load the data and preview the data. Okay. So we got the, this is the same multi-dimensional data set that we had multi-variate data set. We have a bunch of features. Now if you're going to do MDS, it's kind of cool if we can color code. So we're going to do some color coding. I'll show you that right away. Let's do a transpose of the summary statistics so we can now see all of the summary statistics versus all of the features. That's nice. We can look at that. We've done this before. We're experts. We've got to do a min max normalization. We've got to do that again. So we're going to calculate the minimum maximum values. And we're going to go ahead and plot the data and take a look and see what it looks like. Now I'm getting a little bit more fancy here. I'm doing a pair plot. And that might take a little bit of time. Okay, here's the pair plot. We could have got more fancy, but it's the matrix scatter plot. Not a big deal. We've done this before. Okay, other ways to visualize your data. Let's go ahead and truncate our data. We have low, medium, high and very high production. Because if I'm doing MDS, I want to see how high producing wells cluster versus poor producing wells if there's natural groupings or something. Now I could change the color code and look at different aspects, of course, and learn more. Okay, so now I've applied my truncation, low production, medium production, high production, and there's very high somewhere else. Okay, now we can go ahead and now we'll do another pair plot. You guys probably beat me to this one too. Note that ran pretty fast. Okay, so let's go ahead and we're going to do multi-dimensional scaling. But we're going to work with low, medium, high and very high production. This is porosity, acoustic impedance, TOC. I wanted to do MDS going from three features down to two features because we can kind of understand that better. So how do we think about this? You see these pairwise scatter plots? Now imagine we're going to produce one scatter plot that does the best it can to preserve the full three dimensionality of the problem. And that's kind of cool. So we'll get one scatter plot to evaluate. Okay. Now I'll zoom back in and that gets a little hard to read. Okay. I hope you can see that. Okay. So once again, scikit learn manifold import MDS. MDS is our multi-dimensional scaling. Number of components, two, we'll do MDS with two components. We'll go ahead and train it with this data right here, one step, and then we get the result. So we do that. How long does MDS take? No time at all. What do we have now? We now have our 200 samples in a two-dimensional space. That's cool. So now we have all of our samples in two-dimensional space. We'll go ahead and add MDS one and MDS two. Original features. We worked with what? Prosthie, Toc and Acoustic Components. Prosthie, Toc, Acoustic Components, and now we have MDS one, MDS two. So that's projected into that space minimizing the disruption in the distances between those. OK, so we can go ahead and let's go ahead and take a look and see what that looks like. OK, this is one plot describing the full three-dimensional relationship between all of those features. OK, collapsing it into 2D. Whereas before, you really had to look at all three of these plots to be able to see the projections in three space. Here we can see it all in the best we can do while preserving the pairwise distances between all of the samples. OK, what do we learn from that? What we can see and is interesting is we do actually have pretty good separation of low, medium, high, and very high wells. When it comes to this three-verbal in this three-verbal space that we actually have a pretty good separation. The one thing I'd suggest is that we do have some example wells that seem to fit with the medium producing wells. And this is exactly what you do with MDS. Everybody does MDS and what they do is they identify interesting samples, then they label them and they look at them. And they say, why is this well sitting in the middle of the medium production, but it's a low production well? You see that? That's very powerful because imagine we could have done that up here. We could have done that with prosody permeability. But imagine if we have a ten-verable feature space, we can't do it anymore. We can't visualize that. But we can project it in a way that we preserve the distances in the high-dimensional space as best as we can. And now we can identify those wells seem weird. Something's going on. Did something go wrong with the completions? Who was doing that? What happened with that well? You know what? Did they use a different technique that something changed was their different decision? Or maybe you look at a case like this where you have a well that's really grouped with the medium production wells, but it got good production. Maybe we got to identify that well. We got to figure out what did we do right then? Was there something, a different strategy? Something different than happened. Can you guys see how this could be very powerful? Now, remember, you can't go back. So it's not a predictive model. I can ask, what if I drill the well here and back transform it? You see that? There is no back transform, but I can project the data here. The labels are here. Those are the samples. Those are my wells. So I can, in fact, label everything. I could label every one of these data. I could color code them or label them by the depth, the sinewacity of the well. I come up with a deviation that I have while I was drilling horizontally or something like that. Straightness. Okay. Any questions or comments about MDS? Anybody have a data set that you think would be fun to try this out on? High dimensional data set you're trying to find patterns in?
 So, in this lecture we're going to talk about panel. Panel is a dashboarding tool box and application development solution built on top of bokeh. So, most of what you can do in panel, you could do with just really low level bokeh commands, but it would be really verbose. It would take forever to write all the code that it would take to do some of these things. And panel makes it much more easier to do these kind of things. So it facilitates plots with user-defined controls, property sheets for adding parameters in a workflow. This is like data entry type stuff. If you really like, one thing people do like about Excel is just the data entry aspect of it. You can just type numbers into a cell and hit enter in everything updates. And you could build a similar toolbox based on panel where you'd have a format you'd fill out. And then once the values change, the plot would update. All panels for simulations are experienced, custom data exploration tools. That's kind of what we're building or going to build dashboards for reporting. And then like, you know, data rich Python back web servers. So again, what I mean by that is you can have sort of callbacks into Python from your application that you're viewing in a web page. Your interactions with that can cause callbacks that call back into a running Python kernel for form a computation of any level of complexity that you can do in Python and then have it update the panel application. So my scrolling is messed up. Pardon me for one second while I try to get my scrolling fixed because without scrolling, I won't be able to make my favorite comment. It should be working. Well, you can kind of align with the issue. I think I'll be able to see it on the next slide. All right. So what I've done here, and you'll just at the pardon, the fact that this is off the bottom of the screen right now or not completely displayed, but the next slide should fix that. So we just created a function. So all I did here was create a function that creates a plot and creates a map plot live plot. So the function takes two arguments. One is required and the other is optional. The required is the API number. So we give this function an API number and it will create a plot call our database to pull in a production from that API. And then we have a second argument that takes the default value of oil, but you could alternatively change it to gas if you'd like. And so it didn't once we've done the query, it pulls off either the oil or the gas and then just plots it. And if we want to make that interactive, all we have to do. So we have this function create plot and we could call this function on its own. And maybe well, if you're interested, I can show you how to do that. But I mean, there's nothing more than just typing open in a new cell, typing create plot and giving it an API number. Then if we want to make that this interactive, then all we have to do is wrap it in panel interact. So we have this function create plot. We wrap it in panel interact and then the options to that just become we define in this case only two. But we define the API numbers in which we want to have a selector box to display. So in this case, I've only written two, but you could write out a hundred whatever you want. So in this case, we actually have. If you watch this, it updated, right, it updated the label and it plotted the curve differently. So again, I apologize that it's partially hidden and my favorite sort of comment about this is like I'd like to see you do that in your PowerPoint presentation. Because again, this is my slide view of a Jupyter Notebook. And I have interactivity that is calling making live database calls updating the plot on the fly. So this is just a little bit more complicated example that also shows how you can have explicit control over the placement of the widgets. So in the first example, all I did was wrap the function in panel interact and it just took the default values and gave me that little widget, right. In this case, what I do is I define a longer list of APIs just so we have more things we can select from. In this case, I wrap that plot same it's the same function create plot. I wrap it. I pass in this API list as the arguments to the API. And I store that as a variable I and then I call this pp, you know, pretty print function on I. And that tells me what is contained in the eye object is I is an object and let's see what's inside of it. Right. So it's sort of like a matrix. So in the zero zero entry of the matrix in the first column. We have a selector box in the first column first row. We have a selector box called API with those options to find up here. And the default value is just the first one in that list. And in the second row, we have a mat plot live figure. So what I'm going to use is these indexes to control the layout now pant so panel has all of these layout functions. Basically, it's just you create a grid by creating rows and columns. You can also put spacers in, you can use mark down text and other things. So what I want instead of the default. So what's displayed here is the default values. And I'm going to rearrange them right. So I'm going to take. I want the the ner one zero. I will. So I one zero is the mat plot live plot. So put that as the first column in a row. Right. So I have a panel row. The first column thing I'm putting in the first column is I one zero from this matrix. That's the mat plot live figure. And in the second column of the row, I'm going to create a hundred pixels spacer. So that just adds this extra space here. And in the third column of the row, I'm going to create a column. And in the first row of that column, I'm going to put some mark down text. So remember our mark down syntax that double asterisk create bold. So we put some mark down text. And in the second row of that column, I'm going to put the zero zero entry. That is the widget, right. And then that gives me this kind of thing. Now you can see all the all the values are there. And select one. And there you see it updated updates the label updates the data. It looks like I kind of cheated here. I just I just made the longer list, but it's it's just the two API numbers repeated. I didn't go get a bunch of different API numbers, which would be easy enough to do, but I didn't do that for this example. I just repeated that. So you have a lot you can see there's a longer list here. Okay. So that's just using the default panel interact and then rearranging the default placement. And also have explicit declaration of widget. So the fact that I fed it as values of potential values for API, I fed it a list of strings. Well, in that in that case, it naturally, you know, it's going to select the best widget to display that and that that's this object selector box. But if you if you wanted to have, you know, explicit control over exactly what the widgets and you have to go to the panel. Well, actually, I have a slide here that shows most of a lot of the widgets that are available, but you'd have to go to the panel documentation to see them all probably. So in this case, I explicitly define what the widgets are. So I want to have a selector box for the API and now an additional selector box, what that I call reservoir fluid and its options are oil and gas. Okay. So then I take that. I define a new function called reactive plot that just wraps my old create plot. And the reason for that is that I just wanted to appear on this screen, where I use this function decorator here. Okay. So basically I'm putting this function decorator on wrap on this reactive plot, where this explicitly defines API. The API to be this drop down selector box and the OG, which is, you know, just meant to stand for oil and gas to be this drop down selector box. And I'm just passing those into this function. Then I just use a layout again. So in this case, I create a layout called widgets, where I'm where I'm where so I'm actually creating this one first. The widgets, it's again panel column where the first row is the text, the mark down text, the second row is API, the third row is OG. That those are the names of the widgets. So that creates this. And then I have another one called side panel, which is a row where the first column is reactive plot and the second column are the widgets. And so this should be fully reactive again in my PowerPoint slide. And you can also select oil or gas. And you notice over here the gas production, you know, the label change when I selected gas. So it changed to instead of before it said oil barrels. Now it says gas MCF. You can see it put change. So that's how you could explicitly define widgets. Now this is going to figure out what's going on with my scrolling. So I'll just look at it this way. So these are, this is just an example. There's a package called param, which allows for. Essentially declaring parameters as objects to a class that inherits from param parameterized. And any object that you declare in this way will automatically have a widget associated with it. And this is actually what we'll use this later. But this example was also meant to just show all of the available widgets, right? So you can have, you know, a float where you can type text in here and there's no bounds on this. You can also have, you can also set bounds. So that, you know, the user can't type invalid values in here. This is just a display box. It's not editable in any way. This is a string value. You can have slider bars. You can have slider bars with hard bounds. You can have ranges, integer ranges. You can have date ranges. You can have Boolean values. You can have color samples. Date boxes, selectors. We've already seen. In this case, you're actually selecting functions. You can have a list where you can select multiple items in the list, like that. And select files. You can have, you can select multiple files. You can click, you know, have buttons that you click that would instantiate some type of object or something like that. And so this last example uses that parameterize to update a text box, which is going to allow us to select one of three states. And we could add more. There's no, you know, this is just demonstrating the capability. So you're going to select New Mexico, North Dakota, or Texas. And if we do that, then. So what defines that is simply this right here. So I have a class that we call select state and ticker. And it inherits from Param parameterize. So then I define a class attribute, Param from object selector where the default values New Mexico and the choices are North Dakota, New Mexico and Texas. And then what I do is find the default tickers for the default that value, right? So the default value is New Mexico. So I go ahead and call into our database to find the tickers, you know, basically the operators that we have in our database operate into Mexico. And then just, you know, I use the cat tickers by state read to bring it in. That brings in a set of data that I want the parent ticker. If you looked at the data frame, you'd see that there's one called parent ticker. That's the column I want. Then I just want to ensure that that they're, they're strings. In this case, I know I think that's not necessary, but I'm just over killed, I guess, just doing ensure that they're a type string because these objects selector boxes in Param in panel need to be strings. And then I convert the what's a numpy array to a Python list with this to list command. And then I'm going to do a list of default tickers. Those are the tickers associated. These are all the operators that operate in New Mexico or have it at some point. There's some API number associated with all of these operators in the state of New Mexico. Okay. So, so that's all I need to create the two selector boxes is those first three lines of code. So, I remember when I, when I select North Dakota, I want tickers to update based on, you know, the operators in North Dakota. It's a different list. I mean, obviously there's a lot of overlap. Some of the same people. But, but I want to get the correct list from what we have in our database. So if I select a different one, right. Obviously, if I select text is going to be a big list. So, so there's a list of all operators that have ever operated in Texas, right. And so then to get that to update, meaning that I want the values of one selector box to be dependent upon the selection, the active selection in the other selector box. So, I'm going to add that with this wrapper, parameter pins. So basically what this says is I'm watching state for a change. So if I change state, this function gets called. Okay. And then this function takes the current value of state. And then the database, you know, tickers by state gets the new values of tickers sets them to. So the objects of. Param ticker. And then also just select the default or the active selection to be the first one. And so that you see click on this as a check mark by the first one. So if I, if I update this, the first person, the first operator in the list may not be the same for every state. So I can't just hard code an actual operator in there because I don't know that every operator operates in every state. So what I do, just as a default is just actively select the first one. Right. So that when I change states. So this update, I think ADR is the same in New Mexico and let's go back to Texas. As this changes, the active selection will take on the value of the first one in the list. So you see the check mark there. That's the active selection. Okay.
 Now, let's talk about Canier's neighbors. Now, there is a motivation for Canier's neighbors when it comes to a congenital introduction into the idea of predictive machine learning. First of all, Canier's neighbors are completely interpretable. It's very, very straightforward. In fact, what's really cool, it's analogous to the idea of spatial mapping. And I hope you'll see that. It's very much like a way we would use to do spatial mapping, i.e. interpolation between available well data in space. The other thing too that I just love about Canier's neighbors, there's one hyperparameter, really just one hyperparameter, k, and that one controls the degree of complexity. And it's so understandable. So the whole idea of tuning k is going to make perfect sense. So it's a great way to introduce the idea of hyperparameter tuning. Okay. So we're going to get into the idea of Canier's neighbors. And we have to introduce the idea of mapping in feature space. Now, everything's a space, Gaussian space, predictor space, feature space. What do we mean by feature space? I'm simply saying that if we had one predictor feature, hey, do you guys remember, are the predictor features, the inputs or the outputs? So the predictor features inputs for a model one and two. Now, if we work with just two predictor features, we can visualize the feature space, the predictor feature space, like a map. And if you think about it, we've run an experiment, we have training data where we have a variety of observations. And each one of those observations, we know the response. This is labeled. We know that at this combination of input for feature one and feature two, we know the response at that location. So building a comprehensive model that can make predictions for any predictor feature one within that range and predictor feature two within that range is analogous to building a map in the feature space. Okay, if you could imagine contouring, if I contour this up, I would end up with a good predictive model I could use for any combination of inputs. Okay, so this is the concept of mapping in the predictor feature space. Now, let's just think about how we could do that. Now, one way we could do it is we could come up with a really simple parametric model. You remember parametric models. Those are the ones where you sacrifice flexibility. In fact, there was a question raised where I think one person suggested is that really machine learning because it's parametric, a linear regression because we had to assume it's not so flexible anymore. It's not able to fit any possible form of the natural system, but we could we could use a parametric model and multi-linear regression would be a parametric model. Now, if you go back to your kind of fundamental statistics, you remember multi-linear regressions really like fitting a hyperplane or in this case, it's too dimensional. So it's just a plane through the data and it'd be best fit plane and so forth. And so we could do that and that'd be one way to map. Now, that'd be pretty inflexible. It'd be parametric. So we could do it with very sparse data too. So there'd be some trade offs. Or we could try to do and so recall with the parametric approach, we make assumptions about the functional form. We gain simplicity, the advantage of having just a couple of parameters. With that parametric model, there's just three parameters and we can fit the whole thing. The model is generally very compact and portable. And this is the linear model right here. While if we do it in two dimensions, we just remove that. This is what the model would have been. But there's a great risk that the F hat, the estimate of that natural system will actually be two different than the actual natural system and we'll get a poor model. In other words, high model bias model bias. Remember, due to the simplicity of our model, can't fit the natural system. Now non parametric models make no assumption upfront about the functional form or shape. They don't assume that the system is linear or maybe that it's a second order polynomial fit. Or any other type of assumption, they allow the data to control the form of the model. The model's flexible to change to the data. It's form. Okay. Those less risk that your F hat is a poor fit for the actual natural system. This should be a hat here. The estimate of the function will likely be pretty good. Now typically you'll need a lot more data to get an accurate estimate. And remember, non parametric really is parameter rich. There's no free lunch. So don't don't think that if we have a non parametric model that it's a very simple problem to try to estimate. No, no, it means more parameters. It means that we're going to need more data to train all of those parameters. Okay. So we could think of that. Let's think about a non parametric fit to that predictor feature space. And you could imagine if the data I show them all as red dots, but you could imagine this could be a low response. And this could be a high response. And it would be really cool if we had the flexibility to build these types of contours going from low to high in this region. And that would be a really nice flexible model. As long as we can support it with available data. So with that idea of a non parametric mapping and the predictor feature space, we're ready to get started with canierous neighbors because that's exactly what this is going to be. Now, if we were to think about alternative methods to form that map, the map that we just kind of showed with contours. We could do geostatistical methods. In fact, that would be kind of cool. You could do geostats like creaking. Who here is ever done creaking geostatistical methods like ordinary creaking, simple creaking, collocated co creaking. Anybody out here by show of hands. We could definitely spend more time talking about I know one day to we talk to introduce creaking. But we could definitely talk about more applications and opportunities for geostatistical methods inverse distance. We suggested that as a straw man. We said, well, you could do inverse distance. But it does encounter spatial continuity and redundancy of the data. This is firing a few neurons. Maybe. Now moving window average or convolutional methods are commonly used to build a map. And that is analogous to exactly what canierous neighbors does. It's a really a convolutional based method. Okay. Now, just to remind people because it is convolution, what is a convolution operator? What does it really look like? It's not a big deal. The mathematics looks a little bit nasty. The convolution operator shown right here. F convolved over G would look like this right here. Where in fact, what we're doing is we're integrating between negative infinity and infinity of a waiting function. This is a waiting function like some type of moving window waiting function is being applied to this function G of X plus delta. And so when we do this operation, really what we're doing is we're just applying a smoothing window of some former shape and we'll talk about the possible shapes to the original G of X function. Okay. So this is useful. A lot of times in different signal theory, people are doing this in order to remove noise to extract different signals at different scales. And in fact, it can extend to any dimension. I show right here a very simple case. This is technically one dimensional convolution. Well, of course, you can do a triple integral and now you're doing three dimensional convolution. You can go to any scale to do or any dimension to do convolution. Now I feel like the math is great. It's, you know, a very pure way to communicate, but I do like backing it up with visualization. I will beg your pardon and please, please don't be too picky. Yes, I handrew this. So please, if anybody here has a convolution approach that they can apply directly to my screen, you will find that it's wrong, but it's a least representative. If this was your original function and once again, imagine we could be working in 3D 45, the high dimensional space even. You could go ahead and you could come up with a waiting function f right here. And this would be a uniform weighted function. In fact, it would just say that if any data or any information falls within this window, it will get equal weight and the average of all of that information will now be the estimate at the centroid of that window. Now if I convolve the function G of X with the waiting function f, what will happen is you can imagine I could take that function right there. Let me just go to a pointer. I can take that function right there and I can position it right here right on the edge of the data. And at this location right here, I would simply take the average of all of the function G of X within that window. Then I can move over here and move the window. I move over here. Now the windows over going from here to here. All of the averages, the average of the G of X over this window from here to here would be the value I put right there. Now if I do that, I so I take the average from here to here and I calculate a value and I put it right there. If I apply this uniform weighted window to this function right here, it would smooth it out. It would be like a moving window smoothing of that G of X function. And so the f convolved over G would result in this function right here. And it's like if you know someone's more geophysical think of a low pass filter. You would see the general trends of this signal reproduced down here, but the more wavy noisy stuff is going to be smooth out or removed. Okay, so this is a convolution operator. Okay. Now here's the problem. We don't have exhaustive data. I can't build my map by convolving over a exhaustive representation of the response over the predictor space predictor feature space. In fact, if I could do that, why would I do this at all? I don't need to actually build the model. I have the response at all locations. My model would be the ultimate non parametric model. It would be sampling the phenomenon exhaustively, which would be kind of cool, but we can never do that. Okay. So now what I have is I have a bunch of sparse points along G of X. I don't have all of the values at every single location. So now what's weird is if I take that moving window and it's a uniform weighted window. What'll happen is if I put the window right here. I will catch this very high value here. But if I keep moving, moving, moving, and I keep estimating, convolving to this location right about here, this high value will fall out of the window. Bumit's gone. It disappears. The result is that as I do the convolution, there'll be a jump a step because a high value left the window and now the average within that moving window just drops down. And probably it's being driven down by this value right here. And as the moving window proceeds and I go further and further, it's going to jump up when this low value leaves the window and these high values are now dominated. Okay, because we're using a local average within that radius. Okay. Now this is a problem because I don't want to build a discontinuous prediction model. There's no reason to believe the natural setting behaves like that. In fact, I'm more likely to believe that this is an artifact of sparse sampling. And possibly an artifact of noisy data. In fact, this value right here might not actually be a reasonable value. It might be noise. Remember, we have random noise within our model. Okay, so this is this is a problem. So let me ask you, how can we get around this problem of having discontinuous behavior because of perhaps noisy and sparse data. Anyone got an idea of how we could better build our convolution or waiting function. You know, it's the Gaussian disease. You go Gaussian. I totally get it. Gaussian kernels are awesome. And I love that idea. That's a great idea. Thank you so much, Adam. What you could do is you do it. Your Gaussian waiting window. It would be shaped like that that I drew. I hope I drew it pretty well. A Gaussian. And now what would happen is that waiting windows that's moving along. You'll smooth up these discontinuities because a low value will slowly start to add weight. I'm right here. This value starts to enter the window. It will just have a little bit of weight by the time I get centered over this value, it gets maximal weight. And then as I move away, it starts to lose its weight. That would smooth out these jumps. And that's a great idea. That's a great way to deal with this type of noise in the surface. The methodology that's employed within scikit learn is in fact a inverse distance approach for the waiting windows. So it wouldn't be bell curve. It would kind of have this shape right here, kind of hyperbolic type of shape to it. So that's they don't I don't believe scikit learn actually does the Gaussian. A very simple moving window would be a triangular one too. That would definitely help smooth it out. But anything that purplies less weight on the edge of the window. And more weight in the middle would definitely deal with this problem quite well. Okay, so if we did that, we applied our triangular waiting window so and it's okay because convolutions completely general. You could put the equation for your triangular distribution here into your waiting window and you can convolve it. You can mathematically do that. That's fine. You can solve it numerically. Okay, so the thing about so the result is now we get a nice smooth response. So what have I explained? When we do convolution to do our prediction here, we have a choice of how far we go away to find data and we have a choice of how to build the waiting window. So that's going to be our choices. Okay, so now we understand intuitively what those choices mean as we start to build the model or ourself. Now there's something weird. Those tricky machine learning people, they came up with a new idea. They said, well, let's go ahead instead of setting a radius, let's do a K nearest neighbor. In other words, I'll go as far as it takes to find K neighbors. So you indirectly control the size of the window. Okay. So we're going to need to in order to find out what's the nearest neighbors. We're going to have to actually use some measure of distance. So this is kind of interesting because what's happening now is this predictor feature space. This could be porosity. This could be permeability. But nevertheless, I need to calculate distance in that space. Now does that seem weird to anybody that I'm actually going to be calculating distance between samples based on the prosties and the permabilities. It's kind of strange, right? And if you think about it, you clitian distance, the general equation for Euclidean distance, first of all, can act in any dimensionality. You can do Euclidean distance in 10D. That's possible. In fact, the general form is just this right here. You do the square root of the sum of the squares of the differences across all of the coordinates, or in this case, predictor features. So in other words, if I want the distance between these two points right here, I just have to calculate the difference in predictor feature one square it plus the sum of the difference between predictor feature two, that that vector right there. And square that and sum that up, take the square root. And that's my distance in predictor feature space. So this is pretty cool. I can actually do this to locate at an unknown location right here. I want to make a prediction. I could pick the one, two, three, four, five, whatever 10 or so, maybe more. Nearest neighbors by calculating the distances and making sure I pick the closest ones relative to distance. Okay, so far so good. Any problems with doing distance in predictor feature space? Imagine the arbitrary decision to present porosity as a fraction versus a percentage. That changes that once again that would change the variance of that distribution of porosity by 10,000 fold four orders of magnitude. Okay. Now imagine we're doing a distance calculation and an arbitrarily decide to go between Darcy's and Melodarysis or whatever. You'll make one predictor feature dominate the distance calculation. Okay, actually the best way to visualize it is imagine drawing this plot to scale where you actually show this is going between point two and point. And now we got two different methodologies for standardization that are commonly employed not a big deal. They're very similar. It's all about just getting everything with the same amount of variability and range. Now you could constrain the range. And I like that for visualization. It's called a minmax normalization. Psychic learn does it all automatically. It's very cool forward and back transform very easy. So I can take any feature and transform it to have a minimum of zero and a maximum of one. The minmax normalization is really easy. It's literally like a ratio. The value, the specific value at a sample minus the minimum divided by the range of possible values. And that'll give you a one when you're the highest value. A zero when you're the absolute lowest value in the data set and everything else in between. Now standardization uses what's known as an a fine correction. It's a squeezing and stretching and shifting of a distribution with no shape change. It's goal is to correct the mean and the variance. In fact, if anyone's interested, remember this equation. It's so handy. Many, many times I see this get used within oil and gas. An example would be if you get the shape of the distribution from course, because you have good reliable course for the porosity. But now you have a more complete representation of the range of outcomes or better sampling from log porosity. It's pretty common to take the log porosity and transform it to honor the core based mean and standard deviation because that's more reliable, unless perhaps less biased. Once we do devising though, we always got to do that. Okay, so we can use this tool elsewhere. I had to throw that in now. Imagine what would happen if we go from a coup to competence 2000 to 6000. Now I apologize. This should be 10 to the third power. I keep forgetting to change that. So anybody here if you're a geophysicist, you're going to notice that error there. Okay, quick question. When you scale 0, 1, what happens when you have a new sample data that is larger than original say, aha, thank you very much. This is a very good question and let me just say it like this. None of the transformation methods were designed to work in real time. Okay, you see what I mean by real time real time suggests that you have a transform that any new observation can be applied to and you could keep getting new data and just keep applying the transform. Transformations require a static snapshot of the data. Okay, if you get new data, you need a new transformation. You need to reapply the transformation. Even this standardization right here, if you look at the equation, it actually needs to know the original standard deviation and the original mean. That's the original right here. And these are the transform standard deviation. The target transform mean. So in other words, to do this a fine correction right here, a standard standardization, I need to know a priority. What's the mean and standard deviation. Okay. Now, there are a variety of machine learning methods that actually are developed to work with real time data random projection is a method we could cover at any time. If you guys were interested in something like that, it's a really cool tech and I mentioned it during the PCI PCA discussion because it's a form of dimensionality reduction. Okay, so if you have is 2000 the 6000, thank you for the question now. And you go from 0.12 to 0.28. In fact, if you think about it, any distance in this coordinate will seem like it's no distance at all. It's just close. This thing would collapse to a line. In fact, if you drew it to scale and your distances won't make any sense. Now we're going to play with that. I'll show an example that shows that by doing the min max normalization 0.101. We're in a great situation now. We can rotate a vector of same length in any direction and it will always have the same length like it on paper. It has the same length and the calculation will actually have the same length the distances will be preserved. Okay, we have to do that. What happens if you don't do that. I think this is pretty cool. Here's a standardization on a geomechanical property and unconventional, a brittleness ratio. And a standardization on porosity. Now they span from about negative three to positive three standard deviation of one variance of 0. And if you look at the resulting K nearest neighbor model and everything we do is going to be in 2D so you can visualize the entire model, which is very convenient. Can you see that this is isotropic? The resulting model has some features in it and I'll talk about those in a bit. But in general the shape and form of the model is isotropic. Now look at this. This is subtle. But we use the porosity in the original units as a percentage. And we use the brittleness in original units as a ratio, a percentage to. And if you look at that, because this goes between 0 and 100 and this goes between about, I don't know, 3 and 28. Do you see the strations? These strations are caused by the fact that these distances are more meaningful and it causes this correlation structure. If you go back to verigram calculation, this is geometric and is trape. This is a directionality within the model itself. Now look what happens if we go to porosity as a fraction, brittleness as a percentage right here. And if you look really carefully, do you see these strations? At this point right now, this model is effectively a one dimensional model. It really is. In fact, when we're looking for the nearest neighbor, we're extrapolating this distance from here to here is really almost nothing at all. And so the model is basically just continuous in this direction. It's collapsed to a one dimensional model. And that's what's causing that noise. It's like we projected all the data on one dimension. And so you're getting a mixture of high and low values everywhere. It's causing the model to jump around. Okay, with these transformations, can you back transform? So these two transformations right here. And in fact, the answer is yes. And the answer is thank goodness we can. Because if you think about it, if I do this standardization and I do the model in the standardized space. At the end of the day, I don't get paid to come up with, you know, measures relative to standardized features. I need to always go back to the original features every time I build a model, I want to get everything back to the original features. Now, the very cool thing you'll see this was scikit learn. You can apply the forward transform. And then what it does it remembers this transform. And you could just apply a function reverse transform. And it'll do it all for you. And it'll do it on any new observations you get from your model. And so you're going to go, you're going to go. Yeah, we can't do anything really in this transform space. We have to get it back. Thank you for the question. Thank you. All right. Now I mentioned that can yours neighbors is not as simple as the convolutional approach. The convolutional approach that I showed you. It's analogous. But the convolutional approach has a set waiting function that stays constant over the whole space. Okay. It moves. It has the same radius of that moving window, right? What can yours neighbor does it says go as far as you have to to get the can yours date data. Now what's really cool about that. And I think this is a really neat thing. Is that you could imagine you have a place in your data set the predictor feature space. Where you have lots of data. It's densely sampled. And there, if you're estimating at that location, you don't have to go as far away to get five data. You could be another location to predict your feature space where it's more sparsely sampled. And you have to go further away to get your data. Okay. What does this mean? It means that this methodology is locally adaptive. It will mean that larger K. It will mean that in locations where you have denser sampling. The model will change more quickly and have more detail. In locations where you've sparse sampling, the model will be more smooth. So that's actually kind of cool. I like that. That's adaptive where it has more information. It can put more detail in. Now here's a very important thing. Larger K will result in a smoother response prediction under fit. How does that sit with people? Can there is neighbor. If you increase the hyper parameter K will be more under fit. Does that make sense? You use more data. It's less fit to the data. Does it bug anyone? It sounds paradoxical, doesn't it? What's actually happening is you're throwing more data in to get the local average. And so it will go smoother. Okay. Now let's do a thought experiment. What would happen if I do a uniform waiting window? And I have K equal to the number of data available. What would I estimate at an unknown location? The window is massive. I'm including all of the data. And I'm going to use equal weight on all of the data. My estimate is going to be. It's just the answer. My estimate is going to be. It's just the average. And imagine a model where in every single location in the predictor feature space, I estimate with the average. That's a smooth model. That's a completely under fit model. Okay. Now let's do the opposite thought experiment. Imagine if you say K equals to one. I'm only going to use one data. What value would I estimate at this unknown location right here? It's a virus neighbor estimator. That's it. And if you think about that at a data location, that would be completely accurate. It'd be a hundred percent fit at the data locations. And then away from the data. It'd be completely over fit. We're in a model in which we have no random residuals. We completely fit the data. And likely completely wrong away from the data. Yeah. Okay. So we have a choice. Thank you very much for that. I appreciate it. So we have a choice. We can control the degree of fit, the complexity of the model. More complicated models are over fit. Less complicated, complicated models are under fit. We get to control this. But we're going to control the variance bias trade off by doing that. Okay. We've already talked about convolution enough. I think I've shown enough examples. There are two parametric forms you're allowed to use in scikit learn for K nearest neighbors. Uniform in sensitive to the distance from the from the training data that are available. Okay. It's just a uniform window inverse distance as we mentioned before. It's it's I like that the inverse distance because it's not going to result in the discontinities that uniform waiting wood. Okay. nearest neighbor. I think the best thing to do. Why don't we open up the code and get going. I promised you more code today. So let's play around code.
 So this this part of the code is identically what was on the last slide of the panel exercise. What we want to do now is we want to add the ability. So now I have a state and an operator. Let's get the let's get the API numbers. So add we want to add a parameter. And in this case, we want it to be a list selector, not an object selector, a list selector. Now it will allow us to select multiple APIs at once because ultimately we want to take the active selection in this box and have it update, you know, our plot. And we might want to select multiple wells. So go ahead and take a few minutes. I won't I won't let you struggle with this one too long is maybe five or six minutes. But go ahead and think about it and see if you can't add a list selector that will take the active APIs and put them here as an additional selector box. Give that a try. If you if you weren't able to run those update commands, the yet checkout master and whatnot that we ran at the very beginning class today, you might see in your example something a little bit different right here. Just make sure it looks like what what I have. Just make sure it looks like that and it. And you'll be fine. Now the first thing. We basically need to get. The API's so all I've done is type that and I'm not done typing, but I just so we're going to have a data frame that's going to have all of the production information as well as a API by ticker and state. OK. So the ticker. This is going to take two arguments. Ticker is going to be. Ticker and its default value. So ticker this ticker dot default refers to the default value of that. Right. It's whatever the default selection is there. OK. So that's the ticker. State. It's going to be the default value of state. Like that. So that will read that in. If we call read. And. I'm just going to go ahead and remember our drop in a. I think there's. For some reason that in some of the data frames, there is some missing data and you know, just in the interest of simplicity for now, we'll just drop them right. We'll just get rid of those rows that have missing data. OK. So this will return a data frame. You know, if you really need to see what it looks like, you could copy this line of code, open a new cell and just put in the default values. You know, ticker is, well, sorry, ticker is BP. State is Mexico. So let's see. That looks like. So that looks like this. So this has basically all the information we'll need to without having to make another. Well, this is the default, but we'll need one more to update it. But this is the only database call we'll have to make it has everything we need to do the dashboard. So it has. All gas water latitude and longitude as well as API information. But if you notice. The API is a repeated, right? Because there's an API number associated with every row. And there's a row for every month of reported data, right? So we want to get the unique values, right? We don't want. We don't want our selector box for APIs to have a bunch of repeated values in them. We just, you know, if we want to select this well, we just select it once. We don't need to see it three times. So what we can do then is there's a command numpy unique that if we call that, well, let's see. Let's call it on sorry on API. So it doesn't make sense to call it on the whole data frame. We call it on API. And now it's going to return a unique set of the APIs. In other words, it's going to remove all the repeated values, right? And if we look at the. Actually, we'll leave that like that for now. So let's ultimately want it. We want this to work. So let's go ahead and put. We'll call it default. APIs equal to numpy unique. On the producing. Well, data frame. The API column. And then we're just going to. We want this to be. Of type string. And we want it to be a numpy list. So. You notice this is a numpy array, right? And it has a data type object, which is kind of the default catch all data type. We want it to be. As type string changes it to this means a unicode string. And then we want that to be a Python list. We have to it has to be a Python list because that's what. Panel once. Right. All the APIs that BP has in New Mexico. We're going to store that in default APIs. And then we can use that to. Define a new param. List selector. The objects in there are going to be the default APIs. The ones we just found. And the default value will just. Select it to be the first two. And really, I mean, there's no reason other than for selecting first two other than just to demonstrate to the user upon, you know, as soon as you see it that you can select multiple but multiple values instead of just one. And actually would just those lines of code it should produce a widget. So we execute that. So there's our widget. You can see that the first two are selected as we define there. I don't like this annoys me how this it's, you know, by default it capitalizes the the label and the label, the default value of the label is the name of the variable. Whatever you find the name to be. So that kind of annoys me that because API of course is an acronym should be all capitalized. So I'm going to go ahead and add a label to this. So label API. API. So that works in producing this widget, but of course it's not, it doesn't update correctly, right? And that's because we haven't told it to, right? We haven't written anything that would be an analogy with this right here to update the APIs. So let's do that. We're going to write a function. And the underscore in front is just idiomatic Python to imply that these functions aren't meant for anyone outside the class to call. So these aren't member functions that are intended for users to call. They're just sort of private functions that will be called internally only. And it really, the underscore doesn't really do much. It just, it's really just to for our kind of visual reference to say, oh, these are functions that shouldn't be called outside the class. So we'll call this one update API. And what we can do is we can actually copy a lot of the code. So we'll at least copy these two lines down here. And what I'm going to do is just append the variable U to mean like updated. So my updated producing well data frame is going to be this. And instead of using the default value of ticker, it's just going to use the current value, which is self dot ticker. And instead of using the default value of state, it's going to use the current value, which is self dot state. Other than that, everything's the same. I'm going to again, append to you to the front of default APIs, like that. And then I can update self dot API. That's that's the thing in my selector box here with the. Actually, just I like my variable names to be as descriptive as possible. And in this case, we're not dealing with the default APIs anymore, but rather just the API. So like the update, the use stands for updated, you APIs. Okay, they're going to be the active selection. APIs, not the not the default ones, not the ones that correspond to New Mexico and BP, but whatever the active selection in the boxes. So then we can set the values of the selector box. We get that from self param API objects. Those are the things contained in the selector box. We're going to set those equal to you API. And finally, we need to give it a default selection. So we'll just set the default to be the first one. And then the last thing we have to do is we have to add this type of decorator that is going to watch for changes not to state this time, but to ticker. So when we update the ticker, and it could be the fact it could be that ticker is updated by updating state, but it doesn't have to be from a selection. But whenever ticker is updated, we want to update the list of APIs. So now, if we execute that, if I select a different state, the ticker is going to change automatically. And now I wasn't expecting to get an error, but I wasn't expecting it to work. Oh, typo right here. Should be you APIs with an S. So now, if I select a different state, the ticker will automatically update because it goes and gets the new list of tickers, except ADR, whoever that is, they don't have any APIs in North Dakota. They don't have any wells in North Dakota for whatever reason. Well, it's not to say they don't have any wells. They don't have any wells with production. So this up here, production by ticker and state only returns wells that have production data. At least one month of production. So they could have permits for wells they haven't drilled or they could have dry holes, I guess, but they don't have any. Now I don't personally like this to just be an empty box. So it will work if we select a different company like Noble. That should update the list of APIs. I know I'm not sure why that's just one. I know Noble has more than one well in North Dakota. Yeah. This was our good E og. Yeah. There's a full list. So, but again, I don't like it too. You know, when it made that default selection, the box was empty and that, I don't like that. So what I'm gonna do is I'm just gonna, right here, I'm gonna say if you APIs is equal to an empty list, then I wanna set the Param objects to a list with a string that says none, otherwise, give me the values. So what this will do now is in that case where it's empty, so if I change it to North Dakota, that ADR is empty, right? And so now we see none. So it just gives a little more information than a blank box. So I'll pause right there.
 And again, if you weren't able to run those commands this morning, you might see a little bit different than there might be like an extra function here and then what you see down here might be a little different. But otherwise, if you weren't able to run those commands, and you'll see exactly what we basically just wrote. If you don't see this exactly, then you can just go back to the panel exercise and you can just copy this code that we just wrote. You can just copy this and paste it over everything you see here. So don't obviously don't have this other production plot. This is our this is the exercise that we wrote for Matplot Live or I'm just copying it here. It's ready to go because we're going to use it later. So just select state and ticker should be what we wrote. For the panel exercise. So I'm just going to push through for this last 20 minutes here and see what we can get done. And on on Wednesday, I'll have a really clean way for everyone to get caught up to the same spot. Okay. So if you're not don't worry if you kind of get behind or you can't get it to work, exactly as we have it here. On Wednesday, we'll all start with the same spot. So we're just going to push through this last 20 minutes or so with what we have. So basically the next thing to do is to take our list of APIs here and plot them. Right. And to do that, we're going to go back to our bouquet exercise where we had that interactive well location plot this guy. Create create well location map from the bouquet exercise. Let's just take that code. Just just the function create well location map. Take that code. And down at the bottom, paste it. But what we're going to do is we're going to make this function a member function of a new class. Okay. So above where you just pasted, we're going to start to write a new class. So we're going to have to write. We're going to write class. We'll call this one. Select and plot wells. And this class is going to inherit from select state and ticker. So go ahead and put that there. And then you just have to tab over everything to make this function a member function of that class. So we have a new class, selecting plot wells, it inherits from select stick state and ticker. Right now it only has one member function. And in fact, it's not even a member function yet because a member function would have at least the first argument would be self. Right. And then just don't forget when we want to actually run this, we need to now instantiate the correct function. So before we were instantiating select state and ticker, now we want to select and plot wells. So if you did everything correctly up to here, you can actually instantiate this, select and plot wells. And because it inherits from select state and ticker, if you instantiate it, I mean, we're not calling this member function or anything. And then we're not going to instantiate it then it actually just passes the view right through. Right. Because remember how inheritance works. If I inherit, if, if selecting plot wells and herits from select state and ticker, then all of it also inherits all of its attributes and functions. So the fact that I'm just calling param, param is an attribute that I inherited it from select state and ticker. Okay. So we copied over our function, but this function took arguments, long, longitude and latitude, right. And we don't have any longitude and latitude information available to us as of yet. So let's make that happen. The latitude and longitude is in our data frame, the producing wells data frame. So we can actually, if you, if you go back up to where we define the producing wells data frame or up here, we can just get the lat, the longitude and latitude. So longitude is equal to producing wells data frame. And we want to get it. If you looked at the data frame, you'd see that the column label is longitude underscore surface hole. Now remember. I guess I'm going to go back to the panel exercise for a second, because I had these, I had this code down here. So remember when we got our producing wells data frame, this is what it looked like. Remember it had all these repeated values. And we use that unique to get the unique values of the API's. Well, so, so what we did was we said, we called numpy unique on this. And we got that. Let's look at the documentation for numpy unique real quick. And so I'm going to put my cursor on numpy unique and I'm going to hit shift tab. It brings up the documentation. Returns the sorted unique elements of an array. There are three optional outputs in addition to the unique elements. The indices of the input array that give the unique values, the indices of the unique array that re that reconstruct the input array. The number of times each unique value comes up in the array. What we want is the indices of the input array that give the unique value. So in other words, when I look at that data frame again, the indices are over here in this panda series. And you can see like 285 is repeated. So the is only going to take the first one. So the end to see would be zero. So I could use that index to say get a different. Like latitude service hole. If I got, if I used zero, that would give me the unique value of that data, the that point. So the way the way we get unique to give us that is if you look at the documentation as you say, return index equal to true. So there's a there's an optional. Thing here return index. If I said it equal to true, it's going to return two arrays. One of the arrays will have. The list of APIs and the second one is going to be the list of indexes that give me those APIs. And I can use that. To get the latitude and longitude that corresponds to the APIs exactly without, you know, without any additional effort. So. Returning to the auto type dashboard. Where we have this unique function. What I'm going to do is I'm going to set this right here equal to return index. Equal to true. And that's going to return two things. It's going to return the APIs. And. The unique indices. I'll just call it unique indices. And. Because now I've done this unpacking what I actually need to do is I need to move this. Additional syntax to the next line. So I say unique you APIs equals you APIs. As type string to list. And unique indices will be the unique indices. That we need to get the latitude and longitude. Here we're doing it in this update function, but we actually need to do it for the default values as well. So I'm going to in both places I'm going to change it right. So up here, I'll say. Default unique indices. And I'm going to do that again carrots, right? I'll say you can turn index equal to true. Again, don't worry if you're falling behind. We'll. We'll all start in the same. I'm just going to power through these last 12 minutes and we'll all start at the same place on Wednesday. So. Now I have, I mean, my default APIs hasn't changed. I just had to correct the syntax because now it's returning two things instead of one. But now I can use these default unique index to index into my data frame to get me the longitude values that correspond to the unique APIs. And I'll do the same thing for latitude. So I'll just copy that line, change the longitude to latitude. So this sets the longitude and latitude for default. And then I have to also do that again when I update it. So when I update it, I'm also just gonna add values on here so that it returns a numpy array. Remember, if I have a pandas data frame and I want the numpy array, then I can use values to get that. I'm just gonna copy these two lines of code. And I'm gonna put them down here and update it there. So let's see, if that runs without a syntax error, then I must have a typo. Well, here I call it u underscore, u underscore. And let's see. Well, up here I call it producing well data frame. So a couple of typos. Okay, no syntax errors, but we haven't done much yet. So now we're back on this function. And now my latitude and longitude are class attributes. So I can just put self in front of these guys. Then it'll pass the class attributes in, changing and do the transformation. The rest of the function is really the same. So the only thing left to do then is to define some type of view so that I can view everything it wants. So in other words, I need to, I want to put my map next to my widgets. Right. So what I'm going to do is define a function I'll call map view. It's going to be a member function of the class, so it'll have the argument self. I'll have well plot map, which is going to just contain the plot. So I'm going to call create well location map. That'll be stored in well plot map. And then I want this to just return a panel row. With two columns. In the first column, I'm going to put self-param. In the second column, I'm going to put well plot map. And then I'll call map view. We got an indentation error. BPs wells in New Mexico. And if we did everything correctly, we should be able to select a different state. Oh, I already know something's bad going to happen. Well, I thought my map would disappear when, when those didn't update. So I'm going to put a map on the map. Well, I thought my map would disappear when, when those didn't update. But so now let's see if I can select a different operator. Okay, I did not update. So, I should have updated in this function. I'm just looking for my syntax error. Okay. Okay. Okay. Well, not seeing it right now, but it's got to be in this function because the initial, the BP wells are plotted, so the default values are held correctly. It's not being updated, which would be when this gets called. Oh, I see it, I did it again. It should be well, not wells. So let's see if we can get it to update now. Oh, that's what I was expecting to happen earlier. My map disappeared because, if you recall, when I switched in North Dakota, the default operator becomes that ADR, which actually doesn't have any production data. So there's no data in my producing well data frame, the plot, latitude and longitude are empty. Right? So the last thing I want to do, just to make this work, and you have some different options, you know, you could have it just say plot the state with no wells on it. But the easiest thing to do is just to have it not, the only have it change if the list is not empty. So what I'm going to say is, if self.param api objects is not equal to the empty list, then update the latitude and longitude. And so the map will only change if the operators are the, there's actually a list of apis. So now I changed to North Dakota. Those all change my map doesn't update. But then if I select a different operator like say EOG, then I get to see their ones. So now it's working correctly. Oops. I don't know why it keeps resizing my window. Okay, so.
 All right. So what do we do is, you know, I'm kind of wordy. I like to kind of explain things. So I put a little bit of explanation up front here and mark down with a little bit of the equations reminding us about what prediction was. I comment on the fact it's supervised learning. I don't say supervised learning enough, probably. It just suggests that we have the response feature available at the training data locations. As opposed to like clustering where we had no response feature, that was unsupervised. All right. And we have a distance metric. I show the definition right there for a distance metric. This is plain Jane Euclidean distance. Okay. Now you'll notice one thing that's kind of interesting. I should have mentioned this is scikit learn does allow you to use a different distance metric in order to calculate the nearest neighbors. Okay. And it allows a one power. Anybody know what that's called? If you have a power one on that distance metric. Anybody ever heard of that one? It's known as Manhattan distance. Okay. So Manhattan distance or city block distance. And if you think about it, it is the distance between two points. If you can't go on the diagonal, but you have to go like this on the axes. That's why they call it city block distance. Isn't that cool? Okay. That'll help you remember. Okay. So let's go ahead and do this now to create a greater degree of suspense. Go ahead and go to the kernel restart and clear all the outputs. This should run reasonably fast. So let's go ahead and run this live and try some stuff out. We'll experiment with it. Now one thing I want to point out now is that we're still going to use our good old friends pandas. And next week you guys can get deep in the pandas with Dr. Foster. It's going to be awesome with data frames and learning how to work with tabular data. We use numpy for working with multi-dimensional rays. Dr. Foster will get into details there. But we'll use it and we'll just use it as a workflow step, right? Matt plot live for plotting. And we're also going to do we're going to do train tests for the first time. I don't know if we did it yet. I don't think we haven't done train tests. We're going to do split of train test data. And SK learn, scikit learn has neighbors and we can import canyous neighbors regressor. Now regressor because it's a regression model. It's for continuous prediction. There's a classifier. It would be the categorical prediction. The output is categorical. Okay. Go ahead and import that. I just want to point out how amazing it is to me that you can open up Jupyter Notebook grab scikit learn and then just start doing machine learning with your data. I'll show you. It's really straightforward. Okay. Now I want it. I always like making convenience functions because I like my code to be brief and easy to read. And so convenience function here produces a graphical plot of the correlation coefficients. We want to understand correlation before we start. And this is going to visualize the model. This function is kind of cool because you can give it any model and you give it any data and it will actually plot the data on the model in two dimensions like a map, like that contour map where you'll see high resolution sampling of the response from the model, the predictions. And check model is like a cross validation plot with the estimate versus the actual values in some statistics plot it. Okay. Run that code. Now we get the convenience functions. I want to make sure I ran that. We'll go ahead and work with unconventional multivariate. It's another multivariate unconventional data set. I think we used this one previously. We can go ahead and visualize the first seven data samples zero through six. And we can confirm we got Prosti log perm AI acoustic impedance, burn on this TLC, Viternet reflectance and production MCF PDs. So we got ourselves a nice little data set to work with. We can do the describe command and see what the statistics say. And we'll find out that we have 1000 wells. And this is averaged over the entire well. And we have our summary statistics, some means and so forth. We got some negative values here. I did that on purpose to show a little data cleanup. And so we could go ahead and use this command right here to extract all of the data as a two dimensional array. And then we can use numpy array commands to do a really nice filtering. And then basically we have it back. Now there's some deeper details here. I'm going to gloss over such a shallow and deep copies. If somebody is a programmer, you recognize that we actually that get numeric data is a shallow copy because any changes we made here change the original object. I'll tell you the one thing about it is that always pay attention to this concept because when you make a copy of something, if you edit it and that edits the original thing you copied, that's a shallow copy. And I'll tell you in Python, the assumption is basically shallow copy. Now the result is this can cause some confusion for you in a workflow. But anyway, I promise this is not a programming class. So I won't spend too much time on something like that. Okay, so we have reset the negative values to be zero. And you can see the min values for birdleness and TOC are now zero. We've done this before. I won't talk about that again. Now I promised that the standardization was really simple and scikit learn. So look at it. We can go scikit learn pre-processing. This is our data preparation type of methods, standard scalar. Okay, so that's our standardization operator. Then what we can do is we can instantiate it. Anytime I do something like this, I have the standard scalar with its parameters. And I say equals transform. This right here is a case or an object of that type, that class, which is standard scalar. So that's my standard scalar. Now I own that. I get to do whatever I like with that in my workflow. So what I'll do is I'll take and I'll want to work with processing and birdleness as my predictor features. And I can go ahead and say use my transform standard scalar and fit the transform to my features. My features are procity and brittleness. And the result is that I'm going to standardize my features to have a mean of zero and a variance of one. Okay, and then I'll apply them to a brand new feature within my data frame called S-Pore. There's a bunch of stuff here that you're going to see much more detail with Dr. Foster next week. But what's really cool about this is this was the forward transform. And you'll notice as it was asked, I think Heather, you do need all of the data at once. And then you comply the transform because you had to calculate the original mean and variance. Okay, here's the standard standardization of procity and brittleness S-Pore S-Britle. And if you look at it very quickly, you can confirm the fact that they're negative and positive values. You can run summary statistics and you would see that they have a mean of zero and a variance of one. Okay, so we're in good shape now. We got our standardized features. Now there are people here who might be like, well, how do you reverse that transformation? Here's how you do it right here. I can take those features and I can do the my transform. That was my transform, my instantiation of the standard scalar inverse transform on those features. And if you run that, look at that. This is a two by number of data array. And if you look really quickly, what you'll observe is the fact that these are in fact, the original features. It took this value standardized porosity transformed it back to 15.91. That's the original porosity. So I ran the transform in reverse and I got back 15.91 for the first sample. That's correct. 14.05. That's correct, too. That's the brittleness. You see that? So I can go forward and back so easily. And I don't have to do any bookkeeping. My transform transform keeps track of all of the feature values, the means and the variances, the min's and the max. If you do normalization, you could do it with another transform. Okay, we're going to visualize things in the predictor feature space. So we have to set the min and max values. This is for the color bar on the response and for the min max for the range we view within the plot for the predictor features. Let's go ahead and look at the correlation matrix. Not a bad idea to kind of look at the data and do this visualization. We've already done this. I'm going to skip through this pretty quick. Because we've already done this. We've already produced this type of map. Anybody here having Dejavu all over again, right? We're having Dejavu. Yeah, we've done this before. We're experts now. We can do scatterpot matrices. I think every time you do a workflow, it really should be 90% of your time should be data preparation and getting ready, right? So I show some data prep, but really there's so much more we could have done the check and make sure the distributions are great and there's no outliers and so forth. Okay, now what we're going to do is we're going to work with just two predictor features. So we're going to give ourselves a subset of the data set that only has porosity. It's going to have porosity. It's going to brittleness and it's going to also have the production. And so now that's what we're going to work with. And then what we can go ahead is we're going to run train test split. Now I'm going to do something I told you you really shouldn't do or we should be careful doing. I'm going to do a random subset for training and testing where the train test split. I can tell it that test sample size will be 25%. So I'm going to get withhold 10% of the data and then I can use that to test my model. It's not used to construct a model. This is for hyper parameter tuning. Now I do a random state right there. Anybody here ever worked with random seeds? Why do you have a random seed? Remember no computer can actually do random drawing. They do what's known as a pseudo random number generator. And the pseudo random number generator the way it's designed, you put the initial number in and it turns. And usually what you do is to throw away the first few thousand because there's always an issue with a markup chain type of feature where you burn and chain that's not actually got the right statistics yet. And then you start using the string of random numbers. Now they're pseudo random numbers by using the same number for all of us we all get the same string of random numbers. We will all get the same subset of data to work with. Okay, so I do that. So we all have the same look. We all have the same thing to look at. So the train test split is going to randomly remove 25% for testing and we'll train with 75%. What percentage should we use for testing? Anyone have any thoughts about that? I've seen in the literature everything between about 15% for testing and about 30% for testing. And really if you think about it it's a balancing act. If I use too many data for testing, my model's not good because I took away too much good information that could have been used to train the model. If I use not enough data for testing, I don't adequately test the range of behaviors of my model. You see how that's a balancing act? So we're kind of we're kind of stuck with making that choice, right? So there is either a two-step process or a three-step process to do this. I always show the two-step process for brevity of the workflow. Okay, now this is interesting and if you think about it, it could become an end-step process at which point we're kind of getting a little obsessive. Okay, so it goes like this, the philosophy goes like this. I use 70% of the data to train the model. Then I use to train the parameters of the model to get the best fit. Then I use 30% of the data to tune the complexity of the model, the hyper parameters, right? Now what's interesting is at that point, now I just use the model in real world and I think I'm good. Now what people have said is no, no, no, no, no, no, I want to do a three-step process. I will withhold testing data and I will withhold a validation data set. Now the thing I should mention is a lot of the people who do the validation, in fact, call the validation data set what we call testing data. Okay, so I should explain it like that so I don't confuse people. So what they do is they do the training with the training data, then they take a small proportion of validation data that they will use to tune the hyper parameter and then they have an ultimate data set that was left out the whole time and they use that to now see how the model behaves in real world. You see that? And the reason they like that is because that testing data for them was never used to fix the model because what they suggest is your testing data is used to tune the hyper parameter. It's used to influence the model. I want to see what happens when you use data that has no influence over the model. They fill that's more of a true test of the model. That's the three step process. Does that does that make sense? Now now this is where I said it can get a little crazy. Now what do you do if you do the three step process and you test the model and it doesn't perform very well? Do you use the testing data to make improvements on the model? And if you do that now you need a second testing data set to test it again. You see that what I'm saying that you're going to keep going on a cycle. Okay good. Okay so we'll do random withholding of train and test data. Now it's always a good idea to visualize that. It's always a very good idea. In fact my our focus should be on a fair test train split. Now what would be unfair? If I look at the distribution of porosity for training and porosity for testing, if I saw there was no overlap, that the training data was at this part of the distribution from here to here, but the testing was going on over here in this range of the porosity values, that would be unfair. I'm now asking the model to extrapolate. So I want to check that. I want to make sure that in training I have good coverage and in testing I have reasonable coverage. So I'm actually testing other different aspects of the model. Okay the other thing too is I think this is more important. We always want to kind of think more multi-dimensional. A better way to check training and testing is to look in the full dimension if you can. And if you can then you can observe immediately that this is porosity, this is brittleness and they're both standardized. We've already done that. And this is the production metric right here. And so what we can see immediately is that there's a high production here, a sweet spot, lower production here, and there's some non-linear already going on. We can observe that the testing data comes from these locations right here, a 25% subset of this original data. Okay let me ask you this, is that fair train test? Are we looking pretty good? There's a little disturbance for me right here. Because if I look here at that test location, there's no training data near it at all. And so maybe a best practices and look down here too. You have a few samples here. A good idea would be when you apply the test and you check the test results, maybe pay attention to what happened at those locations. In fact, a super awesome epic idea would be to plot the error in the test at the locations of those test data. And then we could kind of understand. You know, if we refer, I'd hate to just look at an average statistic over the whole thing and say, it didn't perform well, but I gave it some really hard samples for sure. Okay. Good, thank you very much for that. And sorry to put you on this spot. I could see you definitely look like you were thinking about something. So I'm going to see people will stop using their video now. Okay. And I do appreciate people interacting. Okay. So here's the cool thing. If you want to do scikit learning, you want to do machine learning, the first after you transform your data, you standardize your data and get ready with it. The first thing you're going to do is you're instantiate your model. So K neighbors regressor was in fact the K nearest neighbors regression model from scikit learn. Now these are the hyper parameters right here. The weights, we have a choice between uniform and distance waiting, right, the uniform window or the kind of hyperbolic distance based window. It's an inverse distance. The number of neighbors is K, the number of neighbors you want to take and guess what P is? P is the distance metric that we're going to use Manhattan distance, Euclidean distance, you could even do a third order. Now that's pretty crazy stuff, but if you want it to, you could do a third power or anything you want to do. Okay. So now we're ready. We can instantiate our model. Now these are the hyperparameters. This will control the level of fit of our model. So let's go ahead and run that. Now the next step here and this happens with every scikit learn methodology, we instantiate the model, nay, like what a horse would say, I guess. And then we go ahead and we do a fit. We've already set the hyperparameters. Now we're going to train the model. So we train it with the predictor features and we got the response feature right there and we can visualize the model. And if we run that, this is what we get right here. So we have the model, we have the model. The colors are the same because I'm just showing the model in high resolution. In fact, to get these plots, I estimated the model I believe at a 100 by 100 matrix. And then I interpolated between the locations. So I can see the full response surface of the model. The data here is the training data. The data here is the testing data. So what you can see, if you look really carefully, you'll see you have pretty good accuracy right here. You have a little bit less accuracy right here. There's some cases where it doesn't look like we did as well. It's because this is the testing data that was withheld. Okay. Any questions? How did we do? Model looks good. It doesn't look too bad. It's I like quantification. We can go ahead and run cross validation like this. The mean squared error in training, the variance explained in training, 0.98. Okay. This is one of those R squares of 0.98. This is where you look at the model and go, wow, that was almost too good to be true, but we did very well with the model. It's not actually that difficult of a data set. It's pretty smooth. We have lots of data to support the model. So we did well. We can also see in the, this should be in testing. I apologize in testing. We got a little bit more error, but the variance explained it translated to same variance, explained same R squared. Okay. And we can see the plot right here. This is in training, the actual production versus the estimated production, actual production versus estimated production. Okay. Let me ask you this. What do you think would be something interesting to do to change this model? What would you try to like to do? We have multiple choices. We can change the waiting window, the number of neighbors. Now, the distance metric, do you want to see that first? What do we say? Should we try Euclidean? Let me, okay, we're scientists, right? We're all scientists. What is your hypothesis? What should change in the model? Let's go ahead. You got to run that code with the Euclidean distance. And now let's go ahead and see what we get. Do you see what happened? Isn't that cool? We don't have those city block shapes because the distances we're searching the way aren't stepping like that. And you can imagine as we go through the data, that step form was caused by the fact that data would be added or removed very arbitrarily just based on kind of how they showed up based on going this way and this way. And if you once you got into the data, it caused those kind of step shapes. Now we have these ray shapes, which are really like they're based on the fact that, oh, let me ask you, why do we have these discontinuities? What do you think? What's causing the discontinuities? Okay, or what could we do to fix them with our hyper parameters? Waiting window uniform, number neighbors five, and the distance metric Euclidean, how could we change this to remove those discontinuities? Okay, so change that to distance. You can do that just like that and you can run that code. I'll let you guys write it into. So we all discover together. Okay, where did it go? Oh, there. Okay, I ran that block. Now I got to run the code again. Is it better? Is it a little bit better? Something's going on here. This is known in spatial estimation as a search artifact. It's usually caused by the fact we're just using way too few data. In fact, even with that distance taper, we're just using so few data that we're just we're quickly stepping. It's like we're using too small of a window. Okay, and so probably that and I wouldn't be surprised if the distance taper is just not enough. I wouldn't be surprised if really the distance taper is an absolute distance taper. So if you think about it, the window has got that shape, but we're truncating the window too soon. Okay, so let's go ahead. Why don't we change the number neighbors? How many neighbors should we try? How about 15? Now, my only concern is this. I tried to run it before you guys because it actually is a little bit slower. Okay, my only concern is as we go to a large number of neighbors, it will change the computational time. Do you see what happened now? We have a better smooth behavior. Things are becoming more average. We're not finding data in much as much detail. Now we can go over here and we can actually actually check and see how we performed. Actually, what's interesting in training, we now have a model with 100% variance explained with round off. Okay, it's rounding to 100% variance explained. In testing, we have about 0.98. We have a little bit of noise. We have a few locations that don't perform well. Now, you remember I said before, we might want to track where the data come from for the test and you could do that. You could actually label this by index number or something like that. You could then identify where are these data that don't perform so well among the testing data. We'll probably find out it's there right there. What about adding control points to drive the trend? Kind of this pseudo well idea, right? Where you're like as a geoscientist or as a geophysicist or a reservoir engineer, I know that we should be going lower here. I want to pin the model, reduce that uncertainty. That's been a long practice within reservoir modeling. Now that's interesting, isn't it? People don't do that. People don't do that, but you totally could do that. In fact, if you do that and you do it really well and you do it to integrate, like, let's say there was some knowledge about the system you want to integrate by putting pseudo information in. We should write a paper on that. We should get a PhD student to work on that because it being, I'm always interested in how do we get more geoscience and engineering into the machine. That'd be a really cool idea. If you have some ideas around that, I also be happy to be happy to discuss around some ideas of how you can do that. Cool. We definitely saw that we did a little bit better by changing that. Whenever you work with a machine, it's always a good idea to try to break it or try to do different things to explore and understand it. What do you want to try out? Should we try one neighbor? See what that looks like? If we go ahead and run that, is that what you would expect? What do you think of that? NASA, I see your mic's off. What do you think? I think overestimates the production or something. Is this overfit or underfit? What do you guys think? Who thinks it's overfit? Underfit. Oh, underfit. Let's do a vote. Say yes if you think it's overfit. Democracy is starting to work. Two yeses. Let me see. Who do we have? Tim, what do you think? You're using just the closest data so you're going to have a complete fit. Exactly. This model will perfectly fit the training data and maybe perfectly wrong at the testing data. If we had a lot of noise in the data, we'd be perfectly wrong. This is an overfit model. Imagine when we were talking about under an overfit, this would be that model that just perfectly goes through the data at every single location. That's the definition of overfit. Let's go ahead and run cross validation on that and see how we performed. Whoa, look at that. Training. Boom. 100%. We nailed training. Look at testing. This right here. Now, it's all rounding up to .98. Verains explained. But I think you'd find that this is probably one of the worst models we've built so far. The data is smooth. The data was noisier. We would definitely have more challenges. Okay. Let's take the other extreme. Let's try the number of neighbors as a large number. And that should take a little while to run. Let's see if that runs fast enough. No, do you see that? 50 is starting to smooth off. What happens if we do 100? I wonder how long that I'll take to run. My compliments to whoever arranged our cloud resources because we seem like we've got, we're running with a portion there. We've got some power there. It's pretty good. We got pickup and torque. Okay. Look at that. Do you see what's happening? Becoming smoother and smoother. Since, you know, since we have such great resources, I'm curious. 200 nearest neighbors. How long is I going to take? That's not bad. This is about as fast as my desktop. This is really good cloud resources. Did other people try to run really large number? Who, what number did you run? Anybody ran something larger? How do you know the maximum number of neighbors? Nasser, thank you very much for asking. Let me ask you this. Let me put it back to the crowd. What would it be the maximum number of neighbors you could ever use? What do you think? Anyone? The number in your data set, exactly how they're now. I was just trying to find that we had a thousand data originally. We did what split? We did 25% for testing. That means that ultimately we could do about 750 neighbors. Now, I don't know if scikit-learn is intelligent. From the standpoint, if it will give you an error, there's always one good way to find out. If you go ahead and we go back here and we set this as a thousand, will scikit-learn tell us? Yep, you see that? Expected number neighbors is greater than or equal to the number of samples. 750 samples are available. You tried to do a thousand neighbors. It works. We could technically try to do 750. Now, I'm really pushing. This is not a linear operator. The computational load is not O to the O. It really is, I think O squared or something like that. By going up by three times, I'm actually going probably almost 10 times computational load. This might take a while to run. I'll leave that running. Look at that. Awesome. That's it. What do we estimate with all we estimate with at all locations? Is this going to estimate with the average at every location? Why not? It doesn't look like it's constant. It's close, but why isn't it not constant? Because remember, we used a distance waiting. If we had used a uniform waiting window, then we would have seen a constant at all locations. It is being locally influenced by the distance of the data. That's enough of that. It'd be fun to run the see how it does in performance. Look at that. You see that? Anybody ever built a model and it did that? The actual production and the estimated production. Look at that. Isn't that crazy? What's going on? What is the variability represented within our estimates versus the variability within the actual values? Is the variability of our estimates in this model larger or smaller than the actual variability of the system? Our estimates have almost no variability. It's all regressed almost to the mean of the data set. Okay. So now when we plot this, we plot the actual versus the estimated, this line is not on the 45 degree line. Its slope is diminished because we don't have the right amount of variability. What this is is we have built a model that systematically overestimates the low values within the system and systematically underestimates the high values in the system. Because we've regressed the mean. This is a regression to the mean. Okay. This, if you want to sound cool in a dinner party, this is known as conditional bias. Your model is conditionally biased. It's biased low in the highs, biased high in the lows. Okay. Every time you see this, let me ask you this. Does this mean you're over or under fit? What I want to give you is a diagnostic tool. If you do a cross-validation plot like this and you see this slope right here, in your mind, think I can improve the complexity of my model. I can try to fit the data more closely. Okay. Because I'm systematically under fit. Okay. So this is a nice little cue to help you out. Okay. So the rest of the workflow, what we're going to do is I just run through a bunch of different examples. Now, we took time. We're basically done with this workflow. We took a bunch of time because I thought it'd be kind of fun for people to try things out together as a group rather than walk through. But if you want to come back to this workflow and kind of walk through some trials, you'll find that we covered much of this. I explained the idea of overfitting with using a large, a small k and underfitting by using a large k with k nearest neighbors. Now, let's just very quickly. Let's tune the hyper parameters. Now, this is really fun. Actually, I really like this. If you run this block of code, what it did is it's going to run every possible model going from k equals 1 to k equals 50. This command from from numpy np gives you an array with 1 through 50 values and there's 50 of them. So to be 1, 2, 3, 4, 5, all the integers, right? Then what we can do is we can plot the result. Look at that. I love this. This is hyper parameter tuning machine learning fans. This is what we're talking about right here. Did that run for everybody? Okay. So what did we do? We reran the model with k equals 1 and then we checked and we calculated a degree of error in testing and that error was mean squared error in testing. Then we repeated it, repeated it, repeated it for all of the 1 through 50 number of nearest neighbors. Then what we did is we reran it with the inverse distance weighted. So we have the arithmetic, which is the uniform waiting window. And this inverse distance is the distance window that we have in scikit learn and look at the results. So let me ask you this. What's better to use a distance window or uniform window in this application? What do you think? Which ones better? Distance. What you can see is that the inverse, which is the distance approach systematically gives us a lower error across different case. So that's a good sign. That's a good validation. That's some hyper parameter tuning right there. Now let me ask you this. What's the best k to use? Isn't that crazy? k equals 2 is the best k. That's crazy. Now I don't like this cross validation that much. This is a, you separate the test, you use the train and you build the model and you check it. There's a better way to do cross validation or hyper parameter tuning. It's called kfold cross validation. Anybody here ever done kfold cross validation? Anyone Tim saying no? Okay, no from Heather. Thank you very much. Heather appreciate the feedback. Okay. What kfold cross validation says, you're going to withhold 25% of data as testing. You're going to run one model and you're going to compare that testing data for that level of complexity, right? And they're like, that's not a great idea because every, you should test at every data location. So what kfold says, I got an idea. If I withhold 25% of the data, let's go ahead, I can do that four times. I can withhold 25% of the data keeps 75% run my test. I can withhold another 25% of the data and keep the remaining 75%. I can do that four separate times. And if you do that, you're actually segmenting the data into four parts and every part of that data gets to be part of the test at one cycle. Then what I can do is I can take the average of the result, the average of the mean squared error over those four folds. And that's more robust because then we kind of average out noise in the data and just we happen to get this one hard data and so forth. We average it all out and all of the data gets checked. And so it's a really good idea. So we can do kfold cross validation. Now, if you want to do that, the great thing about Python, that's epic and really simple is that I can declare a k neighbors regressor. And I can set its number of neighbors as k, I can loop over k. And then the cross validation score approach for doing this kfold cross validation is a wrapper. It's a wrapper. It'll take the estimator my neighbor, my neighbor, my nearest neighbor estimator model, my care net k nearest neighbor estimator model, nade dist. And it'll run it over and over again over the range of data available to me. And now I give it all of the data. I don't have to give it train or test. I just give it all of the data. I tell it the number of folds. Now, I'm a little disappointed myself. Two folds is not enough. Five folds would be much better. Anybody here sitting on a desktop or you've got multiple cores, if you're running at this home, number jobs is the number of cores you use when you do kfold parallel parallelization is trivial because you're going to run multiple times when multiple data sets just send it to a different core. Now we're using the cloud. So we've got a bunch of cores up there. I have no idea how many cores I could actually grab. I have a I have a filling. If I told everyone to do a hundred cores, we'd probably be in trouble. Okay, so I won't do that. I want that. Let's keep it as far that worked. And you can go ahead and run that. It runs really fast. It's done. Now we can look at the result. Look at that. That's kfold cross validation with five fold 20% data used for a test every single time. And look at what we get now. Now compare that to this previous result here. Now every one of the ones we're using here are all inverse distance because we already decided that's just better to do. What's the difference? What do you think? Smoother. This is noisy due to the fact that we're just using the one cycle of test and train. And it has a lot of sensitivity to exactly what test data we used. It definitely smooths out the result. And in fact, with it smoothed out and better performing, we can see that maybe one, two, three, maybe the k equals three is a better model to build. Okay, any questions about can yours neighbors hyperparameter setting by tuning or using the cross validation approach versus kfold cross validation. Any question about this entire workflow?
 Okay, we'll talk about decision trees. Now, we covered canyons neighbors because it was analogous to mapping, it was intuitive. Decision trees, not quite the same as the way we map things, but it's a very compact model. It's very explainable and it's the building block for more complicated methods. Decision trees are easy to understand and they form a basis for more powerful methods. This is what we're going to build right here. This is a decision tree right there. And we're going to build one of those, but I'll explain how we get there. Now decision tree. As I do predict prediction methods, I want to always remind people that at the end of the day, we are really just estimating the F function to go from the predictor features to the response feature and we're acknowledging the fact that we have some source of random error. We don't perfectly fit the data at all locations. We may work with continuous response for what is known as a regression tree or we could do categorical prediction and that's known as a classification tree. Okay, no big deal. We can do both and I'll mention how we do classification when we get done with explaining regression trees. They're not the most powerful cutting edge method for machine learning by by any means, not at all. Okay. They're one of the more understandable and interpretal, interpretal ones. But what's really cool is they are the basic building block for random forest tree bagging and tree based boosting, gradient boosting. And these methodologies are cutting edge. In fact, random forest and many classes of problems provides the very best solution even compared to all this new, new, fan, dangled stuff going on around deep convolutional generative adversarial networks or what have you. This is still a very, very good technology. Okay. So why do we cover decision trees? Because first we must learn about a single tree and then we can comprehend the forest together. The Yamaste. It's kind of deep, isn't it? See everything's so cool. Okay. Decision tree. So that's a idea is I can make a prediction model by segmentation of the predictor future space. Can yours neighbors was mapping in the predictor future space. Decision tree does it differently? It does a segmentation. So visually, this is what it looks like. I have porosity from 5% to 25%. I want to predict the production for an unconventional well just based on porosity. Now I know that would be fraught with all kinds of issues because it's a high multivariate problem and so forth, but I'm just demonstrating. Now what I could do is I could segment into four regions and then over each region, I would just use the average of the training data. Now let me make a couple of comments. The first comment is the regions are mutually exclusive. Region one and region two do not overlap. Okay. Otherwise that'd be weird, right? Because then I'd have locations on the range of porosity for which I'd be joining membership in two regions and you'd have two means from the two different regions. You don't want to do that. You want one possible estimate each location. The regions are exhaustive. There's no holes. There's no gaps. In other words, if I go from zero from 5% porosity to 10% porosity, I have this region, 10% porosity greater than that, like 0.001 or whatever. Up to 15% I have this region and so forth. There's no gaps. My model is exhaustive. It provides me a prediction for all possible porocities. Okay. So this is what I'm going to do. I can form a prediction model through regions and the averages within the regions. Now in order to do that, I need regions. I need to draw regions. So let's go to my three-dimensional features. No surprises. This is the same data set we used within the last Canier's neighbors approach. But you'll notice the color bars different. Anybody here ever use R stats? Anybody ever used R? I don't see any thumbs up. Intra. I, okay, maybe I won't say anything. No, I'll say it. And Dr. Foster is not on the phone, so it's okay to say. I think I'm okay to say this. This workflow was worked up in R stats and I show it in R stats. It's an alternative to Python, of course. It was kind of more focused on statistics where a long time, it was the main way to do it. I would agree with Dr. Foster if he is on the line that there's a lot more attention on Python right now and a lot more power around it and docs and the growth and so forth. I wouldn't recommend it, but ours is still pretty cool. John is really not here with us right now. Okay, cool. All right, he would have spoke up. All right, so we could go ahead and we could draw these regions. Now, the very best regions I could draw given the shape of the problem, this is high production here, medium production here, low production here, would be those lines right there. Something like that would be very efficient. In fact, if you look at the shape of the behavior, those would be very, very good boundaries. Okay, the problem is those boundaries in high dimensional space would be complicated and they would make a complicated model. Let's go ahead. Can I make a deal with you? Would you accept those boundaries instead of these boundaries? We hope that if you had enough of those rectangular regions that we'd be able to capture many features and be able to see a lot. Now if you can go with me and accept, then we have a very, very simple model to work with indeed. In fact, now we work with a model that's a hierarchical binary segmentation in the predictor feature space. If anybody here does any coding, that would be the idea of having a set of nested if statements acting on one single feature at a time. Okay, now I'll tell you about that. I'll tell you how we get there. But it's a very flexible, compact model. It is pretty good. Okay, so we've accepted. Now with every single machine, we're going to do, when we build a machine, we're going to have to calculate a loss function and order to fit the machine. And so in the case of a decision tree, machine learners are funny because the machine learning innovators, they kind of, they thought, well, for everything that we'll use different terms, you remember we had inertia and stress. You remember I introduced that to you? Enersia was used in what methodology? Well, the decision tree people couldn't agree to use mean squared error. They said we'll use the residual sum of squares. Don't worry, not a big deal. It just means they don't average it. They're just taking the sum of the squares. Now this calculation is super easy. You have j regions, capital J regions. So you go little j equals 1 through capital J regions. Now let's take the sum of all data I within the J region. And this is the difference between the true value and your estimated value, which is the mean of the region. And you take the square of that. So that's the sum of the squared error, just summing over all the regions, all of the data in the regions. OK, we'll use that as our loss function. OK, this is going to be our loss function. Now how do we do it? We want to perform a recursive binary splitting in the predictor future space such that we minimize that loss function. We're going to be greedy. There's nothing wrong with being greedy. What greedy means mathematically, it just means that every step we're going to make the choice that minimizes the RSS with no attention to the future. We make no effort to plan the future or anything. It's just that every step, every split, we pick the very best split. It's top down. In fact, what top down means in this methodology is the fact that all of the data starts out belonging to the same region. A decision tree is initialized with all data in one region, one region covering exhaustively the entire predictor future space. Then what I'm going to do is I'm going to take and I'm going to apply a segmentation through each one of the predictor features over the ranges of possible predictor features for that one region. I'm going to find the very best segmentation such that the average in each region will have the lowest amount of RSS residual sum of squares, the very best split. OK, what does that look like? Well, I could take this split right here and this would be the average in that region and this would be the average in this region. I could take this split right here on porosity, this one, this one. Can you guys agree with me that this split would not be the very best next split? Can you see the fact that we have a huge amount of error rate here that we would do very poorly and that this split would be very poor too because just look at how far away we are all the time. But this split right here would be a pretty good compromise in this one right here. OK, so that's what we do. We'll just slide across and we'll pick the one that has the lowest amount of error in residual sum of squares. OK, now we're not done. It's hierarchical binary segmentations, not just binary segmentation. So we're going to do it over and over and over again. So we find the very best first split in porosity, 14.4%. When we scan region one, we scan region two and we find the very best next split in one of those regions. And then when we do that, we say, oh, porosity less than 10%. So now the very best next split is going to be attached here. And what are we growing a decision tree? Now anybody watched that Netflix special Bander Snatch? Anybody see that? Was it called Bander Snatch? The whole point was in the movie, you watch the movie, you get to actually take your controller and decide between decision gates. And if you do that over and over again in the movie, you grow this decision tree of possible outcomes. So this idea of making choices and being able to explore is what's known as the decision tree. The reason we call decision trees is because it's really like a tree upside down with the branches, the branches, and bifurcation upon bifurcation. OK, so we can search and do that. Now let me show you a two-dimensional example just to solidify this. Porosity, birdleness, first best split, my first split right here on porosity. Now I scan across each predictor feature. I scan across porosity in region one and region two. I scan across birdleness in region one and region two. Not at the same time. It's only region by region. OK, the splits only happen within existing regions. I split region two into region two and region three. OK, now I scan across birdleness and porosity over region one, two and three. And the next best split is in region three on birdleness. This is our first birdleness split. OK, you always split only on one predictor feature at a time and only in one region at a time. And I want to illustrate that very clearly. Notice this split does not go across region two and region one. OK. And that's how we proceed. And we just keep splitting and splitting and splitting. OK, how do we stop? What would happen if you let our tree grow too much? Would we get what would actually happen to our predictions if we're predicting with the estimate as the average in the region? Anybody have a crepe myrtle? That's a complete mess. Branches going everywhere. It just becomes such a tangled mess. What would happen to our prediction model? Would we be overfit or underfit? If we come back here and we think about this very best split, the best way to think about it, I think, is to look at this example right here. Now imagine when I pick that next split, I can say, oh, 10% is going to be my best split. I'm going to try 10%. Then what it can do is I can calculate the square difference between all of the data and the average within the region, all of the data and the average within the region, calculate the residual sum of squares. And then I can repeat that again and again and again by putting the split in different locations. Thus split that minimizes that residual sum of squares is my next split. That's the greedy aspect. So I'm literally trying to find the split that limits the amount of error between the data, the training data, and this model that I have. And it's a constant model within the regions, right? What we want to do is we want to do something to limit the growth of the tree because we don't want to be overfit. A good way to do it is apply a minimum number of training data in each region. I love that criteria. And the reason being is because you're estimating with the average of the region, I don't think you can calculate a stable average with only two data or three data. You really do need more data, a reasonable sample size to get a mean. So I don't think your model's reasonable if you have too few data in the regions. OK, now there's another methodology and I do not advocate for it. But it's commonly used. What you do is you put a threshold on the reduction of RSS. I will not accept the split unless it reduces error by this much, this much right here. OK, now the problem with that is there's no guarantee that when I do this, that my splits are going to result in less and less and less reduction in error. There's no guarantee of a monotonic behavior in error reduction. In fact, you can have a little bit of error reduction. And the very next step, greedy step, could be a huge jump in error reduction. In other words, there's a split that enabled a future split that was awesome. So and if you do a RSS threshold, you stop prematurely and you lose out on that really good split. OK, so it's too short-sighted. I don't like that. OK, decision trees, what are we going to do? We don't want them to be too complicated because they get overly overfit. It's better to simplify the tree. If you have a simpler tree, you have lower model variance. There's that term again, lower model complexity, lower model variance, better interpretability. And you'll add a little model bias, but you probably going to be better off. Limiting tree growth with high decrease in RSS hurdles, not a great idea. I mentioned it short-sighted. The best strategy, in fact, is to build a large, complicated tree, let it get overfit. And you've got to love the machine learning terminology. And then we prune the tree. Cool, right? We prune it. Now what's really cool is that we can select a subset of the tree and work with that. And that would be a simplified model. And we can prune the branches in the reversed order that we added them. Therefore, the large, complicated tree has all of the simpler trees within it. It's really kind of cool. I'll show this to you. The methodology we're going to do, we're going to overgrow our tree. We're going to obtain a sequence of best subtrees as a function of complexity. Often we represent that as number of terminal nodes or number of regions in the tree. That's what we call number of regions in the tree. Then what we do is we run k-fold cross-validation to see which one of those trees perform best. And we select that tree. We've already talked about cross-validation and k-fold approach during our last workflow. I won't go into it. But what I'm going to do now, why don't we do code? Instead of showing this to you now, why don't we go ahead and look at some code and work it out through the workflow.
 So now we have this scikit-learn demonstration. Now, John and I came up with this. This is an interesting idea. It's kind of a minimal workflow where you can load up data and then work with a whole bunch of different machines and then try to go back and change the model. So what we'll do here is I'll walk through this with you once. We'll use two predictor features, one response production as it should be, right? Production. Good for value. And then we'll walk through the multiple machines. I'll talk to you. We'll do some, you know, hands-on, where we'll try out some different things to improve the models. Then I'm going to cut you loose for a while where you'll pick your own predictor features and you'll try to build a better model. Now, spoiler alert, I did pick some really good variables, so I may do the best. But don't worry, we'll try as we'll see what we get. Okay, now you guys have been through a lot of Python, you've been through our previous discussions. So some of this I will go through kind of quickly. We're going to import all of our modules, our packages that we need to work with. Scikit-learn packages course for all of our machine learning. We're going to use support vector machines, tree-based estimation. We're going to separate data into test and train split. And we need pandas so that we're able to work with our data frames, numpy for arrays, and matplotlib to plot intake in order to do our data engineering. Let's go ahead and import those packages. And then we'll go ahead and we'll, these functions are for visualizing the model. So we're going to be able to build models and visualize them. In other words, look at the model predictions at all locations, including the testing data locations. So that's a visualization function. It'll be really helpful for us. Let's go ahead and load our data. This data we worked with before during our two weeks ago when we were working together, it's that unconventional data set with prosy log perm to linearize the permability, acoustic impedance, brittleness, TOC, VR, vitrinate reflectance, and production MCFPDs. So, gas production. And so let's go ahead, we'll do some summary statistics. I did that thing again. Let me, do you mind? Let's go ahead and just restart the kernel, clear all the outputs. I always forget to, I'm sorry about that. Okay, let me go back up top here. If you've done that with me, just go back up to the top. So it's just kernel, restart kernel, clear all outputs. And that way we can run it fresh the first time and just see it. It's good. Then you can kind of see exactly what happens rather than it's like a cooking show and I got the pine, the oven already. I like to have it so we can have that experience together. Okay, so I caught up to where we were. Okay, so let's go ahead and we'll do, we'll look at the data. We can preview the data using the head command. We can calculate the summary statistics. We got some negative values. We'll clean that up. We've already done this type of thing before, where we've worked with data frames. Actually, you guys have done a bunch with data frames now with Dr. Foster. So you guys know this stuff really well now. That's great. That's great. The data frames are really powerful. I want to make a methodology where you could pick whatever feature you want to work with. And so these are minimum and maximum values for any one of the features through production so that you could pick your own features. And this will go back. If you, this is for plotting this space, being able to plot the color bars and so forth. I just made a little list there for that. Okay, so then we're going to go ahead. We create our correlation matrix hint. If you're trying to pick new features to work with because you want to outperform my model, look at this matrix and see which ones are best correlated with production down here and see which ones are least correlated with each other. And that will tell you about which features two features would probably help you the best to make predictions of production. Not a bad idea for feature selection. Now, if you want to visualize that, I think that's always a good idea. This is the color. And so we can see the correlation matrix right here. Good. We've done that. The other thing too is if you're going to pick different features you want to work with, you probably want to think about what's the form of the relationship. Brittleness has a really distinct relationship in this case with production. Parosity. Oh, where's my porosity? Oh, that's interesting. Porosity is not showing up here. But permeability right here, you can see its relationship. Let me just make sure that I did not accidentally drop one of my features. Log permeability. Ah, I know what I did. Okay, I've done this before. You see right here what I do is I do and you guys know about data frames now. You guys remember what this command is? I lock. Who remembers what I locked this? Anybody? I know it was covered. Do you remember the concept of yeah, finding a location? Another term we could use is slicing. Slicing and yeah, I like it. You got the right idea there. Slicing. And so what this command did was it sliced out and removed the very first feature from the data frame and kept all of the samples. But you see I ran the command twice and this is a very common mistake you can make with these types of commands because you see how I've got log permeability. Porosity is missing. If I run it again, I now lost log permeability. You see that because I keep slicing every time I slice, I say drop the first column, drop the first column. Okay, when you've done that because I'm writing over top of the data frame, I got to go back and read it back in. If I read it back in, now look what I have. You see how porosities back? This happens all the time when you build a workflow like this where you do something like drop the first column. Anybody know how you could do the coding in a more robust manner so you don't have this issue with reruns? You can actually drop a column by name. And if you drop the column by name, it would only drop it if it has the right name. So that would be the way to improve the code. Okay, so we go ahead and we remove the zero values. We go ahead and we assign. We're back here. We got our porosity back and we look at the matrix scatter plots. So now we're back to where we were before. We got our matrix scatter plot. So now when you look, the first thing I would do for feature selection is I would look at the relationship between production and each one of the predictor features, possible candidate predictor features, porosity, log permeability, acoustic impedance and so forth. Now I, a spoiler alert, I choose porosity and brittleness because I felt they had a really strong relationship. They have pretty high correlations. Brittleness is weird. It has a low correlation because the non-linearness and non-monotonic behavior, but at the same time I could see that it looks like it has a pretty strong relationship. So I knew a non-linear method could get that. The other thing too is I want to look at the features in the correlations with each other. Porosity to acoustic impedance don't have too much correlation with each other. If they were very correlated with each other, like almost correlation coefficient of 0.9, I'd be concerned because they have a high degree of redundancy. They're not providing too much unique information. Okay, so some tips so that you can pick your own features shortly. Okay, now here's where you pick your features. You're going to pick two predictor features and there's a command right here. Poor, brittle, predictor features. That's the code you change. When you change that, the all of everything else will use those features all the way through to do every step. Now I'm going to keep it as porosity and brittleness for this first run. Selected predictor features, index 0, indexed 1, porosity, oh, index 0, index 3, porosity and brittleness, and this response feature is production. Now technically you could pick any response feature. I'm going to suggest we keep it as production. That kind of makes the most logical sense, but you could be doing data imputation. If somebody wants to innovate, try that out. But for now, let's stay together. Let's stay together. Keep it the same. So we can look at the summary statistics. We've got a thousand data of each. We're going to use all of the data because we want to really train up. We want to be able to work with some complicated models. We need a lot of data. Previously, when we worked with this data set, we filtered out and only used some of the data. We're going to now do a random selection. 20% of the data is going to be set as test withheld. 80% of the data is going to be kept in as training data. And we have a random state, random seed. So we all have the same outcome. We all have the same data right now. And so if you run that, then you can do the summary statistics. This is the predictor. These are the predictor features inputs. This is the train. And so we can look at these statistics. We have 800 samples of each, the mean standard deviation, so forth. The test has 200 because we withheld 20% of the data as testing data. And we can go ahead and do for the response feature production, the train and the test. We're doing a good cross validation. We're holding 20% of the data out to see how it performs. Now, this is always a good idea. Look at a histogram of the train test, train test, porosity, brittleness, production. Make sure that you have the ranges covered. You're not doing too much extrapolation. And so forth, we have pretty good representation here. The only thing is you might say that there's the testing data does not test really high values. You might be missing some of that. And you might say that the testing data here is not testing the very high values here too. So you might selectively pick some more testing cases to make sure you get good coverage, really test the model. I like to do a scatter plot like this. Look at the relationship between porosity, brittleness, color code it by production. And this is the training data. These are the testing data right here. And if I was doing this in production, I would actually flag this data right here. I would probably flag that data right there. And I would specifically check and see how it performs. Because if you ask me, that's a little bit of extrapolation. I'd want to know how my model performs with those extreme combinations of porosity and brittleness. And that's always a good idea. In fact, a really good idea would after you produce the model, you can actually calculate the error at the testing locations and plot it up in this feature space and see where you perform well or not. And that might tell you where you need to try to fix up the model somehow. Or maybe there's just something different happening. You're not, you need to actually split up the data and work with it separately. Okay, now we're going to build some models. I'm talking too much. So I'll get through this more quickly. We're going to build some models. The first step is we just take each one of the features and we build linear regression. I'm sorry, disappointing. I wanted to show you this because the one thing I really want to get across with this lecture is how easy it is to do machine learning. In fact, all we had to do was import linear model from scikit learn. The first step was we instantiate the model. We create a univariate linear regression model right here. Then we go ahead and we say fit that model to the training data. And once we've done that, we just say, okay, let's make some predictions over a set of possible production values. We tell it right here. And then we go ahead and do the predictions over all of those values. By doing that, we calculate this line. That's all we're doing is we're just doing predictions at a bunch of different values for. So we get a bunch of actions. Okay, and so once we've done that, we can plot it and we get the result right here. Now, any questions about these steps to build a linear regression model and Python using scikit learn. Good. Now, I mentioned its plug and play, which means that all I have to do is replace this model with a different machine learning model and I can build that model. And it's the same steps every time the exact same steps. Okay, now let's go ahead and visualize the cross validation of the model because it's not good enough to just create this model. This is all the training data. That's the model. That's not good enough. What we need to do is we need to produce these plots right here. This is the predictions we make with our model at all of the testing locations. This is all of the testing data. And this is the error at the testing locations. So data withheld and not used to build the model. That's the error. And this right here is the variance explained the R squared value. So our model explains 37% of the variability. What do you think? Good model. It's a bit of a straw man. We're getting warmed up. We'll do better. We'll do better. Okay. Now I just wanted for completeness. I repeat the workflow. So the minimum workflow to do instantiation, make a model. You'll see later we put the hyper parameters in here, but linear regression. How many hyper parameters? Any hyper parameters and linear regression? Anyone? Is there any way to control the level of complexity of a linear regression model? No, I agree with you, Scott. I see your there's a no there. I agree with you 100%. It really isn't. It really is just the best fit line. Now if you wanted a hyper parameter and linear regression, you'd have to use Ridge regression or last so. And the last so and Ridge regression have what they call a regularization term, which controls the degree of model fit. But that's not something we'll cover right now. Okay, so let's go ahead and we'll for completeness. Let's repeat this with bird on this. Now what do you guys think of my Brinnellness linear regression model? Is it awesome? I agree. That's a terrible model. The failure here is with a parametric model. You assume the parameter form. This is not a linear regression linear relationship whatsoever. Okay. And so we can check it. How do you think it's going to perform? Is it going to be better or worse than using porosity by itself? Much worse, right? So let's see how it does variance explained negative point zero three. Anybody ever seen a negative R squared? Has anybody ever modeled with a negative R squared? This is a special moment. We should mark this in the calendar. What that means is you would have done better with the global average. If you have negative R, if you ever encountered this, you get a negative R squared or negative variance explained. It means you would have made a better model if you would have just taken the average of everything. And so that would have been a zero variance explained. Okay. So that's how you interpret that. In fact, the models misleading. It's systematically just getting it wrong. Okay. And that's the error right there. It's got higher variance a little bit than the data. Okay. Now we get serious. We did unit variant. Let's do by very we're going to deal with both predictor features, porosity and bird on this. We're going to fit this. This process is going to repeat just like we did before. We're going to we're going to import the model linear regression. We're going to do multi linear regression. So first of all, we're going to instantiate the model. It's linear regression again, not a big deal. All we have to do now is when we fit or train the model. We're going to put two features in. We're going to have two features. And so we're going to both of the features right here. And then we're going to go ahead and we're going to print out the parameters from the model. We'll look at the model parameters. We'll go ahead and we'll plot the model. We'll go ahead and we'll check by making predictions that the withheld testing locations. We'll calculate the goodness of that model. Now look at this. This is really cool. Our squared score are two scored is a function built into scikit learn. It allows you to take any testing withheld data and the predictions at the testing location. And it calculates any error measure you want. In this case, our squared. Okay. And then what we can do is we can calculate the error terms at every one of our predictions. These are our predictions. And these are the values that actually occurred at the testing locations withheld. And we'll plot the histogram. So we run that. We get that. This is what we're going to get for every one of our models going forward. Okay. The data plotted right here are the testing data. That's withheld testing data 200 data. What do you think of my linear regression model? Is it good? Variance explained 36%. Anybody remember what the variance explained for porosity alone was? When it comes to a linear fit, brutalness is not helping us whatsoever. It's really not helping us at all. We're we're we'd actually have a much simpler model, a more interpretable model if we just worked with porosity alone. So here, can you see the strikes? Anybody who's a geoscientist when I say strike of a plane, right? Can you see the strike direction here? It tells you which feature is most important. It's basically aligned in the direction of porosity, right? It has almost no influence from brittleness whatsoever. Okay. And this is the error rate here. Okay. That was a simple model. Now, let's build a new model based on a decision tree. We get hyper parameters now minimum samples per leaf. That's the minimum number samples in the in a region for estimating the mean. The maximum depth, that's number of layers or nested decisions within our decision tree. First decision is one layer. Second decision layer would result in four possible regions, then eight possible regions because they go, they keep by puricating on each layer. Okay. And then we have the maximum leaf notes. Now, I don't have that right here, but if you want, you can put that in. And it will cause it to just constrain the number of regions, which is really, really intuitive. Okay. Let's go ahead and run that code. I'm not going to I didn't do any type of tuning or, you know, trying to improve it. Look at variance, explain. Now, you guys already ran decision trees. Can I give you some homework right now? We're all at home. Do you want to go ahead and try setting the hyper parameters and see if you can beat me? Can anyone get a variance explained higher than 51%? If you are lost, if you're not, keep going work, you can you can beat me right here. This is where you set the hyper parameters. Okay. You're instantiating the decision tree regressor, the continuous prediction of production with a decision tree. You can set them right there. Now, if you wanted to change this, you could just go ahead and say, I don't like that. I want to do maximum leaf nodes. You could do that. If you don't put a hyper parameter in, its default is a very large value or basically unconstrained. Okay. So you could do that. You could change this to maybe try to have a certain number regions. Try things out. Has anybody beat 51% yet? I think we're tying 91% you know what's really cool about this model. It's quite robust because this is with withheld data and I'm making a very complicated tree and it's not getting worse. Now, let me try something even bigger. I'm going to try 50 regions. 93% very interesting. Often when you go more and more complicated, it usually kind of rounds up and it starts to come down variance explained in testing starts to get worse. This data set is kind of robust to being over trained to be honest. It's not too hard of a data set to work with. What I mean by that, it's not very noisy. The data if you look at it has kind of smooth shape to it. It does help anybody hire 93%. Decision trees are definitely winning. Were we surprised? Not at all. Right. The linear regression model is just totally inappropriate for the nonlinear brittleness relationship. Okay. And you notice the error is shrunk down to almost nothing. There's additivity of variance. It is literally a case of total variability variance explain variance not explained added together is so really you can see that there's very little variance remaining. It's only going to be about 4% of the total variance variance variance in this error component. That's the variance unexplained. Okay. So now support vector machines support mega vector machines just to give you a couple points before we start to run it is going to take your data set. It's going to project it to a higher dimensional space using what's known as the kernel transform. Now if you want to get into some really trippy linear algebra. It's called the kernel trick. And the reason it's a kernel trick is you never actually need to know the function for the kernel in order to apply it. That's the crazy thing about it. You only need to know distance metrics within this transform space. You don't actually ever have to go to the high dimensional space. Anyway, it's kind of cool. You can pick any kernel you like. Now the kernel you can pick as linear polynomial poly or radio basis function. Okay. So a Gaussian function, a Gaussian weighting function. And you go ahead and put that right there. Then you can set see see is the cost function. The higher the cost function, the more specific the model, the more flexible the model. But the weird thing about support vector machines, you have another parameter right here gamma, which we're not going to do anything with right now, but it actually allows you to change the size of the kernel like a Gaussian kernel. And so you can end up with a very small kernel, very large kernel and so forth. But let's go ahead and just leave it with these two parameters right here. And let's go ahead and run that. What's that look like anybody run it to run fast. Support vector machines. We're going to project into high dimensional space and we're going to come up with a fit surface. It's a linear fit in the high dimensional space. And when it comes back to low dimensional space, it's non-linear. Isn't that cool. So it's linear in the high dimensional space, but it comes back as non-linear. Okay, let's see how long this is going to take the run. This one's definitely the most expensive function that we have to work with here. All right. What do you think? That's polynomial second order. It's linear in that transform of the kernel to the higher dimensional space in our space back in our normal feature space. It's a non-linear fit. Okay, you can see what do you think of that fit does look pretty good? If you're working with a data set that has kind of clean geometries. If you're working with a data set that has some noise in it. If you're working with a classification problem where the classes overlap each other, and you want a robust machine learning methodology, support vector machines are your brand new friend. The really, really cool method. They work very well with really bad data. Lots of overlap for classification. It doesn't care. It's still very robust. Okay, anybody try anything else out on this? This is a cost function. If you increase the cost, it'll become a more specific fit. The polynomial, you can actually set it to a third, fourth, fifth order polynomial if you want it. You could go to higher polynomial or let me just demonstrate this. The radio basis function is super, super cool. If you do radio basis functions, it's like you're doing a Gaussian kernel at all the data locations and the C will be the cost function. It will control the level complexity of that fit. I'll leave the cost function really low. I don't think we'll get a good fit. Can somebody do me a favor and use like a 100 cost function? Can we get some parallel processing going on here? Anybody by show of hands? Can you run 100? Or did you already run the one I said to run? I did that. Then I tricked you into it. Yeah. Anybody done? I'm looking through the names because I remember the last time I taught this, there was one person in the class who always ran it first. I think they'd figured out the secret that the best way to get the computing resources and the run quick was to run when everybody else is not running yet. I don't see that individual. I'm realizing that. I don't have them to call on right now. Anybody done? Nope. Still running. We'll go ahead and we'll let that go. What this last block right here does is it does a visualization of all of your results. All three of your models, your best models, all three of the errors, the cross validation, the distributions with the error. And so that's pretty useful. You can kind of you can check all your models and see how they all perform. And so that's what we get right there. Okay. So this is still running. Let me just. I'll tell you what I'll leave mine running. Anybody who. It stopped running. It stopped running. Look. Holy smokes. All that weight for that. Variants explain zero. It's the global mean everywhere. Okay. So word of vice. If you do radio basis functions, my intuition served me well. I think you need a much higher cost function. It basically with the radio basis function. It was just saying, well, there's no cost to being wrong. I'll just fit the global trend. The global mean, I should say. Okay. I'll go ahead and run that on my own. See how long that takes. What I want you guys to do though is everybody stopped running your kernels idle right now.
 One of the things we've done in this course is provided this learning management system which controls the environment completely for you. But of course, this learning management system is really meant for learning. So it's not really meant to run production models on. So while you'll continue to have access to all of the material that's there for some time to come and you can test out simple things in the notebooks. It's not really intended. There's not enough computing horsepower there to really say train large neural networks or anything like that. We really got the computing power for all the different server instances that you guys are running and choked down quite a bit. But that's one of the reasons even Michael's cycle learn exercise takes a few seconds to run some of those cells because we've got to choked down quite a bit. If you were to take those exercises and run them locally on your computer, you laptop, you'd probably find that they run much faster. Of course, you'd have to have all the same packages installed. So how do you go about doing that? The easiest way and the way that we're recommending, we have no formal relationship with the company, Anaconda. But Anaconda is the world's largest data science platform, if you will. And what it really is is a collection of, well, it's Python along with a collection of third party libraries that you would use to do data science and machine learning. So this is a small subset. This is just a screenshot from Anaconda's homepage. And these are just a collection of the libraries that are installed when you install Anaconda. And you'll see most of the ones are ones we talked about, right? Jupiter, Nampai, SciPy, Boke, Hollivus, Psychic Learn. Those are all things we covered. You know, Dr. Perch covered a little bit of TensorFlow in his class before, MetLife, Pandas, all of that, right? So you can easily install this on your local computer by just the nice thing is it's a one click download and it's cross platform. So Windows, Mac, Linux, it'll work on all of them. It provides a nice sort of graphical user interface to launch your notebooks from to control what will conduct environments. I'm going to talk about that in a second, another thing. So this is, you know, our recommended way to access Python, which should aid in collaborating on some of these things. It controls the environment at least, right? So that everyone's working with the same set of packages and what. So then let's talk about the condo package manager. And I think, you know, these slides are really meant for reference commands. I think the easiest thing to do is just to go back on the command line and practice a little bit or to demonstrate what the utility in this is. Okay. So if you go into open terminal, hopefully you can do that on your own by now. But if you open up a terminal, if you were to type on the terminal Python, you can launch Python from terminal and you'll see some information there that, you know, this is Python version 3.73, you know, and some other information. Well, sometimes, you know, it's usually good to use the latest version of Python. In fact, Python, the latest version is on 3.8 now. And occasionally you end up with old code that you need to run on older versions of Python or older versions, right? And the, you know, it's not just Python in a lot of these machine learning or data science workflows. It's Python plus all the libraries, right? So numpy is managed. It has its own version. Cypius managed has its own version. And what you can end up in, and it's a common lingo in the computer science world is called dependency hell. You have some workflow and you have a lot of dependencies and managing all those dependencies because sometimes, you know, if something gets upgraded, it breaks the code or whatever. What you can do with these condo environments, which we're about to talk about, is create really nice sandboxes for your code. So the default Python that we've been working with is Python 3.73. And, you know, there's all these environments that are automatically loaded. So for example, if we import numpy, it works. You know, if we import scipy, it will, scipy optimize. For example, it, John, I think you misspelled import. Yeah, it works. You know, if we import SK learn, it works. Right? So we've already installed all of those packages. So the way you get out of the Python console, if you don't know, is to type exit, but exit to function. So you have to open and close the parentheses. You can also type control D. So control D will also exit from Python console. Okay. So let's imagine we want to use, create a sandbox environment that has only the packages we need running the latest version of Python. Okay. So what we'll do on the command line, we can type konda. So konda is something called a package manager. And it's the, it's the package manager that is distributed with an a konda. So if you install an a konda, you already have konda, the package manager install. In our case, we're not running the full suite of tools that it's available in an a konda because, well, for one, I just know how to manage the packages on my own through, through this konda package manager. And if you install all of an a konda, there could be a lot of packages you're never going to use. I mean, I can't, you know, if you're not doing Bayesian, Markov chain Monte Carlo, you don't need the honor, right? And so these kinds of things. So in order to avoid bloat, you know, and having too many packages installed and taking up space that we don't need, I just individually install the packages we need. And we do that with the konda package manager. So okay, back to our task at hand, we're going to create a sandbox environment to install packages into. And the way we do that is with konda create. And then we give, we give a name to the environment. Okay. Call it anything you want. I'm going to call it. Pi 38 for, you know, just short for Python 3.8. And so up to now that just issues the name of the environment. And then we can actually go ahead and install packages right now. That we'd want to install. So for now, I'm just going to install Python equals 3.8. Okay. And so if I do that, hit enter. It's going to basically give me, you know, saying the new packages will be installed. And you can just answer yes to this. And it's going to install all of these in an environment for us. To activate that environment, we type konda activate in the name of the environment. So konda activate. Pi 38. Now the, the first time you do it, you'll see this message. Your shell has not been properly configured to use konda activate to initialize your shell run. This command, all of you are running the bash shell. So all you have to run, all you have to do is type konda and knit. Bans. Conduct and knit bash. So if you type that one time, it'll never ask you for that again. It does say that you need to close and reopen your terminal for this to take effect. So let's go ahead and do that. Just close the terminal window and open a new one. Now we should be able to run konda. Activate. Pi 38. You should also notice after you ran konda and it bash and closed and reopened the, the terminal, you notice your command prompt has changed and that you have this little base over here. Base refers to the base environment. That's the environment that I installed. But now we're working in this pi 38 environment. Right. So if now that we're in this pi from 38 environment, if I type Python, you see that before we were running pi from 3.7, now we're running pi from 3.8. Okay. What about importing numpy? No module name numpy, right? It's because we didn't install numpy into this sandbox environment. This, this environment is completely isolated from all the other packages and Python installations that may be on your machine. So we'll go ahead and exit the Python console. And we're in an environment, we can go ahead and install. So we can go ahead and install, for example, numpy. So if we just now just say konda install, it's going to install it into the active environment. And then we want to the name of what we want to install. We want to install numpy. The nice thing about konda. Well, two, two things. One nice thing about konda is that it's smart enough that if you already have the packages in another environment, it doesn't go through the whole process of downloading. So for example, let's say you had 15 environments that all had numpy 1.8, 1.1, 1 in them. It doesn't download 15 copies and stick them all in those environments. It simply downloads one copy and then just links the necessary files into those environments. So you don't get a lot of bloat in your machine. That's one nice thing about konda in general. The other thing that's nice about anaconda or installing numpy from anaconda, because there actually is other ways to go about it. But we recommend using konda and anaconda. And if you do it this way, when you install numpy, you get a version of numpy that's built against something called mkl. So mkl stands for the mass-purnal library. And it's actually written by intel. And it's what it is is a lot of the basic linear algebra services like the low level matrix vector product operations and low level matrix matrix operations. All of that code is written in fact in assembly, but in a highly optimized way by engineers, software engineers at intel who have knowledge of the hardware. And so by simply installing the numpy that's built against mkl that alone will speed up the computations tremendously. And I've actually seen some benchmark studies using tensor flow, which tensor flow is Google's deep neural network framework, which is built on top of numpy that just shows simply installing tensor flow from konda and training a model versus installing tensor flow, you know, in a different way. Where you don't get the numpy built against mkl, you get like a 50% reduction in training time. On certain models just by installing it from from konda. So it's another reason just to use anaconda because you're going to get a lot faster code because you get numpy and numpy is really at the core of all these data science toolboxes. You're going to get the fastest sort of speed from numpy that you can get by using mkl. So anyway, I'm going to go ahead and answer this question. Yes, it's going to install those packages mkl is the largest one. Now if I run Python and then I can type import numpy. And I don't get an error course if I type import scipy. I'm going to get an error because there's no model called scipy. So just to make things a little interesting let's let's go ahead and install at least one more thing. So let's say konda install map plot lot. And I'll just go ahead and say yes. So let's imagine we built some code that relies on this environment. Meaning we wrote the code we tested it. And we know that with all of these packages with numpy 1.8 it works. Map plot live 3.13 it works. And we want to distribute that code. In other words, we're going to send that code to someone else to collaborate with. But that someone else may not have the exact same environment. We're going to encourage them hopefully to use anaconda at least for reasons that I'm about to show you. But they may not have the same exact version of numpy and map plot live. And it could be that you're independency hell when they try to run the code it's not going to work. And so what we can actually do is we can take our current active environment and export it. So what we can do is we can say konda e and v export. And if we just type that it's going to print to the screen all of the packages that are in our active environment. We can use this greater than symbol to redirect the output of what it printed in screen to a file. And the idiomatic file name for konda environments is just environment. .YML. YML stands for YAML yet another market language. So if we were then to look at our file browser now over here you should see environment.YML. And if we open that you'll see what we have there. So the name of the environment is Python 38. The channels there's some meaning to that we won't discuss that at the moment. But then all of the packages and you can see they're pinned down to the sub subversion. So usually the common versioning number of the number to the farthest right is just small change as bug fixes other things. The second number usually represents the minor version usually represents new features we're added. And then the leading number usually represents features that break the application programming interface. So like you have a function named something but you change the argument list so that it doesn't work like it used to. That would be a feature breaking change that would be an elating number. Okay so we have this environment and we can take this environment file and let's just change the pi 38 to something else. So say pi 38 V2. I mean this lack of originality here but just change it to anything else but pi 38. And then delete the last line. The prefix doesn't need to be in there. So we delete that line and save this file. Control S or you can save it from the file menu. Go ahead and close it after you do that. Now what we can do is return to the command line type conda deactivate that will remove us or take us out of the pi 38 environment. We're back in the base environment. This is the one we've been using the whole time. And what we can do is we can create a new environment from that environment file. The way we do that is we'd say conda ENV create the dash f means from a file and then give it the file name environment dot yaml. So conda ENV create dash f environment dot yaml. And what that's going to do is basically recreate that environment from the file. And now it'll have a different name because we changed it. We changed that first line to pi 38 V2. So let's let it do its thing. It did it and it said now to activate this environment pi 38 V2. So then we can say conda activate pi 38 V2. And then if we type Python, we'll see we have Python 3.8. If we type import num pi, no error import map plot, log, no error. As we expect because we installed those packages. But if we import sci-pi, we did not install. They'll be an error. So just to kind of summarize again, what we did was we created a sandbox called pi 38. We installed some packages into it. We exported those packages to a file. And then we used that file to create another environment that turned out to be, you know, just a clone of the same name. But so the idea here, the workflow behind it is that you take this environment file and you check it into your get repository right next to all your code. So that when a collaborator comes along and wants to work on your code or help, all they have to do is download clone the get repository, install the environment file with the command conda envcreate-f environment. And they will have an identical environment to you to the environment that you developed on. So this is a way that, you know, you can avoid dependency hell. And if you follow this kind of workflow, you'll never end up in a scenario where, you know, it's fairly common. I mean, I even get it with my students a lot that, you know, they complain that, well, the code did, you know, they're working with a neighbor's friend or something and, you know, it works on my machine, but it doesn't work on theirs. And it's usually related to this dependency problem. So this is the way you can avoid that. Any questions? And again, you know, sandboxes can be useful for other things, right? And maybe, you know, in just in the maintenance of your own code, you're used to using numpy version 1.16 and you want to test out whether version 1.18 breaks your existing code. Well, you could just go to a sandbox, you know, create a condo environment, install the newest version, test your code. If it works, then you just go back to, you know, you just bump your environment file to include that new version, right? So there's lots of uses for sandboxes. And, you know, if you, if we were to look at my own computers, condo, you can list all the condo environments. If you type condo, ENV list. So in this case, there's just the three that we created. But, you know, if you were looking at my own computer, there'd probably be 40 environments there or more, just for things I'm, you know, a lot of times I'm just creating something temporary and then I forget and I forget to delete it. So I guess that brings up how do you, how would you delete one of these? Well, condo ENV remove and then just give it the name, pi 38, for example. And that will, so now if you type condo ENV list, pi 38's gone. So we can do it again for the other one, 38 V2 and it's gone. So now we only have the basin. Oh, it won't let you delete an environment that you're in. So I have to condo deactivate first. And then I can remove the environment. 38 V2. Okay. Any questions about condo environments? The, you know, just like when we learn to get, I think it's really useful to know these commands from the command line because particularly if you're, if you ever find yourself deploying machine learning applications or data science applications, those are typically done on headless servers. There's no graphical user interface at all. You have to log into the server remotely via the command line and then, you know, issue the commands you need to deploy your application. And in those kinds of settings, you know, any graphical user interface to interact with Git or Anaconda or anything like that is not going to be available. So it's good to know the basic commands for the command line, in case you find yourself in that environment. However, the Anaconda launcher that is installed when you install Anaconda from that one click installer, I mentioned earlier, gives you some graphical user interface tools to deal with condo environments to make it much easier. Kind of like the way that we could also deal with Git through the graphical user interface over here. Right. Anyway, let's look at a couple of other things. So these are all just the commands. This is summary of the commands we covered all of these, right? So this was the first command would create an environment called my virtual environment that would install Python 3 at the latest version, as well as NumPy SciPyPand as a scikit-learn. We can export the environment. We can install from the environment and to activate, deactivate environments. You use the activate and deactivate commands. So other tips, unit testing. We use a little bit of unit testing in the exercises here or you may have, but we didn't actually look at those test files. So there's frameworks for helping you create unit tests. One of them, there's a built-in one called unit test in Python. You don't need to install anything for that. And then there's a couple of other test runners that give you some additional features like py-test and nose. And what these allow you to do is just what it sounds like. Right. And that's why I encourage you from the very beginning to write small functions. Small functions are easily testable. We put in some inputs. They return an output. With that information, we can write tests. If the function, the correct answer to the function with a given set of inputs is 32, well, we can check if it's equal to 32. And we can then automate the test. Again, all of the best code would be code that is written to 100% code coverage. And I'll talk about what code coverage is in a second. So continuous integration is the idea that your tests are automated. So basically all your functions you write should have tests. You should put those, organize them into a test suite that sits alongside your source code in your repository. And when you commit your code, then the continuous integration server will automatically fire up and run your code for you. There are several ones I've listed here that are popular and they are integrated well with GitHub. But it doesn't have to be some commercial tool like this. The continuous integration server could be a program that you wrote. It lives on your server that just monitors or get repository for new commits. And then once it commits made, it pulls over that code and runs the test and then reports them back to you in some way. So nowadays, we've moved all our software over to the, I mostly use Travis. Since Microsoft has recently purchased GitHub in the last year, the Zerr pipeline is actually built in to GitHub now. It's called GitHub Action. So you can actually do continuous integration straight from GitHub. And that's a fairly new feature. Just because I know that configuration files well, I typically use Travis because I've been using it for years. And I've moved over all my important software at least to a continuous integration server tool like these. However, before we did that, we had written our own, basically something that monitored the virtual control system so that when a new commit was made, it pulled over, ran the test. And then if the tests were failing, it would send us emails. It was all automated. Now you can do all this straight from these web-based tools. And most of these you can install, they have enterprise versions that you can install behind the firewall as well, particularly like GitLab. And version control, version control, version control. I didn't spend an hour, an hour and a half on the first day feeding you GitHub's stuff because I know it's not that fun. It's not near as fun as writing code and looking at visualization and stuff. But I didn't do it if I didn't really believe in how important it is. It's so important to follow these guidelines. Because as long as I've been coding, I'm still shocked at how sometimes I'll make the most but not in changes to a code and think that it's not going to cause any failures elsewhere. And sometimes it does. And then I always shocked it, you know, and why? Just some side effect, some function or something, it caused something else to break. And without these automated testing, you don't know that that happens. And you can't always know that it's happened. So I've got an example code base. This was a tool I wrote, you know, I guess a year ago, you can see that there's not even any, the most recent commits was more than a year ago. And this was some stuff that I had built to access our database for production data. And we, you know, for all these exercises. But then later I found intake and it was just a more complete packaged thing. So I gave up development on this package, however, and just pivoted to intake. However, one thing this package does have is a nice display of code quality. And so let's look at some of the things, right? So for example, this badge that says build passing. Well, that's because it's hooked up to a continuous integration server. In this case, Travis. So if we, if I click on that, and in this case, I'm opening a new browser tab. And what you see here is the build logs. So what happened was soon as that last commit was made, this thing fired up a Linux virtual machine installed anaconda installed the exact environment that I wanted installed and ran a set of tests. In this case, there was only nine tests, but nevertheless they all ran. And then after they all passed, and then after they passed, they did a couple of more things. For one, it, it checked, ran a code coverage tool. I'll talk about that in a second. It also read a code formatting tool. There's a nice Python code formatting tool that's really rigid called black. So it actually ran black on my code base, which what it does is it literally formats the code. You have a specified style of your code base. You know, you want four spaces under four loops. You want keyword arguments to have no space around equals signs. You want, I mean, literally every detail of the code. You know, this black, what black does is it formats it for consistency. And the reason, the reason for that is that, you know, as you get a large code base, you really want the code. If you have a large code base, a lot with lots of collaborators, the more uniform, the more uniform your code reads, the easier it is to maintain. Because you're not trying to figure out somebody else's coding style or syntax, you know, the particular way they wrote the code. So in this case, we take, I took all the sort of formatting away from any individual programmer, and it's automatically formatted by the black. And so the entire code base will always look the same in that kind of thing. So that's the continuous integration server and the automated testing that's run there. In addition to that, you have this code coverage tool. So you'll see coverage. What that means, what 97% means, and I'm clicking on this to open a new link, is it means that 97% of the lines in the code are run by the test suite. Ideally, you would have 100%. But I'm missing a few. We can actually look at the lines. You can go to the individual files. Well, I thought you could see here. Should be able to go to the individual files. Maybe we're going to look at that older build. I'm still going to an older build. Now, there seems to be some issue, but last time I checked, it worked. So it should show us the actual source code. And then show us the line that are run would be green, and the lines that are not run would be red. And so that would give us a visual indication if we needed to write more tests. So that ensure that all of the code is run during the test. So like if you have if statements in your code, that would change the sort of trajectory of the output. You need to make sure all those if statements are traversed to get to 100% code coverage. In my case, if I remember correctly, it's saying 113 of 116 relevant lines covered. The three lines that aren't are just errors actually. And I didn't test the errors in sense that they print error messages to the screen. And I didn't test that, but that's easy enough to do. So that's code coverage. Also documentation. So there are tools that can automatically build documentation for you. So returning to the. And returning to the original GitHub repo, you see there's one this is docs passing. So if we go look at the docs, so this is the API documentation for this package that I was writing. And I was writing the documentation to go along. So you see this nicely formatted, you know, an HTML webpage or whatever, with this particular, you know, in this case, to it's documenting the application programming interface to the class. So to instantiate the class, pdbd, we need a username, password, subdomain schema, and there's information about all of these. There's also the class functions. So get. Get production from get tickers by state, get well locations by ticker and state. All of these are documented. Okay. Now how did I do that? Well, let's click on the source. So if you go look at the source, these just this just looks like a regular old Python member function to a class, right? But what I did in the doc string of the function. So usually when you have a string that sits right underneath your function definition, it's called a doc string for documentation string. I wrote this in a very specific way. Mainly that, you know, it has basically a one line description of what the function does. Some tag args followed by some other information and returns followed by some other information. And you know, the type is put into parentheses. So this is the argument name. The type is put in parentheses. Okay. And when I hooked this up, so again, I wrote the documentation as I wrote the functions. Here's the function. I just wrote the documentation. And then I committed it to GitHub. And that's it. I walked away from it. Because I have this set up to this auto doc tool called read the docs, which is kind of the recommended program for doing this within Python. Then I automatically, it turns that doc string into this. Right? So again, you know, here's the exact one we were looking at. Right? So it, you know, it formats, you know, it puts the parameters, puts bullets in front of them, puts makes the argument bold, makes the type, um, italics gives all the other information, provides these hyperlinks automatically and everything. Right? So again, this is automatically generated upon every commit to GitHub. I don't, I don't, you know, I just write the documentation when I'm writing the functions. And this is, you know, how you get users to use your code is you have it well documented, of course. Um, the last two badges are just, uh, PiPy package is, uh, it's called the Python package index. It's sort of how you release code. Um, and then code style black, just again, and it indicates that I've run this through the black formatter so that the code style is all according to that logic. Okay? So that, that's my tips and demonstration for code quality and reproducibility. Are there any questions? Yeah, John, I have a question. Um, this is something related to, I, um, to you being a Mac user. Um, so accessing the terminal, if you're a Windows user, I'm sure a lot of people are, how would you do that? Is there any good? Because you can't just, um, just, um, go to the command line, you know, on your local machine without it. So do you have any recommendations on how to do it as a Windows user? All of the, uh, all of the commands that we've talked about so far are or Unix commands, right? So they're, they're really, they'll only work on a Unix machine. That is a Linux or a Mac, right? Because Mac is also Unix machines. If you have a Windows machine, um, of course, you have the PowerShell, which sort of looks the same, but the commands are different, okay? If you have a very, very new version of Windows, I don't know the exact version, but a very, very new version, they've actually implemented a BashShell in, you know, within PowerShell. So you can actually issue those commands on the, on the command line from, from the PowerShell. Um, but it's one of, it's a really new feature, like in the last year or so. Uh, the, the other way is to use a terminal emulator. Um, and uh, the, the popular one, I think, is called Ming GW or something like that. Um, um, uh, Ming GW terminal emulator. Um, men GW is, I think is what I was, yeah, men, men GW is what you want to look for, minimalist, new for Windows. So that, that's a terminal emulator that allow you to access the somnetic commands from, from Windows machine that one. And, uh, you know, if you're, if you install Jupiter lab on a Windows machine and you open the terminal, then it's going to open the PowerShell terminal. So there will be some difference. Got it. Thanks, Daniel.
 Let's begin by setting the restart the kernel and clear the outputs so we can start this all fresh. Okay, let's go ahead. We got all of the documentation talk about supervised learning prediction with a machine, hierarchical binary segmentation all explained. So you got a little bit of a standalone document. We're going to show you that we can actually represent your decision tree as a set of nest nested if statements, you know, kind of speaking of banner snatch. Did you know that there's people who actually think that our decision making uses a tree? That in our mind, we actually do a hierarchical choice making binary. And that's actually how we do complicated decision making. Interesting. I guess at the end of the day, our neural net is binary, right? It's kind of a signal that fires or doesn't fire and so forth. But anyway, we'll get into that. Okay, so let's talk about to do tree based methodologies. We have scikit learn. We can import tree. Now this is pretty cool. This allows us to be able to access all of the tree methodologies. We have the cross valve score so we can do that kfold cross validation export graphic. It's on your own computer. You're going to find you need some additional installations. It's not super easy. In fact, you have to do some things locally on your machine, even outside of Jupiter to get this to work. Okay. Standard scalar and all the others are old friends pandas numpy mat plot live. They're all there. Let's import them. We're going to have some convenience functions. But now we're experts at this. These are functions to visualize the model to check the tree and see the performance and a function here, which I'm kind of proud of. I did take it from, well, I'm not proud. I'm excited to use from Paul from stack overflow. It's a methodology that turns your tree into code and Python, which I thought was really cool. Okay. So we're going to declare those functions. Let's load the data same data as before. So I'm not going to spend any time talking about it. We're going to get the summary statistics. We had to remove some negative values. Not a big deal. We do that. We got the correlation matrices just trying to give an idea that we should spend some time on data prep. We don't do enough with that here, but I just want to demonstrate that scatter matrix plots or sorry, matrix scatter plots shown right here, where we can look at all the data and see the relationships. We've done that same data set from before. We're going to take porosity, burn on this and production. That's what we're going to work with now. Now let me ask you a question. Do we have to standardize the features for a decision tree? And if I change porosity from fraction to percentage, I'm still going to be discretizing along that range to try to find the very best next split. It doesn't actually matter how much variability is represented on either one of those axes. It would not impact where the split would go. So we actually don't need to standardize with a decision tree. The mean, the clue is this, if you're calculating a distance in the predictor feature space, we most definitely have to think about standardization. Okay, that would include cluster analysis, that include principal components because we're thinking about a variance between the variables and how the variance is explained. We need it there. And that would include K nearest neighbors for sure. Okay, so we can go ahead and we can just work with the raw data. This is the data set right here, porosity, burn on this and production. And we can go ahead and we'll plot up the data and we'll look at the data and we'll see what we're working with as far as the, we've done this plot before. We're experts with this. We got the prostate versus brutalness and production. We're working with it. Let's go ahead. We're going to take the data. I like readable code. So something I'll try to do is I'll take and make a copy of the prostate and brutalness from my data frame and make it part of my predictor features. And the response is going to be production. And so I have nice readable code. I can feed that in. Now remember what we did before with scikit learn is we had the situation where we first instantiate the machine. We make the machine and we declare the hyper parameters. Now, I just want to remind everybody at any moment I can do a shift tab, shift tab with my cursor in the parentheses for the hyper parameters. And I will immediately see what are all of the parameters I can work with are hyper parameters. Now this is really interesting. I can change the criterion. So in other words, what am I trying to minimize residual sum of squares? Now which of those here is they're doing mean sum of squares. So I mean the mean squared error, which basically they don't do the average of it. That's not big deal. You can also find that you have different constraints on the complexity of your tree, like maximum depth minimum number of samples per split, minimum numbers, minimum number of samples per leaf, minimum number of samples per split is how many samples you're allowed to have if you're going to consider a split and how many samples you must have in a final region right here. So you have all these constraints over the model. Okay, so let's go ahead. We can instantiate the model. We'll do minimum samples per leaf, which is the minimum samples per region of five. These I figure I can calculate an average with five maximum depth. Well, if we have a decision gate, two decision gates, three decision gates, that would be a depth of three, three levels of decision gates. So I'm allowing a depth of three. Then I could take my predictor features, my response features and fit my tree, visualize my tree, check my tree. Look at that. That is readable, compact code right there. Whole model being built in one step. This is the result from minimum samples five maximum depth of three. This is my decision tree. This is my resulting cross-stallation of my decision tree. Okay, what do you guys think? Did I get the job? Am I going to be the next star machine learner in your team? Guys like it? Any comments? Anybody bothered by the striations? Kind of weird, right? Somebody want to help me out. What causes the striations in this cross-validation plot? Anyone? The estimated production, you have one, two, three, four, five, six, seven. There's probably two right here, right? Eight. You have probably eight or nine or ten regions. One, two, three, four, five, six, seven, eight, nine. I see nine regions there. You only have nine possible values you can estimate with this model. Because remember, it's going to be constant. The average of a region within the region is the prediction it will make. Whereas the training data, as a whole distribution of productions, your decision tree can only come up with number of region estimates. That causes your model to have this behavior where it's not falling on the 45-degree line. And in fact, it's only predicting with a constant value within each one of those regions. How could you improve that? Because clearly that's error, right? We really don't want it to come all the way out here. It's not an awesome model. What are we going to do to the tree to improve that? What if we were to create a more complicated tree? We could do that. So if you come back here to the hyperparameters, minimum samples per leaf five, maximum depth three, what do you guys want to change? Do you have any recommendations? Yeah. If I go to, if I add one more depth, how much will the regions change or number regions change just normally? I can't guarantee it, but how much do you think it probably would likely change? Would it go up by one region? Would it double? Would it triple? Multiple choice questions here. What would you say? Who thinks that it would, in fact, just add one more possible region? Who thinks it's going to be two times as many regions now? Anyone? Heather said eight. I like eight. All right. What happened? Do you guys see that? We get many, many more regions. When we go into eight, we have blown this up. We have so many regions. Do you see how now we're better approximating this kind of non-linear shape by using lots and lots of rectangles? We get a lot more control on the model. If you look at the predictions, we're now getting to variance explained to 0.97. Now what was our variance explained for our underfit model? This was definitely underfit. Maximum depth of three, variance explained was about 70%. Isn't that crazy? So we jumped up from 70% variance explained to like 98% variance explained, correlation coefficient of 0.83 on the cross-validation plot. Okay. So what else could we change? Any other ideas? You know, eight is a huge degree of complexity. That's for sure, Heather. I think eight is probably as complicated as we want to go. Anybody here comfortable about relaxing the minimum number of samples per region? Remember a leaf is just a terminal node on a decision tree. It's the region. They love the tree terminology, don't they? What do you guys think? Do you think would it be crazy to do three samples in a region in calculate and average? That might be pushing it. What do you think you see that? Is it getting started to become a little unstable? The variance explained is not really going up. I don't think we're really greatly improving the model. And anyway, all of this calculation right now is being done with all of the data. It's not being done as a train test. We're just playing around decision trees right now. Okay. Any questions about this so far? Okay. Let's go ahead and let's go ahead and visualize a tree. So we're going to build that. Oh, let's change the tree. This is going to be crazy. Let's go back to something a little bit more reasonable. Max depth of three, five. When we visualize the tree, the one I built is going to be really big and hard to visualize. Okay. So let's run this again. We get our somewhat underfit tree, but then we can visualize it very easily. Okay. This is the visualization of our decision tree. Now, this is pretty cool. We've got the first decision is porosity less than 15.5%. The mean squared error at that point is the mean squared error of using the global average, assuming a global average. Just remember all the data before the first split is going to be in one region. Okay. Number of samples, 100% of the samples are in that region. And the value right there is going to be the average of that region, 2200 MCF PD production for a gas. Okay. So now we can see true and false. Are we less than 15% porosity? And this is brutal. This is next split. Brittleness is next split and we go down and down and down. We can see the percentage of samples in each one of the regions, 6%, 38%, 33%, only 4% of samples there and so forth. So we can visualize the tree. The cool thing about that is we can also see how the mean squared error is decreasing as we make these splits. We go from here to here, we can see how the mean squared error is dropping substantially. And in fact, I believe these blocks are actually color coded. Are they color coded by mean squared error? I thought they were. Okay. We can visualize the tree. The other thing we can do is we can do proper tuning of the complexity of the tree. And the where we're going to do that is we're going to run many trees. And then we can look at all of the trees and get the error in the trees and we can plot that out. Okay. Now, since we're going to run many trees, why don't we look at our trees? They're very easy to look at. This is my first tree with one decision node resulting in two regions. Okay, that's my first split. This is my decision tree where I said you can build three regions. Okay. So it's got two splits resulting in three regions. Now can I convince you that decision tree number one, the one with one split actually exists within the next tree that all I have to do is remove that split and this goes back to this tree because it's a greedy methodology. It's just stepping forward in the same way. If I tell it make three regions, it gives me the two region tree plus one more split. Okay. So we're just growing branches on the same tree. So I can just prune off branches in reverse order and I can explore all of the different complexities of this model. Okay. Which is really cool. It makes it very simple to do hyper parameter tuning because I have the simple model, the more complicated model, the more complicated model, even more complicated and so forth. And I can just simply adjust between all of these and change the number of regions and see which one has the best result in testing for error. Okay. So there we go and it gets more complicated. If you look carefully, you'll see the split every time. If you look through it, you'll see this part right here got assigned to a new split right there. You see the color? Okay. Okay. And we can step through, step through, step through and the final tree is right there. We're actually getting a little bit over fit at that point. This feature right here, this is starting to feel like we're fitting noise from the data. Okay. Now what we can do is we can plot the mean squared error versus the number of terminal nodes or regions. How are we doing? Which level of complexity is the best complexity in the case of testing? Which one of these trees would you use for real world use? Now there's different philosophies. One is you take the very best tree as far as mean squared error. Another idea would be I say, well, you know, there's a cost to complexity. Let me take a reasonably good tree because this is starting to level off. You've got diminishing returns. It's not really helping you too much to add more complexity. So you could take something that's a little bit lower, I mean, higher mean squared error, but it has a much less complexity to the tree. And so there's a variety of different trees you could pick. If we had noisier data, I suspect that it would have started to rise back up and with greater complexity, we would have actually start to see more mean squared error in testing. For this model, because the simplicity of the data set, we really don't rise up again. Now it's more robust to do K fold cross validation. So take the K fold. I don't know. What do you think for a K fold? How many folds? Two is not enough. Now we have 50% of data in testing. It's probably not a great idea. Four folds gives us 25% data in testing for every fold. We could go ahead and run that. This is a wrapper that uses our tree estimator with different numbers of terminal nodes or leaf nodes or the regions. And it's going to go ahead and run the model over and over again. And it ran. It ran really fast. And then we can visualize it. And this is what we get. This is K fold cross validation error, mean squared error, averaged over all the folds. And we can see what happens as we go from one or this is probably about three nodes all the way to about 28 or 29 nodes. And we can see how our error is decreasing. And we might make different choices based on well, where does it level off or not? Or maybe you come back here and you say, well, I like this. This is good enough. It's starting to shallow out that kind of thing. Okay. Now what we can do is the really cool thing is we can just say, well, 12 was our favorite number. 12 was a reasonable compromise compromise between error and complexity. I could simply rerun the tree saying use 12 nodes. And that's how we prune. We just say, give us those first 12 nodes. We run that and visualize the tree. This is what we get. That's our model. And now what we could do too is we could also interrogate our tree. If you if you wanted to, you could ask the question, if I had a porosity of 10% and brittleness of 30 ratio out of 130, what would be the path I would follow to get to the solution? You can actually ask your decision tree that question. And those ones right there are telling me I would keep going 111. In other words, success, success, success, success across all of those tree nodes, that's the path you would take. Now you could go ahead and change those values and rerun it. And you're still getting the same result there. Let me try a different value. Oh, I must be in a really big space there. Let me try. Oh, okay, I see, I see. So it's telling me right here that these are the choices that I would make in order to follow that path to get to these solutions. So it's telling me how I would track through the tree. Now tree to code is really good. I can take my tree and I can run this and look at this. It defines a function and Dr. Foster will go more into making functions and Python. Then a conditional statement on the first split, conditional statement within that split, the next statement, the next statement. And every time you get to a terminal node, it returns the production value as the average within that node. So this code right here, it's not that bad. If I was to copy and paste it, I could put it into my workflow and create a function that is my decision tree prediction model. So it's highly portable. Your decision tree, you can run that code, get it as if statements and you can now use it in production anywhere you want to. Now a cool thing about decision trees is that every time you make a split and reduce the mean squared error in this case, what actually happens is we can keep track of how many times I use porosity and how much did it reduce the mean squared error. And if you standardize that result to sum to one, you get what's known as feature importance. And so the decision tree tells us that porosity has a feature importance of 54% or 0.54, Brittleness 0.46. In other words, porosity had a slight edge in its importance in helping reduce the error in this model. OK, and we can now visualize our pruned model. If we do that, we get this. This is our final model when we did hyper parameter tuning. That's our decision tree right there.
 And it's just one slide is so short of fact. So big data, right? Michael had his definition of big data, and I don't disagree with it at all. But I think what a lot of times the tech thinks big data is like data that doesn't fit in memory on your machine, right? It's larger than the RAM of whatever machine, that machine could be a cloud-based server or it could be your local laptop, it could be anything, right? But if you can't fit all of the data into memory to do a computation on it, then what are you going to do? You got to do something. The easiest thing to do is that the sort of one step, one small extension from what we learn in this class is to use dance. So what DASC is, is built on top of NUMPAI and PANDAS, and it provides its sort of own implementation of NUMPAI India Rays and PANDAS data frames. But they're distributed, or the data is loaded into chunks, such that they do fit in the RAM. So if you have your data on a hard drive, what DASC can do is read that in the RAM in a chunk, do the computation, store the result, read the next group of data in a chunk, do the computation, store the result. So it has a scheduler built into it that allows it to work like that. And it's sort of the kind of, again, the next thing you should try, if you actually run the problems loading data, because all of your normal NUMPAI and PANDAS commands to modify, operate on data frames, other things, they're all going to work. So on DASC versions of it. So DASC would be the first thing to try if you have really big data that you can't fit into memory. And it's more powerful than just that, but that's sort of the first use case application. Then the Apache Spark project is a very large framework, specifically for working, doing data science operations on really large data sets, really fast, and particularly in kind of streaming applications. It does have, it's written in several languages, Scala and some other stuff, but it does have a set of like Python wrappers to it. So this will be something you would use in the cloud, right? You probably want to, you know, if you really had these, you'd probably just go to a cloud service provider, and they would set you up within Apache Spark environment. But then you would write the commands using Python, and that it has also its own similar version of PANDAS data frames. So you could use PANDAS-like operations. I think it's actually called koalas. So instead of loading like import PANDAS as PD, you import koalas as KD or whatever you want to call it. But then after that, it's very much PANDAS-like, you know, in the way you access labels in the data frames and all of that. But again, the data frames will be distributed across multiple processors in the cloud, right? And it has also ways to handle streaming data and other things like that. Accelerated machine learning frameworks. So we learn scikit-learn and scikit-learn is probably the world's most used machine learning frameworks, particularly for exploratory data analytics and machine learning. It also has its own deep neural network implementation, of course. But when you go to, again, a really big data, really large models, the state of the art nowadays is to use like TensorFlow or PyTorch. So these are deep learning frameworks that were built to do deep learning. But they also, in many ways, do a lot of numpy-like things. So for array computations, you know, you have a vector or an array X and you want to multiply a vector or array Y, you can load that into a Torch tensor and do that operation. But the interesting or neat thing about it is that you can also, if you have GPUs available, you can actually do that operation on a GPU. So if you haven't heard all of the kind of hype about GPUs, GPUs of course, graphic process or units, they were made in the late 90s to basically do 3D, you know, first person gaming, right? Doom and Quake, right? Like what they're really good at is ultra fast pixel rendering. And the sinners, they came out, some, you know, computer scientists started to realize that you could trick the GPU in the thinking it was rendering pixels, but doing numerical computations. And they began to write software that would exploit that so that you could do numerical computations. And as soon as the sort of open source movement got involved, well, then Nvidia saw the utility, and now they have, you know, Nvidia is the biggest player in the game here. Now they build supercomputers that operate at national labs that are full of, you know, have thousands of GPUs in them intended to, you know, do numerical, either physics based or machine learning computations on. And so what, these machine learning frameworks, you know, specifically TensorFlow and PyTorch do is give you, you know, numpy style array data structures that can do the computations and exploit the power of GPUs. In addition to that, they have automatic differentiation implementations, again, on GPUs, so that they can do the optimization routine. So automatic, automatic differentiation is a computer implementation of the chain rule. It gives you, it gives you exact to machine precision derivatives without actually having, you know, manually by hand, compute the derivative. And you need those derivatives to optimize, you know, like any optimization routine or robust optimization routine usually has a derivative computation in it. So what those frameworks do is they give you those array data structures and they allow you to compute derivatives over those array data structures so that you can do optimization. And it's that optimization that you're using when you're training a neural network. Okay. CMK is Microsoft's version, you know, of that. And Keras is something that's built on pop up TensorFlow to make it more usable. So Keras, you know, if you're, you could actually use pie torches or num or pie or num, I'm sorry, you could use pie torches or TensorFlow, just as a kind of a drop in replacement for num pie that works on GPUs. And if that's all you did that, you know, you could use those frameworks directly. But if you actually want to train machine learning models, then Keras, you know, gives a lot of features that makes the hyperparameter tuning and other things a little easier. My recommendation of the platforms, they're all good and useful in their own ways, but my recommendation is to use pie torches. That's Facebook's deep learning framework. There's also, you know, you want to do Bayesian optimization in the Python world. Pi MC3 is a popular package. Pi MC4 is actually built on, it uses TensorFlow as the back in. Bo Torch is again, uses pie torches to back in and it's Facebook's Bayesian optimization framework. The other thing, the last package, it's not really related to big data or anything like that, but it's called X array and it's a package I'm really watching closely because what I, it kind of combines the best of pandas and num pie, right? So num pie, you have indemotional data structures, right? So you can have any number of dimensions. However, you have to refer to those dimensions by their index integer, right? So if you wrote some code two years ago and then you have an array in there that has four dimensions, and you have this, you wrote some code two years ago and you try to go look at that code and you see that, oh, well, you're, you know, my vector index, one, three, 27, 42, right? You have to go figure out what those index integers are, you know, what they refer to in the original data set. Whereas with pandas, we can refer to things by their name, which makes the code really readable, right? If I, if I have a data framing that has a column perosity in it and I'm, and I'm taking proceed and dividing it by a column permeability, if I come back to that code two years from now, and I look at that line, it's going to be really obvious what it is, right? Data frame perosity divided by permeability. What X array does, but, you know, pandas is more or less limited to two-dimensional data structures, things that look like tables, right? What X array does is give us labeled indimensional arrays. So we can, we can assign labels to the individual dimensions, which allow us to, you know, then write our code more like we would write pandas, right? So, you know, it could be a four-dimensional array, but we can refer to the dimensions via their names, right? So, you know, data set one permeability, you know, whatever, right? And it will make the code a lot more readable. The reason we don't, you know, I wouldn't, I don't, even though it's a, it's a fairly new project and I'm really a fan of it, the reason I don't use it right now in the course is because it's not fully compatible yet with SciPy and SciKit learning. They're working on it. It's got some partial compatibility, but, you know, in some cases, you'll probably run into problems where you, you know, try to use SciKit learning SciPy with X array. So that's, that's kind of why I don't, you know, it's something to watch and, you know, if you're just doing, you know, numerical array-like computations, which, you know, if you're used to using Excel to make plots and stuff and you want to just switch over Python, make plots, not really doing machine learning or anything like that, then, you know, maybe you can use X array and get away without any kind of trouble, but definitely something to keep an eye on for the future. Once it has full compatibility with SciPy and SciKit learning, then it kind of replaces NumPy and Pandas, you know, in one package at that point. That...
 Let's just, we're not going to spend to any time really reviewing decision trees because we just covered that. But the motivation here is the fact that ensemble approaches will allow us to build from a simple methodology like a decision tree and build a model that reduces model variance and the result is improved prediction power. Okay, so we want to build something that's going to be stronger on sample methods. Now let's go back. Let me, without reading, okay, we got model variance, model bias in our irreducible error without reading that. What was model variance? Who remembers model variance? These are critical concepts of machine learning. It's really good to have this in your memory. Anyone? Model variance. They're all three components of the error in your model in real world use or as we try to estimate it through testing. They're all three components, additive components of the mean squared error in testing. What was model variance? How about model bias? Anybody remember model bias? Model bias is due to the fact that your model is too simple. Your model is unable to reproduce the complexity in the natural system. In other words, your model is not flexible enough to capture the real complexity. It's an abstraction simplification. So model variance is kind of the opposite. They balance each other. In fact, we call it the model bias variance trade off. Okay, so this is a common concept of machine learning. The model variance was this idea of the variance. If we had estimate the model with a different training data set, model variance is the sensitivity to the data. Now it makes sense then that a more complicated model as we increase complexity will become more sensitive to the data. Okay, a ninth order polynomial, you change the training data and your ninth order polynomial will whip around a first order polynomial. Our line will not change that much as we change the data. Okay, so model variance will go up as the model complexity goes up. Model bias goes down as the model complexity goes up because our model is more flexible to fit the actual natural phenomenon. An irreducible error doesn't change. We can't fix that. We're stuck with it's due to the data. Now what we're going to do is we want to improve our expected mean squared error by reducing model variance. We want to reduce the variance of our estimates in training with our models. So these are all our inputs at testing locations. I said training I meant testing testing locations at unseen locations. And these are the estimates that we get with our estimated model. Now it turns out that one of the best ways that we can in fact reduce error in any system is through the power of averaging. Now there's a mathematical relationship called standard error in the mean. And the standard error in the mean is calculated as the variance of the means is equal to the variance of the original sample divided by the number of samples that you're averaging together. And what we know from that relationship is that if I make an average with 10 samples, I'm going to reduce the variance by one order of magnitude. The variance will drop down divided by 10. That is amazing, right? So this is standard error. And it means that when you do averaging, you reduce variance. So what if what if we were able to do this with our model? What if we were able to perform an averaging to reduce variance, I reduced model variance sensitivity to the data? That would be very, very powerful. Now for this to work, this relationship right here assumes independence between all of the models. And so this is maximal when the values averaged are not correlated. In other words, our models need to also be decorrelated. We have to decorlate all the models. Okay. So what do we need to do? Okay, this is the workflow. This is ensemble prediction in machine learning. We need to calculate multiple estimates for our prediction problem. Okay, we're not going to do just once like we've been doing. We got to do multiples. This requires multiple prediction models. Now you remember we had F hat is the estimate of the function that we used to go from the inputs to the outputs from predictor features, the response features for our system. This is our model of our system. Now we put a little B there. That little B superscript is going to basically represent an index of the models we make. We're going to make B little B through one through big B models. We're going to have a bunch of models, big B models. Okay, we can train multiple models on multiple data sets. But you know, you hear the squealing breaks in the background. These were like, hey, let's do this. This sounds really great. And then we just realized we only have access to one data set. To do this, we really would need multiple data sets that we could feed in to train this model and we would then get multiple models. But we only have access to a single data set. So it seems like we can't do it. Now do you guys remember the bootstrap? It seems like it was so long ago, doesn't it? You remember that? That was like a weekend, like two days ago we covered, did we cover the bootstrap on day one? We did, right? Okay. Somebody remind me what the bootstrap is. I'm taking a break. We're re-sampling. We put a little width replacement in there, but re-sampling with replacement from the original data set. Representivity is essential. You have to assume that the sample set was already representative. And if it's not, you got to deal with that upfront. Thank you very much. Okay. What was the goal or purpose of bootstrap? You're assessing the uncertainty in statistic. You know what's, you've gone a step further. You actually get uncertainty in the sample itself. Because you remember every time we did bootstrap, you remember we did the bootstrap cycling over and over again, Monte Carlo simulation, every time we did that we got a new sample. We got a new distribution. So guess what we can do? We can run the bootstrap right here. We can do this method to assess uncertainty in sample statistic by repeated sampling with replacement. We assume, as Sarah said, representivity. I agree. We got to over the bias, bias in, bias out. And then we have some limitations. Now, I'm not going to dwell on the limitations. Now if you want it, there are advanced workflows that account for spatial context. The bootstrap really does not account for spatial context. There are some limitations because of that because we work in a spatial context. So we got to be, we got to recognize that. Okay, so we can apply statistical bootstrap to obtain multiple realizations of the training data. Now we have YB, response feature, B, basically a new distribution, a new data set to work with. And we got the X's, which are all the predictor features. There going to be little B through one through big B examples. Then we can build a prediction model for each data. I'll call it a realization. It's a new data set drawn from the previous data set. Okay, now what we can do is we can pull all the predictions together. And what I should have shown here was a one divided by B here because this will be the average. We'll take the average of the prediction. So imagine I built big B models. And then I just say make a prediction, big B models, all of you make a prediction. And I take all of their predictions. I average them together. That averaging will reduce model variance. We got our multiple estimates. We reduce model variance by doing averaging. Now if you're doing classification, this arg max operator is majority rule. You simply say, oh, prediction model one said it's going to be sand, prediction model two said it'll be shale. Do do do through all of our B big B models. And then you take the majority rule. That's how you do it for classification. But all of this will reduce model variance. Now remember, model variances are great enemy. And it comes to more complicated models. It's that model variance ramping up with complexity that is the major source of error. We're not worried about model bias anymore. Complicated models tend to do a great job at fitting things. Okay. So we'll do regression. We'll take the average. There should be a one divided by B here. We'll get the average of all of the models or we do the arg max. Okay. Treebacking. What I just explained can be applied in what's known as treebacking. You build an ensemble of decision trees with multiple bootstrapped realizations of the data. Stop. That's exactly what treebacking is. Anybody ever ask you what treebacking is on the street. This is what you can say. And you'd be 100%. You get 100% my class for that. Comments the ensemble approach reduces model variance as expected. It works. Grow the trees as complicated as you like. You can overfit the trees now. The averaging between the trees will account for that. Isn't that great? It's like one stop shopping. We get everything. Now here's something kind of funny that happens. How is it possible that bootstrapped gives you a different distribution? It's because you do with replacement, which means that one realization of the data doesn't actually have all of the original data. It has some doubles or triples of the original data and has some data left out that don't even get involved. That left out data in expectation and statistical average, one third of the data is not used for each tree. You can try to on your own, do a bootstrap, n times from your data, and you'll find on average about one third of your data are not included. They get left out. Isn't that kind of cool? So, but one third of the data is left out. Now what we can do is every single time a data value gets left out, you make a prediction at that data location with your model. That is known as an out of bag sample and it's called out of bag estimation. Now, when you're done, you can take those one third of times that that data value was not used in the model. You look at all the estimates that were made then and you can go ahead and calculate the error. That's out of bag error. You can use that to validate the model as you build the model. In other words, when you do ensemble methods, you don't have the separate training and testing data. It happens under the hood. Kind of cool. Now, admittedly, it is assuming basically a one third testing data set every time, normally, because in expectation, sometimes from bootstrap, it'll be more or less, but there is that basic assumption cooked into it. Okay. Now, here's a problem. The assumption of this variance reduction, the assumption required for it was that the trees or the estimators must be decorrelated. With tree bagging, it doesn't often get decorrelated well enough. This will lead us to random forest, but we'll get to random forest right away. What does tree bagging look like? What would tree bagging be? We could build on ensemble of decision trees with multiple bootstrap realizations of data and we could average them to get predictions from all the models. So we got tree model 1, 2, 3, 4, 5, 6, all of them using different bootstrap realizations of the data. Now I apologize. I did not display the bootstrap realizations. So if you're looking really carefully, and I think some of you are, you'll see the data is the same. I just show all of the data. I didn't actually show. I should have coded it up to do that. I didn't dig into the guts to see exactly what was being used. But you'll notice the trees are different. You guys see the difference? Makes a big difference when you do bootstrap. Look what happens when you take the average of the six trees and now at every single location. So this value right here, this value right here plus this value right here, this value right here, this one, this one, and then you take the average, you get that value right there. Okay? So this value right here is just the average of all of those tree values at that one combination of predictor features. Repeat, repeat, repeat at all possible locations of averaging overall models. Okay? How does that change tree bagging? How does it change this model versus what we see with regular decision trees? Does it look like a usual decision tree? Okay. Okay. Okay. Okay. Okay.
 Variance reduction by having multiple models averaged together. That's what Ensemble Machine Learning is all about. Now there's other methods of Ensemble Machine Learning that we don't cover. One of them is called Boosting. You probably heard Boosting. Boosting is a really interesting idea. The idea is build a really weak predictor, not a very underfit predictor. Get the error. Now build a weak predictor of the error. Get that error. Build another one, build another one, and then build on top of each other. It'd be like a very simple one brand, basically a tree with maybe one or two splits. Then build another tree on the error, the residual from each. You kind of just keep going. You're basically fitting the error. Then you fit the error. Then you fit the error. You'll converge on a pretty good solution. That's another Ensemble method. That's called gradient boosting. Gradient because we actually apply an optimization approach with slow learning and so forth. That's another topic we could have talked about. Now there's a problem. It turns out that tree bagging is not that good. It's okay, but it's not great. We don't ever talk about tree bagging as being really cutting edge. We need another step. It turns out tree bagging that the individual trees may be highly correlated to each other. In the extreme, if all the trees are the same, do we get any variance reduction? I agree Tim. If all of the trees are exactly the same, your estimate will be basically the same as if you just used one tree. You would not have any variance reduction whatsoever. So we really do need the trees to be very dissimilar. Here's the idea. We need to decorolate the trees. The reason the trees become highly correlated is because you might have one dominant predictor feature. Parosity might be the most important. Birtleness has low importance. Every single tree starts with porosity split here. Porosity split there and there in the two regions and so forth. Now they're all going to look the same. It doesn't matter if you bootstrap the data. That one feature is so powerful, it's always going to show up. It's always going to put the divisions in about the same place. Random forest is tree bagging plus for each split only a subset of the M available predictors are candidates for the split. Now you remember when I taught you about decision tree, I said we scan across that feature for each region. We scan across this feature for each region. So we're scanning across all features trying to find the best greedy binary split when we work hierarchically. What random forest does is says for this split right now, you don't get to look at all the features. We're going to draw randomly a subset of features P and we're going to force you to use only those features. You have to ignore the other features. So for that split right now, a common rule is to say square root of M. If you have M equals nine features, you can only consider three features randomly for each split. That forces diversity. In other words, if I have a bunch of trees, some of them will start with a porosity split and some of them will start with a brittleness split by design. We force that to happen. Okay. That's how we do diversification of our trees. Okay. Now here's an example of random forest 300 trees trained to maximum depth of three. One predictor selected for each split. Because in this case, we only have two predictor features. We told it you can only randomly pick from one predictor feature for each split. You don't get to look at porosity and brittleness at the same time. By doing that, we force diversity in the trees and boom, we get the very best model we built so far today. This model right here, it's really, really powerful. There's a lot of really nice things about it. Now, you might be saying, well, come on, Michael, how do we know that this is really a diverse, rich tree or forest? The point is, unless you look at them, you don't know. What did I do? I took my random forest of 300 trees and I pulled out my first six trees and I told them, tell me where your first split is and I plotted it. This is the first split on one tree, another tree, another tree, another tree. Do you see what happened? Hey, the statistics work. We actually got 50% of the first splits and brittleness, 50% of the first splits in porosity. Now I hope you can see that if we had not done this, forced it to split on brittleness, all of our trees would have been probably porosity and they would all have been very similar to each other. They would have looked very similar. And that first split is very important. It's going to affect the tree diversity a lot. Any questions about this idea of forcing it to consider only a subset of features for splits? I've used a toy problem of only two predictor features so we can visualize. In your production use, I can imagine you'll probably be working with like three, four, five, six predictor feature inputs. In which case, you'll see a wide range of different first feature selections. The other thing too is, is there anybody out here who in the back of their head, they're thinking, this makes no sense. These trees are not very smart. They're suboptimal, aren't they? Like the very best first split is not in brittleness, but enforcing the tree to behave in a suboptimal manner, right? Is there anybody out there who's kind of thinking, that's weird. Why would that be a good thing to make a bad model? And so one way to think about this is this. There's a trade-off here, model variance, model bias and so forth. By reducing model variance in this manner with the ensemble approach and diversification of the trees, we actually have a situation where individual models are not as good as that one decision tree we would have built. But the overall aggregation over all of those models, in other words, a bunch of good models, not the very best models, is better than one best model. It seems kind of philosophical almost, right? It's like that idea of having a lot of well-informed people kind of voting and coming up with a consensus is actually better than having that one expert because that one expert is more variable. You see that? They're less stable. Okay. That was philosophical a little bit. Now if you weren't sure whether or not this is different than tree-based bagging that the random force is different than bagging, all I had to do was say random forest, do what you did before, but now you can pick from both predictor features for every split. So I said P is equal to M. If I do that, random forest behaves like tree bagging. It uses all of the features on every split and look at this. My first six trees out of 300. You see that? They're all basically very little difference. The first split is almost the same. Okay. So very little diversification. Now just to be clear about this, random forest does the exact same thing that a tree-bagging approach does, is that when you build a decision tree with random forest or tree-bagging, you're probably using in-expectation two-thirds of the data because of boot trap, just the nature of boot trap. One-third of the data on in-expectation is going to be left out and called out-of-bag samples. And we can predict at the out-of-bag samples and we get an error. Now this is the estimate right here for all of the out-of-bag samples. And what we can do is we can take the average of that and we get an out-of-bag estimate. Now I forgot the one over B again. I have a apologize about that. I need to fix that. Now we can calculate the mean-squared error out-of-bag by basically taking the average of the B divided by three of all of the estimates minus the true values. Okay. So we can do the automatically-do tuning, tuning, model tuning without having to do separation of the training and testing data. We're doing this by design within the model.
 Okay, so we got this workflow right here. And once again, we have some documentation up front talking about a prediction method. Yes. Talking about an ensemble approach. First of all, you got to understand the tree to understand the force. So this is the explanation of decision tree. All approach here it is, we're going to do averaging over multiple models to reduce model bearings. Tree bagging, we're going to use bootstrap out of bag sampling so we can do testing of the model as you build it, no test train split up front and random forest. Okay, let's go ahead. We got a little bit here to load. I'll explain it to you, but there is a kind of a complexity when it comes to tree bagging, but I'll explain it to show you how we do it. Okay, so we import all of our packages, a lot of scikit learn going on now, individual decision trees, the visualization, the trees, the standard scalar and stuff. I just imported all right there. Let's go ahead and get some of our functions. Not a big deal right here. So we can go ahead and we can be able to visualize the model, we can check the model, we'll go ahead and use these functions. Going to load our data, we're going to go ahead and we're going to visualize now I'm running this because this is super fast. When we get into the random forest, it's going to be slow, so I'll stop running and we'll just look at the results. We visualize the data, we go for some negative values. We've done this before. I am just racing through here. Okay, so don't worry, we've already looked at this a couple times. I like to have each code, each workflow have a little bit of this data preparation, of course. We're going to separate out our proximity, permeability to work with and we're going to go ahead and visualize, make sure that we're happy with our data to check our univariate distributions. We don't have to do test train split anymore though, we're not going to do that. We can visualize the production versus brittleness and porosity, that's what we're going to try to model. And I still did a splitter, but we really don't need to do that. Okay, let's go ahead and this are the steps. We're going to set the hyper parameters for individual trees. This is tree bagging. The first thing we'll do is tree bagging. Set the hyper parameters for the individual trees. Now remember, since we're doing an aggregation of multiple estimators, you can overfit. And if you look, look at what I did, max depth of 100, minimum number samples per leaf to, I said, woo, let's go big. And I said 100. I said, go really big or go home kind of thing, right? And what we can do is we can then instantiate an individual tree with those hyper parameters. Now we set the bagging hyper parameters number of trees. Now we have to make a choice of how many trees to build. So we'll say like maybe 100 enough trees and we'll set a random number seed. Why do we need a random number seed with tree, tree bagging? That operation and tree bagging would require random drawing. So in other words, it's interesting. You could technically have a different result because of the fact that you did different bootstraps, got different data sets, got different trees, made different estimates. Now if your number of trees is 100, I'm going to guess the difference between my result and your result. If we were to change the random number seed, would literally be like the eighth decimal point, not a big deal, right? But it's good to educationally wise to explain that, but also make sure we all are on the same page. Okay, now this is where it gets a little complicated. Previously we just had to instantiate set hyper parameters and then fit the model and we're done. Nope. We have to do it in second instantiation because it turns out that bagging is a wrapper. Kind of like the cross validation we did before we're K-fold cross validation. So bagging regressor, you give it the estimator, that estimator is going to be our decision tree regressor and then you tell it how many estimates to make so it knows how many times to do bootstrap and then you tell it what's the random number seed so it's able to do the bootstraps with the random drawings of the data. So the bagging model is going to be run with this encapsulation, this wrapper that takes my decision tree and runs it over and over again. It's a wrapper so it's just running the tree 100 times and taking the average of all the estimates. It's going to be able to do that. Now what we fit, we're going to fit that based on the wrapper which is the bagging model. Now what do I bring this up? Why do they use a wrapper like this in scikit learn? Why did they not just make a tree bagging method? It turns out could you, let me ask you this, could you use bagging with linear regression? Would that make a better linear regression model? Who thinks by show of hands or show yes, hit yes, that we could do bagging on linear regression? And you could say no if you don't think it'd be possible. Three, all right, Greg? What do you guys think? Let's think about the steps of bagging. Take a random subset of the data, fit the model, get the estimate. Take a random subset of the data, fit the model, get the estimate. Now do that enough times for n estimators and get the average of those results and use that as the estimate. Could I do that with linear regression? No reason why not? That would work perfectly well. Now linear regression probably doesn't have a runaway issue with model variance that you would perhaps see with using a decision tree, but still we could do it. This approach of a building, a bagging based estimator could be applied to any other machine. And that's why they made a wrapper in scikit learn because they said, well, why make a decision tree bagging approach when people will want to do bagging on everything? Okay, so that's why we have to do that second step. We take our bagging model, which is that wrapper with the bagging on our decision tree estimator and we fit the model. Now we can go ahead and run it. Now, as I mentioned before, we'll get into, I probably don't want to run all of this. So I hope you guys don't mind. We'll just walk through and talk about it. I apologize for that. It just takes more time. And it would take time now. If anybody wants to run this on their own time, please feel free to do it. Please feel free to change some things and try different things. Okay, so here we have the first example right here. What I'm going to do is I'm going to go ahead and run tree bagging, but I'm going to do it such that I say that the. Oh, where is it? This is a case where I'm actually going to do tree bagging by hand. I'm going to build one decision tree, one decision tree, each one of them has only one estimator. So there's only going to be one decision tree. Do you see these models are just a single estimation model? There's only one tree. You don't see any gradations here. It's just a sharp boundary between each region with the estimator. Then what I do down here is I visualize the performance of my individual decision trees, each one of them bootstrapping the data. I calculate the variance explained right here and they vary some are better than the others. Then what I do with this code right here, which is kind of fun is I take all of those. I actually average them by hand. I take that 10 by 10 or 100 by 100 response surface that represents these models right here. And I just average them by hand using broadcast methods and numpy doctor Foster will talk to you about broadcast methods, but basically I'm just doing a nice averaging. Oh, no, I'm not. I'm actually not doing broadcast members methods. Don't tell them. I'm actually doing a loop, which is kind of the worst type of coding. But anyway, I'll show you how not to do it. Then what we can do is we can take the average of the six models. And that's how I got that right there. And I got the cross validation result on that. So I'm basically just averaging over all of the estimators by hand demonstrating tree bagging. Next, what we can do is we can go ahead and we calculate a variety of different decision trees. Here's a decision tree tree bagging with one tree three trees five trees 10 trees 15 and 30 trees. So you can see the improvement in the amount of gradation. Now, if you look carefully, you'll recognize that this is exactly the same exercise or example that I showed within the course notes. So this code is giving you the way to create what exactly what I had in the course notes so you can do it for yourself. So we can loop over the number of trees and we can observe the transition from this creations and discrete behavior to a more continuous behavior. But is the under or over fit. That was totally under that's amazing. I actually did that pretty good. Okay. So it's exactly it's under its under fit. So I'm going to improve the complexity of the model at this point. Okay. Now I just show examples where I run a variety of different of number of trees, but I use different bootstraps seed numbers for each one of them. And what I demonstrate is with one tree, the models behave quite differently. We have high model variance with five trees. The models start to become more similar 10 trees. The models start to become quite a bit more different. So I'm really curious how long will this take to run. So I'm going to just try it. Okay. So I'm running this right now. Now if you were to run this in your own time, what you'll notice is that because this does take longer the most of the code that I have elsewhere. I don't know about you, but when you hit run and nothing happens and you just sit there, doesn't that kind of bother you. It's calling you're watching toast, you're watching water, try to boil and wondering when's going to happen. So what it is I put a little counter. So this is something you could do with your code. There are more intelligent counters you can do is there's actual counter objects you can use to put it like a progress bar on your code. I just did it manually with the print command. And so basically there's nine trees being built and right here, it's just counting down as it builds the trees. Now, the problem is that the later trees are bigger than the first trees. So it's a little bit of a non-linear counter. It's going to actually slow down a little bit as we get a little bit further along. Okay. I'll go ahead and I'll let that run. We can come back. If it was fast, I did a little test. If it was really fast, I was going to actually try to increase the number of trees for our averaging, but it's too slow. I don't want to spend time sitting here watching that we can look at the variety of different performances for different bootstrap realizations of the data from one tree, get a sense of overall variance in the model performance. By the time we get to 10 trees, the performance of the sets of models is starting to become more similar to each other. These ones here, there's quite a bit more differences, right? So it becoming more stable random forest. Now, no more of this bagging regressor wrapper. That goes away. Why? Because random forest is only tree based. You see that the whole approach around the forest is to constrain the number of predictor features considered for every split. Well, you heard me say split and features for split other machine learning methodologies don't do that. So random forest is uniquely a tree based method. So guess what? Now, scikit learn says we're not going to make a wrapper. We're going to have a random forest method regressor and classifier. Okay. And so I can say max depth random state, the random state because we do bootstrap again, number of estimators maximum features to consider for every split. I said it was one because we only had two features and I don't want to have it picking from both. I had to say one. Then I fit the random forest. My first forest right here. And that's it. And that's how I got the model right here. So it's really straightforward to run random forest. There's no wrapper. You instantiate the random forest with the hyper parameters and you fit it to the data. Now, let me just show you right here is where I looped over over again, where I basically froze my random forest to say only a tree depth of one. And by doing that and only one estimator each time I was able to see these first trees from a bunch of random forest. And I could see that they were diverse from each other now. And there's where I did it for tree bagging in which case all I had to do was say random forest. You can use all features for every split. It becomes tree bagging. Not a big deal. Now, I want to I'm going to finish up right now so we can break if people are getting a bit tired. But what I want to just point out is this. First of all, random forest if you tell it to will provide you a feature importance score. So when you run random forest, you can in fact get the feature importance. And it even provides a measure of error on that feature importance. And you can use that to rank now. Do you remember when we mentioned we didn't show explicitly we talked about feature selection. One approach to feature selection is you run a method like random forest, which is the model you're going to run anyway. And you calculate the feature importance for all of your features. Then what you do is you remove the features with low feature importance and you rerun the model. That is one methodology for feature selection that works quite well. So that is an option random force provide you with a great score that you can use. The other thing I want to show you is this. This is number of trees average versus the out of bag variants, the measure that we get while we build the model to basically help us tune our hyper parameters. If you look at it, look at what happens with low number of trees, you basically have a decision tree. In fact, you have worse than the decision tree. You have a sub optimal decision tree if you just do once. Because you said you can't use all the features. It's not going to be a very accurate decision tree. As you go up, your increase in number of decision trees looks look what happens. Random forest tends to do this. It's very robust. If you have enough estimators, the performance levels off with a pretty good level of performance depending on the quality of the problem and the quality of your data. So that's the first thing that's very interesting is that if you increase the number of trees that tends to average off, there's not as much an optimal. It's more of a sufficient number of trees. Look at this. This is the maximum tree depth versus the variance explained out of bag. What you'll find is with random forest, the same thing happens when it comes to the complexity of each individual tree. In fact, what will typically happen is if your trees are complicated enough, you'll get to a point where you get to a leveling off of the variance explained quality of your model. An additional complexity of the tree doesn't help you. It'll typically kind of level off. Now in some cases you might find it may start to fall down again, but often it's very robust. So in other words, that whole issue of over training the tree overfitting, I should say, kind of goes away. Random forest is much more robust when it comes to overfit. You can use very complicated individual estimators. The averaging takes care of overfit. Okay. Any questions about random forest performance use. Did my other random forest finish running? I'm sure it did. Let's see where was it. What did I run? It was right up here. Yeah, it finished running. Now in your own time, if you want to try something out, or you know what I'll tell you what. I said that these models are going to become more stable. The more trees that are going to be averaged together and we did it by going up to 10 trees. We're about to discuss another machine learning methodology in the background. Why do we run this? And just for fun, pick a number of trees. Pick a larger number of trees. I'll tell you what you can pick any number of trees you like. I'm going to pick something rather large like 100. I know you know what? I'm going to change this whole thing. Let me go one. Let me go 20. Let me go 100. I want to see a wider range of number trees averaged together. Because I want to show you a better visualization of the decrease in modelbearance. Okay. We're just trying something on try anything you like. Now remember, this run has been conducted. So we're changing the random number seed. So every one of the tree bagging models we're using is going to in fact get a different data set for each one of the trees because random number seed is different. So they're going to have different samples each. Then we aggregate over multiple estimators. And we want to demonstrate the reduction in model variance. In other words, this model, these models on this column are different than these models are different than these models right here. They use different subsets of data with bootstrapped to get there. Now if you compare the one tree bagging, which is effectively decision tree on a bootstrapped subset of the data, you'll see that the models are significantly different from each other. And the estimates here to here to here to here to here, they're they can all change quite a bit. Now when you get to 20 trees, look at that. They're starting to get more consistent from each to another. They really are starting to become pretty consistent. Now look at 100 trees. If I have 100 estimators with my ensemble machine learning approach, the model variance in making estimates over the full range of prostitutes, the bread on this look. If you were to difference those surfaces, I think you'd find there's very little difference at all. They're all becoming very, very consistent with each other. Is this what you guys observed when you increase the number. This is a visualization of model variance reduction through the use of multiple estimators and averaging the power of averaging. And that's how bagging works. That's all idea.
 Okay. So artificial neural networks. Now, it's a branch of machine learning that's around this nature inspired type of approach, where we take natural systems and there's many of them. Simulated and annealing is an optimization approach that tries to mimic what happens metallurgy. There's methodologies like swarm optimization, which tries to use the behavior of insects or any type of kind of groupings of animals that behave intelligently as a group and so forth. So neural networks use the brain and they try to mimic the processes that go on in the brain to be able to detect patterns and make complicated models. And they're very, very powerful. Looking at nature for inspiration to develop novel problem solving methods is actually a very cool idea. Nature has been developing and optimizing its system over billions of years. It learns something so we can learn from it. Artificial neural nets are inspired by biological neural networks. I think specifically in higher order mammals. I'm sure the reptilian brains not exactly this. The nodes in our model are artificial neurons. The connections are artificial synapses. So your brain is full of a bunch of nodes and these types of connections or synapses. Intelligence emerges from many connected, not very powerful processors, but there's many of them and they're very connected and it can become intelligent then. We want a prediction system. Now once again, even when we're doing artificial neural nets, remember it's y equals f of x where there's a bunch of x's. It's supervised learning, but it's going to be non-linear. It's going to be very, very powerful and flexible. Okay. Now for the remainder of my discussion and introduction of neural nets, this will be our neural net. Okay. What's our, this is our neural net. It's a single hidden layer. This is the inputs. This is the outputs. This is the hidden layer right here. It's not exposed to the external part of the machine. It's hidden layers. And it has a feed forward neural network fully connected feed for it means that the inputs come into the input node. All these input nodes on the input layer and they're fully connected. If all flows this direction and it goes fully connected, all of the input nodes go to each one of these hidden layer nodes. And then they all go to the output nodes. Okay. And everything's going this way. Just flowing across it or that way. I never know near immature. I think. Okay. The connections, those artificial synapses are these lines right here. This whatever information comes out of this node, the same thing goes on all of these connections. Whatever comes out of this node, the same thing goes to all of those connections. Okay. Information is fed through the system. The nodes are going to take a linearly weighted combination of the inputs and then non-linearly transform them. And we'll explain that. It's a very simple processor. If you think about it, a linear waiting plus a non-linear transform. Very simple. But a large number of these interconnected simple processors are going to have an emergent, emergent prediction of complicated patterns. That's all idea. Just to emphasize, this is the hidden layer, a hidden layer. This is the input layer. This is the output layer right here. Now, we're going to do this when we build our neural net. We're going to add another hidden layer as soon as you add and have two hidden layers, you're deep learning. Okay. You can say deep learning at that point. Okay. Deep learning. Okay. Normalization. You have to do normalization. We've done normalization. We commonly do a negative one positive one, min max normalization on all of the inputs. It's required because the non-linear transform works over a specific range. And if you go outside that range, it loses power. It just becomes very insensitive. Okay. So we want to, we want to maximize that sensitivity, improve activation function sensitivity. Now, it's okay. When we get our predictions, they'll be in the normalized space too. We just back transform them and we already commented. Thank you for the question. We can back transform any normalization, right? Okay. Now let's walk through our neural net. We're going to step through boom, boom, boom from inputs through hidden layer. Okay. So the first thing is we've got our normalization that happened. Then we got the information at the input layer. The input layer is simply going to pass that information on. It's going to take whatever information it is, the input normalized and pass it to this hidden layer. At that location right there, we're going to apply this function. It's a non-linear transform being applied to a linear waiting of the inputs, inputs one through P. There is going to be another term right here, a constant term. It's called a bias term. And based on different neural net designs, it can be different in every node or it might be constant over the entire layer. It's just how do you want to design the system? Okay. So then before the non-linear transform, we hit the bias term. We take the sum of the weights for each one of these streams times the value coming in from the stream. Now we apply our non-linear transform. Sigma is a non-linear transform. Sigma is going to be called an activation function. The key thing here is that non-linear transform allows us to reproduce fundamentally non-linear features. In fact, if we didn't have that, our neural net would technically be a linear system. We don't want to do that. Once we've applied our transform at the hidden layer, then we pass that same signal to all of the nodes in the output layer. It's fully connected feed forward. That's a neural net that we're building. And we go to the output layer with that piece of information. At the output layer, we're going to take an aggregate. All of the inputs coming from the hidden layer, it's fully connected. And it's going to be a certain signal from each one of those nodes. And we're going to do the same thing we just did. Non-linear transform plus a linear weighting. Now, this is interesting. At the output layer, if it's regression and we're doing a regression model, then we're going to use an activation function that is simply die identity. In other words, nothing. It's no transform. And the reason we do that is because we're not really trying to impose non-linearity here any longer. We're just trying to get it into the form for the output prediction. And here in regression, we're good. For classification, we're not good. Because for classification, I want to ensure a probability comes out. And a probability has what characteristic. Yeah. If you go to Komagorov's axioms of probability, probability must exceed or be zero. And probabilities must sum to one. And so this transform right here, this exponentiation is actually forcing the probabilities to be zero or greater than zero and to sum to one. Because this sum of all of the other numerators from the other nodes will result in a sum to one constraint. Okay, so we'll do that for classification, not a big deal. Then we reverse the standardization. If it's regression, we'll reverse the standardization. If not, there's or apply a transform to get back to original units. We don't have to worry about that. If it's a classification, we're already in probability and we're good to go. The parameters you have to train in your model include all of those weights at the hidden layer, we had all of those weights of the input from the input layers. At the output layer, we had all of the weights coming from the hidden layer. So there's going to be p weights. But we're doing the full common at total of the full connectedness. We in fact have p times m weights, just getting into the hidden layer. Then from the hidden layer to the output layer with our single hidden layer system, we've m times k weights. I hope everyone can see immediately that if you're using a pretty wide neural net with lots of nodes in the layers, it's a huge number of weights very quickly gets very large because that common tutorial. Plus we have the bias terms, which will be usually one term per node. So m and k bias terms. So we have a lot of training to do. The activation functions there to impose non-linearly in the system. You know, it introduces non-linear properties to the system without it we'd really have a linear system and increases our power to learn complicated patterns. In my graduate level course, we spend a bunch of time talking about activation functions for this class. I'll just show you some pictures and make a couple comments. But with that non-linearity at all of those little processors with enough connected nodes, artificial neural nets are known as the universal function approximator. In other words, basically artificial neural nets is throwing down a challenge. It's saying, give me a function. I'll fit it. I can fit anything. And if you have enough nodes enough data to support the inference you can do it. Here's some example activation functions, sigmoid or logistic, 10H, re allu, re allu is pretty common. They all have different properties and there's a lot I could say about it. Let me just make a couple of comments. We're going to use the L2 normalization, normalization to do the optimization of the weights. Technically, it requires a continuous function. And that might motivate you to use continuous functions. Like re allu, it has some really good kind of sharpness right here. It can result high result in high sensitivity. It can learn very quickly. So it's kind of all these tradeoffs. There's all kinds of choices that have to be made. I think it's logistic actually has a partial derivative that's equal to itself. It's identity. So basically, it's really fast to calculate. So there's all kinds of choices. I could done that in more detail. How do we train how are we going to train our neural net? We're going to set the neural net parameters to random values. We're going to run a forward pass, run our neural net with the training data. When we do that, we're going to get the estimates with that model. Now, we're going to get estimates and they're going to be really, really bad because we set them as random weights. We've basically uninformed, right? Now, what we can do is we can calculate a loss function. This should not be a surprise. This is the sum of squared error with all of our observations of the response feature at the sample data. Now, what we can do is we can take that and calculate the expected error. The expected L2 error is just going to be the average over all of those locations. So we have the expectation of the error. Okay, an average amount of error. We're going to go ahead and we can say that we want that expectation of error to be equal to a p parameter. And then we can go ahead and solve for the partial derivative of the change of that expected error relative to any one of the weights or parameters in the model. And this is fascinating. We can calculate the error and then we can move backwards through the system using partial derivatives and the chain rule. And we can calculate the change in error versus the change in each one of those weights in the model. Like even though there's so many of them, we can actually do that. That is called a back propagation using the chain rule of partial derivatives. We can go ahead and calculate that. I don't in my gradual of a course, we go through examples and explain that for now. I'll just say back propagation is using partial derivatives to basically partition the error in the result to each one of the weights. Now, if you've done that, you know which way to change the weights or how much to change the weights to improve the model. And you just cycle. Now, the cycling is not a big deal because machine learners like using epic naming. They call it epoch. Geosciences recognize that as an interval of time and geologic in deep time, right. And so they said epoch is going to be every single time I go through all of the data for one training cycle. But then they recognize that there's a trade off. You might want to use different size, what they call batches. In other words, I'll do an iteration where I won't take all of the data that's available to me, not all the training data. Larger, larger batches result in better estimates of error. But slower epochs, it takes longer time in order to do the cycles. And so you can run batches result in noisier estimates of the error. But you can run many, many iterations and you act fact get a better result often. It's better to do small batches and just run many of them. So we don't consider all the training data all the time. There's a trade off. Now, this is an optimization method. There's a gradient here. And so because of that, we have parameters like a learning rate. Anybody's done gradient optimization. Just think of the simple example of trying to find a minimum and a function. If you step too big, you can miss or overshoot the minimum value. Okay. So we reduce the learning rate to ensure we don't overstep momentum prevents our optimization from oscillating back and forth. It forces us to follow our more consistent path exploring through all of the possible combinatorial weights. And so momentum is basically forcing you not to oscillate. It makes you kind of move more smoothly. There's a lot of different hyper parameters and neural nets. We don't have time to get into them all. But there's a bunch of them. And you can change the activation functions to, of course, the design of the net, how big, how wide is the number of nodes per layer depth is the number of hidden layers. Okay. And the number of input layers and output layers will be constrained by the problem, of course, because you have so many inputs and so many outputs. Okay. There's a couple of thermos. Let me just mention one. The no free lunch theorem. It's a universal function approximator. But people were worried people would get too cocky when they heard that. The truth is, you cannot guarantee the models always optimum for predicting new unobserved cases. We could go back 200 years ago to a human, a philosopher. And he had a really good quote where he said, you can't know things you've never seen. You can't know things you never experienced. And that's what I would say about our machines. I say that about every model when it comes to extrapolation. Okay. Low interpretability, difficult to interrogate the model, high complexity model. Okay. Now there's a whole bunch of different neural nets we could build. We're going to do a fully connected feed forward neural net. Now deep learning. We'll do that too. Let's make the hidden layers higher so we can call it deep learning and demonstrate that. I think my example, in fact, is deep learning. Recurrent neural nets is when information flows backwards. It creates memory, very, very good for time based, you know, time series analysis and so forth time based forecasting. And that's what I did in the example on day one that we showed you with the production and the water flood. Convolutional neural nets account for two dimensional three dimensional information. We actually account for spatial locations. And that's why it works very well in images and models auto encoder for dimensionality reduction on and on and on. Lots of different methods.
 Now, this is a very simple problem. So I apologize in advance. Don't judge me. I wanted to give you a very simple experience with artificial neural nets. They are complicated. I thought of the simplest problem I could give you. Okay, but I want to show off the universal function approximator, of course. Okay, let's go ahead. We'll import a couple of packages. Now this is different. Now we're kind of going down a different trail. We're not going to use psychic learn for the actual model. We're going to use tensor flow, which is Google's artificial neural net program. Now here's the problem. TensorFlow is really hard to use. So carous is a front end. It allows you to basically simplify your interactions with tensor flow. It makes it much more understandable. So we're going to use carous. And so carous is loaded in tensor flow is the back in the front end is carous. Okay, now we can interact with carous. Let's load our data. I told you it's going to be simple depth versus normalized porosity. So I have a normalized porosity to work with. Okay, now I'm going to go ahead and I'm going to normalize the depth to and I'm going to repeat my normalization on porosity. I want to make sure I've got proper min max normalization negative one positive one or I'm in trouble. Okay, it's not I'm going to difficulty. Let me prove that to myself. Normalize depth minimum negative one maximum positive one. So I'm good. I want to make sure I do that. I want to mess that up with this normalization in my in my neural net. I'm going to do a train test split. We could do careful cross validation. The model takes a long time to run. I don't want to cycle. I'm just going to demonstrate for a simple example. We're we're going as simple as we can. Let's plot the data. This is my data right here. The original data and the normalized depth versus normalized porosity. Look at data. Do you see why I picked this data set. I want to make something really non-linear. I want to give you something very complicated to fit your neural net. And there's a lot of interesting multi scale cycles here. It's a hard model to fit. So let's go ahead and we're going to specify what are the bins over which we're going to make predictions. We're going to have a set of bins. In other words, depths from here to here. We're going to do a bunch of predictions so we can visualize our model. We'll run to run these predictions. Okay, here we go. If anybody's ever wanted to run a neural net before in Keras, you're about to. Okay, so this is it right here. We can run this. You have one node on the input layer. You have 500 nodes in the first in layer. And it's a rare Lou. Remember that stepy kind of looking activation function. 500 nodes. Now they're fully connected. So the number of weights is 500 times 500 weights plus 500 weights for the bias terms, right, the 500 bias terms. So there's a lot of parameters here. And then we go to the next layer 500 relus. So we got it's a really, really wide neural net. I had a little fun with this. Then what we can do is we can set the optimizer. Remember the back propagation. There's an adem optimizer, which is well known. It's well used. It's got a learning rate. It's got controls over the rate of learning at different times within model construction. And so there's a variety of parameters. I'll just mention learning rate. These sets the one I explained the best. But there's a bunch of other stuff we could talk about to set up our optimizer. So we go ahead and compile it. We say that we want to use a lost function means squared error. And so we can use that for our accuracy at the model. And now we can fit our model. If we do that and we run our model. How long does it take to run? Is a good question. The point is that when we ran this and this is from my last run, we'll take the test data that we separated. We didn't do KFL. Oh, it finished running. There it is. There it is. Who here agrees that that's a universal function approximator in action. Could basically fit anything. Now I want to admit something. Think about the number of parameters I just fit. I've heard of data that I had available. Like that clearly that model is at great risk of being overfit. And I've heard that I've heard the comments before in many tech meetups where people will be like, Jesus, when I work with these really complicated models, they're probably mostly overfit. So we've got to be really careful about our train test on these models. That's for sure. Now the way we did it just by withholding some of the data, we can see the error reduced as we have multiple epochs in which we're improving the model fit. The testing data, it didn't really improve too much. You can see. So really, we could probably stop pretty early and we were already doing just as well with the testing data. Yeah. Now what I've done because we don't have time to do this because this does take a little time. And you may actually benefit from the fact that if you run some of this in your own time, you'll have access to the server going forward, right. So you have time, try some of these experiments. So try different sizes of neural nets. I gave you some of my observations of what I saw when I played around. I tried different learning rates. So you can learn at different rates and see how it actually optimizes whether or not you get to a good solution or not. Try some of that. Now look at this. This is our neural net input one node 500 500 500 one node. The number of parameters. Look at that. There are what happened million trainable parameters in my neural net. And I'll tell you what it says something about the amazing GPU processing methodologies we've ever right now that you can iterate that thing a thousand times in the short amount of period we just did. It's incredible to me. Okay. Now we can visualize the neural net. There's a lot of diagnostics we can do. We can actually visualize all the weights. Don't do it. Don't do it. Okay, I did it. Don't don't do it again. It's not a good idea. But you could try to integrate your neural net. If it's a small neural net, you might learn something, but I'll tell you what this neural net is just way too vast for us to learn from it. Okay. This was neural nets.
Howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin and I record all of my lectures. This is my introduction to data analytics and geostatistics course and we are still in lecture two. This is the third video about probability. Now we start out with basic concepts of probability. What is probability? Frameworks for probability. Talk about frequentist probability. Now we're going to get into the good stuff. We'll talk about Bayesian probability. Let's go and we were talking in the last lecture about a lot of Vendagrams. We talked about the product rule. Let's derive Bayes' theorem directly from that. First of all, the product rule is simply a re-adjustment or shuffling of the definition of conditional probability. If we take the probability of B and we put it in the denominator on the left hand side, that is the definition of conditional probability. We could look at a Vendagram and convince ourselves that that works. The probability of A given B, well we shrink our universe down to only B and we'll realize immediately that that conditional probability is the joint A and B divided by the probability of B. We can see that this equation holds. Now remember, B and A are just labels of events. We could reverse them. We could go both ways. It's not a big deal. We can just switch B and A in different order and we can just calculate probability of B given A times probability of A. So these both will hold. Simple. Now we can go one step further and we can tell ourselves that the probability of B and A is equal to the probability joint A and B. There's no logical reason that switching the order should change the joint probability, the intersection A and B. So we would expect it to hold that these are equal to each other. Now what we can do, we can take these formulations right here, substitute them into this somewhat axiomatic relationship here. And we get the combination of two product rules right here. The probability of A given B times probability B is equal to probability of B given A times probability of A. This is known as Bayes theorem. So this is super awesome. This got us to a very fundamental Bayesian equation that we're going to use to solve a lot of really cool problems. And I'll show you how we can integrate belief into it. Let's just step back and ask ourselves, why do we want to use Bayesian probabilities at all? Why use Bayesian approaches? Well, how are they different from frequentist approaches? We have probabilities based on degree of belief, which I like to say expert experience, I believe doesn't sound so scientific, but expert experience in an event. You're going to update it with new information as new information is available. And we're going to be able to solve problems that we could not solve with simple frequencies. Now, what's an example of a problem we can't solve with simple frequencies? Now, from the book, Savaya, they're to Bayesian tutorial data and lyrics is just an awesome book. And I recommend it. There's a second edition that's out recently. And in the first edition, I'm not sure I didn't check the second edition, but they had an example of the question, the massive Jupiter, I bet you they updated it. Because at the time of the book, 1996, when it was published or when it was being worked on, I don't believe there were exoplanets formally discovered. Now, please don't quote me on that. I'm not an astrophysicist, but from the book, I don't think they had that yet. And so the question became what's the massive Jupiter? And they said a frequentist approach would be measure the mass of enough Jupiter-like planets from multiple solar systems. And then you could from that, formulate a distribution, make an estimate, you know, figure out probabilities through an experiment and multiple measures. Now, the Bayesian approach would be you can form a prior probability and update with any available information. They said in this problem, you could do that. We could use some knowledge about solar systems formation. We don't have an experiment to use, but we could use belief or expert knowledge about systems. And we could update with any other available expert information, new information. And we could get to a probability using a Bayesian approach. So let's go ahead and make a couple more comments on the Bayesian approach. The probabilities are going to be based on a state of knowledge and degree of belief in an event. We'll utilize an assessment of prior to data collection knowledge. We'll start with a prior. We'll update with new information that's available. We'll solve probability problems that we could not solve using the frequentist approach. There's no space outside of the experiment for the frequentist. It's all about counting and ratios. They can't incorporate belief in that framework. Advanced concepts here. Well, the Bayesian approach, it's interesting when we get into confidence intervals with the frequentist approach. We're going to realize that the Bayesian approach actually has what's known as credibility intervals. And there actually a more intuitive measure of uncertainty in a statistic with regard to the population parameter. So kind of cool the Bayesians. There's a win for the Bayesians again. The belief and more intuitive credibility intervals. So we'll get to that later. I just want to throw that in. Get people interested. You can skip to that lecture, but base them. So we just do a simple manipulation of the equation. We take the probability B. We put it in the denominator. This is the most common formulation of Bayes theorem. And what you can see very quickly. We'll make some observations here. What you can see is that we can do a form of conditional probability inversion, which is really, really cool. And I'll show you with a couple examples that we can often calculate this conditional probability B given A. We can't directly calculate that. But we can go from here to this. And this is the one you really care about. And so it's very valuable to be able to do that in version. The other way to look at it is. This term right here is known as the prior probability. That's given your previous belief before you gather new information. This is the likelihood that's integrating new information of data you just collected. This right here is the evidence term. It's a form of standardization. What's interesting is that the evidence term usually is just ensuring closure for the posture and the posture right here is the updated probability. And so what you want from prior new information and a posture is the updated very, very powerful stuff. Now I want to give you one warning here. And that is the prior should have no information from the likelihood or from the new data. This is known as peaking statistically or probabilistically speaking. You can't use information from the new data to formulate your prior. Otherwise you're double dipping. And you will prematurely shrink your uncertainties. And so you're fooling yourself into thinking you know more than you actually do. Like the hood and priors must not have be sharing information. That's very hard to do. All right. So just to rename this and give you maybe more intuitive expression, we could say the prior is the probability of the model. The likelihood is the probability of the new data given our model. The evidence term is the probability of the new data, which is actually really cool. And the posture is the probability model given the new data. It's the model updated by the new data. So this is very powerful stuff right here. Now it turns out that pesky evidence term is often very difficult to calculate. It really is the best way to calculate it is a form of marginalization. And so what we could do is we could calculate the probability in this case probability of a by calculating the probability of a and b plus the probability of a and b complement. And so we would expect that's equal to the probability of a and by the product rule, we can substitute the probability of a given b times probability b plus probability of a given b complement times the probability of b complement. Now, so we went from this original formulation right here to this one right here and you might think, oh, that looks like kind of a step back. It's more complicated. It turns out we have all these terms. In fact, look at this right here. This is half of it right here. So it's just the numerator again. And this part right here is the complement of b not occurring. And you'll find that we can often get access to that. I'll show you demonstrations where we do this calculation of the evidence term. And I'll show you how practical it is. I'll even introduce the concept of true positives and false positives and so forth. So it'll become very intuitive. Okay, so let's give you a class of problems over which we can use this formulation. That will be very powerful. Let me just introduce it like this. It is the class of problems for which you have the probability it looks like it's happening something is happening given something is happening. But you really want to know the probability something is happening given it looks like it's happening. In other words, you want to know the probability something's occurring given a test has shown that it's occurring. You see those are two different things having a positive test that looks like it's happening is not the same thing as it's happening. The test is not perfect. So let me give you a set of examples. Now this first one, I'm not being at all flippant, but it is a good way to get it into your mind. And this would be the case of you go to a doctor's office. And the event b is you test positive for a disease or some type of condition. What you really want to know is event a whether or not you have the condition or the disease. You see that those are two different things. You might have a situation more subsurface speaking or would be the geologist says there's a fault. That's the indicator, the test. But we really want to know if there is a fault laboratory measurement shows some low permeability. Well, there's an error in the measurement. And so we really want to know not the indicator that it's the test being positive. We want to know whether or not the sample really is low permeability and so forth. So we can go through all kinds of problems that are around this area of it looks like it's happening. But what I really want to know is something happening. Okay, given it looks like it's happening. Okay, so let's let's go ahead and just make a couple more comments about these classes of problems. If you look at that equation, this conditional probability, the likelihood is the correct detection rate. The probability it looks like it's happened given something is happening. That's the correct detection rate. And we're multiplying it by an occurrence rate. The probability that it could be happening. Is it a very rare condition? Is it very common? That'll have a big impact on the result. And this denominator is the old detection rate. This is a true positive rate. And the complement was the false positive rate shown right here. Okay, so let's go ahead and pick an example problem and work it out. Now this is a nice and two to one prior information at a site as subsurface environment suggests channel feature exists at a given location with the probability of 60%. If there's a channel feature that might have a major impact on subsurface flow volumetrics, it's important to us. We decide to further investigate this information uses seismic data. The seismic survey can show the channel feature is present with 90% probability if it's present. And if not present with 70% probability if it is really is not present. Okay, so let's go ahead and look at these probabilities and let's put them into a's and b's so we can plug it into our equation to make it simpler for us. The probability will a event is the features present the channel features present in the subsurface b is the seismic shows the feature a complement the feature does is not present in the subsurface. There's no channel. Lization in the subsurface and b complement seismic does not show that is channelization. Okay, so what do we have and what do we need to calculate? We have the probability of a that right there is our prior probability that without shooting seismic there's a 60% chance from our degree of belief before we have the seismic information that we have channelization. Okay, 60% so probability of a 60% 0.6 probability b given a that's the probability of seismic indicators channelization b given there is channelization. And we said if it's present channels are present 90% probability that size of is going to show it. Okay, so 90% probability of being able to detect it if it's there. But we said if it's not present it's going to say that feature is not present with 70% if it's not there. So the probability of seismic saying it's not there be complement given it is not there is 70%. So a little bit more difficult more challenge to be able to determine that it's not quite as high as 90%. Now the rest is just going to be plug in Chuck. Here's the equation right here. We're missing a complement and we're missing b given a complement. How do we solve for those? How do we solve for those? Well, remember closure the closure relationships that we had with the compliments. Okay, now that's enough hints for now. I'll go ahead and show you the solution in three, two, one. Here you go. Okay, so once again just by closure probability of a complement is one minus probability of a that's going to be 40% chance before you shoot seismic of just not having channelization. Probability b given a complement is going to be just simply equal to probability of one minus the probability of b complement given a complement. So we shrink our universe to a complement. And now we can say that the probability b complement plus the problem given a complement probability b given a complement they something one. So we get 30%. We can plug in Chuck and immediately we should recognize the numerator is the true positive detection rate true positive false positive detection rate. This is all detections. The evidence term is the probability all detections. Okay, so there we go. And we get 82% do you shoot the seismic? What do you think? If the seismic shows channelization is present there's 82% chance there is channelization. Prior was 60% chance that's a pretty big improvement in certainty seismic could be valuable. Now the cool thing if you were working in decision analysis those probabilities are exactly what you would need to figure out the value of information and solve for this problem. So, two another win for Bayesian statistics right there Bayesian probability. Okay, here's another example right here will work this one out to one in every thousand blow up per minute. The BOPs has serious crack they just have that problem X ray analysis has a 99% chance of detecting a crack correctly. In other words saying there's a crack if there's a crack. If the BOP does not have a crack there's a 2% chance of the X rate detecting the crack that's a false positive of 2%. Okay, false positive rate. And the rate of blow up per minute cracks is 0.1%. So that is the overall prior of having those cracks occurring. Okay, the blow up per vendor has been X rate and the result is positive. What is the chance of blow up per vendor actually does have a crack. Okay, so let's define events a blow up per vendor has cracks. Okay, the thing is happening. B the test indicates it has a crack X rate test and the a complement and see complement. What do we have so far with the probability of a the prior we said it's going to be 0.1% 0.001 is the crack rate just overall. We said the true positive for the X rate test detecting a crack given there is a crack is 99% and we said a false positive rate of detecting a crack given there is no crack is 2%. Alright, so let's go ahead and work this out. I will show you the solution in 3 2 1 and here it is. Okay, we were just missing the a complement. We had everything else we need it. Not a big deal using closure simple, simple closure. We can say that probability of a complement there is no crack in the blow up per vendor is 0.999. Okay, so then we just go ahead and we can plug in chug and what do we find out. We find that the probability of a crack in the BOP given a test is positive for crack is only 4.7%. Whoa, so what happened what kind of wrecked our test here and what happened here the cracks are very unlikely. The rate of cracks is so so small 1.000.000. Right. And so because of that and the fact that we have kind of a large false positive rate the false positive swamp the equation we end up with a high number here low number here low number here and we get overall a low percentage for the conditional of having the crack given the test was positive. Okay, so this is it now what's really interesting and I did show this lecture previously and I got really good feedback on my YouTube channel and they said well no I object I think it's a good test. Let's talk about that. The prior probability of crack was 1.000. After you run the test you're now up to almost 5%. Now imagine that you've gone from thinking there's 1.000 chance 0.1% and now you've gone to almost 5% probability of having a crack. This is not a conclusive test but this is a very very good indicator if you can do this cheaply and quickly it would actually make sense to run this on the blowout preventors because you're greatly changing that degree of certainty. Okay, so we're greatly updating it could be seen as being valuable still. All right, I hope that was to you. If you want a little more hands on experience I've worked out an Excel spreadsheet where I take this basic abasian formulation and I run the experiment in which you get to change the yellow cells you get change the prior probability of the thing happening. The positive and false detection rates and so forth and then observe the overall conditional probability of the thing happening given the test was positive and it provides all kinds of outputs and explanations and so forth and you can play with the numbers. Not a bad idea. Everybody can work with Excel. And here's some ideas of things to try. If you want to get a hold of this spreadsheet that's well documented it's available to you on my GitHub repository in my Excel numerical demos specific repository under Geostats guy on GitHub. I just want to make some general comments about what we just learned. A combination of rare events and high false to positive rates can make conditional probability of an event given an indication very low. We calculate the posture and we can compare it to the prior and use this to assess the value of the information of the test. So that is that can be very useful to us. We go from the prior probability something's happening before the test to probability something's happening. Given it looks like it's happening from the test and that's a good way to measure the usefulness of our test. Now I went to Wikipedia and they had a really nice example of Bayesian a Bayesian problem and I thought this was a really good one to share. This is different class of problems but it does demonstrate how we can use Bayes theorem to solve things actually that would be kind of tricky without using a Bayesian approach. Okay so we got three machines machine one two and three and they produce products. 20% of the production comes from one 30% from two 50% from machine three they're mutually exclusive. Okay X one X two X three machine one through three mutually exclusive each sample only comes from one but it's exhaustive every sample has to come from one of them. The error rate on each one of the machines are different. Apparently we got some better machines worse machines maybe the crews working them maybe night crew or something but there's a reason there's there's difference. The machine one has a 5% error rate on the products. Probability of error Y is equal to error given X one is 5% probability of error given X two machine two three percent and one percent for machine three. What is the probability of an error in the product overall? Now what's cool what would be hint how do you solve this? What we want to do is marginalization. We want to sum across all of the machines because they're exhaustive and mutually exclusive we can use this formulation right here probability Y is just give me the summation of the probability Y and machine X one or X two or X three. Or the summation of the probability Y given the machine X i times the probability of X i and you do the sum over three machines we can go ahead and get that let's go ahead and solve that. All right so give it a try I'll show you the answer in three two one. All right so this was pretty straightforward. All we had to do is plug and chug we we have mutually exclusive exhaustive events so we're able to just do the summation of the joints Y and X i's over three machines we can expand it using the product rule and we get this formulation right here and so we find out that the probability overall error rate is going to be 2.4%. Now if you think about it it's effectively like a waiting based on the percentage or proportion of production coming from each one of those. That's another way to think about this a waiting of the error rates by the amount of production coming from each one of the machines. Now that was super easy but we needed it it's now the evidence term because that's the probability Y over all and we can use it now to use Bayes theorem and solve harder problems that are less intuitive. What's the probability product came from each machine given an error was observed. Now this is the probability inversion the conditional probability inversion that I talked about before. What's the probability of X i from machine one or two or three given there was an error Y. Okay so we already got the evidence term probability of Y is 2.4%. All we have to do is calculate the conditional probability from Bayes relation. We want the probability of coming from machine I given an error and we can solve it like this. And what you can see immediately this would have been hard to do directly but we can this is much more intuitive and easy to work out and this was easy to work out too. I'm going to show you the answer for each one of the three machines in 3 2 1. Look at that isn't that awesome I just love it. So this is the formulation no surprises we already showed that to you. We can plug and chug. We get the probability of an error given machine one of 5% 5% probability of coming from machine one is 20% 20% the production divided by the evidence term of 2.4% and we get 41% or 0.41. That's the probability if there's an error in the product there's a 0.41 probability that it came from machine one. And machine three and machine two now they're mutually exclusive exhaustive so we would expect if we do the summation over all of these three that it will sum to 1 and that makes sense right the probability machine one given error plus probability machine two given an error probability machine three given an error. Well that should have closure in the conditional sense. Now I provide a really nice example of using Bayesian probability in the case of exploration. And so this is a spreadsheet that's well documented available to you on GitHub at this link it's within my repository excel numerical demos under my count geostats guy. Now what are we doing here? So you have some type of prior probability for the exploration success rate. So what's the probability of exploration success of 35% 10% 45% 20% and so forth. Now if you take all of these I just entered them we can standardize them so they sum to 1 so now 45% becomes 50% probability 25% probability 25% probability for the 45% as the mode and 35% and 55% shown right here. If you visualize that distribution that orange distribution is the prior distribution it's our distribution for success rate prior to new information. Then what we do we enter in new drilling outcomes the new information that we've available so we had 30 wells drilled 10 success 20 failures. We have in the recent drilling campaign a success rate of 33% and you notice our prior was saying about 45% was our mode our most likely success rate in our in our exploration program our wall cat drilling. Now what we can do is we can use Bayesian probability to get the posture to update the prior with the new drilling information likelihood to get to a posture. Now how do we calculate the likelihood distribution not a problem really simple to do because we in fact have the outcomes of the recent drilling we had 10 successes 20 failures out of 30 in total so I can go down here and say what is the probability of that outcome given a success rate in our exploration actually is 0.05. That right there is going to be the probability of the result given the model the success rate of 0.05 and we can solve that for all of these possible success rates in exploration and that will give us the blue distribution right here. Now if you look at it actually makes real sense because you see where the mode is the mode is actually at about 33% or one third so it makes sense that's the most likely outcome given what we observe the data and it diminishes both ways it becomes very unlikely that the success rate in exploration given these observations is something like 80% 90% it's so far away from what we observed in the success rate so we got a prior distribution from belief or previous experience we got a likelihood distribution from new results right from the binomial distribution and we'll talk more about parametric distributions later and we got the green distribution is the updated distribution the posture distribution and so this spreadsheet allows you to actually play around with and use Bayesian opting based updating in order to solve a really interesting problem in exploration and drilling now if you want to work with entire distributions with Bayesian probabilities it turns out that in the case of a Gaussian distribution and we assume the prior likelihood and posture or all Gaussian and in a couple lectures will define and work with Gaussian distributions but that nice bell shaped curb distribution it turns out that we can calculate the up data or posture or mean and variance directly from the means and variances that the priors and likelihoods so it's really simple there's a close form analytical solution it was shown in the survive book on Bayesian updating now let's make an interesting observation if we go down here and look at the variance of the posture you're gonna immediately recognize the fact that none of the means from the likelihood of the priors are in the equation in other words the degree of variability in the posture is not dependent on the central tendency or the location of the Gaussian distributions for the prior or the likelihood this is known as homoscedasticity the amount of variation is consistent over the values of centrality all right we'll get more into that later on I just it just cheers me up to use the word homoscedasticity or heteroscedasticity we'll talk more about it and formalize all of this but we can do Bayesian updating with Gaussian distributions here's another spreadsheet Bayesian Gaussian demo on my Excel numerical demos you can play around the yellow cells here change the average variances of the prior and likelihood distributions and they'll calculate the posture mean and variance and so you can go from prior the yellow line the CDF right here Gaussian the S curve nice shape there you can take the likelihood which is blue you just put that in and you get the green which is the posture the updated with Bayesian updating very powerful stuff now of course we have cases in which we're working Bayesian probability and we're working with multiple mutually exclusive events and our machines were doing that in fact right the product had to come from one of the three machines there was no other choice in that case we can just make the comment as we did with the machines that we can calculate the evidence term in a pretty simple manner we can go ahead and just do the summation of all of the joints now be careful if you don't have mutually exclusive exhaustive events you can't do this but so we can just substitute the probability B is just being the summation of the probability B and a the multiple a events a one through K okay so I hope this was helpful to you this has been a lecture on Bayesian probability so we've covered what is probability in the first lecture just some general comments about frameworks or how we could think of probability the second lecture we caught we talked about the area of frequentist based probability and we in this last lecture this one just now we derived based theorem and then used it to solve a lot of problems that would have been very difficult to solve using just frequentist approaches all right I hope this was helpful to you I'm Michael Perch I'm an associate professor at the University of Texas at Austin I record all my lectures put them online to support my students shout out to you guys keep working we're getting through the term and also to support working professionals or to remove barriers I love the idea of folks maybe high school students or someone who's not currently in university thinking about opportunities to go into science and engineering we need more STEM all right everybody I hope this was helpful take care
to put it into the course notes permanently. But simple models like for example linear regression have high model model bias, but they have low variance. That is, they're fairly insensitive to changes in the data or retraining. And so what I have here is just a set of data points, the blue lines, the blue dots are either, or what I'm using to train a linear regression model. Okay. That blue line that passes through them is the trained model, right? It's the output of the model. Over here, I have the test residual error. So the red dots are my test data, right? So these are, this is, you know, I use the blue dots to calibrate my model. And I use the red dots to assess the predictability of the model, right? So basically what I want to, you know, I hold out the red dots and then I say, okay, well, how good is my blue line here when I compare it with the red dots? And that's what I'm plotting over here. And it's the total residual error. Okay. So each of these, they're just plotted against their index. So for the most part, I've got the scaling fix here. You'll see why in a second. But for the most part, there's low error with respect to the data or low with respect to what I'm going to show you in a second, right? So this is basically the simplest machine learning model we could come up with, right? Linear regression. And here's what I mean by low variance. So if I move the training data around, right? So you can think of this as like taking a different sample set. So I'm, I'm taking those blue dots and I'm just wiggling them around in space. But you could also think of them like those are separate data points. I move them. You can think of that. Those are a different set of data points that I'm using to train my model. And you see that I, as I move them around, the blue line, the model, doesn't change that much. It's fairly insensitive to the data, changes in the data. Okay. Now let's see what happens when we go to a higher order polynomial, right? So here we're adding complexity, right? So in this case, I'm just going up polynomial order. So instead of using a straight line, I'm going to use a cubic line to describe the data. Okay. Okay. In this case, still not so bad, it doesn't change terribly with respect to the data, you know, the variance is fairly low and the error is still fairly low. Okay. And I could go on to the fifth order, but let's go ahead and look at seventh order. Right. So if I have a seventh order polynomial now, you notice that the seventh order polynomial passes through every blue data point exactly, right? There's no error with respect to the blue dots. There is error with respect to the red dots and the prediction. And it, and that's plotted over here. And again, you can see that already there's more error with respect to the prediction than we had before. Okay. But let's look at what happens when I move the data around. So as I move the data around, you see that the error grows enormously with respect to the prediction. So again, we don't care. You know, it's so funny. We've been trained our whole lives and to talk about our squared values and things like that. But all that, all that does is tell us how well we fit the data that we already have. It gives us no ability to predict anything. Right. So you can have the greatest fit in the world. And here's one of them. Right. This is a great fit to the blue data, but virtually unable to predict the red data in any meaningful way. So probably the best is somewhere like a third or a fifth. I think the third is actually the best, right? And it's not great. But you see there, even with the third order polynomial, we're not fitting the blue dots exactly. But in general, we have the lowest mean squared error with respect to all of the data. And this is why it's really, in my opinion, you should always start simple. There's a concept called Occam's Razor. Is anybody familiar with Occam's Razor? Can you maybe hit your thumbs up or yes in the, if you've heard of the Occam's Razor? Okay. So Occam's Razor, let me see. I'm trying to navigate zoom here where I can see the there we go. So I have a couple of people to say yes and a couple of people say no. Thank you for that. So Occam's Razor is a concept that basically says amongst competing models, the correct choice is always the simplest one. And the reason for that is that it has to do with how we fundamentally validate models. We use the word validate often. But in truth, we never validate the model. We actually only, we can never say a model is valid. What we can do is we can test it and try to falsify it. And we can certainly say that it's invalid. So we start with a simple model. And the reason we prefer simple models to complex ones is that they're most easily falsified. If we start with a linear regression model and it doesn't fit our data at all in any meaningful way, it's easy to falsify. And then we can go on and add complexity. And so Occam's Razor stated simply is basically saying that amongst competing models, the correct model to choose is always the simplest one. And the reason for that is begin because of just the process, the way we do the scientific method, the way we set out to validate models. We never actually validate. We can only say that they're not invalid. So we often use the word valid as a substitute for not invalid. In other words, we try to falsify the model. And in the absence of being able to falsify it, all we can say is that it's not invalid. We can't, we can't say with certainty that there's not some scenario, some possible combination of parameters or some possible real data set that would invalidate our model because we don't know. And so again, just to bring home the point here, like I think Dr. Purchan and I are really on the same page with that. I'm not saying that there's not a place for neural networks. I use them in my research. But I don't think we should start with a neural network.
Hey, good day everyone. We're going to talk about the bootstrap right now. Kind of excited about this. We throw bootstrap in right now. It's not really related only to Bavaria statistics you could perform the bootstrap on anything because the bootstrap is a statistical resampling procedure to calculate uncertainty in a statistic from the data itself. That's why we call it bootstrap because it's like that old English saying of pulling yourself up by your bootstrap. You're getting the uncertainty in a sample statistic from the sample itself, which is pretty powerful. So what I'll do is I will show you a workflow for bootstrap. Then I will get out my cowboy hat and we'll do a really quick kind of just start doing a little bit of bootstrap with hat in hand. So let's go ahead and just start talking about the workflow. So imagine you have some type of phenomenon. You have a the subsurface, you have perosity, you have permeability, whatever it is. To see it doesn't matter. You have something that you're sampling. So in this case we're sampling EUR and we're sampling it by drilling multiple wells. And so EUR in subsurface petroleum speak is the estimated ultimate recovery of a well. It'd be measured in the total amount of barrels that you would expect to recover from the well. Okay. So now you have that distribution which comes from N equals 10 samples which were the 10 wells you drilled. And so I just sketched the distribution. Please don't look at it too carefully. I'm sure it's not perfectly correct but I made what should be a histogram of the outcomes from the 10 wells. We can convert that into a CDF and now we have a CDF. It's not a big deal. We can just integrate across CDF from a histogram or from a PDF's per history port integration. Okay. Then what we do once we have the CDF we're going to take and do 10 Monte Carlo simulations 10 because N was equal to 10. So we're going to draw 10 times with replacement from that distribution which is effectively the same as doing 10 Monte Carlo simulations directly from that distribution. So I do the 10 Monte Carlo simulations. They're all independent of each other. There's no correlation or anything here. I do 10 simulations. Every time I do a simulation I get a value. I do 10 times I get a new distribution. Would you agree with me that that distribution will not be the same as the original distribution that we used to build that CDF? There is the possibility it's going to be different. And so we'd expect to be different. If you had a million samples and you were doing a million resamples or Monte Carlo simulations maybe in that case you'll get back exactly what you put in. But for the case of 10 you could imagine that you could have this one sample here drawn two times or maybe this one sample responsible for that little piece of bar there could be not drawn at all. And so the histogram will change based on what happened. Those 10 Monte Carlo simulations. We had a new histogram and you calculate the statistic you're interested in getting uncertainty in. In this case it's just the average of the you are over 10 wells. Then we put that measure here and we're starting to build a new histogram right there. Okay so now we can step forward and we will repeat. We'll do 10 Monte Carlo simulations from that CDF. We're drawing with replacement from the sample set effectively. We get a new distribution it's not the same as this one not the same as the original one. And we calculate the average and we put the average right here. We repeat that. The third time and so we'll call these realizations. Each one of the realizations comes from 10 Monte Carlo simulations from that initial distribution and we calculate the statistic of interest the one that we want the uncertainty in from that new realization of distribution and we plot it right there. You see where this is going I guess I can spoil it now right. Basically you do that L times where L is the total number of realizations and L can in fact be a large number. Remember this computational activity of calculating the the 10 Monte Carlo drawings and calculating the average of them is incredibly fast. So you just said L is large. It a large enough that we now can define this distribution right here which is the distribution of the realization realization averages over all of those sets or L realizations that we have right. So L realization averages. So we have a distribution so that distribution will have like a hundred or thousand samples in it. There will be a distribution and what's very interesting is that that distribution is now treated as the uncertainty in the average given the fact that we had this initial CDF and we had 10 samples in that initial CDF. And so that's the bootstrap idea. You can uncertainty in the statistic from the sample set itself. We took the sample set, delta CDF, we performed N Monte Carlo simulations to get a single realization of the sample set. Then we calculated the sample statistic we're interested in and we repeated L times in order to get a distribution of that sample statistic and that is our uncertainty in the sample statistic. So we get a distribution. So if it's uncertainty and the average EUR, we can now look at the mean. We would expect the mean. There's no bias in the system. We'd expect the mean to be equal to the original sample mean. If we have enough L, we didn't have samples from it. We would get a variance or a standard deviation, a measure of spread. Now, spoiler alert, the spread will actually be exactly what we would have calculated for standard error of a mean. And we've talked about standard error before. And so it's good. We know that this technique works. It matches what theory should do. Why don't we just use standard error? Well, you can bootstrap anything. I can bootstrap to get uncertainty in the average, the variance, the P10, the P13, my personal favorite percentile or anything that you want to bootstrap. So extremely powerful. We wouldn't have theoretical methodologies to get at those, but we can bootstrap it. So the bootstrap approach it was proposed after I'm back in 1982. This is the general workflow that we talked about before. We're going to gather the sample set. Of course, and my students asked about this in class, I really appreciate that. Was the issue around what if you have bias in the original sample set? And the answer is bias in bias out. If you have bias in the original sample set, that bias will propagate through the bootstrap. It won't fix it. There's no fixing it through bootstrap. We'd have to do something prior to the bootstrap exercise. Okay, so we've got N samples. We do N random samples with replacement. So we're going to randomly sample with replacement. I keep saying with replacement because if you if you didn't do with replacement, every time I drew a sample, I just kept it. If I do N times, you get the same as the sample data set. We're going to sample every sample. So you have to replace them so that you get realizations. You calculate the summary statistic of that realization L where we have L through one L equal one through L big L realizations. And then you go ahead and you pool the realizations, the sample statistic from all of those big L realizations and you can summarize the uncertainty. And we've talked about a lot about different types of summaries of distributions. You can of course do a range min max minus the min into quartile range, p 25, p 75 minus the p 25. You could do a standard deviation. You could do anything. Okay, the power of this methodology is that you can get uncertainty. Any sample statistic with any sample, with any shape, it's completely general and we would not be able to do that without bootstrap. So it's very powerful. Now when you're in a meeting, somebody says, oh, we're going to bootstrap to it. You now know what we're talking about. You know exactly what that is. There's of course advanced forms of bootstrap that count for spatial context that count for data collection and so forth. All of these methodologies are much more advanced outside the scope of anything we're talking about here. Okay, let's see. What else? It's a super powerful tool that caveats. Just I mentioned some of these that assumes the sample set is represented. If there's bison, bison, we get propagated through. It assumes all samples are independent, doesn't consider any spatial context whatsoever. They're IID independent, identically distributed samples. If you have a spatial context, you have locations with different kind of central tendencies like a trend model. If you have spatial correlation between samples now we're using journal, on journal, professor journal, previously Stanford, the Skurf Center, his concept of spatial bootstrap is what we need to do and you can do it in Excel. But if you can do an Excel, you can also do it in a cowboy hat. And so this is my cowboy hat. And what I'm going to do is I'm going to take my hat. I'm going to put down on my desk right here and I'll change my view a little bit so you can see my hat right here, my messy desk. And I'm going to take these stickers and put them in the hat. Two of them are orange stickers for my center. Three of them are white stickers. And so what's the statistic of interest for me? My statistic of interest is the proportion of orange stickers because it's burnt orange and I'm here at UT and we got to be kind of like in that burnt orange, right? So basically what I'm going to do is I want uncertainty in the fraction or proportion of orange stickers. So I'm not looking, see I'm looking this way. I'm going to draw randomly in the hat. And so I do a Monte Carlo simulation and my first one is white. I have to take that and put that back with replacement. So it's back in the hat. I'm not looking. I go ahead and draw another one. It's white again. Two whites. Better keep track of this so I don't get confused. Okay, so white and burnt orange. Okay, so I've got two. Go back in here and I'm not cheating and I get, oh, oh, oh, I got an orange one. So that's good. So I got an orange one. And in my hat, is this truly a random process? Okay, did I peak? I peaked. Okay, what I get? I got orange again. And let me see, mix them up, mix them up. And I draw an orange one again. Okay, so what did I get? I got for my first sample set. I got three white and, oh, no, two white and three orange. So I was worried about the proportion of orange. And so my first realization is 60% 3 out of 5 were orange. Now remind ourselves that the data set we're working with is in fact 40% white. And that makes 40% orange. And that makes sense that there's no reason to suspect the law of small numbers. In other words, we don't expect every sample set to be exactly the same as far as the test. Okay, so I could do this again. I could do this again. I could do, it would be very time consuming for me to keep doing this. But I could convince you that we could have an outcome for which I could be very lucky. And I could get 80% orange. I could just happen to draw 80% orange. I could also be unlucky and draw 20% orange. And who knows it is possible. We could work it out using the binomial distribution. We could have the case where we get 0% 0% orange. And we could also have the case where we just happen to draw 100% orange. If I do that enough, what will happen over time is that I will in fact build up a distribution of outcomes. And there would be, I would suspect that since I have in fact two orange and three white, I would expect this would be centered somewhere around 40% and I would expect there'd be some spread where I could go all the way down to zero, all the way up. And not necessarily exactly Gaussian distributed. And it would be a discrete distribution. I'd only have cases of 0%, 20%, 40%, 60%, 80%, so forth. But this would be a distribution of the possible outcomes for the proportion of orange. And that would be my uncertainty distribution of orange. And what's really cool is I did all that by simply resampling with replacement for my initial sample set and by having a very good cowboy hat. Okay, I'm kidding. But anyway, I was able to test that out. Now, of course, that's, you know, having a cowboy hat or any type of hat, I'm not specific to hat type. And doing this type of redrawing is a good way academically to explain the methodology. But we can of course do it more numerically. Monte Carlo is cheap. We can do it all around us. And so what I've done is I have a bootstrap demonstration coded up in Excel. I've done an R2, I think there's something I'm pith on on GitHub, which I've done and other people have done it too. I'm not, you know, but the, so I got a little bit tricky with Excel functions. So what I have is I have data values one through 20. You could adjust the sheet and put more data in here. So I assume I have 20 data values. I draw a random p value. And I do the norm inverse to get a standard normal value basically Monte Carlo simulation standard normal values right here. So this right here is my data set. Then what I do is I use the V lookup command to look up the right index. And I use a random times the number of samples truncated by integer plus one to get a random value between one and 21 and the number of samples. And then I use the lookup to look up by this index and get me one of those random numbers. So effectively this command is just drawing a random value from the set of numbers and to prove it to you. You can see that negative 1.3, 1.938 is right here. It's the eighth number. And if you look through all of these numbers, in fact, come from this list of numbers right here. And if you look carefully, maybe there's some repeating going on. Can I find something really quickly? No. But there should be so here's a repeat right here. You see that? There's a repeat two times that same number was looked up. And you oh, it's only two times. But you might you'll find out. So it's with replacement. We just proved it to ourselves drawing from these numbers. You can find that number is actually right here. So it's number 11 index 11 from that number set. So by doing a little bit of kind of what's the term for it? Just making it work with Excel, I was able to draw 20 times from the set of numbers with replacement. And that's my realization number one right there. Then I did that for L realizations. And so these L realizations would go all the way to 50 realizations. And so this is 50 realizations of those 20 data sets, Monte Carlo 20 times to form new realizations of the sample set. Then what I did is I calculate the mean, the standard deviation, the min, the max, the p 10, p 50, p 90 for each one of those realizations. And then you come down here and I've actually calculated the summary statistics of each one of those summary statistics realizations. It's kind of cool right? And so this here is for the case of 20 random standard normal values. The uncertainty distribution in the min, the p 10 sorry, the mean, the p 90 and the max. And I got a little fancy. I turned them in the box and whisker. I put out wires. You know, I just made a look at. But what's really cool is that this spreadsheet is showing you the bootstrap in action by resampling. It's very straightforward. You can get the uncertainty. And any one of the statistics you could add your own statistics in, you could even do kurtosis, skew, anything you like, the p 13. And so this is available on GitHub. It's a bootstrap demo. It's within my repository with Excel numerical demos. A lot of kind of nice and intuitive demos in order to test out statistical concepts and just do it and excel very simply. So just to get a little bit more understanding from what we just did, I'll give you a couple of questions and get you to work on them and then show you the solution. So from this demo, show that the bootstrap result for uncertainty in the mean is close enough or equivalent to standard error. What is the uncertainty in the p 16 used for a Dexter Parsons calculation? So add statistic in and try to calculate its uncertainty from the L of realizations. Compare the uncertainty in the mean for the case of 20 samples and 10 samples. What happens to the uncertainty in mean as you cut the number of samples in half? I'll show you the answer in 3, 2 and 1. So what do we have here as far as the bootstrap result for the uncertainty in the mean being equivalent to the standard error? We can take one example of where we look at the sample standard deviation, 0.87. Number samples is 20. So sample standard deviation divided by square root of n. We would predict a standard deviation of the mean to be about 0.19. That should be the uncertainty in the mean. The bootstrap result was really close. It was about 0.20 from the resampling approach. So that works. You can show that to yourself. Hit enter, hit enter, re-calc, re-calc and you'll find that things will jump around a bit because of the fact once again that we have only an L equals to 50. If you had L equals to 1000, it would converge exactly down to standard error. What's the uncertainty in the P16? All you have to do is add a P16 calculation to the summary statistics and then go ahead and summarize the P10 and the P90 of that. Note each one of those P16s will require some interpolation because you only have 20 samples to draw from. So it's not going to be completely precise but still works pretty well. Compare the uncertainty of the mean for the case of 20 samples and 10 samples what happens. So what you can see in general is that when we take in from 20 to 10, we go up to a standard deviation in the mean of 0.27. It's about a 40 percent increase or 1.4 multiplier on the on the actual original value. That's what we get. And of course, you could go ahead and substitute one half in the square root and you could see immediately that gives you a 1.4 something multiplier. Then if we do it by bootstrapped, we'll find we get an answer pretty close to it and we can prove to ourselves that the bootstrapped method works and that it's matching the theory from standard error. Okay. So here's another bootstrapped example right here or just an example of a workflow. And so you could be dealing with a subsurface asset where you want to know the uncertainty and porosity for a unit B. You've got 12 wells that penetrate unit B. You're going to assume that all of those 12 wells are unbiased and that each one of them is independent of each other. Therefore, you can go ahead and use them in the bootstrapped approach. You can go ahead and you're trying to calculate the uncertainty in the average of 12. So you would formulate a CDF if they're all equally weighted. That's very straightforward. You do 12 resamples with replacement. You calculate the average. You repeat this step 1,000 times, 10,000 times. It doesn't matter. It's very quick. Just get a nice stable result. Calculate the P10 and the P90 of those realizations, 1,000 or so realizations of the average and then you could go ahead and use that. Now that's now your uncertainty in the average porosity. If you want to do a Monte Carlo exercises, we've discussed before for uncertainty in oil and place. You could go ahead and use a, you could fit that distribution or you could use it as a parametric non-parametric distribution. Excuse me and do Monte Carlo simulation from it in order to propagate its uncertainty through to oil and place or whatever you want to work with. So this bootstrapped approach is very valuable for getting uncertainty in a sample statistic and you can propagate it through all of your workflows. I hope this bootstrap discussion has been helpful to you. Once again, bootstrap statistical resampling procedure to calculate uncertainty in the statistic from the sample itself. You get the uncertainty in the sample statistic from the sample bootstrap. We're pulling ourselves up from our bootstrap. Okay, I hope that was helpful to you. As always, I'm always happy to discuss if you have questions. My materials all online. I'll be getting these lecture notes on GitHub too. The examples are also on GitHub and so I hope this is helpful to you. All right, thank you.
Alright, so this lecture we're going to talk about object oriented programming. I think the easiest way to start here is with an example. Objects are defined by classes, or specifically they are hard instantiations of a class. We'll talk about what that means in just a second. But if we start with this code, in Python you'll start a class with the keyword class. Just like functions or rip statements or whatever, white spaces, syntax, so anything that's spaced over inside the function is part of that function. What we have here is a special function called init. This is run upon instantiation of the class. Again, we'll talk about what instantiation means momentarily. But basically it's run automatically when we create the class, when we create an object from the class. There's another special function called string and this is just a special function that gets called, whenever we call the print statement on an object. Again, the easiest way to see this is with an example. What we'll do is just have an example where we instantiate the object. Basically we are going to create a variable called s that is going to store the object. The object has data, in this case it just has one piece of data that is shape, and it also has functions defined on. So if we call what the attribute is shape, in this case it just returns the string shape. We'll get to something more interesting momentarily. But shape is an attribute, it's said to be an attribute of the object s, in this case. s is just a single instantiation of the class, and we can have multiple instantiations. For example, we could also have another object, ss. If we call the print function on any of these objects, this special function string is going to be run, and in this case it's just going to return the string, I am a shape. That's what we have. Same thing if we were to call this on the other object ss. There's no difference between the two objects at this point. So here's where object ordnative program becomes interesting. What we can do is we can derive a class from shape. For example, we could have a class called polygon. So here's an example of a class called polygon. The fact that we have shape as sort of an argument to the class definition tells us that polygon inherits from shape. Another way you can think of that is that polygon is a shape. In this case, we also have an init statement, and we define the two class attributes in this case, shape, which we'll just set to polygon. And another attribute called side links, which as of right now will be set to none. There's a couple of functions here, compute parameter, which is the sum of side links, and get number of edges, which is the total number of side links. One thing that's not defined here is the string function. But because polygon inherits from shape, it also inherits any function definitions of shape that are not overridden. So in this case, the init function appears in both class definitions. And so polygon will use the one that's here. It overrides the one in shape. But the string function, since it's not implemented, is not overridden, and therefore will be called from the original shape function. So again, let's look at an example. So we've instantiated the class, set it to a, which creates an object that is assigned the variable P, and we can call the print statement on P. And even though it's not defined here, it calls the string function from shape, but then applies the current attribute, self dot shape, which we set to polygon. It is set there so that we now have the correct print statement here. Now we can go on and derive further classes from polygon. For example, we could define a class rectangle. Again, rectangle inherits from polygon, or rectangle is a polygon, which again is a shape. And now you'll see we have a class init statement here that gives us again, the shape is rectangle. And in this case, now it gives us four sides, the length of the sides, which we set automatically to 1111. Now here's where things get interesting because I do not need to rewrite compute perimeter or get number of edges because those are inherited from polygon. All we have to do is instantiate our rectangle. And we can, of course, print, it says I am a rectangle, that's inherited all the way back from shape. We can also call the function compute perimeter. And in this case, it just sums the length of the sides. We can also call, you know, get number of edges, and again, it is four. We can instantiate another rectangle. We'll call this one, large rectangle. And this time, what we're going to do to large rectangle is, even though the default is to set the side links to 111, we can access them as an attribute like this. Misspelling there. And we can actually then reassign that to whatever we want. I called it larger rectangle because we're going to make it something larger. And then we can call compute perimeter on it. And with the new side links, we can see that the perimeter is larger. We can likewise define class triangle, which has the attribute shape triangle and side links 2, 2, 2, instantiating this class and calling compute perimeter on it, gives us something like that. So the advantage of this is that we have two classes, triangle and rectangle that both inherit from polygon, and therefore both reuse the compute perimeter and get number of edges function, as well as the string function all the way back from shape. So using this class inheritance, we have a class that has multiple functionality, but we don't have to implement it all every time for rectangle for triangle. We can borrow code from the class that inherits from, in this case polygon. It turns out that everything in Python that we've used so far is actually implemented as a class, including the common lists that we've been using. So we know we have lists like this. And now it probably makes more sense how the sort function operates because list is an object, specifically it's an instantiation of the class list. And then we can, it has data with the data is one three four in this case, and well, let's put it out of order. One, let's say one four three, and then if we call sort on it, the sort function is a member function of the class, and it manipulates the data in this case in place. And so there we see one three four. So anytime you see this kind of dot in a function call should be really clear now that what it's calling the function on is the data of the instantiated class in this case list. We can have classes that have different behavior or can be instantiated with arguments, just like regular functions. So in this case, our class, we have a different class now that doesn't inherit from anything. It's called car. But when we instantiate this class, we have to instantiate it or we will instantiate it with number of doors. If we set the number of doors equal to, you know, two or something, they're automatically assigned to a class attribute. And then we've defined the string function, which basically says, if the number of doors is two, I am a coop. If the number of doors is four, I am a sedan. And if the number of doors is something else, then it says, I don't know what type of car I am. So let's just go ahead and, you know, we can instantiate this as an object. We'll call coop. And we can instantiate it with number of doors equal to two. And then if we call the print statement on coop, we get I'm a coop or we can have another car that is instantiated with number of doors equal to four. And if we were to print it, it says, I'm a sedan. So there you see that the behavior of the class itself has been changed based on the way it was instantiated. And just as a final example, we can also have, you know, if we set it to something other than two. And then call print on it. So I don't know what kind of car I am. So these are a couple of examples. Again, key things to remember, the init statement gets run automatically. The special string function is what gets called when you call the print on the instantiation of the object. The instantiation of the object is what occurs when you create the object by calling the class constructor as we have here. And in this case, the object is coop, but it's just a sign or sign to a variable coop. But we know it's an object because we know it has this special behavior. It has attributes. And it has functions defined on it.
Hey, howdy everybody. Last time we got into FACES modeling. We talked about the NPS, multiple point simulation and object-based simulation. And we talked about the general modeling workflow. In the general modeling workflow, we'll typically, for the subsurface, we'll involve a series of steps going from FACES and within FACES, continuous properties. And in the case of oil and gas, it would be porosity and then permeability constrained by porosity to have the right correlation between the two. So this lecture right here will fill in that piece. How do we take a continuous model within FACES framework and simulate multiple variables? How do we get beyond just a single variable? How do we go to what we term as co-simulation? So what's the motivation for co-simulation? Well, typically we have to build a reservoir model of more than one property. We don't want to build reservoir models of just porosity or subsurface models of just porosity. We need to know about the permeability, saturations of different fluids. Maybe we're even working with models where we need elastic properties for geophysics or we need geomechanical properties. This stiffness of the material, its behavior's mechanically and so forth. So the typical workflow that we talked about before for oil and gas, which is quite common, would be build a FACES model, have your realizations of FACES. Within the FACES realizations, simulate the porosity. And then within the realizations co-simulate the permeability, as a primary variable correlated to the previously simulated porosity realization. Everything is paired up. We have a realization of FACES. In each of the FACES, you have a realization of porosity and permeability and the porosity permeability realizations are paired to each other. They're correlated to each other. They're constrained. We will only talk about the most commonly applied co-simulation methods. We'll talk about co-located co-crigging and the cloud transform. Co-located co-crigging will just be an extension of our creaking system. Cloud transform is quite a bit different, but it is quite intuitive. And we'll only limit ourselves to workflows that are biberiate. Two variables at a time. We're not going to be talking about some of these more advanced type of workflows that are able to account for a massive multivariate type of subsurface model. Very interesting topic, but we wouldn't have time to cover that right now. Co-simulation is not perfect. But the alternative co-simulation would be to simulate each property independently. So imagine we did that. Imagine we had porosity and water saturation. Then the data looked like this and the R value. The correlation coefficient. The linear correlation coefficient was 0.76. Pretty good correlation coefficient, you can see. And that should in fact be a negative 0.76. Looks like a negative relationship. Now, if we were to simulate them separately, we simulate a realization of porosity at all locations in the reservoir model. When we simulate a realization of water saturation at all locations within a model with no co-simulation, no constraint between the two, you could expect to see a relationship like this. In fact, if you saw any correlation at all, it would be simply because the correlation at the data locations was being propagated into the model. But outside the range of correlation of the data, there would be absolutely no correlation. They'd be just independent of each other. Yeah, that's pretty bad. We'd have lots of implausible combinations. You see that typically, if we had a high porosity, we tended to have a low water saturation. While we'd be simulating values with combinations of low porosity, high water saturation, which wouldn't be physically plausible given this setting. And in fact, if you think about it, that'd be introducing a lot of uncertainty in the model. And so it wouldn't be physically plausible, and it would add a lot of uncertainty. So we want to co-simulation. Now, co-simulation is pretty good. If you take data, you look at its plot again, its relationship, and you go ahead and you co-simulate, you might get something like this. And this co-simulation, we'd have to look at the summary statistics and so forth. The main message we'd like to communicate here is the fact that it's not perfect. We may not honor the complete perfect-biber relationship. And if we do, we may not honor the verigram. The histogram perfectly, there's going to be some compromises. And we'll talk about those, but we will capture the general trends, the general relationships, and so it's much improved. Now, I want to kind of reinforce this idea of pairing the simulated realizations. That is, if we're co-simulating permeability as the primary variable, where it will be related to the secondary variable, porosity, then we will be pairing up the realizations. Realization number one, a porosity will be used to constrain realization number one of permeability. And in fact, it goes further. If we were to look at the simulated value at location U1, a location within this model, it will be paired up with the previously simulated value of porosity. And that will be used to constrain what happens in the simulation of permeability, based on the strength of the correlation. And you will force it, and force the bivariate relationship, correlation coefficient, the shape of the scatter plot between porosity and permeability to be enforced at each one of those locations. And so you could even calculate that previous plot that I showed was in fact taking this location, this location as a pair, and plotting it on the scatter plot right here, porosity saturation. Well, for this case, of course, it's porosity and permeability, but all the same. Then we would scan over all possible locations, get all possible pairs of one variable versus the other one. The ones that are all of the values that are coincidental, coincidental with each other, spatially, they're on in the same location. And we plot them, and that's exactly how we plotted and how we would check the relationship between the two variables. All right. So let's go ahead and talk about each of the methods that we will talk about. Co-located co-creaking is a methodology that focuses on using the creding system to reproduce the histogram in the verogram, and may honor the correlation coefficient, the linear correlation coefficient between the two variables. It does a pretty good job with that. But the cloud transform is a method that will honor the specific form of the bivariate relationship. If it's a little bit curvilinear, if there's heteroscanasticity, if there's complicated constraints, cloud transform is your methodology if you need to reproduce that. But it may not honor the histogram nor the verogram precisely. So if it's a case of each method has its own weaknesses. Co-located co-creaking, if you want to make sure you get the histogram verogram right, and you're comfortable just getting the correlation coefficient okay between the two variables, that would be this would be this method right here. You've got the general direction of correlation that's not perfectly reproduced. You don't reproduce exactly the same form of the bivariate relationship, that's co-located co-creaking. Cloud transform, you'd be able to reproduce the exact form of the bivariate relationship, exact shape, and so forth. But you may not honor the histogram nor the verogram. Both of these methods will start with a complete realization of the secondary variable. So in the workflow, you first simulate a realization of the primary, the secondary variable, sorry, porosity for co-simulating permeability or saturation as in the previous example. And so you start with that realization used. So you use regular simulation to get the first realization of the secondary variable, and then use co-simulation to get a realization of the primary variable, in this case being saturation of permeability, constrained by that porosity. So in order to talk about co-simulation, we've got to know something about cross-verograms. And so you'll note we go very light here. This is very cursory descriptions we won't dive into too many details. Across-verogram is a measure of how two variables differ together over distance. If you look at this equation, you'll readily see that all we did was we took the verogram equation, which was the tail minus the head squared, and we expanded it out and replaced the second term with another variable y. So now we've got a cross-verogram between z and y. And so this is a measure of how two variables will vary together over distance. Now it makes sense that we can do the same thing for the cross, for the covariance to get a cross covariance. And so if we take the covariance equation, we expanded out, we do the same thing to it. What's interesting here is we just changed the term in the head to be the other variable. And so here we're pairing points that are centered by their means, the value, a location, u, the tail location, minus its mean, times the value at location y at the head location, minus its mean. So we're taking two values in space, and we're comparing them offset by lag vector h, but we're comparing one variable at the tail to another variable at the head location. And so of course we can expand that out. And if you have zero mean, of course this will just simplify for us and just become the product of the z variable at the tail, times the y variable at the head, then that would be the cross covariance for lag vector h at, for between variables z and y. Now of course we can take that. And if we standardize by the correlation, sorry, by the standard deviation of z times the standard deviation of y, we get a cross correlogram. And this is pretty cool because this in fact becomes a plot of the correlation coefficient between the two variables versus lag distances. So that's a pretty powerful concept of correlation space. What does it look like? Well if you plot them up, this is what you would get. The cross varogram, just like a regular old varogram, could have started zero. And as lag distance increases, it will increase and will level out at its sill. And the sill as h reaches the range and goes further is equal to the correlation coefficient between the two variables. We say correlogram at zero distance, that's the correlation coefficient between the two variables. Easy. Now what's very interesting though is you could have a negative correlation. And if you have a negative correlation, your cross varogram will go negative. From zero, it will go negative. It'll drop until it reaches the negative correlation coefficient. And they'll level out. That's the cross varogram. The cross correlogram. Now that's pretty interesting. And once again, of course, if we're talking about cross correlogram, if the standard deviation of y and x is standardized to be equal to one, then you will reach, then you will basically have a cross covariance that's equal to a cross correlogram. And so we're just talking about cross correlograms. And so what will it do? Is the cross, I just found a mistake and fixed it. So the cross correlogram will start at the correlation coefficient. So right here, if it was to be, if it was a positive correlation coefficient, or could start right here, if it was a negative correlation coefficient. And at the distance of the range, it will rise if negative, it will drop if positive, and it will reach zero. And at the range, there is no correlation between the two variables. And so this is a very powerful concept. And so we can see the behaviors. Once again, we also have the relationship that the the varogram, the cross varogram is equal to the correlation coefficient minus the cross correlogram. So the same type of mirror image type of relationship between the two that we saw between the varogram, the sill, and the covariance before. Now, here's something really cool about creaking. The creaking methodology is completely general. We could in fact, as we saw before, we can build a creaking matrix with all of the covariances between all of the data themselves. The weights here in the weight matrix and all the correlations, the covariances between all of the data and the unknown locations. This part of the system, the yellow, and the weights right here, we already know, that's a simple creaking system. Now, it turns out that the creaking systems completely general. In fact, if you had a whole bunch of different secondary information at different locations, you want to also calculate their weights, another variable, you could just include them. And if you include them, this whole thing would expand out and you'd have a matrix right here, which would be the covariances between all of the secondary and primary data, the covariances between all the secondary and primary data, and the covariances between all of the secondary data themselves, and this component of the right hand side would be the covariances between all of the secondary data and the unknown location, which is a primary data. And so this would be a direct covariances, which would be your typical covariances, typical covariances of the primary, typical covariances of the secondary, but these would be cross covariances right here, and you would have a set of cross covariances between the data and the unknown locations. Now, it turns out that we can greatly simplify this system because up until now, we would need to know the cross-barogram between the primary and secondary. We need to know the barogram of the secondary, and that would get kind of complicated. We'd have to form a positive definite model, including the primary and secondary data and the cross terms between the two, which is known as a linear model of co-regionalization, which we will not cover in this course. And it's a bit complicated to do, it's more kind of graduate level type of thing we would do. And so we don't want to deal with that. So the way we get around that is we say, let's do co-located co-crigging. And what it'll do is we'll make two assumptions to simplify full co-crigging, and we simplify it down to this matrix right here. Only one co-located secondary variable is considered. So by doing that, we get rid of the full matrix of possible secondary crosses and directs. And we said just take the one co-located secondary value and go ahead and include that with the system. And then we assume that the cross of barogram, because we still need to know the cross covariance between the data, the secondary data, and all the other primary data. And we just say that we can get that as a linear scaling of the regular direct barogram of the primary. So is this a reasonable assumption to make? Well, it turns out the co-located secondary value. If you have a secondary information, the secondary value data or realization at the same location that you're trying to estimate, the primary variable, it's got zero distance offset. It's the most important value. It will screen the influence of all the other possible secondary data in the search neighborhood. So that's pretty reasonable. So let's go ahead and look at this assumption or this methodology more closely step-by-step. Let's build the co-located system. So first of all, let's talk about this idea of screening of the co-located value. And so what are we doing? We have a system like this. We're trying to make an estimate of the primary variable at U0, a location U0, given primary data at location 1 and 2, secondary data at location 3 and 4. This could be completely general. We could have secondary data at location 2 and 1. We could have any type of combination of secondary and primary data in space in order to make this estimate. So we get rid of the non-co-located secondary data. So we throw out location U4, U3. We only retain the value at this location right here, which in fact is U0. Excuse that. So we just retain that one secondary value and we say that one's going to screen all of the other secondary data and that we will call the Markov screening assumption. By doing that, we now get rid of the need to know the diagram of the secondary data. We don't have only one single secondary data and in the matrix it's just going to be this value right here. It's going to be the correlation covariance I should say between the secondary data and itself. And that's just equal to the sill. And if you standardize the sill to be equal to 1, it's just 1. So that's super easy to do. Okay. So that's the first part of Markov, the Markov screening part of the co-located co-creating approach. The second approach is that well we still need to solve for these cross-covariances between the secondary, retained secondary data right here, which let me go ahead and just put that in blue so that we remember it's different. I didn't quite get the right blue. Okay. Between the secondary data and all of the other primary data here, here. We still need to know that. And so what do we have to do? We need a cross-verogram to do that. This is where we bring in a Bayesian updating assumption. You remember Bayesian updating from our days of probability, those days, the beginning of the term or the course. We talked about probability and frequent disinvasive perspectives. What we say is that the cross-verogram between the primary and the secondary Z and Y at lag distance H is equal to the correlation coefficient between the primary and secondary Z and Y multiplied by the primary verogram, the verogram for Z at location H. We're rescaling the verogram by the correlation coefficient in order to update it to give us a cross-verogram. And if you think about it's very logical because if we have negative correlation that will flip the verogram so now it will drop to reach as a cell. If it's positive, the verogram will now rise up to the cell of the correlation coefficient. And once again remember that we're working with in Gaussian framework here so the cell is going to be equal to 1. So all of this is going to work for us. Take the cell, multiply by the correlation coefficient, the new cell is the correlation coefficient as we see with the cross-verograms. So it's actually, you assume the shape and the correlation is going to be the same. The range is the same. In the cross term as the direct term, you're just rescaling it to account for the correlation coefficient. So what's very cool about this is we could populate this entire co-creaking system of equations simply by knowing only one additional piece of information. All we needed to know is the correlation coefficient between the primary and secondary verve. And then we can go ahead and we can do a simulation or a creaking where we go through it all locations. We already have an exhaustive, at least secondary exhaustive secondary data set, the simulated realization of say, porosity for now working with permeability. That's available to us. We can, for every possible location, we have the co-located secondary variable. And we can now do our creaking, including it, and using the correlation coefficient in order to get these cross terms. So once again, the power of this approach is by retaining only the secondary, the co-located secondary variable, secondary data. There's no need for a vergram of the secondary variable. We don't need that. We don't need the vergram between secondary data and secondary data. No need for the cross vergram. We're going to just update it using the correlation coefficient. If the secondary data are smooth, then considering more collocated variables should not even help anyway. So this method should work pretty well. But there is a serious potential problem with excess variance in the results when used in the simulation mode. The excess variance is caused to the fact that we are throwing out information. We're not using all of the secondary data. And so it can cause a bit of a bias in the, in the variance. And if this is detected, often what algorithms will do is they will just scale the variance back so that the total variance is correct. And so that's a general approach. Just once again, to emphasize this idea of updating the vergram, if this was the original vergram of the primary variable shown here with three different structures, you can see the inflection points. This is the sill equal to the variance of the primary variable. This is the sill equal to the correlation coefficient between the two variables rescaling red the primary vergram simply gives us this vergram right here, same shape, same range, different sill. That's all. That's all we have to do. So let's go ahead and look at a typical workflow right now. So we have our secondary information. And that could be a simulated field of some property. Maybe it's based on seismic information like carrots and acoustic impedance. Maybe the acoustic impedance is just available to us at all locations. And we don't need to simulate it. That's perfectly okay. One way or the other, we have a secondary variable. That's available to us at all locations for which we want to simulate the primary variable, which in this case is porosity. We're going to transform each of the variables to Gaussian. Normal score transform of seismic acoustic impedance, normal score transform of porosity. We're going to calculate the correlation coefficient after normal score. Why do we do normal score first? The simulation proceeds within Gaussian space. Everything is Gaussian. The correlation coefficient should be of the Gaussian transform just as much as the vergram should be of the Gaussian transform as we saw before. Then what we're going to do is we're going to then at every single location that we simulate porosity. We're going to take that co-located secondary information from this other realization or model. We're going to include it into the Creighton system. We're going to be able to calculate the cross vergram terms to populate the Creighton matrix based on the correlation coefficient of 0.41 scaling the vergram of porosity so that it has the right skill, a cell, as a cross vergram. And we can go ahead and we can proceed. You imagine you got a random path all throughout this model to simulate the porosity within the faces. Every location, you go and you check what the acoustic impedance value is. You include it in the Creighton system. You use all of the scaled direct vergram terms in the covariance matrix to get the cross terms and away you go. That is co-located co-create. That's all I think I have about it. And here's a nice example of it being used in order to do a simulation to capture the correlation between two variables between the realizations. And so this is one way to be able to get into a model the concept of correlation between their variables such as porosity and permeability, saturation, and porosity and so forth. Now another method we want to talk about was this P field in cloud transform type of methods. The reason we bring them up is actually P field is or specifically cloud transform is used all the time. Patrol, GoCAD, other types of software programs. They have co-located co-create, but you found cloud transform is used an awful lot. So the KID P field is to perform a simulation in two steps. Construct the local distributions of uncertainty and then draw from those distributions simultaneously with correlated probability values. So we're separating the simulation approach into two distinct steps. Now you recall with the sequential methodology, at every location we calculate the local distribution of uncertainty we do Monte Carlo simulation and the correlation is imposed in the creaking. In this methodology the correlation is not imposed in the formulation of the local distribution of uncertainty, but as a separate step. The advantages, distributions of uncertainty can be constructed to honor all kinds of data. You can check them before you make the realization. You can pre-calculate them. You could incorporate other types of information to update them and you could observe directly how they're being updated. So it can be very powerful if you imagine I'm trying to do Bayesian updating as we've talked about before. I've got some type of I treat that as a prior. I've got some type of likelihood and I could update them with additional information. That can be done. Simulations are consistent with the distributions of uncertainty then. Okay, so also there's good reproduction of primary to secondary data in the scatter plot. This methodology is very good at getting whatever the form of the bivariate relationship is. You can get it back. You'll get back pretty close to that form. Some disadvantages include potentially poor reproduction. The history of the verigram. You'll see this methodology in doing all of this may actually change the verigram. The verigram may not be completely correct and so forth. Then we'll all show you that. It's the most common approach within this P field family of a purchases known as cloud transform, which is commonly used in workflows in industry for simulating permeability conditional to porosity. What does P field look like? Let's just take a very simple example. We got the data values. This is one dimensional example. We got an X coordinate right here. I should give credit. This figure is actually from the book of Clayton Deutsch. I appreciate using my lecture materials here. So data values are located here. We're now trying to simulate all locations in 1D1 through 10. We would formulate the local distributions of uncertainty. In fact, you could do this by creating. You could create and calculate at every single location what is the now not sequential creaking, but just regular creaking. We get the estimate and we get the uncertainty at every location that would be perfectly fine. Then what you do is you have these distributions of uncertainty. You want to draw from them in order to build a simulated model. You can't draw randomly from each one of them or you would not reproduce the spatial continuity. You can't draw the mean value at each location or that would just be creaking and we know that that's too smooth. So Mohan Stravastava and others when they came up with this idea of P-field simulation, they said, why don't we just build P values, the probability values that we're going to draw from these distributions with spatial correlation. So you put spatial correlation in the P values. So look at the P values are uniformly distributed between 0 and 1, but they have spatial correlation. This realization number one is the blue triangles and you notice that they're spatially correlated. Red squares spatially correlated. Green diamonds spatially correlated with each other. Three different realizations. Then what you do with those P values is you simply just draw from the distributions of uncertainty. Is it conditional? Well, at the data locations, the distribution of uncertainty shrinks to the data values. So with P-field simulation, we can actually build models that honor the data that have the right spatial continuity as imposed through this correlated probability values that are used to draw from the distributions. So it's a nice little methodology that cloud transform method is a little bit different. What they do with this method is in fact, we take the bivariate distribution, porosity and permeability. We bin it. Now what's very cool is we look at the location at which we want to simulate and we look at the secondary information porosity. And we now look at which bin is it in. And it's in this bin right here. It's a porosity value between 10 and 12.5%. So we build the conditional distribution of values that could be right here. And then we draw from that with Monte Carlo simulation. And we get a permeability value or whatever the primary variable is. Conditional constrained by this relationship between the two variables. If you do that, cloud transform will in fact honor the precise form of this distribution as been into these conditional distributions. Why is this attractive for the purpose of permeability? It turns out that when it comes to permeability, we often don't have a lot of data. The histogram is kind of poorly known. The verogram is difficult to calculate. And so this is not too bad of a methodology from the standpoint that the cross plot between prostate and permeability is probably the most certain information we have. And we're okay with a bit of imprecision with reproduction of the histogram and the verogram. Once again, of course with this methodology, you could impart correlation within the p values that are used to draw from these distributions. And a little bit of a numerical detail here. We condition to local data within permeability by constraining the p-filled to be conditioned to the specific probability value within this conditional that would result in the actual data value that's at that location. So what do we cover? What are we seeing here? Well, co-simulation includes methods that are able to simulate a property realization, a primary conditional to previously simulated property realization, which could be which we will call the secondary. Two simulated properties without co-simulation will only have correlations imposed by data and outside the range of correlation between the of the data, the spatial correlation range, they'll become uncorrelated. So you will lose, you will not reproduce the right correlation between the two variables. So you can't go ahead and just simulate the prerosity, permeability, saturation independently. You have to constrain them with each other. You have to use co-simulation, the primary and secondary data type of approach. If you don't, it leads to unrealistic values. You could have combinations of prerosity and permeability that don't even exist, and you have too high spatial uncertainty. That prerosity information is useful for constraining the permeability information. Two methods have been discussed in this lecture. Co-located, co-creating, we take the full co-creating system, we never showed you that, but we talked about what it would look like. You'd expand it out, and we apply a markup assumption. We only need the co-located secondary data. Anything past that will be masked, markup screening, and it won't be as important. We'll just ignore it. And by doing that, we didn't need the verogram of the secondary variable. We can just, we don't have to worry about that. Basin updating, we still need the cross verogram between the secondary and the primary, because we have that one retained secondary within the system, and we get it by scaling the primary verogram with the correlation coefficient. Now note, this may not reproduce the cloud or the bivariate relationship very well. The relationship between the two variables. It's only constrained by the correlation coefficient, the linear correlation coefficient between the two. So if there's complicated non-linear model relationships between the primary and secondary, it's got heteroscedasticity or something like that. You're not going to get that back. We talked about the cloud transform. It forces the reproduction of the cloud. It uses the secondary information to find out which bin you're in in the cloud, and then it draws from that conditional distribution in order to simulate. It uses a correlated probability field in order to get the right spatial continuity for the primary variable given the secondary, and it can condition the p values to the data values within the conditional distribution. So you honor the data. But it may make mistakes with spatial continuity and the distribution, they may not be precisely honored. So you got to know your assumption, got to know your steps when you make a choice about which type of simulation method to use. All right, cold simulation. I hope that that was helpful to you. I am happy to discuss any time on Michael Perch. I am a professor here at University of Texas at Austin. I'm also the geostat guy on Twitter. I'm super easy to get a hold of. You can find my email on the website for my department, Twitter. You can go ahead and send me a message or DM me if you're interested. If you have any questions, all right. Take care, you'll.
All right, in this lesson we're going to talk about conditionals and Python, conditionals and flow control when launching a notebook. We've already seen conditionals a little bit and that is the test that tests whether something is true or false. So for example, the conditional is this statement. The equal sign is a Boolean operator that is double equals. So a single equals would be a variable assignment in Python. A double equals is a Boolean operator and this whole line together is called a conditional statement. It's basically a test if something is true or false. So we've already seen the double equals. We'll go through several other ones. For example, there is the not equals, which is an exclamation point followed by an equal sign. Of course, 9 and 9 do equal each other. So if we evaluate that cell, we get false. But a separate example that proves that it's true. Right? 9 is not equal to 10. That is a true statement. There is also less than and greater than. There is also less than and equal to less than or equal to. So in this case, that would evaluate to true because 9 is equal to 9. And likewise, there is greater than or equal to. Any of these statements can be negated by putting not in front of it, like so. And they can all be combined together with and or or statements. So for example, we've already seen that that is a true statement. So if we wanted to combine that and test whether two statements are true. So for example, this 9 less than or equal to 9 and 9 less than 10, that would evaluate to true because both things are true. However, if we were to change the second one to greater than, make it the second statement evaluate to false, we would get true false. So true and false is going to return false. There is also the or statement which would return true if either statement is true. So 9 is equal to 9. We know that's true. 9 is greater than 10. We know that's false, but because the first one is true, this will return true. And these and these or and operations can be combined. So for example, we know this first statement evaluates to true. We could then add a third condition and says and something like that. And if we wish we could negate the whole statement like this. So what we'd like to do to create or control the flow of programs is we combine these conditional statements with something called an if statement. Every programming language has an if statement. And Python, the syntax is simply if something is true, for now I'll just put in the Boolean variable forcing it to be true. So if something is true, then what goes in the body of the if statement, I'll talk about exactly how the body is defined in a second. But if the body what goes in the body is something like a print statement in this case. So if true print, this is true. Whatever is in the body of an if statement is in then and over. So we could have multiple lines. For example, we could in addition to printing, we could set the variable x equal to 10. So now since this x is not spaced over, it's no longer the under the if statement. So this will run if true, it will print this is true. It will also assign the variable x equal to 10. And then because I put say print x here, then we can see that it prints to the screen. However, if this were false, and I'll need to restart the kernel to force the variable x to be empty, not assigned. So if this is false, of course it's not going to print, this is true. It's also not going to assign the variable x equal to 10. And so this print statement will have an error because x is undefined. And it does. So instead of actually putting in true or false, what we often do is, we're almost always, is put a conditional statement there. So for example, we could say if 9 is greater than or equal to 9, well, we know that's true. So this will evaluate to true. This statement will evaluate to true, causing what's in the body of the statement to run. So it'll print. It is true. And x will be assigned to 10. We can print x if we wish. Now, in most cases, we don't, I mean, there's no reason we can, we can look at that and verify it's true. So there's no reason to have the computer test for it. Usually the case is that we have some variable whose value could be changing due to other factors. So in this case, I'll just call it variable equals to assign it to a value, say 10, and replace the initial 9 with that. So what this is good saying is this variable, which we can change here, will be replaced there. So this will be saying if 10 is less than or equal to 9, which we know of course is false, then print x. Okay? Again, I'll need to restart the kernel because x has already been defined and you'll see that this returns an error. Okay. So however, if I make variable 8, now this evaluates to true and it works again. Now we often don't want errors to occur if we make assignments. So it's usually the case that we'll have a so-called else block. So if this is true, do this, else do something else. In this case, we'll just have it assign x to 9999. And this will never fail given any variable, I'm sorry, given any value of variable rate var. So if I run this for that or that, or that, it's always going to run, it'll never produce an error. And in the event that it's false, it assigns x to 9999. So we can also, so this is called if else, we can also have something called else if. So else if allows us to have another conditional statement. So in this case, if the first one is false, it will move on to the second one. So what we'll do here is we'll say, we'll change it to this. So we're going to remove the print statement. I don't think we need that anymore. So in this case, this will be evaluated in the order that they're read as you go down the page. So if the first conditional statement, if variable is less than 9, x will be assigned to 9. If that becomes false, it'll test the next conditional statement in this ellipse block. So this will test if the variable is identically equal to 10, then set equal to 10. Otherwise set x equal to 9999 and then print x, right? So of course, we can, this is simple enough that we can just know what's going to happen up front, right? The variable is 100. The first one will be false. So we'll move on to the second one. The second will be false. So we'll move on to the else statement and there there's no test. This just occurs in the event that both of the other two are false. So if we run this, we'll have 999. However, if we change it to that, right? If variable, which is 8 in this case, 8 is less than 9. So x is assigned to 9. And the other parts of the if statement are not even run. If variables identically equal to 10, this case, the first conditional statement is false. The second one is true. So x is assigned to 10 and this thing exits. So it never goes to the else statement. And again, what we had in the beginning. All of these conditionals on each line can be replaced by something very complex. So again, it could have these multiple conditional statements, truths with ands or ores, negation operations, anything like that is also valid in these. It's good practice. It's not required in Python, but it's good practice to put parentheses around the conditional statement just so you can see exactly how it's evaluating. Particularly, this is important when you have multiple condition statements. So in this case, the parentheses force this first combination or to be evaluated before the and operation. Also, it's good to know that these things actually short circuit. So in other words, if you have something that's true, if an if statement that's true, it will evaluate that in an x at the if statement. The other ones will not be evaluated. So if the variables less than 9 is true, nothing else will be evaluated after that. Of course, in this case, it wouldn't change the answer, but you might have scenarios where it would be. For example, if the variable is less than 9, x is 9, however, if the variable is identically equal to 8, then x is equal to 8. Let's run that and see what happens. With an input variable of 7, so 7 is less than 9, x will be assigned to 9. However, if we change that to 8, now 8 is less than 9, which causes this first statement to be true, x will be assigned to 9 and the code will exit. So this statement that checks if the variable is identically equal to 8 will never be run because it's been short circuited here. So that's just something to be aware of. If you want to do some, you'd have to come up with a more clever logic. For example, if you wanted anything that less than 9, except identically 8 to be equal to 9, then what you'd have to do is say less than or equal to 7, or... equal to 9, identically equal to 9, something like that. So this would say anything less than or equal to 7, or exactly 9 gets set to 9. So in the case when this is 8, this would be false. This would be identically equal to 8, x would be assigned to 8, and the program would exit. However, if we make x large, something above 9, and we get the 9, 9, 9. So you can also nest if statements. So just to start something clear, we'll start something new. So we'll just say if true, well, let's do something interesting. Let's just say if var will set to either true or false in this case, so we'll just set it to true. So if var is equal to true, and then you can say if x will have another variable called x, if x is less than 10. So in this case, and I just put the spaces in here for clarity, there's no requirement that there's spaces in the vertical rows just for readability. So if this is var is true, then we'll go into this block, and in this case we'll test to see if x is less than 10, we'll print x, and otherwise we'll print x plus 100. However, if var is false, the program will just print exit. So in this case, if we execute it, because x is 9, which is less than 10, it prints x. However, if I make x 11, it's going to print 111. If I make var false, then it's just going to print exit. So through these combinations of if statements and conditional tests, we can create a lot of complexity in a program by flow control essentially. So as variables change, due to the output of other functions or calculations, the code can take a different path. If something is true, do take one path, if something is false, take another path. The best way, of course, to get used to these is through trial and practice. So we'll practice lots of these conditional statements.
Audi everyone on my go perch on associate professor at the University of Texas at Austin, where I share all of my educational content to support my students with evergreen content that outlasts the semester, and also working professionals who are interested to learn new skills in data analytics, geostatistics or machine learning. Alright, it's been a while since I recorded a video. I looked through the lectures and I realized there was a glaring gap, and that was, I teach my students that if what you do does not impact the decision, you don't add value, and I look through my lectures and I realize that I don't have a decision making lecture. So let's go ahead and let's get into the concepts of decision making. We have built uncertainty models. Go through all of my previous lectures, you're going to see a lot about how we build uncertainty models. Now we need to make decisions in the presence of uncertainty. Decision making. Alright, if you want to see how to build uncertainty models, go back to my lectures. There's so many different ways that have been taught and demonstrated. Basin updating, taking a prior updating with new information through the likelihood function and then going to a posture, bootstrap to get uncertainty in model inputs, Monte Carlo simulation to propagate the uncertainty through a transfer function to value, creaking to get spatial uncertainty. Locations with the best estimate and a measure of the uncertainty in the estimate, creaking is your new best friend. Indicator creaking for the uncertainty which accounts for softness and the data, and perhaps even spatial continuity that varies by category. Very, very powerful things. Simulation, that's the entire workflow where you have the ability to build subsurface spatial models with heterogeneity, apply the transfer function and truly build distributions of uncertainty and complicated things like connected poor volume or flow rates or the homogenization volume variance relations of lithium. It doesn't matter. These are very powerful methods. So the prerequisite for this lecture is you've already calculated an uncertainty model. You have it. These are examples right here as I mentioned before. Cover factor of a mineral, the hydrocarbon resource in place, water, the, that can help inform say the flow rates you'd see at a brand new well. Now we have to make a decision with that uncertainty model. What are the decisions we make for the subsurface or spatial? There's many of them. Number location of wells for extracting or exploiting a natural resource. What is the location and timing of injection to do pump and treat? Some type of environmental remediation of a contaminated site, dig limits and mining sequence of selected mining units in order to be part of a mining plan. Sure. These are all examples and I hope you can see these are in high value decisions. Now here's the paradox. We have to go from this continuous uncertainty distribution of all possible outcomes to a discrete decision. Now there is one single best decision for every possible subsurface model that represents uncertainty. We represent subsurface uncertainty as an ensemble of models and there is one best decision per model, but that is not useful to us. What we need is the one single best decision and we can't make probabilistic decisions. Let me illustrate this a little bit better here. What we have is multiple models, one, two, three, four, five, representing some type of resource in place, an outline of where we think it could be. And you can see it's uncertain because it could be this, it could be this, it could be this. Then what we could do is we could go ahead and for every one of those models we could come up with the optimum decision. Put the well here, put the well here, put the well here to maximize the value for each one of them. At the end of the day what we'd have is a whole bunch of realizations of the decision. What can you do with that? Absolutely nothing. In fact you can't do anything with that. It would not be logical. It makes no sense to say take an average or the centroid of those. No, no, that's not going to work. What we really truly need to do is go through all the models and find that one best choice that for each one of the realizations is not going to be optimum. It's going to be good, but it's not guaranteed to be optimum for each. But globally over all of them, it is the single best choice. Let's talk a little bit about this. This going to a discrete choice is really related to the concept of the integer programming, the problem of trying to make predictions or work with models in such we can't use fractions. It has to be discrete. There's also concepts of optimization combined with simulation in order to find the very best choice and experimental design. This idea of capturing multiple uncertainties in all of the decisions and putting it through and try to sample it, understand the response surface and optimize over that. But then a day, this is not an easy problem. I want to give you a couple of warnings here. Typically, it's going to be high dimensional. There's going to be a large number of parameters in the actual subsurface model and also the decisions we're making. There's interactions. We'll have multiple decisions and the first decision you make can influence the optimality of the second decision and maybe even preclude certain options. And everything is multivariate. We're going to have not just where do we put the wells, but we need to know exactly how we complete them and how do we manage them? Are we choking them? What kind of flow rates do we allow? These are complicated problems. So high dimensional, there's interactions. It's multivariate. Let's go ahead and walk through the basic workflow for decision making by maximizing expected value. What did Profit Workflow goes like this? We're going to, first of all, we're going to calculate down certainty model. We're going to L equals 1 through L subsurface realizations. Here they are. 1, 2, 3, 4, 5, 6. These are all subsurface models and I want to remind you that the actual uncertainty model is L subsurface realizations. Now when I say that, I should also mention that I'm kind of truncating the definition here because we could have scenarios where we change the inputs, the model inputs, the decisions, the assumptions that capture uncertainty in the modeling decisions. In other words, we might even take the entire, in this case, it's porosity. We might take the porosity distribution and shift it low because we say, with that many wells, we don't really know the distribution. It could be low. It could be high. It could be medium. Those cases in, we're changing the decision of the model, that's a scenario. Realizations are different. We hold constant the model choices, the decisions, the assumptions. We just vary the random number seed and we allow them to statistically fluctuate. That's capturing the fact that between a data point and another data point, you don't actually know what's going on. There should be uncertainty and that's captured through realizations. From now on in the lecture, I'll simply say subsurface realizations, not scenarios. I'm going to reserve that word for the next step. Multivari. It's going to include all of the features we need to get to value. In other words, I'm showing porosity right here, but we would expect co-similated permeability, probably within the constraint of faces and maybe with, of course, saturations and so forth. They can be massively multivariate, in fact. But uncertainty model is represented by multiple models of the subsurface, that's step number one. Step number two, establish S equals one through big S development scenarios. In that previous case right there, we have to make a choice. What is our choice? We can put a well here, a well here, a well here. So I'm making the problem very simple. We're just picking the next well location and we got three choices, where to put that well. Now you'll notice that these are discrete alternatives. We don't have a scenario 1.3 or 1.5. It literally is just one, two and three. Those are our choices. Includes all details needed to simulate the extraction from the model. So the well location, completion types, if we're doing mining, the dig limits, the sequence at which we're removing the selective mining units and so forth. It has to include everything. For example, in this one, I just show three wells and I illustrate by showing kind of schematically the drainage radius around the wells. What distance I expect to drain with the wells. Next step, we're going to establish a profit metric. The profit metric allows us to go from the subsurface realization L, the development scenario S, and allows us to calculate something that's closely related to often value. Now of course, there's a lot of other drivers such as maximizing recovery of an important national resource, protecting the environment and so forth. These are all important aspects of value. So I'm not just saying dollars. Transfer function is going to allow us to go from the specific subsurface realization and the development scenario S. Two, this measure of value and I'll just say profit metric right now, but it's value. So in this case right here, if I place the well, this is my choice number two, I will extract 13 million barrels of hydrocarbon. The estimate ultimate recovery I imagine. Okay, so the input is subsurface realization, development scenario and output is going to be some measure of value. The thing we must ensure is that the units of this profit metric really should be in the thing that we're trying to maximize. That's the way to make this work very well. Get as close to be as complete. Don't stop short. Get to the thing that we're trying to maximize. The thing that really matters to us. Next what we're going to do is we'll do the full commonatorial of calculations. We'll take the subsurface realization one through six. We'll take all of the decisions, scenario one through three and we'll apply them all and we'll calculate the value. Now here illustratively, I've just shown for the first and the last model all of these profit metrics for each one of the possible decisions. We're going to repeat that over all L and all S. We'll take that full commonatorial. You can imagine building a very nice table with L rows and S columns, however you like to format. Next what you're going to do is calculate the expected profit for each one of the development scenarios. This calculation is the expectation of the profit metric for the specific scenario. Now a reminder and expectations of probability weighted average. I hope that's helpful. I have an entire lecture on statistical expectation from earlier this course. The expected profit is going to be calculated with a probability. Now this probability lambda L is going to be the probability of the specific subsurface realization. I used lambda because it can be seen as a weighting on the specific subsurface realization. The calculation, let's say for the example of development scenario two, will simply be one divided by the sum of the lambda's times the product of the lambda's multiplied by the specific profit metric for each one of the models. Now if all of the models are equal probable, all the subsurface models have the same probability of occurrence, then the lambda's just going to be equal to one divided by L. And I hope you can see this is simply the arithmetic average of all of the profit metrics over a specific development scenario. Okay, so that's the expected profit. Now what we've done is we've calculated the expected profit over all of the development scenarios, one, two, and three. And what we do is we pick the development scenario that maximizes value, the profit. And so really if you think about a machine learning speak, you'd be picking the arg max, which one of the integers gives you the maximum value. That specific scenario, in this case, it would be scenario number one, maximizes expected profit. Now this is a good time to sit back and ask ourselves about uncertainty. We have a methodology by which we can go from uncertainty to make a development decision. What uncertainty distribution is best is a narrower distribution, always good news. Take an example right here, the yellow distribution has much higher variance than this nice symmetric Gaussian-like-looking pink distribution. A higher variance distribution will have likely more downside risk. You see this, we've got higher density over here on the low side, but then we've got more upside potential. In many examples, it turns out that the positive skew results in outcomes that have a great impact on the operation. In fact, those few times that you sample out here on the curve, it occurs in the subsurface, results in few wells that actually pay for the entire project. So the boomers, the really good producing wells in mining, it would be those selective mining units where you happen to get really, really high grades, you just excellent locations within the mine. So it turns out it really depends. Now let's kind of think a little bit more about uncertainty making estimates in the presence of uncertainty. How do we make an optimum estimate? You cannot provide an uncertainty distribution to operations for decision making. If they ask you, what do you think is the connected poor volume in the subsurface? You can't give them an entire distribution. You have to give them a single best estimate by which they can make an operational decision throughout in the field. They have to make a single choice. We must provide a single estimate. We must choose a single estimate in the presence of uncertainty. Now whenever I say this, students often ask, well let's just take the expectation. So let's think about that. We take a distribution of uncertainty, we give the statistical expectation, a probability weighted average, seems very logical. But when you think about it, what the expectation is doing is assuming that the cost or the loss from over an underestimation are symmetric. And in fact, that it's squared. In fact, if we are underestimating by twice as much, it would be four times as costly. So we're assuming a squared loss. This is in fact analogous to an L2 norm when we're doing machine learning and other tips optimization. But what if the cost of overestimation is greater than the cost of underestimation? In that case, you're risk averse. You don't want to overestimate, you can actually give a lower value than the expectation to account for that asymmetry in the cost. What if the cost of overestimation is less than the cost of underestimation? In that case, you're risk seeking. It's better to estimate higher than the expectation because underestimation is more expensive. Now let's go ahead and take these concepts and let's formalize them as what's known as a loss function. Loss functions. A loss function will help us make decisions in the presence of uncertainty. It allows us to make a single estimate from a distribution of uncertainty and we can quantify the loss in a function like this. The way the function looks is the y-axis is the loss or cost. The x-axis is going to be the error. In other words, the error is equal to the estimate minus the truth. Negative values are suggesting increasing degrees of underestimation. Pause the values or indicating increasing overestimation. Right here you can see our good old friend here, the parabola. Basically a L2 or a squared symmetric loss function. Now of course I want you to also notice that what we'll assume is that if we are correct, we have zero error, there's not going to be any cost or loss due to error here, right? Now there's of course much more complicated examples of loss functions and you can design them using knowledge about domain expertise about the specific problem. Here's an example right here where the cost of overestimating is higher. If you look at it, it has a steeper slope than the cost of underestimation and the cost of underestimation at some point, it doesn't hurt us to further underestimate. This often happens. This type of threshold behavior could happen in facilities development where at some point it's a break over. You make a choice and now it's not going to have any further effect. Now let's go ahead and illustrate the concept of a loss function with a really nice example here. What we're going to do is when estimate, rain, whether it's going to rain or whether it's not going to be a sunny day and the decision we're going to make is whether or not we should carry an umbrella. Now in this case overestimation as you estimate rain, but it's sunny outside. There's no rain. Under estimation of the probability rain and you estimate no rain, but rain happens. Okay, let's go ahead and talk about what the loss functions would look like for the decision of should you carry an umbrella. And what you're going to find out is it really depends on the context. What if you're going to the zoo? Now I'm a father. I have three kids. I went through the stage. My kids are in college now, mostly. And I went through the stage of taking young kids to the zoo. Multiple strollers, backpacks, maybe even a kid on your back. All kinds of like bags, diaper bags. You're basically, it's amazing how much you have to carry around. And so you can imagine if you're doing that, if you're going to the zoo, the umbrella is just another thing you have to try to manage in addition to all those kids and then running around and all the stuff that's happening, that loss is actually really high to just having that umbrella and you didn't need it. But what happens if it rains? Well, I got to tell you that the zoo, as a parent who's done this, there's lots of places that go inside. You can go look at the nocturnal animals. There's other places where you can get a little bit of shelter where people go eat and so forth. You might get a little bit wet. But I got to tell you, after like dealing with kids at the zoo and all the stickiness and so forth, the good news, that might just actually clean you up a bit. So it depends on how you see it. But I hope this is helpful. What if you're going to an interview? What happens if you go to an interview and you overestimate the probability rain and it doesn't rain? Well, maybe you look good. Maybe the person who's interviewing you says, well, look at Michael, looks prepared, thought ahead, check the weather. Very good. Carrying an umbrella, maybe carrying an umbrella looks distinguished. Now what if you go to an interview and it rains and you have no umbrella? You show up, you're all wet. It might not look good. And then of course, at some point, you're completely soaked and maybe it doesn't have any further loss. You couldn't get wetter if you were to go swimming in your clothes. Now speaking of swimming, what if you're going to the swimming pool? Well, if you carry an umbrella and you're going to the swimming pool, I don't know, maybe that's something you want to do. But you know, it's not really necessary. Is it just loss? It's just something to handle. You might even forget it at the pool. But if you're going to swim pool, you're already planning to get wet. It's not a big deal if you get a little bit wet on the way, right? Okay. So I hope you can see that it really does depend on the circumstance. Now how are we going to go ahead and make an estimate in the presence of uncertainty to support decision making? What we're going to do is we're going to quantify the loss, the cost of over and under estimation in a loss function. We're going to apply the loss function to the random variable, to the distribution of uncertainty for range of possible estimates. We're going to calculate the expected loss for each of the estimates and we'll make the decision that minimizes the loss. Okay. So here's the loss function right here. I'll just redraw it right there. There's one we showed on the previous slide. And what we have right here is how we calculate the expected loss in general. We'll take the integral from negative infinity to infinity of the loss. Now look at these terms right here. You notice I flipped them around. No, that was not an accident. In order to do this calculation, in fact, we need under estimation on this side over estimation on this side. So we're going to flip the function horizontally. Then what we'll do is we'll multiply it by the probability of that occurring. So in other words, the loss of an estimate that we want to make is going to be equal to integration of the error of that estimate relative to the truth, which is a random variable. And this right here is going to be the probability of that specific true value. It's a random variable again. All right. Now let me just show you the calculations schematically and then we'll do nuts and bolts. I'll show you an actual output numbers. We take the distribution of uncertainty for Z, sorry, showed it to my Canadian friends. Z. I'm going to go ahead and pick an estimate, Z star. I center the loss function, which is flipped on that value. Now what I do is I put that function there and I calculate the integration from negative infinity to infinity of this loss multiplied by this probability, this loss, this probability. So you can see high loss here, but very unlikely to occur. This has low loss and right in the mode where you want it. So that probably boasts very well for the expected loss of this estimate and then it rises up and here it has just leveled off. If I go ahead and I do this integration, I'll get a value like this. Now we'll go ahead and we'll take the blue value right here. Now you'll see all of this really high cost right here for overestimation. We're estimating really low. So there's not really, there's not a good chance of any overestimation really, just a little bit right here and it has low probability. This rises up right here right through the mode. That's kind of bad news, but then it levels off. So we get a value that's a little bit higher in the form of one. Now this is all bad news because what's happening is we're estimating very high, but the cost of overestimation is the worst and you see we have these high loss going right through the mode. You'd expect that expectation to be quite a bit higher. Okay, I hope that was helpful. Let's go ahead and do nuts and bolts. We're going to take a similar problem. We're going to give ourselves the discrete problem. So in other words, how do we solve this? Discreetly, the discrete approximation will be the expected loss of an estimate is equal to the sum of the loss over a bin. We're going to bin at a certain estimate and the values, the true values are binned. We're going to go ahead and multiply it by the probability of that bin. Okay. The normalized histogram of the uncertainty in Z will take our estimate. We'll just put it right here in the middle. That bin nicely. Then what we can do is we go ahead and we do this calculation. The chance of having zero loss because we're completely accurate would be 1%. You see that? It's very low probability. If you take all these probabilities, they sum to 100%. Just the normalized histogram once again. Then the loss right here of about 50 million, I'm just kind of interpolating here nicely. 2% loss right here of about 100 million. See that go across there? About 10%. And so forth. You do the calculation of this probability times the loss, probability times the loss. Just summing all of those, you get $111 million. That's your loss for an estimate of 100 million barrels. Okay. You see that pretty straightforward. Now let's go ahead and move the function to another estimate. For brevity, I'm not going to do them all. I'm just showing you some examples. Take loss function. We shift it over. And now you can see what's happening. We got this higher cost of overestimation is starting to kick in more. And you go ahead and you get 1% chance of, ooh, 210 million, very high loss, but very low likelihood. And all the way to zero here, all the way back up here to to a 25% chance of having 100 million dollar loss and so forth. You take the sum of all of these probabilities times their losses. Once again, those probabilities are going sum to 100%. This is an expectation calculation. You get 60 million for an estimate of 400 million barrels in place. Let's go one more time. Let's illustrate a very high estimate. And you can see once again, we get these really high losses here because of the cost of overestimation is higher than the cost of underestimation. It's got a steeper slope. And you can see the result right here when you go through and you do all the summation of all the probabilities times the losses. You get 152 million, not unexpected. It's going to be the highest one right there. Now we could have done every one of them. And we would get a nice function and we could take the minimum of it. And that would give us the estimate that minimizes the expected loss. Let's go ahead and show an example. This is one of my favorites. There is a famous geostatistician, Mollandstrvastava. Now Mollandstrvastava actually is famous in Wired Magazine for cracking, interior lottery scratch card. And you can read about that. Look that up. It's really interesting. In addition to beating lottery's, Mollandstrvastava also wrote probably the very best paper to illustrate and describe making estimates in the presence of uncertainty. Let's go ahead and look at his examples. It goes like this. How much solvent should we inject to assist with sweeping oil? Now let me just explain it. We got injector and we got a producer. And when you inject solvent, one cubic meter of solvent can liberate one cubic meter of oil. What you need to do is calculate how much poor volume is connected between the two wells. And then you'll know exactly the best estimate is exactly as much solvent as you have poor volume. We're assuming very simple piston displacement, no kind of heterogeneity beyond. We have shale that's impermeable and sand, which is white so the black is the shale. So the shale is just a barrier, it can be a barrier or a baffle. You see that? That's a barrier right there. This right here is blocking. The injector can't even get through. Now what he did was he did a little calculation. He looked through and he said, okay, this right here will definitely not flow. So it's disconnected sand. This right here, this right here, you see that? Went through and calculated the poor volume, accounting for heterogeneity and this concept of connected sand. Okay. So when he did that, using like 500 realizations, a sequential indicator simulation with clearly a highly anastropic verigra model, long verigra range and the horizontal, less than the vertical, of course. What he found was he got this distribution right here. The connected poor volume went from boat. I'm not sure exactly what the ultimate minimum was, maybe 75,000 or so meters cubed, all the way up to about 97,000 meters cubed. Now the point is this is you have to go to ops and give them an estimate of connected poor volume between those two wells. And you cannot go to ops and say, the average, the expectation is 87.8000 meters. You can't do that. You have to actually count for the cost of over and under estimation. We got to formulate a loss function. Now the reason we'll have to develop this problem and I believe it's because the loss function is super intuitive. It goes like this. What is the cost of over estimation? If you were to over estimate by X meters cubed of solvent, you just waste the solvent. So in other words, if you think the poor volume is 80,000 meters cubed, but you, it's actually 75,000 meters cubed. There's 5,000 meters cubed of solvent that just cycled through and did nothing. It's wasted. What's that loss of over estimation? It's linear and it's going to be simply the cost of solvent per meter is cubed. You see, that's just the value of the solvent, the cost. If you underestimate by X meters cubed, what's going to happen is you're going to save money on the solvent. You didn't have to use the solvent. You didn't use it. But you're going to leave a certain amount of oil behind. So X amount of under estimation is going to be equal to the cost or the value of the oil minus the value of the solvent multiplied by X. That's a slope right there. You'd imagine that the oil is going to be much more valuable than the solvent cost. That slope you'd expect would be steeper than the slope here for just the cost of solvent. So we have a really nice, simple, simple loss function to work with. Now what Mohan did next was he took different ratios of the value of oil, the price of oil, to the cost of solvent and he calculated the best decision for each one of them. Okay. So for a 10 to 1 ratio, where the cost of oil per or value of oil per meter cubed is 10 times the cost of the solvent, then you're going to want to estimate high. The cost of under estimation is going to be much higher than the cost of over estimation. Now if you go ahead and you take a case right here, which is kind of a little crazy to imagine that the, we'll get to a point where the oil value to the cost of solvent ratio is 3 to 2. That's getting pretty close right there. Then you're going to definitely want to estimate very low because now we're reaching a point where the cost of over estimation is starting to become more significant relative to the cost of under estimation. Okay. So I hope this is useful. I really, really like this problem from Mohan Stravastava. Thank you very much, Mohan, for that. Let me go ahead. Let me demonstrate a couple of hands-on exercises. The first one is an Excel demonstration of using loss functions to make the optimum estimate. Now I do things in Excel. Sometimes I have people or students I'm teaching really struggle with Python, so it's not a bad idea to have some Excel demonstrations. In fact, if you go to my GitHub account, I in fact have Excel numerical demos, which is a repository full of well-documented Excel spreadsheets with data science, geostatistics, probability statistics, concepts demonstrated. I think that's a great idea. This is your making with loss functions is shown right. Let me go ahead and grab that. It's right here. Okay. So what it does is you have an uncertainty distribution. You can go ahead and change the distribution. Look, I made it a little bit wider. This is the uncertainty model. Then what you have is you have a loss function, the cost of under estimation, over estimation, assuming linear, you just change the slope. And then what it does in Excel calculates the expected loss as a function of the estimate and gives you the very best estimate. Now if you look over here, you'll see it doesn't use VBA. It just does the full common tutorial of the calculation. Calculates the expectations over that full table at high resolution to give you this result right here. Super, super simple to work with. Now if you go back to my account, what you're going to find on my GitHub account is that I also have Python numerical demos. And then the Python numerical demos, I have this demonstration right here. Some decision making in the presence of uncertainty, interactive decision making. So in my GitHub account, Python numerical demos, I have hundreds of well documented data science, geostatistics, machine learning, statistics, probability type demos. Many of them interactive, which I just love. I really love that. Interactive decision making is shown right there. Now what I'll go ahead and do is I'm going to open that up locally. And here it is right here. I'm going to go ahead and run this. It's got lots of documentation, import some packages. Here's the dashboard and a voila. Here it is. Check it out. I really like this. I have to admit, I just think this is really cool. I mean, can I get a little bit bigger? I think that's about, oh, there we go. That should fit on the screen. Okay. So now what it's doing is you have the uncertainty distribution shown right here. And of course, you can change, look at that. You can change the width of the uncertainty. I'm showing an experimental histogram sampled from it, just to demonstrate the samples that we're used to make the calculation. And what we have right here is the cost of under an overestimation, the slope, and you can put a power on it. So you can have like quadratic or cubic and so forth. And there's sampling rate right here. If your computer's running slow, just increase the step-step size. So you don't have as many calculations. It should run pretty fast. This is the distribution and dash line right here. This is the resulting loss, expected loss, for all the various estimates of values. Now let's just go ahead and just demonstrate this. We'll increase the cost of under estimation, which way should our estimate move? Should stay at the expectation, should get lower or higher. If it's more expensive to underestimate, what's going to happen? Look at that. We estimate higher. We don't want to underestimate. So as I increase the slope here, look how our estimate is shifting from the expected value right there or from the mode. Now, OK, there we go. And you could go ahead. Wow, that's pretty cool. If we make it like five times more expensive to underestimate, look at this. Our estimate here is shifted quite significantly. Now you could go ahead and play with things like this. You can change it to a quadratic for the under estimation and linear for the overestimation. And you can see the result right here. Now what's really interesting is that quadratic has a need effect because what it does, it makes the cost of under estimation actually not that expensive in the short term. Like for small amounts of under estimation, it makes it actually not so impactful. But then, if you really underestimate, it ramps up really fast. And you see because the distribution, it's shape, we actually shifted our value even lower. Did you see that? Yeah, that was kind of surprising. Look at that. See if we go back to the linear, it shifts back up. Because it creates more expensive. It's more expensive for under estimation, for just small underestimates. You see that? OK. I think this is great. I really do have a lot of fun playing with these numerical demonstrations. Well, there's a lot of things you can try out. Try increasing the cost. We did that. Try changing the standard deviation of uncertainty model. And try other types of kind of non-linearity and so forth. I think it's a lot of fun. OK, let me just wrap up. By commenting on the major things we looked at, joint optimal decisions is what we have to do. It'd be very easy to do L suboptimal selection, but we can't do anything with it. We can't support operations with that. No. Decision making with uncertainty. We can solve for a set of discrete development scenarios and for subsurface realizations. We can calculate the profit. And then from that, we can calculate the expected profit over specific development scenarios. We can pick the development scenario that maximizes profit over the geologic or subsurface uncertainty. Estimating with uncertainty, we need a single estimate. We have an uncertainty distribution, but we can't give ops a distribution. We have to pick the very best estimate. And if you're thinking to take the expected value, you have a loss function implicitly in your head. And that's symmetric and quadratic. The world isn't that way. The cost of over and under estimation are not always symmetric. Not always going to be simple quadratic or second order. And so we can account for it with a proper loss function. We'd calculate the choice or estimate that minimizes the expected loss. I hope this was helpful to you. Once again, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin, where I teach and connect research in spatial data analytics, geostatistics, and machine learning. I share all of my lectures to support my students with evergreen content that outlasts the semester. And in fact, I hear from them. They tell me all the time I get emails from former students saying, I'm still using your lecture content. And I think that's awesome. I'm still using your code from GitHub. In addition to that, I like to support working professionals, shout out to working professionals, learning new data science skills. All right, everyone stay safe. Bye.
In this lecture, we're going to talk about another form of flow control in Python, and that is loops, looping structures, specifically four loops and while loops. So we'll start with four loops. I'm going to start by defining a list, and I'll make the list just for demonstration purposes or to kind of show how general looping structures are in Python. We'll start with a list that's heterogeneous. I will say it wouldn't be a normal thing to do to loop over heterogeneous lists necessarily, particularly in numerical processing. Typically, we're looping over to perform computations or something like that. And that wouldn't be the case if you had this heterogeneous structure. In this case, we have an integer, a floating point number, and a string. So we're going to create this list, and then the syntax for a for loop will be for, I'll say, item in list print item. And so if we just go ahead and execute this for now without kind of further ado, you'll see what happens. It prints the items one by one to the screen. Now this small loop is the same exact thing as if I were to do this kind of thing. So that is assign the first value of a list to item and print item. And then reassign item to be the second value in the list and print that. And then reassign item to be the third value in the list and print that. These two things are identical. And in fact, what I've written here could be said to be the unrolling of the for loop. Of course, if our list had a million items in it, we wouldn't type out a million lines of code like this. The for loop is much easier to read. So since you've seen the equivalence of the two, let's back up. In fact, I'll just delete this cell and back up to the for loop here and investigate what's happening here as each iteration through the for loop occurs. And to do that, I'm going to use something called a debugger. So what I'm going to do is I'm going to put what's called a break point right here on this line. And when I run this cell, every time through this loop, the code is going to stop on this break point on the second line and we'll be able to inspect the current values of the variables over here in this variable inspector. So let's go ahead and just start it. And you can see the orange line highlights the line it stopped on and over here, there's a lot of information. The main thing that we're interested in here, you can ignore most of it. The main thing we're interested in here is the value of item, right? So item, name of course is item, it's of type int integer and its current value is one. If I were to hit play here, meaning this will restart the for loop, it will go on to the next iteration, you'll see the first item printed to the screen here because this line of code hasn't been executed yet. When I hit play here, this line of code will be executed and the loop will repeat itself. If I hit play again, you see that the first item was printed to the screen, we've stopped on this line of code again. And now the current value of item is two, we can see it's of type float, it's of type float because we forced it to be that by putting the decimal place there. And of course, its value is two. If I hit play again, you'll see it print to the screen, there it is. And an order line here indicating that the code is stopped on this line once again. Now the value of item has been reassigned again, this time it's of type string and its value is hello. And if I put print to play again, you'll see that print to the screen and this time this debugging process will stop because we've exited the for loop. And as is the case there. So you see this is gone. It's often the case when using for loops that we just want to iterate over a monotonically increasing set of numbers. In that case, say the number 0 to 10 or 10 to 100 or something like that, we can use the range function in Python. So in this case, I'm going to say for i in range 10. And I just use i here in place of item. This is a variable. It could be anything. You can name it anything you want. In this case, what we'll do is we'll actually print say i plus 1 and execute that. And you see, of course, we know in Python is 0 index. So the range function actually starts from the number 0. So what's printed out here is 0 plus 1, 1 plus 1, 2 plus 1, 3 plus 1, 4 plus 1, all the way to 10. Another example of what we can do with the range function is again used to, I guess, just for demonstration purposes instead of using i, I'll use j. Again, it's just a variable. It could be anything. In this case, we'll go from say 10 to 21 printing. In this case, I'll just print j. So there we get from 10 to 20. That just due to the nature of the range syntax. It takes up to 21, but not including 21, in increments of 1. It's often the case that as we're looping over something, we want to store the values in a list itself. And we can use, we already know, we've learned about lists. And if you recall, the list append function. So if we start with an empty list, I'll just call it a list. And we'll start like that. And we can just say for item in range 10, we want to take a list and append item squared. So this will give us 0 squared, 1 squared, 2 squared, 3 squared, 4 squared, and so on. If we execute this, it's going to appear like it didn't do anything, but of course the updated values are stored in a list now. So each of these items, as it took on each value from range, was squared, and then its value was appended to a list. There is a shorthand syntax for constructing lists on the fly like that. They're called list comprehensions in Python. And so to repeat this same task as a list comprehension, the way we would write that is using square brackets, we would say, item squared for item in range 10. And you see we get the exact same answer. This may be a little hard to understand at first if you don't truly understand looping structures. So if you don't have a firm grasp of the structure of a loop, then it might be best to hold on to this for later until you have a better grasp of that. And we can revisit list comprehensions in the future. List comprehensions are in fact quite faster in terms of computational time than doing it this way, but nevertheless they end up with the same output. So another type of looping structure in Python is called a while loop. So a while loop basically evaluates continuously, meaning it will continue to loop while some condition is true. So in order to illustrate this again using the debugger, we'll first write our loop. Let me start with that and then I'll explain what's happening. Okay, if I go ahead and execute this, we'll see what happens. So in this case, it prints the numbers 1 to 5 to the screen, but let's take a little closer look at what's happening, and then we'll examine it in the debugger. So initially we set a variable called stop condition equal to true. So the first iteration, this will say a while true, which means it's going to execute the body of the loop. Also we set an initial value of i equal to zero. So what we're going to do is upon execution will print the current value of i will set the stop condition equal to this conditional expression. So the first time through the loop i is zero, i less than 5 is true, therefore stop condition is true. i will then be incremented by 1, so i will, you know, as initially zero will add 1 to that and making i1. The loop will loop back around, evaluate the condition, stop condition, which we already know through the first time through the loop was true and it will continue. So it will print the next value of i, which is 1 on the second iteration through, evaluate stop condition, this will be either true or false, and then it will increment 1 again. So what we're going to do is we'll turn the debugger back on, we'll set a breakpoint at this line, and we'll run this code. Again, a lot of stuff over here, most of it you can ignore. The one thing we're interested in here mainly is the stop condition. So we see that initially stop condition is true and as we step through the loop one iteration at a time, you see we've printed zero to the screen. As we hit play, it'll print the next iteration to the screen and the next one and the next one. If we go up here to stop condition, it's still true. We'll take one more step, take a look at stop condition and now stop condition is false. That is because as we know when i was printed to the screen here, now i must be equal to 5, 5 less than 5 is false. Therefore stop condition is false and this time we will exit the loop as soon as we exit past this breakpoint here. And that's what happened. I don't like to use wild loops because in the event that stop condition is never met, this will run indefinitely. It will lock up your computer. It's called an infinite loop. I always feel like it's better to have a maximum number of iterations that will occur during a loop and you can achieve a similar structure by using a for loop with a fixed amount of iterations and then using an if statement for the stop condition. So let's look at a for loop that replicates similar behavior. So in this case, I'll say for i in range of 100, this is going to set a maximum value on the number of iterations that can occur, will be 100, we'll just print i and then we'll have an if statement that says if i greater than 5 break. So what this will do is if this evaluates to true, the break will exit the for loop. So it did in this case print i before the break statement. So in other words, on the last iteration through the loop, i was equal to 6. It printed it, this was not evaluated to true until the code got to that line and then the break statement was executed. So that actually replicate the exact behavior as above, I would actually need to change this just to 4 and then we can have the same type of behavior. But again, now we have no danger. For example, if I made some type of typo in my code, that would say for example, try to test if something that i is equal to 4.4, which will never be true because i is just in the integer range of 0 to 100. So this will never be true so our break statement will never occur. But in this case, the maximum number of times we'll print the screen is just 100. So while it's a little bit unsightly, it does stop eventually. So I prefer to use a for loop with a break statement in places where you might be tempted to use a while loop. However, that's just a matter of my preference, either is acceptable. So there are a couple of other conveniences in Python looping structures. Well, first of all, we can loop over two pulls in the same way that we looped over lists. Of course, two pulls are immutable, so we couldn't do any kind of a pinned operation or anything like that. We do have, if you wanted to, in addition to knowing the current value of what you're iterating over, you wanted to have some type of count over that. There's a function called enumerate. So in this example, again, let me define a two pull that this time is going to contain a list of strings. Well, two pull strings will say. And if we loop over it, again, we'll just print the screen. It will print one by one the items in the two pull. However, if we want to count of that, meaning the integer number associated with each place in the two pull, we can use a function called enumerate, which will return the items in the two pull, but in addition to that, the count. So we'll use this kind of syntax. Count will be the integer number, 0, 1, 2, 3 item will be the actual string that's there. We'll modify this print statement a little bit, I guess, to be a little bit more illustrative of what's current. So there you can see it should be clear now. The first item in the two pull, the string a, has the count 0, meaning it was the first entry, two pull 1, 2, string 3. So that's what the enumerate function does. It returns the item in the list and the monotonically increasing count of iterations as the loop progresses. We can do something similar with dictionaries. So let's start with a dictionary. If we loop over a dictionary, we can actually have the looping structure return both the keyword and the value. So here we have to add the function items to a dick and that will return the keyword and the value. That's printed to the screen. If you only wanted to print the keyword, for example, then you could use the function keys. As we see there. So this is a couple of additional functions or utilities that are often used in four loops.
Adi everyone I'm Michael Perch, I'm an associate professor at the University of Texas at Austin and I record all my lectures and put them online. This will be a lecture about frequentist probability. We'll take on the frequentist perspective proudly and show how we can count our way to probability. I want to acknowledge the fact that some years ago I was supported and helped as I just joined UT by Dr. Dr. Hardari and Lake who provided some previous course content used in this course and some of these slides probably greatly modified but a few of them are still retained some of that essence and ideas from them. I want to recognize them for their just wonderful kindness to me and helping me get started with my courses and providing some content. Alright, so let's talk about frequentist probability. What is it? How do we define frequentist probability? It's a measure of likelihood that an event will occur. Now if we have a random experiment and well defined settings such as a coin toss, we have good control of the experiment. We can go ahead and define event a and a will be a certain outcome. If it was a coin toss it would be heads occurring with the coin toss. Okay, you flip the coin, you get heads and you say that that was event a. We can define any event as being a. The probability of a occurring which will use shorthand notation of p a is going to be equal to the ratio of the number of times that a occurred over the total number of trials or experiments. And what we'll say is that n must be large enough. Now we'll get more accurate the more experiments we run but we'll talk later about the false belief in the law small numbers. And how dangerous it is dangerous it is to use too small of an experiment but it has to be a large enough number of trials. So what would be an example of this? Well, what's the probability of a dry hole for the next well and or having sandstone occur in a reservoir unit or exceeding rock perosity of maybe 15% or anything it could be. So we define a we run experiments they should be careful experiments will get more into issues around stationery and so forth when we get more geospatial. All right, so the frequent is probability is all about experiments and counting get the ratio that's your probability good to go pretty simple stuff. Now we're going to use then diagrams a lot to define probability logic based on this frequent is approaching will build up from it will get all the way to conditional probabilities and Bayesian approaches for the next lecture. And so we're going to talk about then diagrams let's just make sure everybody's on the same page this is a then diagram right here. It is a great tool to communicate probability and so what are some of the things that we can observe directly from a vendor first of all we're going to have samples that drive this and they will they'll be an experiment and samples from the experiment individual outcomes then from those outcomes we will define different events a B those are two events. Now if we have a certain outcome then we'll define that from that sample as being event a occurring or event B and they could jointly occur together or not okay. So then what we're going to do is we're going to have omega space now this is the box now if you're one of my students in my class and you're about to take one of my midterms please draw the box okay because the box is all possible outcomes. Omega just as we mentioned in the previous lecture and the problem is if you don't draw the box I can't really understand the Venn diagram right because this is all possible outcomes now the ratio of the area of B to the size of the box is in fact the probability of be occurring right. So in this case what would you estimate maybe about 20% maybe maybe 25% probability of be occurring and maybe more like 10% or so probability of A occurring and I'm estimating based on size of the box now we could get out a tool and measure the areas plenimiter or whatever and kind of figure that out but we'll just eyeball it for here okay. So got to draw the box if the box was way out here then you could see the probability of A and B would be much smaller and if the box was wrapped right around this tight probability of A and B is much larger okay now it's an excellent tool to visualize marginal joint conditional probabilities we're going to get into that in this lecture so I want to find that now so let's get let's get a really simple example. So we determined faces in a set of well cores and we sampled at say one foot intervals along the walls we got 3000 data or omega would be the 3000 measurements from core okay so far so good then we can go ahead and define event A and B and so maybe our event A and B could be A is sandstone and B is some type of shell or something like that so we define our events and so instead of using the using A and B I'll just call it shale and sandstone now we can go ahead and take the result from the 3000 data and we can draw our venn diagram and this is pretty cool because we have all possible outcomes the box omega we have the probability here of sandstone would be this area right here the probability of shale would be this area here and the overlap between the two would be a combination which we could define based on lithophaces being some type of intramedic sand shale type of lithophaces and everything outside of this union of sand and shale would be other faces something else happen maybe there's carbonaceous type rock wacky stone bounds don't or something there might be something else going on in the system okay so that's a basic example of a venn diagram I hope this makes it clear for you now let's go through a variety of different probability operators and these are the basic building blocks to probability logic and so the first one is the operator union of events now this is pretty straightforward we represent the union of events by this you symbol so it's easy to remember and it is technically for the set of all possible outcomes X that it would be the combination or the or operator so what's what are the set that include x within a or x within b now visually this is why we like venn diagrams it's really easy to understand because the union operator is like if you were drawing in venn diagram it's the weld operator it's basically taking these two shapes and welding them together and this is what I'm trying to calculate a or b would include the intersection of a and b also it includes just b and just a so it's this weld and so this is the probability I'm trying to work out now the intersection operator is different the union operator was the or operator the intersection operator is the and operator both must occur together a and b okay now the way I remember it is the symbol for the intersection is this upside down you and to me it looks kind of like an n so I think up n intersection uh it's kind of a way I remember it so that's the and operator we'll introduce it more formally later as a joint probability we'll get into that later and we'll we'll use different notation at that point too all right so unions and intersections now let's talk about complementary events okay so the complement and so if I have a event right here I could also have the not a event a did not occur and we will call that the a complement a superscript c for complement all outcomes that are not a now we can also have mutually exclusive events this is pretty obvious events that don't intersect they don't have common outcomes the intersection of a and b the and operator is a null set it's an empty set they never occurred together and in the venn diagram we would see it like this they don't overlap there's no purple area between them all right so basic probability now we can go all the way to exhaustive mutually exclusive events and so this is a sequence of sequence of events whose union is equal to the overall sample space so the union of a one a two up to a n different events is going to be equal to omega and I should also state that there's no intersection between them so any pairwise intersection a one and a two is null set now if we have exhaustive mutually exclusive sequence events the venn diagram would look like this now if you look really carefully it's either red or blue there's no purple there's no overlap there's no gap between a and b there's there's no not a and not a it's all gonna be either a or b okay so that's exhaustive mutually exclusive events we're going to use that to solve some interesting problems later on so now the venn diagrams that have shown so far have been pretty simple probability of a is equal to the area of a divided by the area of omega but of course it could get really much more complicated we can have all kinds interesting intersections and unions and we can work with many different events at the same time let me give you a very simple example here but getting a little bit more complicated the probability of drilling a dry hole in the next well that could be a complement because a was drilling a well that was not dry producing well encountering sandstone at that location in space if you got good rock at that location that's b exceeding some type of critical threshold for porosity to have goods amount of volume in place that would be event c and so we might be interested in the combination of a dry hole having sandstone and also having good rock porosity in other words this could be like a seal failure where you actually good reservoir rock but there's no fluids in place that we want to recover and so we could go ahead and look at the venn diagram of the combination of all of these events and now we're interested in this right here now of course this intersection right here would be the good rock no feels seal failure at all all right so our venn diagrams can get much more complicated now good thing to do is to give a little more thought to what an event is how do we define event and how do we relate that back to the original sample set and this is somewhat trivial but it gets our minds in the right place about it now I want to acknowledge I show small sets here because of just it would just be tedious if I showed you many samples but in the back of our minds remember N goes to infinity that part if we're going to do any probabilities we really need enough samples and I've only shown you a case right here of like what seven samples so just keep that in mind that's just four demonstration okay so petrophysicist has measured porosity from seven core samples from a carbonate formation and they have these values right here they've been sorted conveniently in order and we want to define three different events porosity less than 15% so that's a low porosity rock and that's a and you'll see a goes from here to here the first three samples porosity greater than 20% that's the really good stuff that might actually be if the permeability is also high might be a thief zone might be an area where we just have really high production and something we should be aware of and C would be this intermediate and it's got a low overlap with A and it goes up to right here 17% okay so we have defined three different events and we could go through and work out the unions the compliments the intersections of these different events just to make sure we test our knowledge so I'll invite you to pause the video right now and work through this on your own I'm going to go ahead and three two one and show you the answers so really straight forward right here we have in the case of the union of A and B it'll just be the A events 10% 12% 14% and we're welding on we're adding in the 25% that's it right there so the intersection between B and C well B and C don't overlap that's why it's nice to draw these boxes and look at it like this so it's a null set and the union of all of them will be all of the values excluding the one that was not in any of the sets so I hope that's useful now to calculate the probabilities while we're going to assume it's enough samples and we can go ahead and just use the frequentist approach which counting and ratios we already counted now we do the ratios and so you can confirm this there were seven samples and so now we're talking about the union of A or B is going to be just four divided by seven the first three here from the set A and this one adding to four and so we get that as a probability and so forth the if we take the union of all of them now this is interesting I just realized I have a little error here so I apologize for that the union of A, B and C actually missed one of the values so in fact I should have removed it and it should be six out of seven so I was just testing you okay anyway I'll get that corrected it's not bad it just shows we're all human so let's go ahead we've just demonstrated this idea of intersections unions compliments and this idea of calculating frequentist probabilities from a simple sample set let's go through a couple more probability concepts non negativity normalization these are fundamental probability constraints and they build on the basic axioms that we talked about in the previous lecture from Koma Goroth and so what do we have here first of all probability must be bounded between zero and one we understand there should not be negative probability that would clearly end the universe it was p paradoxical that doesn't make sense at all but also makes sense that probability of an event can't be greater than one super probability you know it doesn't make any sense right 100% means it's gonna happen are we paranoid you know like it's really should not go higher than one closure that all possible outcomes if you take all of them that's a probability one nothing else can happen now we're not talking black swans you know anticipate it to type of outcomes but if we assume we know all possibilities then for one of them to happen probability one and null sets probability of something that never happens is going to be zero now complimentary events if you have complimentary events a and not a now we know they're gonna be mutually exclusive and they're gonna be exhausted because all possible outcomes are a or not a there's no other option and so we have closure and closure means that if we take those probabilities and sum them together it's equal to one as we can see here in the vendirogram a a and not a actually does cover the entire area or all possible outcomes omega all right so let's talk about another probability operator let's talk about the addition rule and this is a common mistake people may because they forget to subtract the intersection so this is how the rule goes the probability of a or b the union operator is equal to the probability of a plus the probability of b and then we have to subtract the intersection of a and b the intersection right and so what are we doing here we take the probability of a plus the probability of b now look what happened we just double counted the intersected area so we got subtracted once now in the case we have mutually exclusive events we could generalize even further this would be any number of possible a i's events a i 1 2 3 4 up to any number what we can say is that the intersection between a i a j where i is not equal to j is going to be a null set they're mutually exclusive they never overlap each other and that case we use comma goofs third axiom which was in the previous lecture and this you'll remember it's exactly what this is and which we can just simply sum the probabilities because they're mutually exclusive to each other there's no intersections no double counting to count for now given what we've just talked about we could go ahead and solve this problem right here now what are we going to do let's calculate the probability of a b intersection a and b union a or b for both of these cases this case right here prostate permeability samples it's a bivariate distribution or sample set two variables or features and for this one right here now this will help us understand the addition rule in that idea of double counting now hint here this is literally just point counting in other words just count the number of samples in a b over the total number of samples okay i'll show you the answers in three two one so this what it looks like pretty straightforward probability of a just six samples in a there's a total of 20 samples and so we get 30 percent probability of a just frequentist counting and ratios we assume we have enough samples and is large enough of course again and we keep it kind of simple here for demonstration now probability of a and b well there's no intersection between them so that 0 percent a null set of samples 0 percent probability and the probability of a union with b the a or b operator now we can see that we do the probability of a plus probability of b there's no intersection and it's just 60 percent we can count the points in both confirm that works same thing with this figure right here but we have an intersection 10 percent of the samples are in both a and b and we can quickly observe the union operator here and see that if we do 40 percent plus 30 percent we would get 70 percent without doing the subtraction and very quickly you can confirm to yourself you've actually created a bias you double counting subtract the 10 percent you get 60 percent and you can quickly see that this makes sense that we do have a total of 12 data here that is 60 percent it works out if we do the proper addition rule with the subtracting to remove the double counting okay so that's cool addition rule example to demonstrate it now this is where things get real we're getting into conditional probabilities now and I think this is the real power I think there's so many great things we can just do with conditional probabilities okay so what is a conditional probability what's the probability of event b occurring given a occurred now the notation will use is the probability of b this horizontal line will read that as given and that's event a okay probability b given a how do we calculate that the equation is going to be as follows the probability b given a is equal to the intersection probability of a and b divided by the probability of a now let's go to the venn diagram and see what that means we have made the statement that a has occurred okay so we no longer deal with omega in a conditional probability we have to conceptually shrink the space of possible outcomes to only a because we said a occurred now our universe is only a we can't see anything else and so the venn diagram was here with omega now this is our new venn diagram it's only a can occur and so if you look at that what's the probability of b given a occurred it's the ratio of the intersection a and b divided by a it makes perfect sense if we shrink our universe so every time you hear a conditional probability think of shrinking your universe kind of funny all right so now let's generalize because we've introduced the idea of conditional probability and we've defined it by this equation right here probability b given a is equal to probability of a and b divided by the probability of a let's put some words around these terms this is a conditional probability we just stated that this right here and we could say it as probability of a and b or just intersection a b is a joint probability it's a joint probability because it considers more than one event at the same time and this in the denominator is a marginal probability because it's only considering one thing at a time one event at a time now how do we visualize and think about this how do we kind of put this all together I would draw a plot like this now if you look at it we have a distribution of porosity and if you're only concerned one feature at a time or one event at a time this is going to be a marginal distribution just porosity measures this right here is a distribution of permeability it's also a marginal distribution and if we took all the points and we plotted and modeled them between porosity permeability that's a scatter plot we could fit some type of model to it some kernel density smoothing estimator of density and we'll get don't worry we'll get more into this we'll talk about distributions in the next lectures it would have some type of shape that point cloud and we can model that as a distribution in 2d and we're considering two variables or features at a time and so that would be a joint distribution now what we could do moving forward from that is we could calculate from those distributions probabilities so I could pick an interval of porosity between here and here and from that distribution I could calculate the number of outcomes that have that criteria porosity in that range and that's a marginal probability if I pick an interval on permeability and I calculate the probability of being between those values that would also be a marginal probability now if I took both of those a constraint or bin in porosity a constraint and bin in permeability that would create a square right here of criteria porosity interval permeability interval and I could calculate a joint probability from the joint distribution okay so far so good this is really kind of cool stuff so we could now have joint probabilities from combining these two criteria each of them individually would describe a marginal probability now if I was to define a condition given porosity from here to here I could go to this point cloud and I could look at all the permeability values that meet that criteria and I could build a distribution right there that would be a conditional distribution and if I was to say okay give me a value or probability being from here to here that could be a conditional probability I've shrunk my universe to that porosity constraint and now I get a conditional probability of permeability given that porosity constraint okay don't worry we're going to reinforce this with some really nice practical examples let me just define some of the terms again that we just talked about marginal probability probability of an event irrespective of any other event you don't consider any other event conditional probability probability of an event given another event is already true it already occurred you shrunk your universe and we use the notation x given y and so forth okay and joint probability probability multiple events occurring together now we can use the notation x and y or x intersection y later on I'm going to get a little bit lazy and just break it down to just x comma y to keep the notation really simple and compact now given what we just learned about joints and conditionals and marginals we can kind of expand our concepts of conditional probability we can talk about a general form or recursion of conditional probabilities and this is like this is really really interesting so let's walk this through one step at a time the statement is that the probability of c given b and a so you see we've got a conditional and then we also have a joint here in the condition that's super cool is equal to the probability joint a intersection b intersection c divided by probability a intersection b now let's go to the venn diagram and prove that to ourselves okay we said the probability of c given b and a okay so let's take b and a that's this area right here so we shrink our universe to that and now we need to calculate the probability of c given that so that's going to be this area right here which is the intersection a b and c divided by a and b this entire area right here so I hope you can see by the venn diagram that that holds up now let's do a substitution recal the definition of conditional probability the probability of b given a is equal to the intersection probability a and b divided by the probability of a so what we can do with that now is we can reorder this we can turn it into the probability of a and b is equal to the probability of b given a times the probability of a we're just take this denominator out and put it on the left hand side we can now substitute for the denominator in this first equation and we get this form right here you see that now this is super cool because if we take this form and simply take the denominator out again put it on the left hand side we now get the joint probability of a and b and c is equal to the probability c given b and a times probability b given a times probability of a now do you see that pattern it's a super cool pattern it's a recursion of conditionals the joint right here can be solved for as the probability of c given b and a now we can take that apart as probability b given a and now we just take the probability of a okay that pattern if you look at it is can be expanded like this to any number of possible events now why do we care about this well when we get into many different machine learning methodologies such as naive bays we're going to find we use this to solve for a joint probability that would be very difficult to solve for otherwise we can use this recursion of conditionals to make it a much more practical problem okay so we went a little deep right there we got into something a little bit more complicated if you understand this you're going to begin pretty powerful on these conditional probabilities okay so let's make sure that we understand what we're doing we'll finish up with a little closure and conditional probabilities and we'll do an example okay so closure of conditional probabilities it turns out we have the same closure rules but in this case we've shrunk our universe so it's a little bit different so the probability of a given b plus the probability of not a given b is also going to be equal to one now you see how this closure is working we shrunk our universe to be as occurred so take the venn diagram b is occurred we shrink our universe there now you can see very quickly that the probability of a given b this intersection plus the probability of not a given b this part right here is equal to one because we shrunk our universe to be only b can occur now so they have closure now and of course it works with a or b or any one of the events we're dealing with okay so given all of all that we've learned about conditional probabilities let's go ahead and solve some of these now you'll notice I made it a little bit tricky you don't need to actually measure the areas you can actually communicate all of this using the concepts of probability of a b and so forth intersections so go ahead and solve for each one of these conditional probabilities okay I'll show you the answers in three two one and here they are okay so what happened so this is interesting this first part is trivial case one is trivial because remember the numerator of a conditional probability by definition we'll have the joint the intersection between a and b and in this case they're mutually exclusive that's equal to zero null set and so all of the probabilities are just zero here now in the second case we have something really interesting going on here we can take the probability the definition probability the intersection a and b divided by probability b and what we're going to find out is that if we shrink our universe to be a always occurs and so in this case it's just equal to one they're both equal to each other this intersection of a and b is equal to probability of b okay now in the other case we shrink our universe to a and what's really cool is the probability of b given a just becomes the ratio of probability b divided by probability of a now I hope these examples really helped you solidify the concept of conditional probability now I provide one more example here this is just more point counting hint hint not a big deal and we have event a is porosity that's good it's above 15 percent permeability good above 200 milla d'arces will be our event b and let's go ahead and solve for by point counting assuming we have a large enough experiment you remember that probability of a given b probability of b given a now I'm going to ask you a bonus question and that is how much information does b tell you about a and vice versa are they informed of each other do they share information doing porosity does it help you know permeability okay so go ahead work that out see if you can get the bonus question too it's pretty cool and I'll show you the answer in three two one and here it is okay simple we can just color code and draw our boxes here event a event b and both of them occurring together and we can easily solve for the probability of a given b the intersection eight points out of 20 just be alone 11 points out of 20 and so we get eight divided by 11 and so forth now how much information does b tell you about a and vice versa the way to think about it is this the probability of a just alone the marginal would be all of the points in a divided by total number points and that's 10 divided by 20 50 percent the probability of a given b is different and we already saw for it up here as eight divided by 11 so the probability of a alone was 50 percent probability of a given we already knew that we had good permeability is 73 percent there they are different significantly different now we're getting statistical testing significance but let's just say those are quite different so if I know that the permeability is good it does tell me a lot about the porosity my probability good porosity jumped up from 50 is 73 percent and we can do a vice versa so what's it tells us it tells us that we can't work with a and b independently they're sharing information now we'll get more rigorous defining independence here shortly now here's a nice little example right here where we'll talk more formally about the idea of calculating joint marginal and conditional probabilities and we'll do it from a table of frequencies what have we done we got a feature one fraction of shale we bend it up we got feature two porosity and we bend it up and you could imagine we could take all of our point cloud and just count the number of data points inside of each one of these bins we're just bidding up a scatter plot and counting the frequencies in each bin so that we can easily start to make some calculations okay so the joint distribution is really discretized here as a bunch of frequencies now we could turn those frequencies though there's joint counts into probabilities just simply by dividing by the number of total samples in other words the number samples in this combination right here this joint count is one the total number of samples we have here I think is about 20 percent and so we would expect that this probability joint right here before percent we could go through the entire data set and do that we'll do that on the next slide the marginal distribution how do we get the marginal distribution it's the process of marginalization in other words what we're going to do is if we want the marginal distribution of x we're going to integrate from negative infinity to positive infinity over the joint distribution over the feature we want to remove in other words it's a integration or if we bend up the joint it's a summation so if I want the marginal fraction of shell 10 percent I'm just going to sum over the column I'm integrating over porosity to remove it is that cool and if I want the marginal of porosity 15 percent per say I would simply sum over the row and that would and then all I have to do is divide by the total number of samples that's marginalization I've removed the influence of the fraction of shell by integrating it over it okay so conditional distributions you remember all that talk about shrinking our universe we're still shrinking our universe so we can solve for the conditional simply by looking at the joint and dividing by the marginalized okay so what does that look like if I want the conditional of porosity 20 percent given v-shale of 10 percent all I have to do is take this joint and divide it by the total number of samples in this column where we have in fact we're conditional we shrunk our universe to only v-shale 10 percent the fraction shell can only be 10 percent this is our universe now we take that joint divide by that new subset of just the column and we get the conditional okay so let's go ahead let's start with the joint probabilities really simple so we can go all the way through our data set and simply take we had one sample here divided by 25 total samples that's 4 percent two samples here three samples here not a big deal so that's how we got the joint probabilities now to calculate the marginal probabilities all we're going to do is sum over the columns to marginalize to get the marginal for just fraction of shell sum over all of the rows to get the marginal for porosity and for the conditional probability we shrink our universe we're basically standardizing by the count the marginal within each one of the rows or the columns okay so how let's go ahead and work that out I'm going to ask you to go ahead and do marginalization for both v-shale or fraction of shell and for porosity and to then calculate the conditional probability given porosity is equal to 15 percent for v-shale given porosity is equal 15 percent I'll show you the answers in three two one right now okay so what do we got here really really simple to get the marginal probabilities we're going to integrate over the feature we want to remove so we got v-shale right here and so we want to know the marginal v-shale given 10 percent we're just summing 4 percent plus 8 12 percent plus 4 percent 16 percent there it is boom you get 24 percent just keep going that's all we're doing for the marginal of porosity we just sum across the rows and you can confirm very quickly 25 percent case that's 8 percent the 5 percent case that's 12 percent down here not a big deal the conditional probabilities from the joint probability table very straightforward we just go ahead and shrink our universe we said given porosity equal 15 percent porosity equals 15 percent is right here this part of the table is the whole world now so the conditional v-shale equal 10 percent is simply going to be the case of 4 percent divided by and that's 24 percent that's going to be equal to 1 6th 8 percent divided by 24 percent 1 3rd and so forth we shrunk our universe every time you hear a conditional probability think about shrinking your universe all right now there might be some of you out there who want who want a little bit more hands-on opportunity I put on github at this address and my github repository on excel numerical demos an example in excel where I gave myself a frequency joint joint frequency data set with much as samples you can edit that at will it calculates the marginal frequencies and then converts them all to joint probabilities marginal probabilities and then it calculates one of the conditional sets of probabilities conditional of v-shale given a porosity equal to a specific value I think 4.5 percent go ahead you can play with that and get some understanding about joint conditional and marginal probabilities and distributions probabilities are these individual measures the distributions when you take all of them together so let's go ahead now and shift gears a bit we're going to talk about the multiplication rule now what's really cool here is that this is going to lead us directly to Bayesian probabilities so we're almost there we're getting there all right the multiplication rule this should not be a surprise at all in fact if you look really carefully it is the reordering of the definition of conditional probability if I took this probability of a and put it in the denominator on the left hand side that is the definition of conditional probability so no surprises we simply pulled the denominator out put it on the other side and so the multiplication rule is the joint probability of a and b intersection is equal to the conditional probability b given a times the marginal probability of a okay now if events a and b are independent of each other what that means is that this given a should have no influence on the probability of b knowing something about a should not impact the probability of b if they're independent so the probability of b given a is just equal to the probability of b we can substitute this part right here and so given independence we can say probability of a and b is equal to probability of b times probability of a under the assumption of independence okay the general form of independence of mutual independence between all of our events is we can say the intersection of all of our a i's i one through k different events is just going to be the product sum of all the probabilities we're just going to multiply them together so an example in the case of three k is equal to three is the probability of the intersection or joint a one through three is simply the product sum or multiplication of all of the probabilities okay this is pretty cool stuff so if we assume independence we can solve a lot of problems in a very simple manner let me give you an example right here event a is well event b is porosity greater than 10 percent we have the probability of oil 30 percent probability of good porosity is 50 percent what's the probability intersection of a and b and here's another one where we expanded to three possible events all right i'll show you the solutions in three seconds three two one all right very very simple here so if we go ahead and have assume independent events we can just multiply the probabilities by each other and the result is we get probability of a and b is probability of a b times probability of a 30 percent times 50 percent 15 percent and with three events too we just keep multiplying them by each other to calculate the probability now given that we said that if events are independent we can find them just by the product sum we can use that as a definition of independence or even a check for independence so if we assume independence we expect the probability of a and b to be the probability of b times probability of a we expect the conditional probability of a given b to just be equal to probability of a and the probability of b given a to just the probability of b and we have the expanded general form in the case of the intersection of a i through k as just being the product sum of all the probabilities the marginals okay so we can use this as a definition of probably of independence and we can use it to check for independence now i would not say we're proving independence i would say that we can show that there is not likely independence if we violate that relation but i would not suggest that we're completely proving independence okay so let's just let's just put it that way it's a little bit of a soft check so let's go ahead and give ourselves an example we got well one through well ten we're going to have a top middle and bottom unit and for each one of them we can only have one face face is one two or three event a one is face is one in the middle event a two is face is three in the bottom now using these definitions you can use any one of them go ahead and try to disprove that we have independence here okay so i'll give three seconds and then you go ahead we'll look at the answer together okay three two one here's the solution a simple way to do this is to just i always like drawing circles boxing things off making clear what event a one and a two are so a one was middle is face is one so those are all of the a ones the circles down here are the a two's face is three in the bottom of the the bottom unit or base unit so we can calculate the probability of a one probability of a two and we can calculate the intersection a one a two we had two times that it both occurred out of ten and so we have 50 percent 60 percent 20 percent we can do a check probability of a one times probably of a two is equal to 30 percent this is not equal to the intersection between the two and so we can say that we don't feel this is independent now once again very difficult to prove to prove independence but it's pretty easy to show that it doesn't meet the criteria for independence so therefore we suspect it's not in the pen it now this is very useful because imagine out in the field if you had a data set knowing that there's a degree of dependency between the middle and the bottom is a predictive model it helps you make choices in some cases you may only know the face is the middle not know the face is in the base maybe you didn't collect core down there you can use middle to make a prediction of the bottom because there's some information being shared between them all right we're going to stop there for right now we've covered a lot of probability concepts around frequencies probability in the next lecture we're going to go ahead and get into Bayesian probability and that's going to be a lot of fun I really love that stuff but I hope this was useful to you once again I'm Michael Perch I'm an associate professor at the University of Texas at Austin I share all my lectures online I share content almost daily on Twitter I'm the geostats guy on Twitter on GitHub I have a repository with all of my worked out examples that is geostats guy repository on GitHub and I of course have this YouTube channel which is geostats guy lectures all right I hope this content is useful to you everybody take her
Alright, in this lecture we're going to talk about Python functions in a little more depth than we previously have. So we've seen functions like this. So this is a function. Name of the function is My Sum. It takes two arguments, A and B, and we're just going to return the sum of them. We call the function like this. What we haven't talked about before is what type of arguments A and B are. In the way that the function is currently written, these are called positional arguments. In other words, the value of A is always going to be the first argument that's in the first position of the function list. Same for argument B. B will always be the second. There's another type of arguments that we can have. They're known as keyword arguments. And in this case, the difference between a positional argument, a keyword argument, is given by a keyword argument always as an equal sign and some value. So for example, we could have something like this. In this case, A and B are now keyword arguments. And their default values are two and three in this case. So I can actually call this function as I redefined it without any arguments at all. And if I do that, the default value of A is going to be two and the default value of B is going to be three. So if I execute this, we get what we expect. With keyword arguments, we can call them as we would positional arguments before. So we can say give it two values, four and five. And in this case, A will be, take the value for, B will take the value five, and we'll get the sum. We can also define them in reverse order. And I guess, let's to do that, we need an operation that the order matters. So let's say my subtract. And instead of adding the two numbers, let's subtract them from one another. And in this case, then we can call the function. As long as we specify the argument, we can put B before A. But again, we have to specify what they are. And in this case, we specified the keywords in the opposite order that they were originally defined, but the operation here still works correctly. So it's five minus one and not the other way around. So those are keyword arguments. There's another type of argument called a variable argument, and that's specified this way. So here we have this asterisk before our variable variable arguments. What variable arguments is going to allow us to do is put in any number of arguments in the argument list. It will store those arguments in a list that you can then iterate over, for example, with this for loop, which computes the sum of the variable number of arguments. So again, the syntax for variable arguments would be to have an asterisk in front of them. I guess I should say that you can mix and match positional arguments, keyword arguments and variable arguments. So you can mix and match variable and keyword and positional arguments. However, the keyword arguments always have to come after the positional arguments. So in this case, the variable argument is like the second positional argument. And if we notice this when I call the function, these numbers right here add a 13, the default value of 2 is then being added as well, given me the result 15. So that's an example of mixing and matching the different types of arguments. There's one final type of function that's very useful, particularly in defining very small functions inside of other functions, things like that. And these are called lambda functions. So the syntax here, again, this is generally intended to be like a one-liner and not a very which would not be a terribly complicated function. In this case, the variable for the function is x and the body of the function occurs after the colon sign. So in this case, something like if we wanted to have x squared, this would define a function that takes an argument x and then of course, squares it. So if we want to call this function, it would be like this. We can also specify this as a keyword argument. So we could give it a default value of 2. In that case, we could call the function without any arguments. We could also have lambda functions that are functions of multiple arguments. So in this case, say we have a function that's equal to or a function of 2 arguments x and y and then just returns say x plus y. So this is a very, very simple implementation of our original MySum. And we can call this with the two arguments like that. So this is an example of a lambda functions or in other languages, there are sometimes called anonymous functions as well. So you might hear them called that as well.
Hey, howdy everybody. So now that we've covered CREING for spatial estimation, and we covered these sequential Gaussian framework for simulation of continuous variables, now we should cover the idea of indicator based methods for the purpose of estimation and simulation. So what can we say about indicator based methods? We can use them for the purpose of estimation and simulation when we're working with categorical variables, and specifically we benefit from the fact that we're able to explicitly control the spatial continuity of each category. In fact, the most common use of this type of methodology, indicator based methods, are in the cases where we have categories or discrete variables. We can also use them for the purpose of estimation, simulation of continuous variables, and benefit from the added explicit control of spatial continuity for different magnitudes. In the last section we talked about the fact that the sequential Gaussian paradigm results in a maximum entropy type features. These are recognized as very short or maximum discontinued, I should say, for the extreme values and greater continuity for the values in the middle of the distribution. This of course can be problematic if we consider the most important values with regard to our distributions across the impermeability, are in fact the values at the very ends of the distribution. Those are the very low permeabilities and prerosities that may form our barriers and baffles, and those at the very high may be our conduits, the flow, have the most important impact on flow and behavior. In order to work with indicator based approaches, we need to indicator code our data. It's interesting, this indicator coding is effectively a probability coding based on categories or thresholds, categories of we're dealing with categorical data, or thresholds of we're dealing with continuous data. And it requires us to infer indicator verigrams for every one of the categories or thresholds that we're working with, a little bit more work, but the concepts are quite straightforward, once we understand verigrams and typical type of simple creaking and so forth. So what are we doing when we are using indicator methods? So let's talk about the indicator coding. We're transforming a random variable or function or the data to a probability relative to a category or threshold. What does that look like? We've denoted like this, the indicator transform at location U, U bold as a vector or location vector, for the threshold or category ZK is the indicator threshold for a categorical variable. And what is it? It's what is the probability of a realization or data value being equal to a specific category. So the indicator transform at location U for category ZK is going to be one if at location ZU, the random variable or the data is equal to that category. And it's equal to zero other ones. So give you an example. We have threshold two, which is equal to two, that's categorical, so it's in fact a category. And we have a data value at location one, U one, which is equal to two. The indicator transform will then be equal to one. 100% chance at location one, that we have category one, because the data value at that location is equal to that category number two. And we could have another example here where we have a threshold of one, but then we have a random variable. We have some type of soft constraint in for data, not hard data. And so the indicator transform would indicate that we have a 25% chance of that category occurring. It's not one or zero because it's soft constraint. The indicator transform we have a continuous variable is now we're going to be talking about what the probability is for realization to be less than or equal to a threshold. And so our indicators define like this, where it's one if the random variable or the data is less than or equal to that threshold. And zero other wise. So examples would be given a threshold number one of six percent. It could be porosity. The data value at location U1 is in fact 8%. What's the probability of it being less than or equal to 6% at zero? So the indicator of coding is zero. And of course we could have a location, well so we have location two, we have a threshold 18, and at that location we have soft information described by a Gaussian distribution. And we calculate the probability of being less than or equal to this threshold of 18%. And that turns out to be 75%. That would be the indicator or probability coding of the data. Now just in case this concept of indicator coding is not clear to you. I have provided a table where we have a set of data, location one, two, and n. So there's n data. And we have three different possible categories one, two, and three. And we have the indicator transform relative to category one, category two, category three, at all locations. And so the indicator transform for the first location, which has a true value of three, it's hard data. The probability the category is equal to one, zero percent, two, zero percent, three, hundred percent. Location two, the actual category at the hard data is equal to one. So the transform is for category one, a hundred percent, for category two, zero percent, zero. And you see hard data will result in these categorical hard data will just be simply one or zero if we have the actual value in that location. And so this is a probability coding. So we take our original data vector, which has one column by the number of data, and of course they'd all have locations and so forth. And we convert it into a array with k columns, where k is the number of categories. And so that's what indicator does it actually converts our data into k separate sets of information or probability code. So first we could look at the case where we have a variable. And so now we have data values that one, two, and n is 12 percent, 4 percent, and 17 percent. And our thresholds are 5 percent, 10 percent, and 15 percent. These could all be parosities again. What's the probability that 12 percent is less than 5 percent, zero, zero for 10, and one, it's less than 15 percent. Probability of 4 percent being less than these thresholds, 100 percent, 100 percent, 100 percent. 17 percent is larger than all of the thresholds, zero percent for each of them. And so you can code your continuous data relative to each one of the thresholds like this. Then we can use these indicator transforms or indicator-based probabilities for spatial estimation for each one of the categories or thresholds. And so what we're doing with indicator-creeing is we're going to discretize the interval of the variability of the continuous attribute z into a series of thresholds. If it's categorical, of course it's a bit different, but we'll discretize it into thresholds. Then we'll perform indicator-creeing to estimate at those thresholds at the unknown locations using the indicator transforms for that threshold of all of the neighboring data. And indicator-based verigram will get into that. And then we'll say that that is going to be equal to the probability at that unknown location, the estimate at that unknown location, of the true value being less than or equal to that threshold given n where n is your available data within the neighborhood. And so this is very powerful. We actually are estimating a conditional CDF or conditional distribution directly at the unknown location with no distribution assumption, but as a series of thresholds. For a categorical case, it's very straightforward. In that case, we're going to be estimating the probability of being equal to a specific category. And if you imagine if we estimate the probability of being in each of the categories for all possible categories one through K, that in fact would provide you with an entire distribution of uncertainty for the categorical or discrete case. And just to reiterate, we use the term conditional CDF or we can call it a CCDF is a CDF and an unsampled location estimated by local data. It's conditional to the data. As opposed to regular CDF, we calculate from the data. Now we're talking about a local estimate of a CDF conditional to neighboring data. Indicator creating, what does it look like? Well, this is how we could express it. In fact, we are going to estimate through indicator creating the probability at an unsampled location for a specific category or the cumulative probability at a threshold and we'll do it based on a linear set of weights, apply to the indicator transforms of all the available data. And we will also account for the global proportion if it's categorical or the global cumulative probability if it's continuous, the initial input distribution. And so one minus some of the weights will go to that. And so if we have no information, we just estimate with the global distribution continuous or categorical. And this is pretty powerful. What does it look like in practice? Well, you have an unknown location, you have data one, two, and three. The data will be broken up and the distribution at the unknown location or broken up into a set of thresholds one through four. And we have the data will be coded based on those thresholds. So you could imagine that they're based on the magnitude of the data values will have a transition over the thresholds from zero to one zero to one based on the actual data magnitudes. And so then that every one of those thresholds will perform indicator creaking and we will estimate a cumulative probability at that threshold. And we repeat it at the next threshold, the next threshold, next threshold. And in the end, we get a local conditional distribution at that unknown location. And so this is very cool. We're estimating directly the distribution of uncertainty, comparing contrast that to the Gaussian type approach or the simple creaking type of approach where we were estimating the mean and the estimation variance or the creaking variance. And then using a distribution assumption to then get a local distribution of uncertainty in this method, we actually build the CDF up by discretizing the problem at different thresholds estimating their cumulative probabilities. And then we'll use a piecewise linear model or some other type of interpolation method. We'll have to do some talix extrapolation. And there we go. We have an entire distribution available to us that we can then use for any purpose and we'll make a comment on what we're going to do with that next. Of course, if it's categorical, this works perfectly well with categorical cases. Our classes are going to be K1, K2, K3, different categories. We're going to estimate independently their probabilities at the unknown location, unsampled location. And we're good to go. We now have a distribution of uncertainty for categorical and distribution uncertainty for the continuous case at the unsampled locations. Well, so what do we have to do to make this happen? We have to use the indicator transform on all of the data. We have to calculate an indicator verogram. And then we have to use indicator creaking with the transform data and that verogram to estimate the probabilities at the unknown locations. And so let's go in through each one of those pieces. Let's talk first of all, we've already talked about the indicator transform. Let's just kind of finish a couple points really quick about the indicator verogram and what that looks like. So what do we have? How do we calculate the spatial continuity for the indicator approach? This is the indicator verogram right here. It looks just like the regular verogram. In fact, the only thing that's different is that now we are using the indicator transform of the data and we're doing that for each one of the thresholds or categories. So you'll notice that our indicator verogram is not just gamma. We put an i gamma indicator at h. It's not just h. It's also going to be a function of the threshold. It'll change with each one of the thresholds. Another thing that's very interesting about this, the indicator for hard data is either 0 or 1. So what do we expect? If we look at the summation, we're taking average one half the average of the summation of this square difference. Well, if both of them are equal to 0 or both of them are equal to 1, i.e. they both belong to the same category or they both aren't in the same category or they're both either above a threshold or both below the threshold for the head and the tail in space, those cases will always render a 0. Because we'll have the 1 minus 1 or 0 minus 1, that's 0 and you square the 0, you get 0. And so that doesn't contribute to the sum. There's no contribution to the sum. If they're different, ones inside the current category and ones outside the current category or ones below the current threshold and the other ones above the current threshold, in that case we get 0 minus 1, which would be equal to negative 1 squared, it becomes 1. Or we get 1 minus 0 squared, that it becomes 1. So what is it? An indicator verogram is going to be equal to 1 half the proportion of pairs that change. So we can look at the indicator verogram and interpret it as the probability of change over distance. And that's very powerful. Another point we should make and we've hinted at this before, you're going to need a different verogram model or you should build the verogram model. In other words, calculate the verogram, fit the verogram, interpret it and fit the verogram model that's positive definite, teach one of your thresholds or to each one of your categories. And so you'll have to do that. Now you're going to standardize them so that each have a cell of 1 in order to do that. You have to count for the fact that the variance of the indicator is actually going to be equal to P times 1 minus P, which is the proportion of 1's and so forth. But you go ahead and you standardize them so that they all have a cell of 1 in order to work with them. And then you're going to model multiple verograms that are positive definite. Now, just the warning. For the first cutoff, you can't say 100% nugget effect, second cutoff, super high continuity, third cutoff nugget effect. Again, if you if you modeled willy nilly so that they are changing dramatically between thresholds, of course, that's a contradiction. You'd expect some consistency between the indicator verograms. They can't just change dramatically. And so what do you do? You want to use data to support the modeling and make sure that you're using data consistently. And you also want to when you do build a model, think about making the changes smoothly varying between them. If you don't, you'll probably run into issues with order relations and we'll talk about that more later. And also in addition to that, by working in that manner, it's a little bit easier to infer the parameters in the case of sparse data. Back from helping a student who just showed up. And so how can it help us with sparse data? Well, we have information from each one of the thresholds or cutoffs can help each other. Note here we say cutoffs. This is how we posed it in the book. But we're referring to thresholds as we've been talking about right now. This would be one through K thresholds as described up till now. So some general comments about the indicator approach estimation simulation. You're going to need a verogram for each threshold or category. So it's a bit of a more difficult inference problem. But we'd argue that it provides greater flexibility because you can extreme the spatial continuity for each one of the thresholds or categories. The resulting model of uncertainty is not Gaussian. In fact, you can avoid that whole maximum entropy issue and create models with greater continuity for the extremes than you could with the Gaussian assumption with say sequential Gaussian simulation as we discussed last lecture. More readily you can integrate data with different degrees of softness into your conditioning. And we will talk more about how we incorporate soft data pretty quickly. You may also work in force. You can also use this approach for sequential indicator simulation. And we'll demonstrate that as an extension on the indicator creaking right away. The methodology for sequential indicator simulation is commonly used for categorical variables like bases and it's sometimes used for continuous variables like perost. So let's go ahead and go through the indicator creaking workflow. We're going to loop of course across all locations for which you do not have data available. We're going to also this in the additional loop in addition to what we saw with the simple creaking workflow. We're going to loop over all thresholds or categories for each threshold. We're going to find all the relevant data and we're going to code that data to indicator based on the current threshold or category. Then we're going to use that indicator coding at the current threshold will be able to make an estimate of the probability either the probability being within a category or the probability cumulative probability at a specified threshold. We're going to correct that distribution for order relations. We'll get a little bit more into order relations later. And then we're going to use that distribution. Now, so this is very interesting at all locations when we do indicator creaking. We're not going to have a single best estimate like we had with simple creaking. Instead, we're going to have cumulative probabilities at thresholds so we're basically estimating a complete CDF or it's going to be the probabilities of being within each category. So we're estimating directly a categorical PDF either way, we are getting at a continuous or categorical categorical conditional CDF at the unknown locations. So if somebody asked you to do indicator creaking and build a single map, you could ask, well, what do you want to see? Because I have fact have multiple probabilities describing an entire CDF. So what can you do with that? You could get a measure of the uncertainty probability intervals, percentile ranges or something like that. That could be useful. You could assess the probability exceeding certain given thresholds. That could be useful. For instance, what's the probability that you have a contaminant concentration that's greater than some legal limit? That could be very useful to you. You can try to do an expectation type estimates where you look at all of the thresholds and you look at the distribution shape and you integrate and assess what is the expected value from the distribution. You can do that too or you could do a storkastic simulation. Let me go ahead and illustrate what indicator creaking looks like. We'll do it by hand on PowerPoint. So we have an initial distribution, cumulative distribution function is shown right here. And so this distribution is for say, porosity. That's over all of the possible data we've available to us. Now we're trying to make an estimate that this unknown location given the data, one, two, three, four locations. And we have assigned four thresholds to our CDF to describe it. Now what are we going to do? While we signed our thresholds, we need to calculate indicator verigrams for each one of these thresholds. So you do the indicator transform. And so I've just provided an example right here. And so we have 12, 19, 14 percent and 15 percent. What's the probability of being equal to or less than those thresholds? And this one should be one. Thank you for the correction in class. I think that was Alex. Appreciate that. And so we would assign 0, 1, 0, 1. And you can look back and you can say, yeah, 15 percent is greater than that would be 0, 14 percent equal to 19 percent greater to 12 percent is less than. And so we would have indicator coatings like that. We can then calculate the indicator verigram of that data set over all of our available data, even away from this location where estimating all of the data that's available to us. We would calculate the indicator verigram. Next, we would perform indicator based creaking, which simply would be taking our indicator verigrams to populate a covariance matrix. We would have a set of these matrices for each threshold for performing an independent creaking for each threshold. The first threshold, we calculate all of the co-verances between the data and themselves, the data and the unknown location. And when we do that, we get a creaking estimate of the probability, cumulative probability of 14 percent of being 15 percent. That means we have 15 percent probability at the unknown location having a porosity less than or equal to 14 percent. Cool. So we can repeat that again. We do the indicator coating at the 16 percent threshold. You'll notice that we had a change now that this data location, because it was 15 percent is now less than 16 percent where it was greater than 14 percent. And so now we have 111. You'll notice that over the thresholds, as we go through them, more of the data get coded as ones. So it makes sense that there is some monotonic, not perfectly monotonic perhaps, but there's an tendency towards increasing as we make these estimates. That makes sense. So we get another estimate 60 percent chance of being 16 percent porosity or less. And we finish up with all four categories, or thresholds, I should say. And now we have four points describing our CDF. Now that's not enough to do Monte Carlo simulation. We could use some type of piecewise linear interpolation or some other type of interpolation. And we have to do some work to estimate the tail extrapolation decision needs to be made. Once we've done that, we have a complete CDF. And we're good. We got a CDF. We know the uncertainty at this location. Well, once you have that CDF, you could do Monte Carlo simulation permit and get a simulated continuous porosity value. If you take that value, put it in with data, treat it as data. And for all subsequent calculations, you do the indicator transforms on that too. That would be very powerful. That would now you have a simulation method. Now I should mention you would pre-calculate the barragams indicator, barragams over all of the available data with the indicator transforms. You'd get one barragum for every indicator transform or every threshold, I should say, or category. And you would systematically use that. The new simulated values would not be used for barragam calculation. They would only be used for conditioning the additional indicator creaking calculations. So you'd have to calculate all of the indicator transforms on each one of those to proceed. So one of the great strengths of the indicator based approach is the ability to directly integrate soft data, not just hard data. Now hard data is not a big deal. In fact, we've been showing you examples up until now of a lot of different hard data. This is exactly how hard data would be coded with the indicator based approach. The probability of being less than or equal to a threshold would be zero until the point at which the data is less than the threshold and then it would jump up. And so the data value would just, it would just jump up as soon as we got to the actual value, say, 15% porosity here. But the cool thing about the indicator transform is that you don't have to provide a zero or one. You can in fact say, well, listen, the porosity is between about 17% and about 33% and in between there's a probability based on this curve right here. That would be soft data. You're encoding it as saying that, well, yeah, it's more likely in the middle, but there's some probability to be on the tails. You could even do an inequality constraint and that's really cool. What that means is that for thresholds that are between 20 and 30, you just don't say anything. You don't have any information about how it's behaving. But you do know that the data is showing you that the value must be greater than 20 must be less than must be less than 30. And so this constraint is saying the data tells us it's between here and here, but doesn't tell us anything else. And so we can actually encode that in an indicator transform as this inequality constraint. All right. Now let's just mention order relations. What does order relations correction do? And so this is a problem that we have is that if we're calculating in indicator based approach to get a CDF. We in fact are going to be calculating at each individual threshold. The value. So look, we could have a value here, a value here, a value here, a value here, a value here, here, here. Now if I was to go ahead and draw that CDF, I hope I can convince you that that is not an appropriate CDF. The CDF to be valid must be monotonically increasing or have zero slope. If it's decreasing with negative slope, it suggests negative probabilities, which are not possible. And so what do we do instead of using those original points, what we simply do is we impose a non negative slope and we go through and we connect the points we go to this point here. And then we say, well, the next data is down here, but we can't go there. So we'll just go across next data still low will go across. Okay, we're fine now. Okay, now we'll go to the next data point next data point. Okay, oh, this one's down low again. So we'll just go across to it and then we'll go up to one and then we're good. Now you'll notice that would create a bias. We would be higher systematically. And so we've got another possible solution. So what we do is we go backwards. And we go down to that first point. We said, well, we can't go up or negative slope going the other way. So we'll just go across, go across. Okay, now we're good again. We can go down to the next point, go down next point. Oops, we can't go up. So we just go across. And then what we do is we just take the average of those two lines. And that becomes our solution. And that avoids any bias and imposes the constraint of non-negative slopes. And so that's order corrections. Now a categorical variables, of course, we're going to be estimating the probabilities and unsampled locations of each one of categories could be boundstone, wacky stone, mudstone, and so forth. So we will be considering a set of categories. The point we should make is that the order relations correction for this is pretty straightforward. If these don't sum to one, all we have to do is just divide by the sum of the probabilities. Each one of them divided by the sum. And that would standardize them all to spend some to one. And so that's the approach we're correcting order relations in the case of a categorical variable. Let's look at some example indicator simulations now. We got a categorical indicator simulation right here, taken from a paper by Michael 2009. And it's sand, silt, or clay, yellow, kind of a gray and a black color. And so if you look at them, you can see the general type of behaviors. If you look carefully, you might notice or convince yourself that definitely yellow has different spatial continuity, definitely a different proportion than of the others. It's more continuous though. And you will see that they can each have their own continuity, their own proportions, they could even have trends and so forth. And so this is the type of simulation that could be used to then constrain porosity, permeability within each of the faces. Then we have this example right here. It's a continuous simulation in which we have a V-shell model. We have multiple models, this realization one, two, and three, and it's taken from the book, Mejana and a verma in 1994. And so you can see the basic kind of spatial continuity. Notice it's not Gaussian. We don't see maximum discount connectivity of the highs and the lows, but in fact, they have each have their own spatial continuity. I should mention that there is kind of a degree of, of, of discontinuity between the thresholds. You can actually probably estimate where those thresholds are based on where things get a little bit broken up somewhere around yellow to green, somewhere around green to blue. There's some things getting kind of broken up a bit. That's a known artifact with continuous indicator based simulation that you don't reproduce quite the right connectivity between the thresholds. It'll a little bit too broken up. And so of course, that of course, that could be a concern. And that's a reason why many don't like to use this approach, but the same time it is a possible approach to work with. Now, what else do we have available to us? Well, what I did was I went and excel and I created an example of indicator based simulation. I just put it together as a nice little template for the students to get a little bit of experience working with it. So let's go ahead and look at this sheet right here. This is the one where I have indicator based. I should set indicator based creaking. And so as usual with my Excel spreadsheet, you can change the yellow, but you can't the orange or the calculations and there's intermediate calculations and so far for the creaking systems. But let's go ahead and just look at it and understand what's going on here. So what do we have? We have the input where we say we have category one, two, and three. So you get to decide what are the categories one, two, and three or quite straightforward. Then you have the proportion of each. We set the last proportion to be one minus the sum of the other two so that we sum from one as we should. And then that's plotted down here. So that's the global proportion of each one of the categories. Then we have data. The data one, two, and three and their locations one, two, and three. And then we have the unknown location right here. Now I'm not sure if that's one, two, and three. I just picked and said one, two, and three. Let's see, 50 by 40. So this is one, 20 by 90. This is two, and this is three right here. And the unknown location is right here. The data values that those unknown locations are set to be toggled by what are the categories available previously. And these are the indicator transforms for each one of them. It's quite trivial. And then you can specify the barogram models for each one of the thresholds or categories. They each have their own barogram models. Nugget effect will be the sill or one minus the contribution of the spherical so no nugget effect here 300 range. And then what will happen is you will then have the creaking system notice there's three separate creaking systems one for each one of the indicator transforms for category one, two, and three. Using the specific barograms for each one of those categories. And the result is that you get a set of weights that will be applied to the indicator transforms of each one of the categories in order to get the estimate. And so if we look across and see where we got the creaking estimate that's being made for each one of the proportions, you can look at it very quickly. No, excuse me. You can look at it very quickly. You can observe that in fact that estimate is simply going to be the indicator transform of each one of the data applied right here multiplied by the weight to each one of the data. And then we'll take this one minus some of the weights and we'll apply that to the global proportion. And by doing that, we get our creaking estimate of the proportion of one at the unsampled location. And we get the creaking estimate of proportion of two or probability of two at that location and the creaking estimate of the probability of three at the unsampled location. And that's the resulting distribution right there. And I suggested a couple of ideas. Let's just try one or two of them out right now. First thing we could do is let's set the ranges very small. Okay, we set the ranges very small. What happens? At that point, we're now estimating with the global distribution. There's no information from the data whatsoever. Now on your own, go ahead, take the sheet and go ahead and try some of these other ideas out. Place at one of the data at the unknown location. See if we have an exact estimator. Does it estimate with the category at that data location? Set all the data to the same category. See what happens then. So there's a couple other things you can try out. Let's make a summary here. So summary of indicators. So first of all, you need indicator transform. It's a probability coding. You can work with thresholds of a continuous variable or you can work with specific by category assignments if it's categorical purpose. You can work with it for spatial estimation. And in that case, you're estimating directly the CDF from your estimate in the CDF by estimating the cumulative probabilities of thresholds or estimating the probability of being within each of the categories for discrete discrete case indicator for spatial simulation. You can, what you can do here is you replace simple ordinary creaking with indicator creaking in the sequential context. You do Monte Carlo simulation from the CDF that you got from the indicator approach. You transform the value with indicator transform based on each threshold and you use it for data for the subsequent indicator creaking. So that's all we have to say about the indicator methods. As always, I hope that this was helpful to you. I'm always happy to discuss. I'm easy to find. I'm Michael Perch. I'm a professor here at the University of Texas at Austin. I'm also on GitHub and on Twitter. I'm the Geostats guy and I'm always happy to discuss. All right. Thank you.
Hey, howdy everyone. I'm reporting from a different location today. I was working from home and I thought I'd put together a quick lecture on Crigging and so this is my office at home and So last time we got into the issues around trend modeling we were building trend models in order to take and Decompose the total variability of a spatial system in the two buckets on one side We had that which was trend and considered known and it was typically non-stationary We would have a non-stationary mean or variance or whatever the statistic was we would model it Then what we would do is we would then have a Residual and if we're talking about a mean that's very straightforward We just take the data values to track the trend we get a residual the residual now be stationary and we can go ahead and try to model that using our geostatistical methods The trend is considered known the residual is considered not known or unknown and we will use Statistical geostatistical methods to try to figure that out. So we have Remove the trend we're working for residual and we're ready to do some creaking creaking for the purpose of making estimates in space So what does spatial estimation look like? Well, let's go ahead and take a very simple example and Illustrate consider this situation we have this area of interest delineated by this oval type shape here and We have data points one two and three those locations one through three we have Data available to us so we know what's going on now the problem we have is that we would like to know what's going on at this unknown location right here We want to understand what's going on this location. We have not sampled that location So at the data locations one two and three we have a Z measure. It's not a random variable It's a data point so we showed in low caps Lower caps we have our usual location vector U in bold. It's a vector Location one two and three those are our three data three data locations The Z could be any variable of interest. They could be Corrosity it could be permeability for most of the discussion today. Why don't we just assume that we're dealing with porosity So now we're going to try to make an estimate at this unknown location How are we going to do that? Well, there's many ways we could do it a very convenient way to do it would be to form an estimator that looks like this Now when we say Z star we're talking about the variable of interest at the unknown location star means an estimate You not because we're dealing with the unknown location will just call not or zero and so when we think about making an estimator This is a very logical way to do it What we have here is an equation where we're taking the sum of For each of the data one through n there's three data available to us We have a weight that's assigned to each of the data Lambda is the weight and Z or Z I should say is the data values at those locations. So we're just waiting the data with a linear weighted Average of the data effectively or linear weighted estimator of all of the data So far so good But we could be concerned about biasness and we could imagine that if in fact the sum of the weights is less than one or greater than one We could imagine that we would actually be adding bias to the system How do we count for that? We take this term right here? We just simply say take the sum of the weights to the data and We're going to take one minus that so the remainder in order to get us up to a sum of weights equals to one And we'll put that weight on the global mean That's a very straightforward way to do it. It does provide us with Unbiasedness and so it's useful now you could also imagine there might be a circumstance in which we don't think this data It's useful for this unknown location and in that case we could imagine the sum of the weights being very low and It would make sense that much of our remaining weight or that large remaining weight would be applied then to the global mean So this makes sense. It's a pretty rational type of an estimator to work with now Of course we want to account for the fact that we're dealing with a phenomenon that is usually non-stationary many things we work with Non-stationary the mean is changing locally the proportion is changing locally So if we go back to the mean we could imagine that what we could do is just Recast this equation but in from the perspective of residuals a residual work And so we have Z estimate but at the Estimate location this location here we subtract the mean at that location and We have the data and at the data locations We subtract the means at the data locations and so you can see there that we're working with residuals at each location And so you could imagine that given that we could say why residual is equal to whatever the Z value is minus the local mean and if you do that this entire equation shrinks down and just now it looks Very simple why star at unknown location the estimate of the residual we know the mean so we'll have the full estimate given the mean again Is equal to the sum of the weight supply to all of the residuals at the data locations? And so we're fine we now have an estimator that we can work with and you can confirm for yourself that this actually now Is also accounting for that unbiasedness constrained in fact? We will still have the some of the weights being applied to the mean and if you think about some trends here Because of the fact that when we work with residuals the mean is now zero So whatever weight is not applied to the data is in fact being applied to a zero mean and so everything's working out We got unbiased as working with residuals. We can account for non-stationarity. We feel pretty good about that So now the question comes up. How are we going to get the weights? And so let's just look at this problem right here What would be the weights that you would want to apply? What would be the criteria that you would use to decide on the weight or the relative weights between each one of the data values? Data point one and two are both very close to the unknown location Data point three is much further weight would we want to give data point three less weight? Then one and two it makes sense that closeness is going to matter Now what about the fact that would data point one and two are very close to each other? Does that affect the weight that you give to each of them? What if data point two was on the other side the same distance away? Would it get more weight and would data point one have more weight then? What if I drilled four five and six right here? Would they continue to get the same weight as one two and three or would they impact the weights? It seems that data redundancy should also matter and then finally What if I told you the direction of spatial continuity was like this? That there is much greater spatial continuity here and very poor spatial continuity in this direction here This is a minor. This is a major direction of continuity in that case We might see data point three getting more weight than one and two even together And so what's that telling us we want to count for closeness? We want to count for redundancy of the data. We also would want to count for spatial continuity and coming up with these weights So Waiting schemes we could propose equal weighted on all the data we could do that Equal waiting would just simply be the weight is equal to one divide by the number of data That of course would not be very satisfactory That would not account for any of the things we just talked about so wouldn't account for closeness redundancy or spatial continuity So we're going to just throw that out. We're not going to do that What about inverse distance? In fact many people know about inverse distance waiting It's commonly used for interpolation and spatial problems where we have some type of sparse data The methodologies very straightforward the weight is simply one divided by the distance between the data point in the unknown location raised to a power And that power is giving you sensitivity of distance if you use a power of three You say high sensitivity to distance and And in impact of that is if we made an inverse distance map from this data That we would see huge influence a data point number two in this area much less from data point one and three and Your model becomes more locally specific less averaged more bullseyes within the model If you use a power of one it's got a lot less sensitivity to distance and as a result of that You would see much more smooth type of behavior a lot less locally specific to the data around you now This part of dividing by the sum of the weights is just to ensure it's a standardization to make sure that the sum of the weights is equal to one What's the problem with this? counting for closeness Not accounting for redundancy and not accounting for spatial continuity at all directions of spatial continuity or anything like that So it's not a great method to use either So we want something that can account for closeness redundancy and also spatial continuity concepts of spatial continuity how we're gonna do that well Let's go ahead and pose a New construct we're gonna take our Linner system our our system of estimation here based on weights Apply to all of the data with the residuals to get the estimate of the residual so far so good we know about that and Then we're going to define estimation variants estimation variants will come up many times So come up when we talk about machine learning and in estimation variance is very straightforward It's the expectation of the Estimate minus the true not available but true value at the unknown location And we're gonna square that and so this becomes a variance If it's centered on zero, it's just a variance so we have estimation variants and what's very cool about this We know how to work with expect expectation It's very simple. We got a quadratic. We can expand the quadratic We expand the quadratic we get of course Y star u squared We got this other term here, which is the unknown value squared and Then we have this product term here now It's very cool is for each one of the Y star estimates We know the equation that we use to get it. It's a sum of the weights applied to the data And so we can substitute that into the equation and when we do that we get these terms right here Now it's very cool about this now is if you look at this term right here We got a double sum double the weights so we're now going to be summing over the data with themselves Comparing all combinations of the data and their weights so far so good But this term right here is the expectation of Data located at index i data located at index j And if you think about it that is if we consider it centered with a mean of zero again That is equal to the covariance between data location i and data at location j We can substitute the covariance in there This term right here we got a single sum which is going to be over all of the data with the unknown location and So if we look at this the expectation of the unknown location times the data This is in fact if we're assume once again a mean of zero Is going to be the covariance of the unknown location with the data and This term the remaining term Simplifies for us it turns out that it's simply going to be the variance of the problem Because we're just simply looking at the variability of at all of the Unknown locations is simply going to be the variability of the problem. That's just simply a variance And so we have a variance term here will denote it since we're using covariance as the covariance at the zero distance Which is equal to the sill which is equal to the variance? Okay, so this is what we get we get for the estimation variance or the performance of our estimator we're going to get a Value here, which is this this component here, which is the redundancy component right there Then what we have is we have and look it's redundancy we're simply looking at the Coverances between all of the data with each other so it's accounting for all of the combinations data and how they're Correlated to each other or what the covariance is between each other This term right here is the covariance between the unknown location all the data This is a summary of the closeness of the data to the unknown location and This component is the variance and this makes sense because if you have a problem for which the variance is very large You would expect systematically for the estimation variance to have a potential to go higher And if you have very small variance of the problem well, it's an easy thing to estimate the estimation Varences are going to be uniformly low So we've got estimation variance now where engineers and so what are we going to do? We want to find the weights that will minimize the estimation variance if we do that We could all agree that that would be the best weights to use we want to minimize estimation variance have the best estimate possible And so we take that former equation and we apply the partial differential of it relative to the weights and if we do that There's a there's some derivations we're skipping over of course through the purpose of this course They're available in the standard textbooks You could find them within the Purchan Deutsch book of course Googlers. I'm sure it's in journals book and so forth Maybe I'm strong asked about an ice expo You'll find that This will actually simplify to this equation right here a Summations weights multiplied by the co-verances in the redundancy term Minus two times the co-verances between the data non-normocation and So what does this result in it results in a system of equations? We've got any equations and unknowns this is the simple creaking system Now let's go ahead and define creaking really quick and then we'll go ahead and look at it in matrix form which I think is much more efficient and so what do we have here? We have Creaking is defined as an estimation approach that relies on linear weights that account for spatial continuity data closeness and redundancy the weights are unbiased The estimators unbiased and it minimizes the estimation variance Okay, so let's go ahead and look at in matrix form we had system of equations Let's go back to our problem with three data one two and three here. We got an unknown location right here two dimensional problem If we look at the system of equations, we're gonna have a weight applied to the covariance between Data point one in itself weight two Data point one and two and so forth All the way to the covariance between the unknown location and data point one and you carry that through through data One two and three three equations Now you might be wondering about how we got covariances we modeled the verogram You can recall from the previous lectures on spatial continuity We talked about the fact that the Verogram can be directly related to the covariance by the sill Covaryns at h is equal to the sill or variance minus the verogram at h and so We can take this system of equations and and look at it within matrix form and what do we get? The left hand side of the system is the Covaryns is between all of the data and themselves This is really cool because this part of the matrix this this part is this matrix I should say captures all of the information about Parallys redundancy between the data This side is the coverances between the data and the unknown location it captures all the closeness and This array this matrix right here is simply or vectors just simply the weights that we're trying to solve for How are we going to solve for it? We populate the coverances using our positive definite spatial continuity model Then we invert the left hand side Multiply it with matrix multiplication by the right hand side calculate the weights very simple You could do it in excel it be very straightforward So what are some of the properties of simple cream? First of all if the covariance model that we're using to describe the spatial behavior of the phenomenon of interest is Positive definite. There will be a solution. There will be a unique solution to that matrix system We can go ahead and calculate the optimum creaking estimate and the associated estimation variance or error variance The creaking estimator is unbiased if we take the estimation I'm sorry the expectation over all of the locations the data Minus the estimate at or the true value minus the estimate We'll find that in fact that's equal to zero. It's unbiased as an estimate We know already we talked about a little bit about the derivation That we expect it to be a minimum error variance estimator That the weights are solved for that minimize the error variance Now in case you don't believe this go ahead just try to pick weights go and just pick weights anything different than what creaking gives you Calculate recalculate the error variance and you're going to find out that it's always going to be higher So we call it the best linear unbiased estimator because of these general problems Now think about creaking creaking is very cool because creaking is like a good friend The good friend that gives you a good estimate of something tells you what they think is going on But also says how bad is the estimate? They're very honest about the estimate they're giving you so if you take the previous error variance equation And you substitute in it the constraints from the simple creaking system you get this nice tidy equation right here Which will tell you what is the we'll call up the creaking variance What we can also call the estimation variance and this is an assessment of how good your estimate your creaked estimate is an unknown location Very very powerful the other point to assistant in exact interpolate in other words at the data Locations creaking is going to give you the data value at the data location Another point we should make is that the creaking the creaking variance can be calculated before you actually have data This is very powerful. That's because it's homoscodastic The creaking variance doesn't depend on the values at the locations It simply depends on the fact that you have data at those locations So if we go back to this example that we kept looking at We could go ahead and say well What what would be the uncertainty at this location? Given these three data you could calculate it with the creaking system. That's fine But you could say what if I had data here? What if I drilled this wall right here or if I sample With a drill hole or soil sample whatever is in space If I take a sample here how will it impact uncertainty here? With the creaking system you calculate the change in the creaking variance Knowing the data value here because it's homoscodastic. It doesn't depend on the data value Now of course if you get a new data value right here and you find that it contradicts your trend model Or if it changes your spatial continuity concept of course that's a whole different situation What else can we say creaking takes in the count distance from the information That the right hand side of the equation the system We got the left hand side that accounts for the redundancy within the data and it counts for the spatial continuity because you're using co-currences creaking smooth In fact if I was to calculate creaking estimates at multiple locations here here here and here and here They would all be very similar to each other problem They would be very smooth taken join that's because at each location creaking is trying to give you the very best estimate at each location It's not trying to give you the right spatial continuity It's not trying to honor the global distribution of the porosity or whatever property we're working with Just trying to solve the problem of getting one Best estimate at a time. What's that mean? It means that creaking is too smooth and The cool thing about it is you actually know how too smooth it is and we can use that when we get the simulation to correct for it One more comment or a couple more comments about creaking If the data is all outside the range of spatial continuity from the unknown location the simple creaking weights will all equal zero and the best estimate If we take the sum of zero times all of the data values get zero you can estimate with zero for the residual add back in the trend You're estimating with the local mean the trend model or if you have a stationary phenomenon the global mean We also have an interesting thing about creaking and that is you can screen data So I give you a one-dimensional example where we have data at locations here here here and here And let's just consider the two closest data to this unknown location right here. So this is where we're trying to estimate What's very interesting is If we were to go ahead and calculate the creaking weights We would find out that this would probably have a positive weight here But this data closer to the unknown location and in 1d It's perfectly screening the data behind it. We might expect negative weight on location number three What does that mean while for working with residuals like the residual v-shale mean of zero These would be negative data values multiply a negative weight you get a positive value When you get to pause the value times the value here if you sum this all up you can actually extrapolate outside the range of the data Creaking can extrapolate through negative weights Negative weights happen because data is screened Another way to think about screening in 2d would be if you were to imagine that you have the situation like this And you have an unknown location right here and you have data value here If you could imagine screening kind of like you're shining a light From the unknown location and so any data that falls back here Is being screened by this data value right here And so that's another way to think about screening All right Another interesting property of screening is known as the string of If you have a string of data and we often do in the subsurface because we sample data along drill holes and wells and so forth If you have a string of data and you're estimating an unknown location of weight from it You can have the possibility of having a distribution of weights look like this Typically it's not this bad. It'll be like a spike here then a mount here spike here What's going on? Well, what's going on is that you might have more weight here because this data values closer to that maybe even spatial continuity is greater in that direction Closeness But remember creaking also considers data redundancy And to creaking Which doesn't know that there's a boundary doesn't know that perhaps This is the top of the layer. This is the bottom of the layer It might not know that And so creaking just sees this data value and this data value is being the least redundant data values And they can perhaps receive more weight Programs that use creaking in them will typically limit the number of data per well to try to avoid this problem Okay, let's go ahead demonstrate creaking in excel and we're going to go ahead and play around with it and look at some different configurations If you go on GitHub Geostat guy you will find a Sheet just like this called simple creaking demo. It's in my Excel numerical demo repository And so what we can do here is we can vary all of the fields in yellow to change the problem So the data locations are shown here the unknown location is here If I want to move the data values around I can go ahead and just type in new numbers here And you can see it jumped around I arbitrarily just gave it data values I just decided to use the global mean from the mean of the data Three data is not enough to get a mean but I just did that for the demonstration we get residuals for doing the residuals Take all of the data locations work out the distance matrix Distance between the data themselves the diagonals can be zero because that's data one with data one data two with data two and so This is the distance between the data and the unknown location This is the barogram model now I kept the sheet very simple. There's nugget. There's spherical Structure you're allowed just one structure isotropic with a range 300 data sizes 100 by 100 And then the covariance matrix is calculated directly from the distance matrix using the barogram model We invert the left hand side multiply by the right hand side to get the weights We get the sum of the weights We get the weight that's assigned to the mean which is one minus the sum of the weights And from that we get the creaking estimate and the creaking estimation variance So let's try a couple of things this will be kind of fun So the first thing we can try to do we take that one data point that we just moved We can move it back Why don't we put it kind of close To data point number two We could do that by saying okay 25 and then we can move it down to 60 Okay, so just take a look at the weights right now and when we do that look what happens You notice that data point number two dropped in weight This data point here Then increased in weight, but it's like they're sharing the same weight Everything has become equal weighted Okay, so this is very interesting. It's almost like it's equal weight it. Why is that? These are closer to this unknown location, but they're redundant with each other This is further away remember the problem is isotropic And so they're all getting the same weight because redundancy is knocking down the weight here Closeness is kind of poor for this one so it knocks it down for that one there too And so that's kind of interesting now what we could do that's kind of cool is we could go ahead and try moving this data closer So now let's move it in just a little bit so we move it to 75 while in the same distance So now we have the same distance away and what's happened this is very interesting We have if you look at the sum of the weights right here They're just a little bit larger than the single weight given to this data It's accounting for the fact that they're redundant Okay, let's try something else Let's go ahead and take data point number one Let's move it over and then let's move it down What happened? Let's even move it in a little bit closer I think what if we bring it all the way into 20 Okay So if you look right now we put this data value number one right by number two number three is over here The weight on data point number one is negative It's starting to screen data point two is screening data point number one and so it's gone negative That's very cool. Let's go ahead. We'll move that back Let's get it out of there and put it back I don't know around this there And we can move it in just a little bit closer Okay Now let's decrease the barogram range so everything starts to go out of range What happened? We have a 50 meter range We're still got this stuff these data and range, but they are getting less weight If you look at the sum of the weights now it's equal to point eight The sum of the weights that are the mind one minus to some of the weights that's applied to the global mean is almost point two Let's decrease the range again Ah, it's 20 now So now all of the data are out of range the sum of the weights is equal to zero none of the data is informative about the unknown location And the weight given to the mean is equal to one The creaking estimate is the global mean and the creaking variance is equal to the cell The nugget effect component plus the spherical structure which means the uncertainty is equal to the total uncertainty The problem to the variance of the problem All right, this spreadsheet once again simple creaking demo is available in excel numerical demos within the geostack guy GitHub repositories Okay, so I just put a couple of exercises here. We just did them I think it's very useful to explore and get experiential learning with creaking That's why I put in excel so people can just try it out Learn it that and feel kind of get a sense of how creaking behaves. I think that's very powerful Okay, so what's next? Now there's a different type of creaking which is called ordinary creaking And if ordinary creaking what we do is we say let's add an additional constraint Let's say that the sum of the weights from a sequel to one That's a pretty reasonable thing to do if you do that what happens Is that you just have to expand the system of equations You have this component here, which is just saying if you do matrix math saying wait one plus wait two plus wait three is equal to one That's all it's saying so it's very straightforward Now what's very cool if the weights the sum of the weights is constrained to equal to one If we go back to original estimator here and we look at before we did the residual removal and all that We would realize that if all the sum of the weights is equal to one you don't you no longer need to know the global mean In fact, what ordinary creaking does it estimates the local mean the um that mean locally from the available data And so this is very powerful. What does it mean? Ordinary creaking relaxes the assumption of stationarity of the mean it estimates the mean locally within the local window using the available data Okay, let's just make some points about creaking Creaking procedure for constructing minimum error variance linear estimate at a location where the true values unknown It's a spatial estimator The main controls on the creaking weights closeness redundancy and the vera Simple creaking does not constrain the weights Works well for residual from the mean or local trend mean but assume stationarity So you got to work with a residual that's going to be stationary or work with something that doesn't have any trends in it Ordinary creaking constraints some of the weights to equal the one therefore the mean does not need to be known It relaxes the assumption of stationarity It locally is estimating the mean and then making the estimate based on that many different types of creaking Generally they vary by the way that they deal with stationarity some include methodologies then actually try to estimate the trend model at the same time as Estimating the optimum weights We won't cover that right now Two implicit assumptions are stationarity we talked about already that in some cases we work with residuals like simple creaking We'd want to most of the time work with residual and in other cases like ordinary creaking We've relaxed stationarity a bit We allow the mean to locally vary Ergo density is also an important concept. It's a bit complicated But it gets down to the idea that the space of our which we are estimating is large relative to the correlation range Creaking and then it results in the stability of the statistics that we're working with It'll be more important when we talk about simulation methods Creaking is not used directly to map spatial distribution of an attribute It can sometimes we'll do it when we have something that's very smooth and or we just need to work with interpolations But we emit that creaking is too smooth. It's just giving us the local best estimate We can build them a map of that But we got to be careful because it's not representing the right total distribution right total variance And it doesn't actually reproduce the right spatial continuity if you took a creaked map and calculate the distribution Or the spatial continuity be way too low variance and it would also be Way too smooth the spatial continuity short range continuity would be very smooth This all gets corrected soon in simulation So I have an example where I have a workflow in R for doing creaking I will go ahead and record that as a separate lecture and talk through that I think that's a very useful opportunity. I will also talk about methodologies for using gslive and I will show some Python too So in summary we've just covered creaking The next recorded lectures will include Illustrations or demonstrations of creaking And then after that we'll get into simulation As usual I'm more than happy to answer any questions or discuss geostats at any time you may reach me by email Or you can reach me of course through twitter on the geostats guy on twitter I hope this has been helpful to you. All right, take care
Alright, in this lecture we're going to cover Python data structures. Data structures are a word terminology used in computer science to refer to different types of containers that store data. And there are quite a few different ones. We're only going to cover the most common ones used in Python. That is list, tuples, and dictionary. There are others and other programming languages have more linked lists, other things like that. We're just going to cover the three main ones from Python. So a list, I'll just start with some simple syntax, a list. Here I'm going to store it in a variable called a list. And it is a list always started with brackets, so square brackets. And then what goes in a list is kind of arbitrary. So a list can contain numbers. It can contain, of course, those are integers. It can contain floating point numbers. It can contain strings. It can contain a mixture, a heterogeneous amount of data. So in this case an integer, a floating point number, and a string. And I want a bull just to complete the common data types. So lists are useful for storing data, and then we can extract data from them. So for example, if I wanted to get the first element of the list, that would be the number one in this case, the integer one. Now Python uses zero indexing. So here we have zero, one, two, three, four. I'm sorry, just zero, one, two, three. So the first element would be the zero index would return one. The one index would return 3.4. The two index would return the string a list. And the three index would return false. You can, a couple of ways, you can add things to a list. One way is to simply use the plus operator. So we know a list plus, and then I can add something to that list, for example, another string. In this case, I use an extra set of brackets there, and another string is itself contained in a list. Let's remove that for the time being. So now we have added that. Of course, this addition operator only pins it to the end. There's another way to do that with using the object-oriented approach. That is by using the append function. So if I wanted to append another string to a list. In this case, it appends it and reassigns it. So if you notice, perhaps it's useful to go back up here, because in this case, we've added this to a list. But a list is unchanged. So the original list, a list, still contains with what we defined it appear. And we simply added it to actually have it read. Add it to the list and remain there. We'd have to do a reassignment like this. So the append function is another way to do this kind of operation. So you're going to add something to a list, and a list will then contain the original part plus the additional part that you're adding to it. Additionally, you can insert things into the middle of a list. So we can use the syntax insert. Here, the notation is the location that you want to insert. So in this case, let's insert something between 1 and 3.4. That would be into the 1 index location. And then let's insert the number 10.4. And then look at what a list is. So there you can see it was inserted. You can also do so-called slicing operations. So we've already seen how we can get part of a list by, say, we want the first entry. We know that's the 1 index entry. We know that's 10.4. But if we wanted to get, say, all the numbers from the list, which in this case are the first three entries. So in that case, it goes from 0 to the 2 index. 0, 1, 2. In this case, the slicing notation here, this colon means take from 0 to 2, that part of the list. I'm sorry. So 0, 1, 2, 3. So the slicing notation will return up to the third element, but not including it. And so there you see the, what's returned there. So let's assign this part of the list to a new list. We'll call numbers. And then we can actually call, since we only have numbers now, we can call another function that's defined on lists and that is sort. And then if we look at what numbers is now, you can see the numbers have been sorted. So 3.4 has been moved ahead of 10.4. There are many more functions that are defined on the list data, data structure. I'll leave it to the documentation, the Python documentation to discover what the rest of those are. The last thing I want to show is that you can actually, or two more things. The first one is that we can actually reassign things in the list. So if I wanted to replace the number one with, say, the number 100, the number one is in the zero index location. We've already seen how to take that part. We can actually go ahead and directly reassign it. So in this case, we'll reassign it to 100. And then we'll look at what numbers are so that you can see that that's been replaced now. The last thing is just that we can have a list of lists. So in this case, I'll define a variable list of lists and use this type of notation. So we have, you know, the first entry in the list is actually a list itself. And that list contains one in three. And with this, we can use the indexing notation again. Say I want to get the first item in the list of lists, which would be the list itself, one, three. That would be the zero index. And that's what's returned. But then I can go ahead and index further into this. So let's say I wanted to get three exactly. Then that would be located at the one, the first entry of that. And so you can have this kind of nested taking of nested lists. And it kind of follows similar to matrix notation. So if you think of the first item in this as the first row in a list, then that would be the zeroth row. And then the one would be the second column. So that would return three is shown here. So those are lists. The next thing we'll talk about are tuples. You might think of a tuple as kind of a constant list. So a tuple is defined not with square brackets, but rather parentheses. And again, it holds data. And the data can be heterogeneous as we had before. We can take parts of, so there's what's in the tuple. We can access parts of the tuple in the same way. Another nice thing about tuple is we can do this clever unpacking. So in one line here, what we can do is take the first element of the tuple. We'll be assigned to x. The second element of the tuple will be assigned to y. And the third element will be assigned to z. This is said to be unpacking a tuple. So I do that, then say print x, print y, print z. You can see that each of those are now stored that way. The one thing we can't do with the tuple is we can't do that index reassignment. So we saw earlier we can take, say if I want to take the word hello from the list, but I cannot with the tuple reassign it. It's going to give me an error when I execute this. So tuples are kind of, they're said to be immutable. It means you can't change. Once they're defined, you can't change what's inside of it. And so it's a good way to protect your data. If you know that you're going to store data and you don't want it to change in any way, then it would be better or wiser to use a tuple. And in some cases it can be more efficient in accessing that data and other things. That fact that it's stored in a tuple. The last data structure we'll talk about is called a dictionary. And dictionary is a data structure that contains things of the type keyword value. And I'll just give you an example of what I mean by that. So, have a dictionary we'll call my dictionary. Dictionaries are always use the curly braces to define them. And in here you'll always have a keyword. The keyword will always be a string. So in this case, I'll use the keyword first name. And in this case, the value could be any valid type. So it could be a number, a floating point numbered integer. It could even be a list. It could be another dictionary. It could be any valid Python type. In this case, I'll just use the string, John. And we'll have another keyword as well. We'll call last name. So keywords are always strings, but the values are arbitrary. So for example, we could use the keyword age. And the value in this case would be 100. So then to extract things from dictionaries, we're going to use the keyword. So in this case, for you can also, there's another way to get things from a dictionary. And that is with the Git command. So if I say Git age, now you might ask, well, why would I use one over the other? Because they both returned the value. And this includes an extra syntax. But the nice thing about the Git command is that you can actually give it a default value. So in the event that the keyword is not actually defined in the dictionary. So for example, if I tried to get, you know, say, data of birth from the dictionary, in this case, I won't get an error. If I were to use the original syntax, I would get an error. You see that because data birth is not in the dictionary. However, what I can do with the Git command is give it a default value. So in the event that data birth doesn't exist, I can assign it to a default value. So that would return the default value because data birth doesn't, in fact, exist in the dictionary. So that's a useful thing. There's other things that we can define or other functions that are defined on dictionaries. Like for example, if you just want to get the keys, the keywords, you can call the keys function that will return the three keywords from the original dictionary. You can also have, just like we had lists of lists, you can have dictionaries that have as values of some of the entries additional dictionaries. So for example, I'll use something like this where if we wanted to then grab what the value of linear solver is, we would use this kind of syntax. That's going to return another dictionary and so then we can tack on something like that to the end of it, to grab the exact part we want. So again, this is just a primer for the basic data structures in Python. We'll get a lot of practice using these.
In this lecture, we're going to talk about MatplotLive. MatplotLive is kind of the default standard plotting library for Python. It's been around quite a while, I think, since 2004. It's really mature and stable. And I think most people would agree it's kind of the default plotting library for scientific computing. There are other more modern ones that have their uses, but it's always good to use MatplotLive. And one of the reasons is because MatplotLive can probably create any type of 2D visualization that you've ever seen. It's basically it has all the bells and whistles, although sometimes people complain because to create very extravagant figures, then it takes a lot of commands to do that. And I can't deny that. Sometimes it takes quite a few commands to get the figure you want, but nevertheless, it is possible. One of the things I always like to start off with when I talk about MatplotLive and I say that it is possible. There was big news in 2019 for the event Horizon Telescope collaboration. We're able to essentially capture the first images of a black hole. And the images that they released, of course, came from the result of analyzing the data. And they did that in Python and produced a figure in MatplotLive. And the figures here. So the image that you've seen of the black hole is actually a MatplotLive figure. So primarily MatplotLive will be used for two-dimensional plots, so we'll see it also has some ability to do three-dimensional plots and visualizations. The thing to keep in mind here is that MatplotLive plots always contain axes, which contain an individual plot. This is typically kind of the surface area of a plot. I have a figure coming up that'll talk about that. And figure which is the final image that can contain one or more axes. So typically when you're talking about axes, you're thinking about the lines or the data on the plot. And this is not to be confused with axes, ix, is, which are the actual coordinate axes on the plot. The next figure helps highlight that. So what the area of the plot here is what you would consider the axes. In these cases, the red line, the scatter plots, the blue line would all be three individual axes as shown here. And the figure is kind of the area that surrounds that, that contains the coordinate axes, the tick marks, the title, everything else. So this figure comes from the MatplotLive frequently asked questions, and it just defines a lot of the things that you might want to have control over. The major and minor ticks, the major and minor tick labels, the x-axis label, the y-axis label, the markers for the different types of plots. You can have circles or pluses or diamonds, all different things like that. You can have a background grid lines, you can have legend, of course the title of the plot. And so this is a nice figure that describes all of those things that you might want to have control over. And the nice thing about MatplotLive is you do have full control over everything you see, so you can make the figure look exactly how you want it. So let's start with a basic example. It can get any more basic than this. We have to import MatplotLive.pyplot as PLT. This is the standard import. And you know, this is the kind of idiomatic way to do it. You'll always see this and stack overflow and another documentation that this is a standard import from MatplotLive. There are several ways that you can instantiate. So this returns a figure and axes object. As we were talking before, all MatplotLive figures have images have a figure and an axes object. And this is one way to create that object with the PLT subplots command. You can also create it with figure and other things. And you can use subplots even when your intention is to only create a single plot. There would just be one row and one column. And so in that case, I almost always use the subplots command to instantiate the figures. Again, that returns a figure object in an axes object. And in this case, what we're going to do is add a plot to the axes object. So in this case, just a couple of numbers, 0, 1, 2, 3, and this is a Python list. Being input and they're just going to be plotted against the indices in this case. So in this case, you just get a straight line. And we'll set the x-axis to some numbers. So here's a pretty plain plot. Now, it's interesting going on. We'll show on the next slide how we can change the plot style a little bit. So in this case, the only thing we're doing is adding a grid. We do that with the axes grid command. But we could also set the y-label. We could go as far to modify the fonts of the x and y-tick labels. Again, you have full control over the way everything looks. We can also use numpy data. So it's fully compatible with numpy. Map plot live is. So in this example, we'll import numpy. We'll create a range of numbers that goes from 0 to 5 in steps of 0.2. And then we're going to plot basically that range of numbers against itself to create a linear line. And then we're going to plot t squared as a function of t and t cubed as a function t. In this case, we're also going on and adding some labels that will be used in a legend, as well as setting the x-label to t in the special syntax here. The dollar signs actually render the t as a letech math font. So you can look at more documentation on that or have another example later that shows that in a little more detail. We're going to add a grid. We're going to add a legend. And this is the type of plot we get. So the plain plot styles in map plot live are just a white background with kind of, you know, the things you've seen so far. There are many built-in styles. These are, you know, special kind of backgrounds and line styles, thicknesses, other things that are set as defaults. And so here's an example of the same, what's the same example we just saw. However, this time we're going to use the style 538. So if you're familiar with the website 538, Nate Silver's website, he has a very particular style of his plots. And so this style was meant to mimic the style of that website. And so that's why you see the 538 there. My favorite style, xkcd. I'm a big xkcd webcomic fan. And you know, when I first saw this was implemented in map plot live. I just thought it was humorous, but I have actually used it. So this is just a comic type font. But it can be useful to draw figures like this, perhaps when you're presenting presentations and you want to suggest that a trend in the in a data or trend, but in a relationship between two variables. But you want your audience to know that it's not real data, that it's just a hand drawn data or a trend. And so I've actually found some use in using this type of a comic font. Because you rarely want to produce more than one flaw or one plot or many plots with this type of style. In this case, I've used this with plot xkcd. And that will only apply the style to the commands that you find underneath the with statement. So only, you know, this is an indentation block, just like a for loop would have or a Python function. And so in this case, it just executes what's underneath that with the styling and the nexus and returns to the normal styling. If you'd like to look at all the available styles on your computer, you can run a plt.stile.available. And so in this case, these are all the types of styles that are available. And you can basically test them out to see what which one you like the best. We've already seen in previous lecture, we talked about pandas and this built in plotting capability. Of course, the built in plotting capability in pandas is very nice to create quick look plots, but a lot of times the styling is not exactly what you want. So in this example, we just read in a data set and plot the porosity permeability relationship and a scatter plot using the built-in data frame plot command. And we end up with this type of plot. But if we wanted to look nicer, we want to change some of the things with respect to the plot, what we can do is actually instantiate a figure in an axis from that plot log. And then we can actually pass the axis as a keyword argument into the pandas plot command. And once we do that, then the pandas plot command will plot porosity permeability on that axis. And then we get full control. So in this case, I've changed the labels to be the Greek symbols again through their Latak math syntax. So this generates the fee that you see there, this generates the cap of that you see there on the y level. And then I've added a grid as well. So that's just an example there. A little more complicated example in this case, I'm using the same data frame from pandas. And I'm actually creating a real subplot in this case, one with one row and two columns. Then I'm using the data frame built in plotting command for histograms. And I'm creating two histograms, the porosity and the permeability and passing in the axes that I created. Now this subplots because there are two columns, axes is actually a tuple which contains two items. And so when I'm passing in is actually a tuple, and those are plotted against their respective you know data. And then I have the ability to go ahead and make some changes to that. So the default for pandas histogram is to put the title at the top. That's not typically what we do with figures or figure labels. So I set the title to empty to clear that out set the ax labeled a permeability. The x label to permeability, y labels a number of occurrences and then just set the y limit for demonstration purposes to be the maximum of this histogram. Because it's a tuple, I have to access the first axis through you know its indices. So zero there. So all of these commands that are applied to the zero axis are being applied to this first plot here on the left. The next one, the one index, are associated with the permeability and those are being plotted on the one on the right here. And I just noticed that this should actually be porosity. So you see there's a little bit of styling change because of something that's been run later in the notebook but nevertheless making it interactive but nevertheless the label was in fact updated. So this is a nice reference figure. I got to give credit to this website, practical business Python. A guy has a real nice write up on that plot live on that website and he generated this figure with some data, you know, it's basically some stocks or ticker company information versus revenue. And but he labeled then you know, so this is the final plot that was created through a number of commands but then he basically labeled all those commands. So in this case it was created from a data frame called top 10 with using the data frame bar H, you know, kind, this this type of command but he then passes in the axis that he created with the subplots up here. And so in that case then you can make changes which you know he created this vertical line here. He set some excellence added a title to the entire plot other things like that. So I found this really nice kind of summary of the different commands, different things you can do in this case on a bar on a bar chart but a lot of the things apply to scatter plots, line plots, other things like that. So we can do contour plots in map plot live. In this case I'm loading in some reservoir oil and gas reservoir called niche niche lick. I'm loading that in from a data set as a numpy array. In fact, the three numpy arrays containing the x and y positions and the z locations which are the depth. Again, I'm going to create a create a figure and add a contour plot to that and a color bar and so then that's that's how I do it with these commands here. So that's a that's a contour plot. Likewise, I can have a surface plot and this is where the interactivity comes in. So in this case I have to load this map plot live notebook which makes an interactive plot in a Jupyter notebook along with some other imports from map plot live. And then again creating a figure this time adding an axes to it that is a 3d projection. Our z data has some nands in it when we create the contour plot the nands get whited out so they're not they're not displayed but it will cause issues in a surface plot. So we're going to set those to zero just using the standard numpy fancy indexing there and then we'll go ahead and create the plot with the x, y and z data again specifying a color map and and no edge colors and adding the color bar here. And so in this case, you know, this is you can see the different depths of the reservoir and we can actually rotate it around within the notebook here using this interactive surface plot. So there's other plotting libraries for Python. All of these are newer, more modern, then map plot live. All of them can usually create great looking plots and much fewer commands the map plot live but they are somewhat more limited in their overall capabilities. So bokeh is a great, a great plotting tool that's really targeting interactive applications so it gives you the ability to say zoom in on your data and do other things like that. And it's really made for targeting visualizations in Jupyter notebooks or HTML web pages. So you can generate HTML that what bokeh actually generates JavaScript that can be then run in an HTML web page or web browser. All of these is a program that's actually the interface is very similar to bokeh but what it allows you to do is actually choose your back end and you can use either bokeh or map plot live. So map plot live I would often say is better for creating publication quality, you know, print type figures. Whereas bokeh again is more for HTML or web based outputs. And so all of these actually allows you with one set of commands to create figures that can be then chosen to be rendered with either back end, the map plot live back end or the bokeh back end. Plotly is very similar to bokeh, interactivity, web based, other things like that. All Teres is the newest of the group. A lot of people came to Python from a different programming language, possibly R. R has a plotting package called GG plot, which is based on the so-called grammar of graphics. And all Teres is the same. It's very similar. It's based on a Vega, which is also a grammar of visualization. It's a declarative programming language within Python and it has some really nice plots. So there's links there to all of those different plotting libraries and you can go and visit those and see the types of the galleries and different types of plots that each of those can make.
Howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin, and I record all of my lectures and put them online to support my students with evergreen content. Even after the term is done, why not? Come back, keep learning about statistics, and also working professionals. Anyone interested to learn about this? Maybe we'll get some people to join us and STEM. Alright, so let's get started. This lecture was an important gap in my introduction to data analytics and geosatistics course. What I realized is I neglected to include a section on model checking. Now what's very interesting, this is somewhat related to any type of numerical model that we build that could be applied, and so I think this is an extremely important subject for us to cover. Okay, so let's go ahead and get started with model checking. So what's the motivation? Why do we want to cover this? Well, we need to check the performance of our models. And we'll talk about what we mean by performance, how we will, the metric or the criteria by which we will check our models. Bad models will lead to bad decisions, and there are many modeling decisions, many modeling inputs, therefore there's plenty of opportunity for blunders with our models. We make mistakes. So the best practice is to check the final product, check the model to make sure it's sufficient. Okay, so let's talk a little bit about model checking and refine what we mean by it. First of all, we're going to have to describe or define better what is a subsurface model. Now we've been talking about this through the entire course, but let's just make sure we're on the same page. It is a numerical model that integrates all available data sources together. It's bringing it all together. It's informed by statistics that we calculate from the local data and from available analogs will incorporate our geologic or subsurface engineering knowledge, and we'll use those statistics and pose them on the model. The results of the models, many decisions, often the result of a very complicated workflow. And I've worked on many subsurface models, and they often are quite complicated at the end. A suite of models to represent uncertainty in the subsurface. Okay, so that's what we mean by subsurface model. This is important because this will help guide us in what we need to check. Now what are we going to try to check? What do we want to assess with regards to our models? Well, we have all of those inputs to the model, the data and the statistics. So it makes perfect sense that we should check to make sure the model honors the data, the model honors the statistics. I'll give you a very simple example. You had an input distribution. It could have been probably represented by a PDF or CDF for a property of interest. You want to make sure the resulting model actually honors that histogram. It's not systematically bias shifted. Maybe the variance is too narrow and so forth. You need to check your distributions. It was an input you expected as an output in the model. Accurate spatial estimates. This is critical. You want to check the ability of your spatial model to make estimates away from the measured data. You want to make sure it does this over a variety of configurations and with sufficient accuracy. We also want to make sure we have accurate uncertainty models. The uncertainty model at every location, we have a model of the range of possible outcomes, the distribution of possible outcomes. You want to make sure that that's fair given the amount of information available and given all of the various sources of uncertainty. We've got to check that too. We've talked about what the spatial model is and we've now mapped that to ideas about what aspects of the spatial model should we be checking. Let's talk about how we're going to do that. First of all, let's check for the reproduction of the modeling inputs. We put something into the model. Let's make sure it comes out in the model that there is what we call reproduction of those statistics or those statistics are honored. That's the type of language we use in the modeling world. First of all, to be a little bit more specific about modeling inputs, we could look at a model which represents a volume of interest. This is x. This is y. And we recognize the fact that we had measured spatial data. In this case, it would be well logs and core data that's been interpreted. And we have interpreted through a geologic knowledge and maybe some engineering inputs, a trend in the feature of interest. And that's represented by these contours right here suggesting a general transition from high to low or low to high. And we have various statistical inputs that we're using to constrain the model. For the property of interest, this could be porosity. We would have the PDF. We have carefully modeled that, debiased it from available data. And now we impose it on the model. We expect that to be reproduced. The verga model for porosity is shown right here. And perhaps we have a relationship with covariates such as permeability. And so we'd look at a scatter plot between porosity and permeability. And we expect that to be reproduced also. So those are the model inputs that we're dealing with. Now, let's go through those inputs and talk about how we would check them. I want to check the data at the data locations. Now, this might sound trivial. And you might be surprised, but I'll tell you what, subsurface data is very expensive. In fact, many different sources of spatial data is expensive. And the model will immediately lose credibility. If you have a model over a mesh of cells over the volume of interest, and at a location for which you have a measurement, that cell does not match that. Your model will lose credibility and accuracy can really be a problem. Consider what would happen if I'm concerned with flow behavior around that well. And the permeability at that cell location is too high or too low. It will change the rates of production at that location. You will not get accurate estimates. Many geostatico-modeling methods actually enforce data reproduction. In fact, if you go into the codes, what they'll do is it'll do a painting of the data values onto the co-located grid cells. Right off the bat, and you'll ensure that it has reproduction. Now, and I also want to comment on this, and we do not cover this in the course. That is, we really should be considering the well values scaled up to try to account for the fact that you're going between dramatically different scales of over the well bore or the spatial samples to the model cell, which could in fact be pretty typical for it to be 50 meters by 50 meters horizontally by one meter vertically, or a little bit larger than that. And so, because of that scale change, there may be some level of mismatch, and the best practice would be to compare the scaled up well data, scaled up to the model grid cells to the values in the model grid cell. They'll be consistent if you got the model right. So, it's pretty straightforward to check the input statistics against the model statistics. Just empirically, we can go ahead and calculate distributions and verigrams of our models. And I'll give you a very simple example right here. We built five realizations based on the distribution. The black line is the CDF, the input CDF. The red lines are the output distributions for each of the five realizations. And we can either look and see that we have a little bit of fluctuations or changes, but on average, we're in the right place. There's not a systematic bias. In fact, these fluctuations are expected. They're known as ergodic fluctuations. They're statistical fluctuations. And they're actually can be related to the length scale of the spatial continuity versus the size of the model. If you have small continuity or range of continuity relative to model extent, they will dampen quite a bit and vice versa. Okay, so now here's two more examples right here. Overall, pretty good performance could be a little bit concerned with the performance of the CDF right here, but I would really want to calculate many more models to see if maybe I just happen to have five realizations that are a little bit low on the cumulative probability on the lower end of the distribution. Now, the verigrams, not a problem too. We have input verigrams for two different directions, the solid lines. This is isotropic. This is anastropic. And you can see the solid lines or the inputs, the dash lines are the outputs. And you can go ahead and compare just like with the CDF, we'd expect it to be centered, but have a little bit of fluctuation. Overall, pretty good performance could run some more models to really check. These are the ideas of checking the data and checking the input versus model statistics. Now, let's talk about cross validation of the estimates, the spatial estimates. This is really important because we're building spatial models. I want to be able to check the ability of my model to make an estimate away from the data locations. In other words, I have a volume of interest. I have the available sample spatial sample data right here. I want to see if my model is able to make good predictions away from that spate, those spatial samples. What we're going to show you is we'll do this by producing a plot like this, the estimate versus the truth. And we're going to do this using withheld data. Now, this is critical for our assessment of resource in place or contaminant at specific locations or groundwater resources. No matter what it is, it's important to be accurate away from the data. And if we don't do this well, we're likely to make poor decisions with our models. We'll make poor development decisions. We'll put the groundwater wells in the wrong place. We won't do the subsurface contaminant remediation in the right place. We need to get this right. But here's a weird problem. And that is we don't have the data away from the data. I know that sounds axiomatic and a little bit silly, but it's true. We want to test the performance of our model away from the data, but we don't know the truth away from the data. So we're going to have to do something a cross validation method. Now, we'll talk about two different types of cross validation. What we do is we split the data into a train and test subset. They're mutually exclusive exhaustive groups. We don't throw anything away, but we'll take 15 to 30% of the data and we'll declare its test data. We pull it away. We hide it. We don't look at it. Don't peek. Okay. You're not allowed to use it at all. So we got a subset of our data as test the red spatial data right here. Now what we do is we build our model and we calculate the estimates at those locations and we can plot the estimates versus the truth. We bring the truth back. We unlock the vaults. We retrieve it. We've done modeling. We're not allowed to cheat. Okay. And we go ahead and we check it and we see how they relate to each other. Now, we've got to be a little bit careful here. We got to ensure that the difficulty of these estimates is a simulation of the real world use of the model. In other words, it would not be okay to use testing data that are only very close to training data. That's too easy. That's way too easy. And likewise, I don't want to use testing data that are just really far away from the training data. That would be too hard. Unless that's the intended use of the model is always to make estimates very far away, then probably you'd want to do that. We're simulating the use of the model. Okay. Now, that's the first method. The second approach is known as K-fold cross validation. We're going to have training and testing. But you know, it's funny. If you think about it, you didn't get a chance to test with all of the data. You only the 15 to 30 percent. So what you do a K-fold cross validation, you pick a K. In this case, it's going to be three. You divide the data up into three folds. You'll see the color of the dots, green, red and yellow. Here's a green, the reds and the yellows are the three folds. Then what I do is I'm going to loop. I'm going to go ahead and I'll say, take the first fold as test, build the model. Now, see how we performed at those first fold test locations. Okay. Start all over again. We're going to build three separate models, one model for every K-fold. And we're going to take the second fold as test, build the model. And then we'll do the third fold and we'll repeat. Now, to be a fair test, the test data cannot inform any part of the model, the verigram, the distributions, the trends, none of it. You really have to have amnesia to do this right. You have to say, I built the first model with the first fold as test. Now, I forgot what that looked like. I forgot everything I learned and I start all over. Otherwise, you'll be cheating. You'll be using all of the data for all of the models and that will make it too easy. This does require K models to be calculated, one for every one of the folds, but it is more robust because you get to test at all of the data locations. And so you'll find that you'll get a more reliable assessment as long as you don't do any cheating with this. Okay. Can't use data to help. That's in the test set every time you build. Okay. So how do we summarize accuracy as we're doing this? Because we already made the scatterplotten, the scatterplot was great estimate versus truth. That's pretty cool. We got to go beyond the scatterplot. And so what we'll do is we'll calculate the error. Now the truth is the truth. That's what we should be estimating. We should have been on the line, but instead we estimate it here. And so the error in the estimate is this horizontal segment right here. We'll call that the delta of the property at the locations u1 through 5. Okay. Now a really common measurement that's used to summarize error would be mean squared error. And for that, all we have to do is take the average of the square of all of these delta. If you do that, you're good. That's a good summary measurement that we can use. What are some interpretation cues that we can take from these cross-validation scatterplots between estimate and truth? I think it's good. I'm going to show you three pathological cases and see if you can follow along as far as interpreting them. The first example I'll give you is called conditional bias. What that means is you systematically overestimate the lows and you systematically underestimate the highs. Another way to think about it is your estimation model has two little variance. And so this slope is in fact inflated. You're not on the 45 degree line. This commonly happens when we underfit our models. Truncation. This is another common problem. And usually it's due to a mistake with the algorithm, a blunder with the data and so forth, where what's actually happening here is there's an artificial truncation in our estimates. All of the estimates that should have been in this area are collapsed onto this line right here. We should check for truncation. That's a pretty severe issue with our model. Systematic bias. What we see here is that the values we estimate are systematically too high. Now it's always possible we could be in this situation when they're systematically too low. We want to be right on the 45 degree line. So we don't have a systematic bias nor a conditional bias related to higher low values. This is really animated. Now, funny. Okay. So these are some interpretation cues. Don't throw away the cross validation plots. There's information there. Look at the distributions of error. Look at the mean squared error. But also look at these plots right here. Okay. So let's talk about how we check the entire uncertainty model. This is something most people think elect to do. And I think it's very powerful. So geostatistics, a branch of statistics, is all about building uncertainty models, which means that the unknown location we get more than just an estimate. We get the entire uncertainty distribution. They would make sense for us to check the entire distribution to make sure that's correct. Okay. Not just the single estimate from the distribution. That's really kind of throwing away a lot of good information. We need to determine if the uncertainty model performs well. Is it a fair uncertainty model? It's wide enough. It's not too narrow and so forth. We're going to use a modified form of cross validation. The original paper on it is actually actually available from Deutsch, my PhD advisor from 1996. And we have a lot of details around it in the book, the textbook we worked on together, along with other sources. What can go wrong with an uncertainty model? So let's abstract a little bit and look at some distributions and look at some data and think about what could go wrong. So firstly, we could have too low of uncertainty. In other words, the distribution of uncertainty we made looks like this, but when we take it to a wide range of special locations, which have that same distribution of uncertainty, the true values are just always outside. This is the case where you fool yourself into thinking you know more than you actually do. And this commonly occurs. We omit it important on certain resources. What we have here is the data are like this distributed with a mode probably here, but our uncertainty distribution that we use and what compared to all of the data for which we had that similar uncertainty model, the true data followed side. This is bias. The two, the result is we just have too many true values that are outside of our confidence intervals of this distribution. Our uncertainty model is just wrong. The third thing that can happen is you have too high uncertainty, too many truth values inside the confidence intervals. This is interesting. You might think that's a good thing, but imagine your boss asked you to make an estimate and you say like for instance porosity and you say to your boss, it could be anywhere between 0% and 100%. I don't know. Is that a useful uncertainty model? It's too wide. We can't make uncertainty driven decisions based on something that's just way too cautious. You have to be conservative and careful, but at the same time you can't inflate uncertainty to cover your basis. That's not a useful model. Okay, so what do we want to do? We want to build what we call an accuracy plot named by the original publication. It's a method to cross validate uncertainty. Now the approach is this. We're going to withhold testing data and estimate the uncertainty distributions at the testing data locations. So far so good. This is basically the same as cross validation for estimates, but we're considering the entire uncertainty distribution. Now we're going to take the true values, the data values at those locations with the uncertainty distribution and calculate the cumulative probability value. We'll find out if it's a p 50, a p 10, a p 13, or whatever it might be. Then for a set of symmetric probability intervals, i.e. the p 10 to the p 90, the p 20 to the p 80, and so forth, we'll go ahead and calculate the proportion of testing data from their cumulative probability values that lie with the nendientiable. For example, if your data had a cumulative probability value of 64%, it would be within the 40% probability interval going all the way from 30% all the way up to 70%. And so forth. Then what we're going to do is we're going to plot the proportion of data in the interval versus the probability interval. I'll go ahead and show this to you for a single confidence interval. In the case where we have only five testing data. Now, in reality, we would have many more testing data. We get better resolution on the result, but I'll show you this trivial example by hand. Okay, so what we have here is we're going to have five testing data, five different locations. We're going to estimate the uncertainty distributions and we're going to calculate the result for the accuracy plot for the 80% confidence interval. I've shaded 10%, 10%, 80% confidence interval in the middle, 80%, and so forth. And I took the true values at those locations with held testing data. I put it back in calculated it's cumulative probability. In this case, we're working just with the PDF. So I just plotted the data value. And so we can see the true values here. Now what we do is we're going to go ahead and we're going to look how many times do the true values fall within the interval? One, two, nope, that's out in three, four, four out of five times we fell within the interval. Okay, so now what we can do is we can plot the proportion of data in the interval 80% and the confidence interval, the probability in that interval was 80%. So the result is going to be this circle right here. Okay, so we just did it for one confidence interval. We could repeat that over and over again. What does that look like? We simply just have to label the other intervals. In this case, we only have five data. So we really can only calculate the 20% the 40% the 60% the 80% so we'll go ahead and do that. So I labeled those. You can see 20, 40, 60, 80% and I went through check it. See if I got it right. I hope I did. And I calculated the number of times or just counted the number of times. The true data was in each one of these intervals. I got 80% 80% 60% were in the 40% interval and 20% were in the 20% interval. Okay, so what can we see here? We can start to make some interpretations. Clearly, we want our points to be on the line. It makes logical sense that for the 80% probability interval, confidence interval, I would expect to see 80% the data in that interval if my uncertainty model was good. Okay, was fair. And so we can see here that we had too many data within the confidence interval here. So let's go ahead now. We can make some interpretations. If we have points above the line like this on our accuracy plot checking our uncertainty model over over all of the testing data, that indicates that we're accurate, but we're in precise. The uncertainty is too broad. We probably don't have a bias in the model, but our uncertainty is too wide. So that's in precise. That's we want to have something that's precise and accurate. How do we get that? Well, I'll tell you right away. If we're on the 45 degree line like that first example, we calculated, then we're accurate and precise. That's ideal. That's what we want to do. If we're below the 45 degree line, we're inaccurate. We're in precise. The uncertainty is too narrow. We're fooling ourselves or our models just biased. We're shifted. Okay, and we don't want that to happen either. So these are the interpretation cues that we can use with these accuracy plots. A very powerful methodology to be able to test or check the model, the uncertainty model for our spatial model. Okay, so we covered just now model checking. We talked about the checking the model inputs. We talked about cross validation for the accuracy of the estimates and cross validation for the accuracy or goodness of our uncertainty model. I hope this was a useful discussion for you. I'm Michael Perch. I'm the associate professor at the University of Texas at Austin. And I teach and conduct research on spatial data and lyrics, use statistics, machine learning. If you're interested in collaborating, if you're interested in working together on a research project, if you're interested in going to grad school, so forth, go ahead and contact me. I'm always happy to discuss anyway. Everyone, thank you very much for tuning in. Stay safe.
All right, so in this lecture we're going to talk about NumPy. NumPy is an external or third party library, meaning it's not part of the standard Python installation. It can be installed in various number of ways. All of them are pretty straightforward and easy, but nevertheless, it's a third party library for doing numerical computations. That is, you know, array operations or operations on arrays of numbers. You might have heard that Python is slow and part of the reason that Python in its, you know, pure form, pure implementation, pure Python, not using a library like NumPy. Slow is because of something called quickly called duck typing. So if it looks like a duck, then it is a duck. That means, you know, if a variable looks like an integer or a float or a complex number, then the runtime engine when Python executes the code, it treats it as such what it looks like. And this causes a lot of overhead at runtime, trying to determine, you know, what type of a variable is. And the solution for that is to use so-called NumPy data structures. So these are data structures as objects, right? We know a little bit about object oriented programming. So these are data structures as objects that have a single type and contiguous storage. So contiguous storage, meaning all the data lays next to each other in memory. And this can be important for minimizing cache misses and other low level performance issues. It's implemented in C. And if you're careful, if you're careful in the use of NumPy, you can actually expect near C-like speed in performing computations on the entirety of an array. So let's look at an example of, you know, kind of how slow Python is or a comparison to NumPy. And to do that, we're going to use the built-in time function from ipython that's built into Jupyter notebooks when you're using a standard ipython kernel. So what we're going to do here is we're going to use a Python list comprehension to add one to a million numbers. So this range function, of course, will give us zero, one, two, three, four, five, all the way up to a million. And we're going to use this list comprehension to add one to each of those numbers in succession and store the result in a list. And so if we do that on my computer here, you'll see that it winds up with 131 milliseconds. So if we do the same operation in NumPy, by the way, we import NumPy as, you know, the standard way to import NumPy as to import NumPy as MP. And then we use the MP namespace to create an array of numbers that go to a million. And then we just simply add one to those and NumPy is smart enough to cast this plus one operation across that array of numbers. And so they perform, if you were to view these, the output of these arrays, you'd get identical answer. And in this case, you can see it's something like a hundred times faster to do it in NumPy. That is again on my computer. The real key here is not the absolute speed, but the relative difference between the two numbers, right? This NumPy implementation is much faster. So NumPy also gives us something called universal functions. These are vectorized functions that operate on arrays in an element by an element fashion. So if we wanted to say take the sine squared of each number, again, up to a million, to do that in Python, we have to basically put it in a loop or a list comprehension like we've done here. And we individually take the sine of each number and then store those values as we go over the list. We can however do this in NumPy like so. So we have a function sine that's defined for NumPy arrays. When we stick an array, in this case, again, zero to a million integers in this array, it will take the sine of each element individually in an element-wise fashion. Again, also squaring those numbers here. So again, the result of these two operations would be identical, but the NumPy version is much faster. There are many ways to create arrays in NumPy. We can start with a Python list. If that list is initially given as a list of integers, then the resulting NumPy array will also have the type integer. If we were to basically use the decimal point in any of those numbers, then we would get an array of doubles. Or if we were to use the special syntax for complex numbers, we would get an array of complex numbers. So in this second example, it is an array of complex numbers. We can also create a range of numbers. So in this case, it would go from minus 10 to 10 and steps of 2. And even though we use what look like integers as inputs here, we can be specific about the data type. And a lot of times I'll do this for readability, it's be very specific about the data type. So in this case, the data type is a float, which is a single precision 32-bit floating point number. We can create a linear space. So in this case, it would be a set of doubles, floating point doubles, that go from 1 to 4, and there would be six of them equally spaced in between. And we can also read data from a file, like so. So there are other ways to create arrays, but these are the most common ways to create them from Python lists or specify an A-range or linear space. So NumPybe has an array of many utilities for getting information about arrays, slicing other things. In fact, the slicing notation in pure Python actually was borrowed later on from the NumPy slicing notation. So if we start here, we can create an array. This would be just a list of integers from 0 to 9. You can call this function reshape on it, which will take the one-dimensional array that was originally created, and it will reshape it into a 3 by 3. So it looks like a matrix in this point. And you can see the number 0 to 8 there. If we wanted to get a part of that array, we could do it with this type of notation. So the colon means take all, in this case, rows, the first, all of the first dimension. And so this means take all of the rows in the 0th column or the first column. So in that case, 0, 3, 6, that would return that part of the array there. Again, NumPy arrays are objects once they're instantiated, so they have attributes. In this case, one of the attributes is shape. And when we call reshape on it, we're actually changing that attribute. So the array or the object is stored in X, and it has an attribute shape, which tells us it's a 3 by 3. We can reassign parts of arrays using this kind of notation. So the syntax here is, again, along the first dimension of X, go from the beginning to the end in steps of 2, and along the second dimension of X, go from the beginning to the end in steps of 2. So that's what we do here. We'll say in steps of 2, we'll take 0, 2, and in steps of 2 along the rows, we take 0 and 6, and then 6 and 8 along the columns. And so that gives us why there, this little piece of our original X. And we can actually use this to reassign parts of the array. So in this case, we want to reassign the 0, 0 entry to 100. We would have this type of output. Now here's something that you might not expect, is if we look back at the original X, so remember why was assigned to a piece of X. And when we changed Y, we also changed X. That's because X or Y, rather, is just a view into X. It was not a complete copy and memory. It's just a special view into that part of X, which we assigned here. So any changes to Y will also change X when done this way. And this is just real, you have to be careful to know about this functionality because it could catch you if you expect that Y is a copy and that you want to leave X unchanged. If you want to actually do that, what you should do is make what's called a deep copy. So in this case, we're explicitly going to call the copy method after making the reassignment to the part. And in that case, then, if we reassigned part of Y and then look at what X and Y are, we see that X was remained unchanged as we made this assignment to the 0's or entry of Y equal to 200. Another thing we can do is very efficient and compact finite differences. So from the definition of a derivative, so say we have a function F and we want to know how it changes with respect to X, then we can approximate this from the limit, say, as delta X goes to 0, I'm just essentially rewriting the definition of a derivative. Like so, of course, the definition of a derivative takes this limit here. However, in the discrete case, we can't actually take delta X all the way to 0, although we can approximate it to be very small. So this is an approximation of the derivative. And so what we can do with this is if we take, say, for discrete data where this might be XI, a point XI, and we'll call this XI plus a half, where delta XI plus a half is simply X. I'm sorry. Simply X at I plus 1 minus X at I. And if we imagine we have some function that's smooth, but we discretize it at individual points, and then we give these points a label, say, call this one XI and this one XI plus 1, then approximation to the derivative here, this line, could be taken with this formula here by evaluating the function at XI plus 1. Well, what we can do is we'll call this distance delta X, right? So this is XI plus 1 is the same thing as saying F at XI plus delta X. So we'll evaluate the function at this point, evaluate, subtract the evaluation at that point, divide by delta X, and that gives us approximation to the slope of this line. Of course, the approximation becomes more accurate as delta X is smaller. We can do this in NumPy very efficiently in a compact way with this type of operation. So in this case, we have an array X, that's just a discrete set of numbers that go from 0 to 40 in increments of 2, and then we'll square that to get Y. So we want to compute the derivative of Y with respect to X. What we'll do here is using this syntax, this computes all of the changes in Y across the whole array. So starting from the second element of Y to the N and subtracting from the beginning of Y to 1 from the N, that gives us our delta Y and same thing for our delta X in the denominator here. And so then the resulting approximation to the derivative of the curve is that. And if we plot this then, you can see, so our original Y of X is this black line, and if we compute the derivative, these numbers right here are actually plotted against X in this red line here. And of course, we know that the slope or the derivative of X squared is 2X. And if we were to evaluate the slope of this red line, we would see that it is in fact 2. We can do more sophisticated so-called broadcasting. So earlier we saw how we could broadcast the sine function or broadcast the operation of adding 1 across an entire array. We can actually do complex broadcasting against multiple dimensions in an array. So here we're going to start with three dimensions. We'll call red, blue, and green. Each of these are two by two arrays. I'm sorry, each of these are two dimensional arrays, 800 by 600 in each of those. We're going to use those three arrays to create another dimension of an array. We'll call RGB. And so if we look at the shape of RGB, it is 3 by 800 by 600. We can then transpose that array to make it 800 by 600 by 3. And so the outer dimension is then 3 and we can multiply by a 1 by 3. So in this case, we'll multiply by a half, a half, and a quarter. And that multiplication is broadcast against the whole array. So all of these individual, the red is multiplied by half. The blue is multiplied by a half and the green is multiplied by a quarter. Here's the resulting array, which is hard to visualize. But an easier way would be to actually look at the images that are created. If we consider those red, green, and blue values to be color values of a pixel, our original image looks something like this and our scaled image looks something like that. We can see that it's more green because we had more influence from the green component coming in in the scaling. We can also do something called fancy indexing. So with fancy indexing, we'll start with this original array X. We call like this. And we've already seen that if we wanted to take the component of X, the third index is 0, 1, 2, 3, we would call it like this and return the value there. This is no different than a Python list, of course. But what we can do with a numpy array that we can't do with a Python list is we can do this so-called fancy indexing. So if we wanted to take various numbers from the array given in a list, so the 0th component, the 3 component, and the minus 1, which indicates 1 from the end of the array, then we can do it in this way. So again, we get just the 0, the 3, and the last one, which is 8. So this is called fancy indexing. If we notice, we cannot do this with a pure Python list. So a Python list, if we just created here, 1, 3, 5, 7, we don't wrap it in numpy array or anything like that. We have a pure Python list. If we tried to take just the second and third components of that list, then we would get an error. This fancy indexing works for multiple dimension arrays. So if I reshape X into a 3 by 3, set that equal to Y, and then take the 0th component, that would mean to take all, that would take the 0th row, so the entire row here, 0, 1, 2, and that's what we get. And we can also do it in multiple dimensions, so we can take the first and second row, so the 1 index and 2 index row, which is the last 2. And we can also even get fancy than that and take the 0, 1, 2 component, or I'm sorry, the way you would read this would be the 0, 0 component, the 1, 1 component, and the 2, 0 component. And that's what we get here. So right, 0 is the 0, 0 component, 4 is the 1, 1 component, and 2 and 6 is the 2, 0 component. So that's how we read that kind of thing. You can also have Boolean arrays, these are very convenient, so we'll do, just start with the group of random integers, and then we can actually set a condition to say that wherever x is greater than 0, then return the element-wise conditional statement. So wherever x is greater than 0, that is wherever x is a positive number, it will return true, otherwise it will return false. And so you see that, you can compare, 1 is positive, 2, 6 is positive, 3 is positive, 2 is positive, 2 minus 4 is negative, false, right, and so on. Then we can take this index array, which we've, I've set to the value index there, I can take that IND array and use it to actually fancy index back into x to extract the actual numbers. So if I wanted to get the positive numbers from the array, then I could do this kind of operation. So we use this often in cleaning data and other things, or truncating data. It's also a convenient function called where, which basically allows you to say, where true, so where index the array is true, then in that case I want to return those components of the original array x, and otherwise I'm just going to return string. This is kind of a kind of a trived example, and you'll see, because the data types in NumPy have to be contiguous, it converts the whole entire output array to a string, so these 1, 6, 3, 2, those are actual strings, not numbers anymore. But nevertheless, it just shows the kind of things you can do. So again, the syntax here is where IND is true, return those components of x, otherwise return something else. And you could change this to say, return 0 in those cases. And that's, you get a different answer, right? So in this case, where true you'd get the numbers, and otherwise you'd get 0. So the second argument is kind of like the otherwise in an else statement or else if statement. So that's a brief overview of NumPy.
In this lecture, we're going to talk about pandas. Pandas like NumPy, which we've already talked about, is an external library that we must import into Python. And it is really the primary external library that people use for transforming data. We'll talk about some of the ways we can transform data and other things we can do with pandas in this lecture. Just like it's idiomatic to import NumPy as, you know, MP. Similarly, you'll see this all over the documentation everywhere, stack overflow in the web that it's often, or it is the case where it's idiomatic to import pandas as PD. So once we've done that, we can read in some data. In this case, we have a data file that is a comma separated value file. This is a common format for storing data. And we read that in the pandas with read CSV. And this read CSV command is very powerful and maybe in and of itself enough reason to use pandas in some applications because it has very sophisticated ability to ignore certain headers and footers and parse dates. And just in general, parse data from a file in a very efficient and clean way. Once we read that in, it's stored as a so-called data frame object. So DF is an object. A data frame object that is part of pandas. And one of the member functions that is defined on data frame objects is head. My default head will print out the first five values, the first five rows of a data frame. So it kind of gives you a preview. Turns out this data set actually has 200 rows. And we'll rarely look at all of them because it's just difficult to visualize on the slide. So you'll see this often as head. Head does accept an argument. The default is to display five rows, but you could give it an argument if you wanted to display 10 or 15 or 20 more of the data set. So in this data set, we have X and Y. These are geographic spatial locations. A facey threshold. They're just either an integer one or two. It corresponds to either a sandstone or some type of mudstone. Parosity values, permeability values, and acoustic impedance. So this is the data set that we're reading in. Another member function of the data frame object is describe. So we can call describe on a data frame. And it will automatically generate some useful summary statistics. For example, you know, the count is simply the number of values for each of these labels. The mean is the average value, standard deviation, min max and different quartiles. So this is useful to get a quick look of a summary of what's in the data frame. We can take parts of the data frame. We're familiar with slicing type operations in NumPy. And in some cases, these indexing and selecting operations correspond to those as we'll see. It's often the case that we want just one column. If we want to do that, we can refer to it by its label. Again, so in this case, I'm just pulling off the porosity column and printing out the first five rows. You can pull out multiple columns from the data frame by passing it a list. So in this case, the porosity and permeability. You'll notice over here on the side, there's this monotonically increasing set of index. These are just the labels of the rows in PAN as we call those index. A lot of times, they're just monotonically increasing integers like this. However, in time series data, it's often the timestamp over here as the index. We can do different types of label-based selection. So we can use the member function LOPE in this way to select not only the column, but the row number as well. So in this case, we're going to take the 1 and 2 index. So in this case, the index serves as the label. So we're going to take the 1 and 2 index row and take the porosity and permeability columns. LOPE also supports numpy-style slicing notation to select sequences of labels. So again, we'll take from 1 to 2 the indices and also from porosity to acoustic impedance, in the data frame. So if we go back and look at the original data frame, you'll see that in the order that they were originally, it was porosity, permeability, and acoustic impedance. So we can use this numpy-style slicing to get everything in between. We can also get index-based selection. So we have ILOPE for that. In this case, we're going to refer not only to the index, the row by its index, but also the column by its index. So again, if you go back and you were to look at the original data frame, you'd see porosity and permeability are the three and four indexed from zero columns of the data frame. And so that's just another way to get information from the data frame. Often the reason we use pandas is because we want to refer to the data from its label. So this is less useful except perhaps if you're maybe looping over and you want to loop over the integer columns instead of the labels. But often, we prefer to use pandas simply because we can refer to the columns via their label, which makes our code more readable. If we want to do an operation with porosity, it's nice that we can refer to it by its label instead of the three column. ILOPE supports the Python list style slicing. We can get data, the underlying data as a numpy array. So for example, if we wanted to sort of ignore the index and just have it return an NDE array that is a numpy array, we can say the data frame porosity and then we use the attribute values. So that will ignore the index and just return the underlying data in the data frame. Of course, this is a numpy NDE array and we can then use any kind of standard numpy operation on it. We're going to look at some other types of transformations of a data frame. For example, the original columns as they were in the file, the column labels were kind of long. Faces threshold 0.3 permeability could be impedance. If we wanted to create shorter labels so that we could refer to them any more compact way, we can use the data frame rename command passing at the argument columns as a dictionary. So the syntax here is the original column name and the new column name that you want. We also use the in place command or argument here, which in this case will transform the data frame in place, meaning it doesn't return a new data frame. It actually changes the data frame in memory in place without returning a different data frame. We can add a new column to a data frame. You'll notice that data frames often use syntax is very similar to Python dictionary. So with the Python dictionary, we, if we wanted to take, get the values or set a new value keyword pair into the dictionary, we would use this type of syntax where this is the keyword that would go into the dictionary and then what it's equal to would be the value. So very similar here in pandas, we want to set a new column with the label 0. And in this case, the column is just going to be a numpy array of zeros. That is the original link to the data frame, 200 in this case. And then we're just printing out the first five rows. So you see over here, we've added a new column to the data frame. We can also then remove columns. In this case, we're going to just remove the column that we just placed there. We're going to do that with the drop command. We give it a label. In this case, if we want to drop a column, we do have to specify the axis because the default is zero. And that would be a row. In this case, we want to drop the column. So we'll say axis equal to one. And again, we want to do it in place, meaning the zero's column will be permanently removed from the data frame in memory. And again, we're just printing out the first five rows with the head command. We can also remove rows with the drop command. In this case, the axis is defaulted to zero, which implies a row. So in this case, if we wanted to drop the one index row, we could do it like this. And then we're printing out the first five rows. And if you notice the index here, the one row is missing. So we've removed it. Also, notice in pandas, you can stack these member function commands. So in this case, we're dropping and then immediately printing out the data frame of the first five rows. We can sort data frames. So in this case, we start to recognize that a lot of these operations are similar to the things that you would do in Excel. So we have this kind of two-dimensional tabular style data. And we want to sort it. And in this case, we're going to sort it based on porosity in the descending order. And again, do it in place, which will change the order of the data frame in memory. In this case, I'm printing off the first 13 rows. And what you'll see is that the original indices are all jumbled up now. We can reset those indices with the reset index member function. Again, in this case, in memory, in place. And so you'll see where this just resets the indices to be monotonically increasing. And again, we're printing off the first 13. The field of data science feature engineering is a term that's used to transform existing features into new features that might be more useful for analysis. And so in this case, we're going to create a couple of new columns in the data frame. One of them is going to be just a conversion from the fractional porosity as it was originally to a percentage. So in this case, we're going to have a new column that's called percent porosity. And that's just the original column porosity times 100, of course. We're also going to have another column called PURM-POUR ratio, which is the permeability column of the data frame divided by the porosity column of the data frame. Notice that we can do these mathematical operations directly on the data frame columns. We don't have to extract the values of the numpy array. These type of things will work directly. So again, printing off the first four five rows of the result. You'll see that, in fact, the porosity has been converted to a percent. And this is the ratio between the two columns of permeability and porosity. You can do other things, just more feature engineering here, and using some numpy commands. So we're going to create another column that we'll call porosity type. This is either going to be valued as a string high or low. And we're going to use the numpyware command to determine that. So basically, if you remember the syntax for numpyware, the first one is a Boolean array. The second one is the value if true, and the third argument is the value if false. So in this case, what we're saying is if the value of porosity in the data frame is greater than 0.12, we're going to give it the label high. Otherwise, we'll give it the label low. And then I'm just using some numpy style indexing, fancy indexing, to get a few of the values that are high. In this case, it's just the first three values of the data frame we're printing out. You can see all of those are high. And then I'm also just wanted to grab some of the low ones. In this case, you see you have to go to index 165 before you any of the low ones appear. Just you can see that these are the other index. These are a different location in the data frame. So a similar operation, but instead of just assigning a higher low value, what we'll do here is say, where the porosity is greater than 0.12, we're just going to pass the permeability through. And if it's less than that, we're going to just set it to a very small number, 0.001. We'll call this permeability cutoff and add a new column to the data frame. And again, what I'm doing here is just showing you, in some case, the first three where the porosity value was high, the permeability is simply passed through. And the first three where the porosity value was low, we cut off the permeability to a very small number. Again, this is just an example. It's very often in data that we have missing or invalid data. And so pandas has quite a few built-in features for dealing with missing data. In this case, the first thing, our original data frame did not have any missing data. So let's add a missing data in there. So we'll use the add command, which is similar to low in a way, but is only used for accessing a single value, and it's very fast. So in this case, we're going to take the one index of porosity and set it to none, which, and then if we print off the first three rows of the data frame, you'll see that there is actually a nan there, not a number appearing in that location. So we explicitly put it there in a location that we know, but it's often the case that when we have data or we read it in, that it's unknown. The is null member function will return a boolean array that identifies rows that are missing. So in this case, I'm going to again use the numpy style fancy indexing to basically find the rows in the total data frame. Of course, in this case, there's only one because we explicitly set it, but here we're printing it out, and in fact, you can see it's the one index row that this is returned. So we're using fancy indexing. This df porosity is null returns a boolean array, right? Truser falses true where we have missing data. And then we're just printing it out with this fancy indexing command. We can drop missing data from a data frame with the drop built in drop in a command. So in this case, we have that data frame that had the missing data in one index, and we can just simply call drop in a again, we'll use in place equal to true, which will cause the data frame to be changed in place. And it doesn't, and this function will not return a new data frame. Yeah, we can then again print out the first three rows. If you recall, it was the one index where we had a porosity value that was missing, and now you see that it has been dropped from the data frame. We could re-index this. We've seen how to do this. Up to now, we've been dealing with data that was read in from a file. We can also create new data frames from scratch. So the first thing we'll do is use numpy to create a list of an array of random porosity values that go from zero to point two. And then we'll use the Cosini-Carmen relationship to convert those porosity values into permeability. So we have this nonlinear relationship between porosity, and we'll use that to give us permeability values. So as of right now, porosity and permeability are just numpy arrays. If we want to stick them into a data frame or create a data frame from scratch from them, first thing we can do is create a dictionary. This is just a regular Python dictionary, where the keywords are going to be your desired labels in your data frame, and the values are the data, in this case, the numpy arrays. Then we'll pass this dictionary as an argument to PD data frame to create the data frame. So in this case, we have a data frame, DF, new, that we create on the first line. And then we just do some sorting, because later we're going to plot this. And so we want them to be monotonically increasing. So, and because they were generated randomly, there's no guarantee that that would be the case. So we're going to sort them based on porosity in place, and then go ahead and re-index it with this ignore index equal to true right now. And then print out the first five rows. We can also merge data frames. So in this case, we're going to take two selections from our original data frame with ILO. And then we're going to, and so we're going to call those DF1 and DF2. Again, it's just the first three rows of the data frame, and from index two on to the end for the columns for DF1 and DF2 is similar, except now we're just taking the rows 10 to 13. So now we have two data frames, and we can then concatenate them with this command. So, pandas PD can concatenate the data frames in a list, and the access, which we want to concatenate it on, right? And this is an access equal to zero refers to the rows. So here, I left the indices in there so that you can see zero, two, three. There's our first three, and now the 11, 12, 13. So you can see that what the data frame displayed in the bottom is just simply the two data frames above it stacked together on top of one another concatenated along the rows. Another nice feature of pandas is the built-in plotting. So if we just call plot on a data frame, it will plot all of the columns against their indices, right? And so in this case, this is what we have, right? If you recall, our original porosity values were randomly generated, and then we had a nonlinear relationship between them for permeability. However, it's not really useful here to plot against their indices, though, that's just a monotonically increasing set of numbers. There's a couple of things we can do to get the plot to be more useful. One of them would be to set the index to porosity. So if we set the index to porosity of the data frame, and then call plot on it, now we have, you know, porosity on the x-axis and permeability is the y-axis displayed in the label here, and you have this relationship. We can also just do this more explicitly from the original data frame. So if we take the original data frame, in this case, I'm calling a scatter plot, so plot dot scatter. And then I'm explicitly giving it x and y-axis labels. And so in this case, a scatter plot, because the default would be a line, though, the dots would be connected. In this case, we're just plotting the individual data points as dots, circles. And we're going to set the x-axis to porosity and the y-axis to permeability to get this nonlinear relationship. Finally, you know, after we've done some transformations on a data frame, it might be useful to write that to a file so that we could then import it into Excel or send it to someone else for further manipulation. And the data frames have this built-in two CSV command, which can then you just give it a file name, in this case, 200 wells out. And we're going to remove the index from the data frame, because again, those kind of monotonically increasing integers aren't that useful. In this case, we wrote to a file that's on my hard disk here, and I can use the Unix head command to actually look at the first five files. So we don't get the nice display that we get in a pandas data frame, but it is essentially the same thing. The first row here are the labels, and then all of the values below it in commerce separated value form. There are many other things you can do with pandas. The documentation is really great, just lots of examples online. For example, you could have some complicated function that you could then map across or apply to every value, every row for a label in a data frame, which would give you very much Excel-like features, but far more powerful because you have the full power of Python and NumPy to do the manipulations.
Hey, howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin and I record all my lectures. And this is the lecture getting us into probability, part of the second main unit within my introduction to data analytics and geostaticist course. So let's get into probability. We'll talk about very quickly, just kind of fundamentally about what probability is. And then we'll switch into frequentist probability. We'll have a bunch of vendiograms. We'll talk about probability logic. And we will then lead ourselves derive our way to Bayesian probability and show how we can use Bayesian probability formulations to solve unique problems. So let's get into it. What's the motivation? Why do we want to cover probability when talking about data analytics? Well, first of all, we need probability to support decision making. Data analytics and statistics are all about supporting decision making. And we need probability when we're supporting decision making in the presence of uncertainty. It's essential to quantify probability. It motivates decisions. What's the probability that we'll have a successful well, be it hydrocarbons or water or whatever it might be, would help inform the decision of is it worth it to drill the well? What's the probability that a valve has a crack in it? You've done some type of scan on it, x-ray type of imaging or such. And you say it looks like it has this probability of having a defect or a crack or some damage to it. You can use that to determine whether or not you should replace the valve. What's the probability that a seismic survey will find at Reswar? How good is it? How accurate is it? What have we found historically? Well, that'll help us defend if you think about it. The whole area of decision analysis is all about trying to find the value of that new information and to balance that with the cost of that information and we'll determine whether to acquire the seismic. What's the probability of Reswar seal will fail? Well, if you're doing carbon dioxide sequestration, CO2 sequestration, that's essential. We don't want to inject into the subsurface only to lose containment later on. We need to guarantee those volumes remain in place for extended periods of time. Most of our decisions in the subsurface and I would say many spatial settings require us to quantify probability because of uncertainty. And that helps us make better decisions. By communicating uncertainty, our work is used directly in the decision. And as I always tell my students, if your work contributes directly to the student, you have great line of sight to value and your work will be seen as valuable and you'll be rewarded for it. So it's a win-win for everybody. Now, with that motivation, let's get into probability and talk about probabilities together. So let's start. What is probability? How do we work our way to it? Now, what we can do is we can go back and look at some of the work of the past. And Komagorov gave us three axioms around this concept of probability. He said that it's going to be a metric or measure that follows these three axioms. Probability of an event is a non-negative number. There is no such thing as negative probability. It wouldn't make any sense. So the probability of an event a and a could be any event at all is going to be greater than or equal to zero. That's the first axiom. So far so good, right? Not too bad. Probability of the entire sample space are all possibilities, which we will always represent as omega, which is all possible outcomes must be equal to one. If you take account of all possible things that can occur and nothing outside of that can occur, then the probability of that happening is one. It's 100 percent that one of those things must happen. So probability of omega, the complete set is unity or 100 percent. And then add activity of mutually exclusive events or unions. In other words, what is the probability of the union of events a subscript i where i goes from one to the total number of different events that can occur. And what do we know? Well, if they're mutually exclusive events, then we know that we can just sum the probabilities over those possible i through whatever number of possible events. So in other words, if we have only two possible events a one and a two, then we can solve for the union, a one or a two as simple as summation of the probability of both of them. So these are fundamental axioms that any measure of probability we come up with should follow these and then they'll be valid. So this is this seems pretty fair. Now, let's think about how we can actually come up with the measure. What are the different fundamental ways that we can go ahead and calculate this representation of probability? Well, let's back up and look at four different ways that we can think about probability. The first one is we can think of probability as being a long term frequency. Now, we'll introduce this in the next lecture as being the frequentest perspective. And really what we say is the frequentest simply run an experiment and count the outcomes. They take the ratio of the total number times that a event occurred over the total number of experiments that were run and they say probability is that ratio. Now what's very interesting is this approach requires repeated observations of an experiment. Now, if we get deep into the details of this, you'll find in fact that in the purest form, frequentest probability doesn't actually exist outside of the experiment. Only have that probability when you run the experiment, only for the samples you ran the experiment on. And that's that's kind of the kind of the most purest interpretation of frequentest approach. You'll see when we talk about frequency approaches, we're going to relax that. We're going to say, yeah, we ran an experiment, we had an observation, we're going to use that probability elsewhere. We'll assume stationarity. We'll get into these terms later on, but we're going to relax a little bit. But let's just leave it at this. Frequentest approach is based on long-term frequencies observed over an experiment. Now, there's another category that's very interesting and that's physical tendencies, propensities of the system. And this I actually really appreciate as a geoscientist and engineer, because it's based on the idea of knowledge of the system. You understand the system, know how the system should behave. Can't you use that to work out probabilities with regard to outcomes from that system? I'll give you a very simple example. If you toss a coin, the probability of the coin landing on its edge is in fact, it can't happen, but it's basically very, very small probability. I forgot, I looked it up at some point, it was like one in 10,000 or something like that, that that ever happens. So, we suspect that if the coin is not weighted, that really should have a 50, 50 of landing heads or tails, we don't need to run that experiment. We don't need to use long-term frequencies to measure that, because we know the coin. We know how the coin was designed. We know how it's the density in the coin that it's not weighted in a specific way. We know about the dynamics of that system. And so, we can expect to buy physical tendencies to have a 50, 50 probability of heads and tails. Now, there's another approach, and that is based on degrees of belief. We have a probability it's based on our certainty about a result. Now, what's interesting, but this is very flexible. We can assign probability to anything. As soon as we say we're going to use belief, we can solve all kinds of problems that we may not have been able to run the experiment. But we can use some other assessment and when we get into the Bayesian approaches, as we introduce it formally, we'll talk more about what a prior is. But that's the whole idea. It's based on your experience in which leads to a belief. It's very flexible. We can sign probabilities to anything. And then what we'll do is we'll update as we get new information. We'll talk much more about that. That gets us into a more of a Bayesian approach, as opposed to the frequencies approach based on experiments and counting. Now, there is a fourth category. And to be honest, I find this a little bit difficult to differentiate from the third category. And so I don't tend to use this too much going forward. But let me just for the sake of completeness, just mention it. This idea of degrees of logical support. It's a generalization of classical logic. It's basically like three, the degrees of belief, but they say it's more objective because they said we won't allow feelings. And I'm sorry, to me, it gets very hard. I feel we're splitting hairs at that point because of the fact that as soon as we talk about belief, it's really hard for me to kind of separate and say what's feelings versus belief and what's experience versus belief versus feelings. Okay, so let me all believe it there. As far as saying that there are really three main categories from which we can calculate our probabilities. Now, what we're going to do in the next lecture is we're going to dive into this idea of the frequentist probability approach. And so we'll talk about how we can use experiments and counting. We'll do a bunch of Venn diagrams and then we'll build up from there and get to Bayesian probability. I won't treat number two or number four really in the subsequent lectures. We'll build from one, two, three, and then we'll finish up there. Okay, so I hope this was a useful lecture to you. It was short. We will get into much more discussion right away here. I'm Michael Perch. I'm an associate professor at the University of Texas at Austin. I'm on Twitter. I'm the Geostats guy. And I share daily content on data analytics, geostatistics and machine learning. And I promise only just a few kayaking pictures and maybe a couple of pictures hiking and dogs and such. But in Star Trek. But anyway, so I hope this was useful to you. All right, take care, everybody.
Hey, howdy everybody. So we have finished our discussion here on spatial statistics and geostatistics about verigram calculation, verigram interpretation modeling. We learned how to use it within the creaking system and now we're going to talk about simulation. Now we're motivated by the fact that there is something just wrong with creaking. What's wrong with creaking? While we look at creaking, what we'll find is that it is an estimation method. It does its job very well. Its job is to get at each single location in space, the very best value, the most likely value at each location, the creaking estimate. But if you take all of those estimates in space, they're too smooth. In fact, we don't expect that creaking would honor the global distribution. All of those estimates are way too smooth. There's not very much variability in them and so the variance is going to be too low. Also, what's interesting is we tell you that we use the verigram to make a creaking estimate. But if we calculate the verigram of a set of creaking estimates, we would not reproduce the verigram. It would be too smooth. If we had spiracles, if we had spherical structure, we would expect that the verigram would maybe have a Gaussian structure, or maybe a longer range. It would be smooth, specifically in the very short range. So let's take a very simple example right here. We have a residual V-shell. So we have each rendit, this V-shell, or fraction of shell, count variable. And the black line would represent a creaking estimation. We've got the data. Data dots are represented in the data values. It's one dimensional example just for illustration. We've got data at 1, 2, 3, 4, and 5 locations and you can see the respective dots. The creaking estimates are going to be at the data, the data locations, plus or minus my drafting ability. And if you look at that, that's a creaking model. I'll add distances greater than the range. We go to the global mean. That's simple creaking. But the actual phenomenon we're trying to estimate, the inaccessible truth, in fact, looks like this. The red line. There's heterogeneity in it. There's variability in it. And so we need something that will provide us models that represent the entire distribution. Have the extreme values that aren't too continuous. If we're going to use these models through the purpose of any type of subsurface forecasting, it's important to get heterogeneity. You can't lose that. So we need a method to be able to correct for that. That's the simulation method. That's our motivation. So let's compare estimation with simulation. The two workflows. What are they trying to do? Well, estimation is going to honor the local data. So will simulation. That's not a problem. Now, with estimation, if you have a nugget effect, you might have a bit of discount unit data locations. Estimation is also locally very accurate. It seeks to get the very best value at each location. It smooths. So it's appropriate for some type of visualization of trends to understand general features. But it is inappropriate for any type of a transfer function or process that requires heterogeneity in order to get a good estimate. There is also no assessment of global uncertainty. We'll get into what that means. Simulation, check, honors the local data. But in addition, it actually honors the whole histogram. The entire CDF or PDF is going to be reproduced. It's also going to honor the spatial variability. And so now it becomes because we've got the full variability. Remember, variance of permeability and spread of permeability matter, Dexter Parsons was all about that spread. Remember that we also carried about spatial continuity. We showed examples of which we saw that flow and recovery factors were impacted dramatically by different spatial continuity. So you'll be very happy to know we get the histogram. We're also going to get the right spatial continuity. We can use it. We also get alternative realizations. And all we have to do is change the random members to get that. You'll see that shortly. And we can do that to account for spatial uncertainty. And so now an assessment of global uncertainty is possible for us. So just look at the definitions of estimation. Simulation, we talked about what they do differently. Estimation, a method to calculate the best estimate at each location. So that's any type of spatial estimation, the best value of each location. It focuses on local accuracy. Globally, it's too smooth. It's not realistic. It realistic. If you take all of the estimates jointly, it's not right. Simulation, the method to calculate a good reasonable estimate at each location. You sacrifice local accuracy. But because you focus on global accuracy, having the right distribution and the right barogram, so you say it's okay, I just need a good value, a reasonable value at each location. But I want to get it right globally. So let's talk a little bit more about the smoothing effect of creating. And we'll understand what's going on here. Creeing is locally accurate. It's too smooth. We can visualize with it. Can't use it if we need heterogeneity. We said that. The variance of the creaked estimates is too small. The variance of the creaked estimates, so the variance of all the estimates, why star representing these estimates, is equal to the total variability minus the simple creaking estimation variance. Well, you would expect what do you expect for the variability over the entire model for the creaking estimates? Well, they should have the same variability as the initial sample data. That would be what we hope for sigma square. We expected to have the same variability of the entire sample data. Instead, it has this variability, the variance minus the simple creaking estimation variance. Therefore, we know that simple creaking is too smooth. And we know that the amount it's too smooth by is the simple creaking estimation variance. So just consider the following. Just try to imagine different circumstances to kind of rationalize through this. Simple creaking variance is zero at the data locations. And that makes sense at data locations, the estimation variance is zero. There's no uncertainty whatsoever. And there's no smoothing. In fact, creaking doesn't do any smoothing at the data locations. If you were to have a model that was just data values, the variance of that model would be the right variance. The problem is when we move away from the data. So if we move beyond the range of the data, so we get far enough away from the data that there's no spatial correlation with the data, the creaking variance is equal to the total variance. It's maximum uncertainty. And all estimates will be equal to the global mean or the trend. Well, if we're talking residuals, it's going to be the global mean, which is zero. If we have a set of estimated values in the all out of range of data, and they all have zero, the variance is actually zero. There's no variability complete smoothing. How much variance is missing? All of it. The simple, the simple creaking estimation variance at all those locations will be equal to the variance. It's all missing. It's too smooth, completely too smooth. So of course, spatial variations and estimation variance at all locations can be dependent on the data and the verigram. Specifically, the verigram ranges can be very important. That'll tell you whether or not you have some correlation or not. And the nugget effect, of course, will be important too. It'll tell you whether or not you jump up in estimation variance as soon as you get away from data. So let's put together a proposal to correct creaking. Let's design a methodology whereby we can correct creaking so that we don't have this issue of not reproducing the histogram nor the verigram. So let's be concerned about the missing variance. So we know that the missing variance in the in the estimates is the creaking variance. And we know that we can use a simulation method to correct the variance to get the right history. And that simulation method is to add, we take the creaking estimate and we add a residual to it. The random residual will mean of zero and variance equal to the missing variance. So we will just add it back. Now we've treated this before. We dealt with the fact that variance is additive. We've been doing it all over the place nested structures and verigrams. We've been talking about adding different components of the variance that describe different spatial frequencies. We talked about it when we talked about expectation. And so we take advantage once again of this idea of the additivity variance and we can add a residual in and then add get that variance that was missing. And we want to correct the code variance. So we have the right verigram. So the verigrams reproduced in our models. The method we're going to show for that is sequential simulation. We're going to add the simulated values to the data as we proceed sequentially. And that will impose the right spatial continuity between the sim-light values. Simulation reproduces the histogram honors spatial variability the verigram. So then it becomes appropriate for any type of process that requires us to understand spatial heterogeneity, which for us dealing with subsurface, many problems are like that. In fact, creaking is really is used most in settings in which you have dense data. And you have a transfer function that's more very simplistic. Something that's maybe volumetric calculations and so forth. You may not need the full range of heterogeneity if you're going to be scaling up to some type of larger aggregate volumes or concentrations. But it still may be important. Okay, so simulation. It's also going to allow us to assess uncertainty globally through multiple alternative realizations. And so now let's prove this that proved that this will work. Let's go ahead and get into a couple of proofs. The proofs have all been taken from the book Geostatistical Resort Modeling. The second edition was Perch and Deutsche. Of course, I believe the proof was, of course, in the first original edition by Deutsche Lohm back in 2004. So full credit for that. So recall this simple creaking estimator right here. Simple. We're working for sigils. We don't have to talk anything about one mind some of the weights. That all gets cooked in as soon as we're dealing with the residuals. We also showed a very brief summary of the derivation of simple creaking that resulted in this system, linear system of equations. And we can go ahead now and do a check. Let's check. Let's calculate the covariance between the creed estimate and one of the data values. So we got a creed estimate at a location, unknown location. We got the data values, I should say, between the data values. And so we can go ahead and calculate this. If we do, what we'll notice is if we assume zero, we can expand it out like this. It's just the product expected value of the product. And if we go ahead and we calculate the expected value, the product excuse that comma shouldn't be there. Then we would be able to substitute for the estimate, the estimator shown right here. We go ahead and we simplify that further. And what you'll find, in fact, is that the result is we can substitute the simple creaking constraint from here into here. And in fact, the covariance between the estimate and the data locations is equal to the covariance between the locations and the data. So what's that tell us? The covariance between the data and the estimates is correct. It's imposed by the simple creaking system that part of the covariance is correct for us. So the problem here with covariance is interesting because we know we're okay between the data and the estimate location. Well, let's look at the different parts of covariance and let's check and see where the problems coming. Between the data values themselves, we know that's correct. The variance is correct. The verigram is correct. Everything's correct at the data locations. Between the data values and the predicted or estimated locations, it's correct too. And we just showed them. The problem comes about between the predicted values. And we know that intuitively. If I make a creaking estimate at this location right here, I make another creaking estimate at this location right here, they're estimated in the pen and of each other. There's no constraint to impose the right spatial continuity between the two. So, and we also recall that the variance is still too small. We're going to get to that. We're going to get to that. We'll get that treated with the residual. And so what do we have to do? Well, we need to add the missing variance in. That's a we propose that we're going to work with a residual. We know the variance is supposed to equal to the cell, which is the sample variance of our data set. The station of variance should be constant everywhere. There's everywhere. There's nowhere to no reason to assume that at the data locations that the variance should be lower or higher, we're assuming the stationarity here. Although the covariance between the creaked estimates and the data is correct, the variance is still too small. And we've talked about the fact that the missing variance is the creaking variance. So we need to add back a missing variance component. We're going to add back in the missing variance. But we want to make sure that we're not changing the covariance reproduction. Because we said we're doing the fine job with the covariance between the data and the estimate location. So let's check that. Let's check if we go ahead and we add a random independent component, zero mean and correct variance to our previous creaking estimate. We'll call that a simulated value. Let's check and make sure that we haven't messed up the covariance between the simulated values and the data locations. And so we'll go ahead and check that. We'll check the covariance between the simulated values and all of the data locations. And of course, once again, assuming a zero mean, we can just explain that as a product, the expected value of the product of the two. And once again, we can substitute in for the simulated value, these two components right here. We've done that right here. We can expand it out. Now what's very interesting is that this component right here will go to zero. It just disappears. The reason being is the residual and the data are independent of each other. And we know from expectation as an operator that the expectation of a product, if they're independent random variables, is equal to the expectation of the first, the multiply by the expectation of the second, the expectation of the random residual, its average is zero. So in fact, that's just going to disappear. We're left with just this side right here. And so therefore, we know that once again, by substitution with the simple creating system, once again, we find that in fact, the covariance is correct. We have therefore, we find that the covariance between the simulated value and the data is equal to the covariance that we had before between the estimate and the data locations. So we haven't messed up the covariance. The covariance is still okay between our data and our simulated location. So let's go ahead. We've shown that this will work for us. We've shown that we're in good shape. We can move forward with a procedure where we can fix creating so that we can honor the histogram and the verogram and also get a model of global uncertainty. So first of all, let's define all of the separate concepts in sequential Gaussian simulation. First, we have sequential because we're going to sequentially include all of the simulated values and we're going to treat them as data. So we're going to simulate a value after we simulate it, we'll put it into data set and we're going to treat it like its data in order to impose the correct spatial continuity or correlation between all of the simulated values because we know creaking gets the correlation spatial continuity correct between the data and the new estimate locations. We'll take advantage of that by sequentially including our simulations to make sure that we get the right spatial continuity Gaussian because we're working Gaussian space. The data is transformed to Gaussian before we work with it. We calculate the verogram in Gaussian and in doing this, if we calculate a local estimate, the conditional local conditional distributions based on a local estimate, a mean, coming from the creaking estimate, an variance coming from the estimation variance or the creaking variance, with those two parameters that we assume Gaussian, we now know the full distribution of uncertainty at each location. Remember in Gaussian space, everything is Gaussian. All of the conditional joints, marginals, everything remains Gaussian. And so we take advantage of that. Simulation because we use Monte Carlo simulation to draw from this Gaussian distribution, the local conditional distributions that we calculate from creaking, we draw from them with the Gaussian, with Monte Carlo simulation and that allows us to be able to add in the missing variance and to construct multiple equal probable realizations. Okay, so here's take number one. I'm going to have two takes at this. Take number one, we'll kind of more words at the overall sequential simulation framework. Take number two, we will show pictures and kind of show it more illustriatively how it gets done in an actual grid. First step, transform the data to standard normal. That just means Gaussian with a mean of zero, a variance of one. All your work is going to be done in normal Gaussian space. Go to a location, perform creaking to get a mean and corresponding creaking variance. Creaking, we talked about in the last lecture series, we know how to do that. Draw a random residual that follows normal distribution from mean of zero and a variance of the creaking estimation variance. Add the creak to estimate and the residual together to get a simulated value. Now, note, what I'll show in subsequent steps and when talking about this will suggest that we combine those two steps in one. All we do is instead of using a residual, we just say draw a value of a mean equal to the creaking estimate already shifted and the variance equal to the estimation variance. It's the same thing as this two step process. Then we're going to add the simulated value at that new location to the data set. We're going to treat it like data. That way we impose the right code variance between the simulated value, we just simulate it and it all subsequently simulated values. The key idea is that we use those previously creaked simulated values and then we get the right spatial continuity between all the simulated values. Now, we're going to visit all locations and random order. This avoids this is just a practical approach to avoid artifacts. Something I like to say to my students is, have you ever flown in an airplane over a farmer's field? When you look down, can you tell which way the tractor goes? The answer is, yeah, you can see striations in the field caused by the tractor. It's movement through the field. That's kind of what happens if you use a regular path when you're sequentially simulating. You get a high value, you can propagate it across and it can cause striations or features and we break that up by using random path. You back transform all the data values and simulated values when the model has been completely populated. You create another equal probable realization by starting again, but with a new random number. So that you will in fact get a different random path and you'll do different Monte Carlo simulations and it will change up the model completely. Quite a bit actually. Okay, so why go see simulation? Well, the local estimate is given by creaking and if that's given to us and the variance is given to us by creaking, however what shape of distribution should we be considering? Should we be working it? And so by using Gaussian with the two parameters, the creaking estimate and the creaking variance, we therefore know all of the parameters so that we know the entire CDF to do Monte Carlo simulation with. The other advantage is that normal Gaussian distribution is that the global distribution will be preserved. If we transform the Gaussian and then we do a bunch of Monte Carlo simulations from different Gaussian distributions. The ultimate distribution at the end will also be Gaussian and so the whole thing will remain consistent. Everything in Gaussian space is Gaussian. We're going to transform the data to Gaussian to normal scores standard normal in the beginning before variegated orography. And the reason being is that the varogram is being used in Gaussian space so you should calculate the varogram on the Gaussian transform of the variable. We're going to simulate 3D realizations in normal space. Then when we're done, we're going to back transform when all values are done. We've got the whole model popular. Now there's never any free lunch. My advisor always told me during my PhD. And so there is no free lunch here too. The price, there's great mathematical simplicity to the Gaussian assumption. But the price of this mathematical simplicity is a characteristic of maximum spatial entropy. Practically defined, we say that the lows and the highs, the values that the tails of the distribution are as disconnected as possible. Now we could say because of that, we should be concerned about using this for permeability because it could affect conduits, barriers, baffles and so forth. I would say we should be careful with it with prospi permeability. We should also be thoughtful about trend models and so forth. So we would expect the Gaussian distribution to maximize this entropy. So maximum spatial disorder beyond the varogram is a consequence. Maximum disconnectedness of extreme values. The median values have the greatest connectivity. And so we would expect and then symmetric disconnectedness, the extreme low values, extreme high values will be just as disconnected. There would be a symmetry around the center of the distribution. Okay, here's take number two. That was take number one for explaining sequential Gaussian simulation. If we got that far and you're still looking at this video saying, I don't, I still don't know what that is. Well, give me another chance. Let's see if I can get you to understand this. So these are all of the steps. A little bit wordy. Let's walk through them and then I'll show it to you in pictures. Establish a grid network coordinate system flat in the system. Assign the data to the grid account for any type of scale change if there is a scale change between the data and the size of the grid cells. Now, I should note up until now, this has been just like creating any type of method where you're trying to estimate within a framework. Transform the data to normal space. That's new to Gaussian space. Now you can calculate the verga. Okay, so here's the here's the simulation. Determine a random path through the grid nodes that don't have data. Find the nearby data. Go to the first location. Loop over the nodes. Go to the very first location. Find any data within a search and any previously simulated nodes. Construct a conditional distribution by using creaking. Draw simulated values from the conditional distribution. That's Monte Carlo simulation. Assign the simulated value to the grid as data. Loop to the next location. Keep going until you fill the grid up. Now step number six is just checking. After you fill the grid up, just check. Do you honor the data? Check. Honor the histogram. Should be standard normal at this point. Do you have that? Do you honor the verga? Check that. Once you're good with it, back transform from normal space back to the original distribution. Ah, you'll reproduce the original distribution. That's important. You restore to the original framework. If you flattened and you had some type of undilation or stratigraphic complexity or maybe some faulting or something, you restore it back to that state. Then you could check at that point. Did I honor the geologic concepts? Geophysical information, production data, and so forth. To calculate multiple realizations, just go back up to the top. You could probably go right back up to just determining a random path and you could loop through. Now you may want to change distributions and verga grams and so forth. If you fill that, that's an uncertainty. But in general, we would just go to a new random path. Draw new values with Monte Carlo simulation. And we would get a totally different realization. We call those realizations. Let's talk about the steps with more pictures. We got our carosity distribution. We're trying to simulate it on this grid. We got a well-2 or a well-1. It's an anti-cline original framework structure. We flatten it and we assign a grid network. We assign the data that's been interpreted upscaled to the grid cells. So now we have data populating our grid. And you notice that there's been some upscaling. The distribution may have changed to reflect that upscaling. Volume variance. We get less variability if we have a larger scale that we're working in. Now we transform the data values to normal space. Standard one, zero, mean one variance, Gaussian distributions. So our values have all been transformed. We transformed this upscaled distribution to Gaussian. Go ahead and calculate the verigram. And so we calculate and model the verigram in the primary directions. We've done a lot on that. We're good to go. We determine a random path through the grid nodes. Now you'll notice on this slide, I got lazy. I'm going to defend it and say that I didn't want to fill in every cell. It would be too noisy. But I got tired of doing this. So I went up to, I think, 13, my favorite number. So I went up to 13. But you can imagine all these empty cells would also have values in it. It's a random path going to all cells. Eventually, every cell that doesn't have data will be visited. Okay. Then for the first node on the random path, we're going to go ahead and look around ourselves. We're going to look at the search neighborhood. We're going to gather all of the data and previously simulated nodes. We got two data right there. We're going to construct the conditional distribution by doing creaking. We do our creaking. We get a creaking estimate. We get a creaking variance. We assume Gaussian. Boom. We got the whole entire distribution. We know the entire distribution now. And so we have this distribution of uncertainty at that location with regard to what's going on. And so that's super cool because then what we can do is we can convert it into a CDF. We do Monte Carlo simulation. Monte Carlo simulation, draw a random p-value uniformly distributed between zero and one. So we preserve the original distribution. Take that value and sample from the distribution, the CDF, basically the inverse of the CDF for that p-value. And we get a specific value negative 0.6. It's Gaussian. They're Gaussian values. That's why it's okay to have negative even though it's a porosity. It's a Gaussian transform of porosity. So we can negative it. And look at the grid node. We now have that value. We've assigned our simulated value to the grid. It's now there. Ah. So now we go to the next location. We gather up the data. We don't have any previously simulated nodes at this location. And we perform our creaking. We calculate the creaking estimate, the creaking variance. We do the inverse of the CDF. That comes from assuming a Gaussian distribution. And we get our realization to put at that location. We go here. And we go to all locations. Now pretend this entire grid is filled up by a Gaussian distributed values. Probably negative three to positive three or so. And so that's we would expect it to complete. I got tired of filling them in. Then what we could do is we could check the results. We could check the distribution to make sure it's Gaussian zero one. We could check the program to make sure it's correct in Gaussian space. We calculated in Gaussian space. So we make sure everything's okay. Make sure we honor the data at the data locations. We should. We assign the data to the to the cells where the wells or whatever data source intersect that those cells. So we should have the data right at the data locations. Then we take all of those Gaussian values and we back transfer on them back to the original distribution. So you can see all these values are prerosities measured in a fraction. And so we have all of our values at all locations. And so now we have a reservoir property model. Honors the data at the data locations. Honors the histogram. The complete histogram is matched. And it honors the barogram, the spatial continuity model. And so here we have it. We have a realization that drawn schematically with some kind of a contour kind of look. And we would restore it to the original grid framework. So we're just going to deform it again, restore it back. So we have our well data. We have our heterogeneity model. And at this point we probably want to check the geologic concepts to make sure it matches any type of geophysical production data and so forth. And we can start doing some type of flow simulations seeing how it behaves. We start to learn from our realizations, Eureka, a realization from our realizations. That's what happens. And we can calculate multiple realizations by simply just going back to the step of assigning a random path. If we use a new random number seed, we would get a different path. And at every single location, we would get a different Monte Carlo simulation. And that would have a different influence on all of the other subsequently simulated values that are using the previous as some values as conditioning data. And so in that way, you get a totally different realization just by changing the random number seed, honoring the same histogram, honoring the same barogram, honoring the data, but expressing spatial uncertainty. We still don't know what's going on between the data locations. So let's just take a look at some simulated realizations. Excuse me, I'm overlapping the plot here, but you can just see it's a simple spherical barogram from no negative effect. Here's an exponential barogram. Here's a Gaussian. They all have the same range. The range is 600 meters isotropic in all three cases. Here's two realizations for each of these barograms. They're both of the same distributions too. And you can see the color scales here. They're porosity units going from 0 to 30 percent porosity, which is the orange type, orange colors. And so we can see the overall type of behavior, the spherical versus exponential, which has more short scale variability, but has a little bit greater and long range continuity. And then the Gaussian, which is very smooth in the short distances, but is also has very much like the exponential and as far as long range continuity. Now what's very instructive is we could go ahead and look away from data values. And so one of the best things to do is pick the corners of the model. So here we are, a data point at the x or the cross right there. At this corner of the model, we have a very low porosity less than 5 percent. If we go over to this model, in fact a yellow, it's 20 percent. What does that mean? We're so far away from data that we would expect the uncertainty spans the entire range of the input distribution. And so that's what we're seeing there. And if you look all over the model, you can convince yourself that at this location here, this location here, they're both very high because you have a high data value conditioning. But then away from it, it stays high, then it drops down to a medium value. Here it stays high and even gets a little bit higher. And so away from the data, you'll see lots of fluctuations. You look across these simulated realizations. You'll see that over and over again. That is spatial uncertainty. You know the global distribution, you know the verigram, you know the data, but you still don't know what's going on away from the data locations. And the amount of fluctuation will depend on the amount of correlation with local data. Here's another example where all we did was take this spherical with a range isotropic range of 600. And we just went ahead and varied the relative negative effect from 0, 50 to 90 percent relative negative effect. And this is pretty instructive because you can see the same long range features in all three. It's just the amount of short scale noise within each of the models. And you can do that with simulation. Another thing that's very useful is look at multiple realizations of simulation, realization number one, two, and three, and compare them to exactly what you would have got with creaking. And so here's the ordinary creaking. And you can see that if we use the same color bar on all three, the creaking looks almost completely green. It's away from the data with a little bit of negative effect. It drops really quickly or it goes, it goes very quickly to the global mean. Now we spent a lot of time during this discussion trying to convince you that we need to reproduce inputs, statistics. We need to honor, well data, we want to honor the data, but we also need to honor the histogram, the CDF, and the verga. And what I'm going to tell you now, I hope it doesn't shock you, but it turns out that there may be fluctuations in the way that we honor those statistics. These fluctuations are known as ergodic fluctuations. And so what can we say about them? When will we see these ergodic fluctuations? Well, in general, if the model is large relative to the spatial continuity. So the big model with short spatial continuity, we expect these fluctuations to be minimal. They shouldn't really be very obvious. This shouldn't be an issue. If the model is small and the spatial continuity within that model is very long, then we expect that these fluctuations can become extreme. So let's take those previous models that I just showed. And let's go ahead and look at for the case of the Gaussian exponential. And this should be labeled as spherical here at the top. Here is the CDF. The black line is the experimental CDF. It's step E. Step has steps in it because it's experimental based on Q data. And we have the resulting CDF's as continuous lines because they're very much more smooth and continuously. These are based on the entire model, a lot more points. And you can see that there's some fluctuation. It's not too bad, but in general, there's fluctuations. The verigram, you can see that we have the, there's lines, there's black and red lines, which are for the different directions, the experimentals. And then we have the dots represent the actual experimental verigrams calculated from the simulated realizations. And we can see there's actually some pretty good fluctuations here. So what do we say in general? What are we looking for? Look at the CDF's, you'd hope that the fluctuations should span the CDF, the target CDF, and not show any specific bias in one direction or the other that they should have results at a higher and lower. And the same thing for the verigram, as we see here, where there's some higher and some lower. We could get concerned, we could look at this and say, well, look, all the realizations seem to be higher. What's going on here is an issue with the variance being inflated with this model. Should we check to make sure the cell is right? We may want to check our models further with regard to these fluctuations. All right. So that's the end of lecture on simulation. Now we will get into indicator-based simulation, co-simulation. How do you deal with more than one variable at a time? So we're not done yet with this spatial. We're going to have a couple more lectures before. And I think we may just jump right into some machine learning and some uncertainty analysis at that point. As usual, I hope that this was helpful to you. I'm easy to get a hold of. I'm a professor at the University of Texas at Austin. You can find me, look me up. And I'm also just that sky on Twitter. And so you can see some of my content and what I'm doing there, I announce things almost daily. All right. Thank you.
Howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin and I'm going to explain to you the Bayesian coin example from the book, Sevilla 1996, a Bayesian tutorial. Now, so first of all, start out by explaining the problem to you. Then what we're going to do is we're going to look at some interactive Python code using interactive plotting and widgets to be able to make the problem, play with the problem, learn something from the problem and learn a little Python while doing it. All right. So what is the problem here? We're going to have a coin. We're going to pull it on my pocket. We're going to show it to you and you have to make an assessment of is this a fair coin? So you want to know what's the probability of that coin having heads? What's the probability of having tails with every single time that it's flipped? And so we'll focus on heads. Of course, tails is just going to be the opposite the complement. So we're going to use Bayesian approach to solve this problem. You're going to be able to design a prior model of how you think the coin should perform, which is shown right here. This is the prior. And you're going to be able to use your belief. Maybe you look at me and say, well, perch seems like a very honest person. So I anticipate that this is going to be a fair coin. So what you have to do is you got to design a PDF representing the probabilities of different probabilities of heads. Now if we look at this plot down below, this is a fair coin. That would be 50% probability of heads. If you absolutely believe that I am an honest person, and there's no way I would be walking around for a weighted coin in my pocket, you put a spike right there and you'd say, nope, Professor Perch is very honest. It must be a fair coin. Or what could happen is you could say, you know, I really think that Professor Perch is kind of a joker, the type of person who would be, you know, walking around maybe a two-headed coin in his pocket. And so really you believe that the probability of heads, your prior model is this 100% means that's a two-headed coin. Or you could be in the situation where you think Professor Perch is walking around with a two-tailed coin in his pocket. Probability of heads is 0%. Or maybe you think that I have some type of advanced capability or knowledge around making weighted coins. And in fact, I've created a coin that has some type of distribution where more often than not, it will be heads. But with some probability density function, it doesn't, of course, perfectly. You know, it will have some type of waiting. Your most likely estimate is that maybe I have it weighted so that it's about 80% of the time going to be heads. But you're uncertain. You don't know. You just think that I've somehow waited this coin. But you're not certain about how much it's likely to be heads. You just think it's weighted. Or finally, you might say, I just watch a couple of videos on YouTube. I don't really know this guy personally very well. I don't really know what to think about this coin. I have no information. And so you say, well, it's uniform. I have equal probability across all of the from a fair coin, all the way to to tailed coin, to a two head coin, to something weighted towards heads, to something weighted between tails. They all have equal probability. Because you really don't have anything to say about the coin. That right there would be a naive prior. Okay. So this is the choice you have to make. You have to formulate a prior model in order to put into this equation. And what's really cool, we can solve this. You don't see when we solve it, we're going to actually bin the problem over a range of different probabilities of heads. And all we have to do is come up with a model that has a description of the density or likelihood for each one of those bins. So as long as we have this at each one of those bins, that's our prior input into this equation. And we can solve this over all of those bins. Now what I'll do next is I'll talk about the likelihood model. All right. Now let's talk about the likelihood. So the likelihood is this part of the equation right here. It's the probability of the outcome from the experiment given the coin. So likelihood function right here. Okay. So how's it look? So we already got the prior model and you drew a function. You said purchase honest. It's going to be a, I'm certainly going to be a 50, 50 coin. Or I don't know, perch, complete naive or weighted one way or the other with some uncertainty. You decide it. Now what you're going to do is you're going to take that coin from me and you've got to run an experiment. And you're going to keep tossing the coin. Every time you toss the coin, you're going to get heads or tails. Tails, heads, heads, heads, tails, and so forth. You can just keep going and you'll keep tossing the coin and you'll get a certain number of possible outcomes. You'll have a number of heads and a number of tails. And if you take both of them, add them together, that's going to get you the total number of trials. Now what you need to do is for all of the possible, remember, we discretized the probability of heads where that would be 50% right there and maybe 55 or 60 or 70 or 80, all the way up to 100% right here. We need to calculate for each one of those probability of heads in the coin. Now we got to calculate the probability of the experiment, the tosses given that coin. And the great thing is we have the binomial distribution. The binomial distribution equation is shown right here. It provides a PDF probability density function for the following. The total number of trials, total number of successes, which we will call heads, the probability of heads to the power of number of heads. Well, guess what? For this case here, it might have been 5%, 6% or something. We go ahead and put that for P. And we look at our experiment. We got a number of heads, number of tails. We can put that in. Number of heads is the X. Number of total experiments is going to be hit number of heads plus number of tails. And we can go ahead and take one minus this probability. And to the power of the number of trials minus the number of successes. This is the probability of the failures. And of course, this is the choose operator and choose X. So basically, what is this? It's assuming independent success failure, binary outcomes with a probability of the success of P. You have this many successes. So independence, just the probability of success, time to probability of success, time to probability of success, gives you the probability of that combination. One minus, this is the probability of failure, total number of failures. So this is assuming independence of all the probabilities. And this choose operator is accounting for the combination of ways it can happen. Because we could have heads, tails, tails, tails, heads, tails, or tails, tails, heads would all be cases of two tails, one, heads, for three tosses. And by we have three combinations to get there. And so the choose operator accounts for that. All right. So we get the binomial representation here. So we can just plug and chuck. All we have to do is supply into this number of heads is X, number successes, the number of heads plus number of tails is going to be the big N. And so we're able to calculate. And what we'll find is if we an experiment that was 50, 50, what we're going to find heads and tails is well the distribution that's nicely shaped. And I'll come back down and the mode will be right at the 50%. Now if we run fewer experiments, what'll happen is the distribution won't be a steep. It'll have more variants because you have more uncertainty. And if we run many, many, many experiments and it's 50, 50, the uncertainty would go down. So that's the binomial distribution that we're working with. Now if we were closer to the edges, it can of course become skewed. So don't don't mix mix it up and think that's a Gaussian distribution. But if you write on the edge, it'll become skewed and have that shape. So that's how we're going to calculate the likelihood. Now what's really cool is once we have the likelihood for all of these probability of heads for the coins, we can take the prior times the likelihood and we're able to go ahead and solve for the posture. Now I should make a couple of comments about the evidence term. The evidence term is really just going to be a standardization of force closure. What we can do is just solve this for all of our bins and then divide it by the total sum over the bins. That will force the area under the curve or the summation across all these probabilities to sum to one. That's closure. That's exactly what the evidence term would be doing for us. So we'll just do closure at the end through a standardization, dividing all of the probabilities by some of the probabilities to force the posture distribution to sum to one. All right. So that's exactly how we're going to get this done. Let's switch the code and take a look at the code. So here's the Python code. It's an adjupiter notebook and we're going to be able to accomplish this approach of basing updating the figure out of a coin is fair based on a prior model and a likelihood model come from the binomial distribution and a set of coin tosses and to get to a posterior distribution and we're going to do it all interactively. Okay. So this is what it looks like every time I do one of these examples in jupiter notebooks, I like to put a little bit of some links introduction, explain the experiment. So this is all explained right here kind of short and to the point. We're going to go ahead. We want to work with some parametric distributions. I ended up working the binomial and the Gaussian distribution. I tried experimenting with the triangular for a prior. That was kind of cool. I ended up going towards a normal distribution. I thought that was better control of the experiment. But of course anything could be done. You could even try to hand draw or edit in the values if you like. Let's go ahead and import those packages to work with. We're going to also need i, pi, widgets and can import interactive. And that's how we'll get our interactive plotting going. And we're just going to work with mat plot live. I like doing things really simple. We'll need numpy to do some array math broadcast methods that are nice and quick and we should be good to go. Now here's the code really, really straightforward. What I need to do is I'm going to declare four widgets to work with. Now each one of the widgets are going to describe different parts of the inputs for the experiment. The first widget is simply going to be the coin bias. That's my prior model. And that dial will go between zero and one. And that's where I think the mode or the most likely value is for the coin. If I think the coin is spare, I'll put on 50% heads. If I think it's bias one way, like more often getting heads, I'll put go above 50% and so forth. Now then I also want to put in a coin uncertainty term. I'm using a Gaussian distribution for the prior. And so you can change the dispersion. Maybe you say it's 50% of high certainty low variance. That's exactly what that is. And then you'll be like, Hey, purchase honest. I know this coin is going to be good. It's going to be fine. Or maybe you're going to go ahead and say, you know what? I don't know, perch very well. A naive prior in the case of Gaussian distribution is a very wide dispersion relative to the problem. And that result in the prior actually having no control in the problem. We'll see that. Okay. Okay. And then the other widgets we want to work with are B widget is going to be the wall, the B and the C are going to be the outcome of our experiment. And I thought a nice way to do it would be after over the experiment, what was the proportion of heads observed? So this is a proportion. It's easier that way than having separate heads and tails dials. I thought that was clean. And then the C dial or slider is an integer slider, which is the total number of coin tosses. Now what we're going to observe is as we shift that to a higher proportion of heads, the likelihood term will shift to be more the probability of the tosses given the coin will indicate a higher probability with regard to these higher bias terms within the coin. And as we increase the number of tosses in total, the uncertainty is going to shrink. So we're going to get to see all of that. That's really cool. The interactions between everything. So we have defined a function here. It's going to read in the information from the slider A, B, C, and D, A and B are prior information, the mode or mean of the bias, the variance of the bias, the prior, and C is going to be the total proportion of heads in the experiment. These are the total number of trials or coin tosses. Okay. So we're in pretty good shape now. I like to use regular, you know, simple, easy to interpret code. So I'll say, okay, just tell me the number of heads, number of tails from that experiment. And so both of those should be integers. Then what I want to do, I want to discretize the probability of heads from zero to one 1000 times. So I get nice, clean functions to look at. They're really nice and clean. I'm going to define the prior. The price is really simple. I gave the mean. I gave the scale parameter that's measure of dispersion as D. And I for closure on the function, I force the sum of all of the bins to be equal to 100%. I just like to do that. It's very clean for me. If I looked at the probabilities on a table, they would be valid probability values. And I just, I like that. Okay. So now we're going to go ahead and define a plot with the prior distribution. We just plot the prior distribution. We're going to have a two by two matrix plot in this function. Now you notice this is a little bit inefficient because every time you touch a slider, it's going to reproduce all of the plots. I didn't do anything fancy with trying to only update certain plots. Remember, I just kept this very simple. There'll be a little bit of flicker and it shouldn't be too bad. It's not that heavy computationally. Okay. So we'll set the limits on the X zero through one. The X axis is going to be the probability of heads from my prior model. And we'll put labels on the plot. We'll plot it up. The second plot is going to simply be a pie chart. And now I got a little fancy with the pie chart. I'm kind of proud. What it's going to do is you enter the total number of trials or number of tosses and proportion of heads. It's going to just show you proportion of heads, proportion of tails and the size of the pie chart increases. You do more trials. It's kind of fun. Okay. So we go ahead and we have that. But we also want to create the binomial distribution. Wasn't that great? In SiPi, we actually have the binomial distribution already coded up for us. We can get the probability density function discretized. And so all we have to do is tell it how many successes did we have? How many total trials did we have? And what was the probability of heads that we're assuming for the coin? And we do that for all those bins. There was a thousand bins from zero to one hundred percent chance of tails, a hundred percent chance of heads and everything in between. And we'll get the binomial distribution. And we can look at that. That's our likelihood function. It's going to be our likelihood function. It'll be really useful. So we can see how the experiment is providing new information. Then once we have that likelihood, the final thing we can do is we'll just take that. It's a one dimensional array of the probabilities discretized across the completely tail-weighted coin, double tails, all the way in double heads and everything in between, we can just apply Bayesian updating, the prior multiplied by the likelihood. Now I mentioned, I haven't calculated the evidence term. The evidence term is just there for closure. So all I have to do is take that one dimensional array of posture values right here, calculated some and divide it by itself. That's that's closure. It's going to force closure so that all of these probabilities sum to one hundred percent. And so that works. And then we can plot the posture. Now in order to get this to come alive to start being interactive, all we have to do is call this widgets dot interactive output. We refer to the function that does our plotting. And we just tell it to link up and use all of the ABCD up above here. We defined as widgets, float sliders, and then it enters your slider for the total number of experiments. Okay, so that'll make it come alive. Now I put this command here, clear output, weight equals true because I was having some flickering problems and you'll see what it does. It makes it just sit there and it doesn't immediately update. And I think that's useful because it seemed to me that it was trying to update every time I moved to the lobit. It was to do updating and replotting the charts and it just made it a bit laggy. So this is improved the performance. All right, so now what we'll do is, well, I just realized and I just realized that I'd forgotten to import widgets. And so when I ran it, I got an error. So just to take you back up here and mention, we need to import interactive and also widgets. Interactive will do the connectivity of the widget to the function for the plotting and the widgets will allow us to have the access to the sliders and all. So we go ahead and run this code right here. And now we have our function. We have our widgets. We have our function. What's missing now? We need to display it. When we run this code right here and I already ran it just now, but we run it. Look at this. We got our widgets. Now I didn't mention it, but we did use an H box in order to organize our widgets. And if you look back at the code, you'll see I actually did take the widgets, define all the widgets and then create an horizontal box of widgets with all them listed. You can do so much to build like a full UI user interface with all kinds of how does it scale when you move it, when you shrink it, how does it how does it format? I actually spent a little bit time as doing full stack development C++ and worked with QT for a while. And it's it's kind of getting like that where you can kind of control everything really nicely with the display. Okay, so now what'll happen? Let's go ahead and just change the coin bias. Okay, so let's take a look at what we've actually done. So as soon as you move one of the dials, it comes alive because I put that weight command and they're not a big deal. Okay, so let's go ahead and make some changes. The first thing is we said 50 50 proportion heads observed in the experiment and we did 100 coin tosses. So that's literally 50 coin tosses head 50 coin tosses tails. Let's go ahead if we look at the likelihood term right here and we go ahead and increase the number of coin tosses. Look at what happens. It's not cool and you see this plot grows. Now we have 610 coin tosses. Do you see what's happening? This distribution is becoming narrower. We're conserving the area under the curve and it's becoming narrower because there's a more coin tosses where much more certain with regard to the actual experiment to outcome and what it's telling us about the bias and the coin. Okay, we got more and more information. Now look what happens if I change the proportion of heads observed in the experiment. Now you see how that distribution shifted to 30%. We could even take it all the way down to 10%. You see what happens. If I take it up to 90% or 80%, 90% heads right there. Now the likelihood function is changing significantly. You're getting a lot of good information. Okay, so let's go ahead. Let's shift the total number of coin tosses small again. So you see how shrinking, shrinking, the little bit of flickering but not too bad. Let's go ahead and take it to something really small. Okay, so proportion heads 50% and we had only five coin tosses. Now what's going on? You can't have half coin heads when you only have five coin tosses. Let's go ahead. Yeah, good. Six coin tosses, 50, 53 heads, three tails. Look at this. A very broad likelihood. It's getting quite naive. You have very little information. Now let's go back and look at the prior distribution. Let's say that you decide that I'm a fair person. So you put the 50% is what you think is the most likely value for the coin. You think it's most likely a fair coin and you think it has very low uncertainty. In other words, you put a spike on 50% said purchase fair. If I think the coin could have a little bit dust or dirt on it, that might actually change the heads, you know, probability of heads. Maybe just a little bit around 50% 50, 50 but it should be fair. Now look what happens. You have a naive likelihood. You've threw the coin very few times and you have a very specific prior and look what happened. The posterior is basically just mirroring the prior. You didn't have any really not too much new information. Okay, so now let's go ahead and say that you increased the amount of information in the likelihood. You did quite a few coin tosses. Now this is going to grow. Look what happens. You see how this is growing right here. You had a prior that said purchase fair, his coin is fair. You had a likelihood that showing the same thing and you ran a lot of experiments showing the same thing and now your posture is even more certain that my coin is fair. In fact, if you keep running the experiment, look at what happens. You see that's not great. Look at that. This is providing a lot of information and the posture is saying, yep, this coin certainly is fair. Now we can do some other things. Let's go ahead and say, well, what if you run the experiment? You thought the coin is fair. You run the experiment, but then it something goes weird. It looks like as if the coin is weighted towards 70% probability of heads. And so you ran quite a few experiments. It's actually pretty narrow on that. Now look at what's happened. Your prior was purchase fair. Your likelihood is taking you towards a biased towards heads and your posture is going to be a compromise between the two. Its mode looks like it's somewhere around 0.55.57, but the experiment was saying a mode most likely, the coin is somewhere around 0.7. Yeah, yeah, which is right here, 0.7, 70% chance of heads, but your prior was 50-50. And so you're somewhere in the middle. Okay, so we can see all kinds of interesting things happening right here. Let's go back to your prior and let's say that you were really uncertain. You didn't really have much information. You perched. I don't know. I watched some of his lectures. I'm not sure if he's got cheating with a coin. He's got double head, double tits. You see how this is very uniform. It's a uniform distribution. We put a huge variance or dispersion on it. Look at what happens here. The likelihood term becomes the same as the posture. The prior provides no information. Okay, I think this is so much fun. This was an example of using great Python packages like iPyWidgets for interactivity with MatplotLive. Not very fancy, not very difficult. Actually, if you look at it, we were just able to find some sliders, put them in an H box, define a function that created the plots, and just tell it to link them up. Take the sliders, use it to populate the values into my function, and just told it to go ahead and create those plots. We now have this nice interactive GUI that we can play with and understand the coin example from the book Savaya 1996. All right, so I hope this was helpful to you. I'm Michael Perch. I'm a professor at University of Texas at Austin. I teach and conduct research in data analytics, machine learning and geostatistics. We have some exciting new papers that just came out. Brand new novel ways to model the velocity fields and intergranular scale for intergranular scale, the poor structures of rock. I have a new paper that just came out with modern ways to use machine learning for building, subsurface models using deep convolutional generative adversarial networks, and even some really interesting spatial data analytics to methodologies to drive decision making in subsurface development. That came out just in December. Anyway, I am always happy to discuss, hey, take care everyone. Bye.
Howdy, everyone. I'm Michael Perch. I'm an associate professor at the University of Texas, and I record all of my lectures. Now, this is part of my lecture series in the introduction to data analytics and geostaticist course for undergrads. And in this lecture, we get into the concept of spatial bias. Where does it come from? And what are we going to do about it? Now, what's the motivation for talking about spatial bias? Well, what I would say is virtually all of our subsurface data sets, all of our samples that we get from the subsurface or any type of spatial sparsely sampled setting is going to be sampled in a biased manner. And so what does that mean? Well, it means we can't use raw statistics from that data to support decision making. If we're sampling perosity of the subsurface and it's biased high, we're going to think that there is more poor volume if we take the raw or what we'll call the naive statistic, we need to do something about it because otherwise we'll just think things are better and then they actually are. So let's get into what are the issues? Why do we actually have sampling bias? Now, take a look at this data set. This is why this is x. This is a two-dimensional plan view of the subsurface. These could be multiple vertical wells. And those are the average perosity values through some interval in the subsurface where we have a reservoir. Now, if we look through the data set, this is the pretty awesome, I think the plasma, maybe inferno color bar. I think it's plasma from matte plot live. And if you look at it, what you're going to observe is that in general that we have denser sampling where we have high values, the yellows and oranges, and we have sparser samples where we have the lower values, the more of the purple type of colors here. Okay, so clearly we've sampled in a manner where we're more densely sampled in the high values. Okay, if we went ahead and calculated a statistic from that, we would find that the average over this area of interest would be too high. We created a bias in the sampling. Okay, so let's just think a little bit about what's the source of this bias. And we'll do a little recall in the previous discussions on statistics. We talked about issues around sampling and getting our statistics. And we talked about the fact that data in the subsurface is collected to answer a question or multiple questions. How far does the contaminant plume extend? You can sample the peripheries of the area of interest, the phenomenon that you're trying to understand and see how far it goes. Whereas the faults you're going to sample or drill based on where you think the fault might be and try to bound that uncertainty. What's the highest mineral grade or porosity or permeability? You're going to try to sample the best part of the reservoir. How far does the reservoir extend? Offset drilling. Okay, we're also going to sample not just to answer questions and minimize uncertainties. We're going to sample in order to maximize net present value directly. Now we maximize net present value by minimizing uncertainty. Don't get me wrong or by finding out how far the reservoir goes. But sometimes we just want to get the best stuff out of the ground. We want to get the best production rates, the best mineral grades, find the thickest part of the aquifer with the most amount of water. Whatever it might be, sometimes we are drilling, not for information, but to exploit the resource. And so that's another driver for where we sample. Now it's not all just because of a wanted to exploit the resource or reduce uncertainty. We also sample in a way that's limited by abstractions. We might find that we're not able to access all parts of the subsurface equally safely because of surface obstructions, because of geomechanical constraints and the way that we drill and so forth. We might not be able to run an evaluation on permeability on the lowest permeability rock, because it's just not competent. It would take too long to run the test on permeability. Okay, so there's all kinds of reasons we can't sample the whole subsurface in some form of random sampling or equal likelihood of sampling. But that's what we would have to do. If I wanted to sample the subsurface such that I could use the samples directly to calculate a statistic and deem that that would be representative, I only have two approaches available to me. Random sampling or regular sampling. Regular sampling once again, assuming that you're not aligning with natural periodicity of the spatial phenomenon, i.e., the phenomenon does this and you're drilling all of the highs and missing the troughs or vice versa. Now let's assume random sampling. What would happen if you went into your asset manager, your team lead, and you were to say I want to drill the next well in the Gulf of Mexico, $150 million maybe for production test, and I want to do it using random sampling. Of course that would be ludicrous, that would be ridiculous. We can't change the way that we sample the data. We can't not first statistical representative, but us with the Geostats knowledge and data analytics knowledge. What we do is we need to address the sample bias after sampling. So what I would say is our rule is never use raw spatial data to calculate statistics without making an attempt to assess bias and correcting it. All right, how are we going to accomplish that? We got two methodologies by which we can do this. The first methodology we can use is we can do declustering techniques. So we want to adjust and get the histograms, the summary statistics, right? Declustering will do it by assigning a weight to each one of the data, the each data value. Okay, and so you could imagine previously you had a table with a column of values at every single location. Now you have a column of values at every single location and a column, a column of weights. And so for every data value, you're going to have a weight, every data value, you have a weight. And that weight is going to tell you about how representative or how sparsely sample, I should say, each one of the samples is. I'll get into details around that. Okay, so we're going to be able to then use those weights in any one of our statistical calculations and I'll demonstrate that for you. The biasing techniques are different. The biasing techniques say you did not sample the entire distribution. You just missed the worst part of the reservoir or you missed the best part of the reservoir. And so you'll use some form of secondary information to fill in missing parts of the distribution. More complicated. I'll explain it to you. I'll show you an example, but it's definitely requires a lot more trade craft. It's not a simple plug-in play type routine. All right, so I'll stop right there with this video. And next, we will get into the details of declustering. And then after that, we will get into a discussion around debiasing. Well, I hope this was helpful to you. Once again, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin. And I record all of my lectures to support working professionals and most of all my students shout out to you all. Hope you are finding these videos useful. All right, take care.
Hey, howdy everyone. This should be a very short lecture in which we're going to motivate or describe, define spatial continuity and motivate us to try to calculate and to work with spatial continuity. So in order to motivate us and to show the value spatial continuity, what we're going to do is give ourselves a very simple example. In this example, we have a 1000 meter or 1 kilometer by 1 kilometer unit of rock. We're going to show a two-dimensional example simply because it's very straightforward to visualize, but of course this could be done in full 3D, no problem. And we have this area. We have an injector where we're injecting water. And we have a producer set of four producers where we're drawing oil from this unit of rock. Now, of course, this problem could be completely general to any type of spatial phenomenon. We may be concerned with a number or some type of resource trees. We may be concerned with fish. We may be concerned with even groundwater or any type of spatial type of resource. We could have the same type of problem and we could be talking about some type of a function to work out the response of that area to our scheme for exploitation or development. Now, because we're talking about oil and gas in this example, we're concerned about the porosity and the permeability as being the variables that we need to map and understand their spatial continuity in order to assess the permeability. In order to assess the response to our exploitation scheme. And so that we're working with apples and apples, we will use the same porosity distribution on every single example. We will use the same permeability distribution. So the two PDFs are shown here just sketched in for all of the examples. So it's apples to apples. We're comparing the exact same rock unit distributions, univariate distributions, but the spatial continuity varies. So here we go. There we have two examples. So let's ask ourselves the question. Does the spatial continuity of these properties matter to the response of the subsurface? So let's consider I'll just show permeability. Permibility is going to be directly related to flow in the subsurface. Let's go ahead and just look at permeability. I've got a log scale for permeability going from one milledarsie to 100 milledarsies. And so I have my example number one here, example number two. Now recall, they have exactly the same distributions of porosity permeability. The only thing that's changing is spatial continuity. So P10, P90, the extra persons, I should say, will all be the same same static oil in place for both these problems. So when I start to inject water here, what do you think is going to happen? Which one of these reservoirs would you rather invest money in? And so let's go ahead and we'll start running a very simple flow simulation. Run the flow simulation and we are plotting the time of flight. We see the permeabilities for the two cases and we see time of flight. And so time of flight is going to be standardized by poor volumes injected. And so very quickly you can see overall the rate at which water is replacing oil and flushing from the injector towards the producers, the four producers in the spot spot pattern. And so which one of these do you think would have higher recovery given these two different patterns of time of flight? And so if you look at it, you'll see that the example where you have this type of very short scale discontinuity. In fact, I'd suggest that there's no spatial continuity here. These are like as if we just randomly picked values and signed them to the grid from the distribution of permeability at all locations. Here you can see you have a lot medium scale of kind of a medium type of spatial continuity. I would say the spatial continuity up to distance of about maybe a hundred meters or so, this a kilometer here, you get the sense that there is some persistent correlation over space. And when you look at its behavior, you can see that there are dramatically different time of flights. In this case right here, we have a phenomenon that's behaving homogeneously heterogeneous. It really is the same everywhere. It's just the values are drawn from that distribution, but there's no real spatial structure. And you get a very nice piston-like displacement with the flow of front just expanding in a circle towards the four producing wells. And as a result of that, you get very, very nice recovery factor. You should expect to get most of the oil swept. It looks pretty good. You'll see some hot pixels, which take longer to get into because the fact they have low permeability, you might still have a low good bypass, but in general, you get good recovery factors. In this example right here, you have more spatial continuity, but as a result of that, you've got pockets of low permeability, high permeability, and it results in baffles, kind of barriers almost, and conduits where the water is preferentially moving towards the producers. And as a result of that, you're not getting all of the coverage or leaving the lots behind. And so we see in this case that spatial continuity is causing a more complicated type of setting as far as what's happening with the transfer function. That is our recovery. Now, if we look at this example right here, I'll pan back and forth. See what's happening there. All we did was expand the range of spatial continuity to larger distances. And as a result, you get this big pocket of high permeability going transitioning to low permeability. And now when you look at the overall sweep pattern, you see that there in fact is not even very good sweep to these other two producing wells. And so we're not recovering very much of that area as far as the well goes. We're preferentially going these two wells, we're leaving these wells out. How about this example right here? Let me pan back and forth. The previous, the first, the second, and third. Now what's happening here? We have very long range spatial continuity in one of the directions. But we are lacking in general. We don't have very good spatial continuity in other directions. So we have a high degree of anastropi in this spatial continuity. And as a result of that, look at what's happening. We're not even sweeping to this well, to this well. We're just preferentially just like just very quick flow right to that well there in the south west part of our pattern. And so now we're getting very poor recovery indeed. So what can we say? Does to the question does spatial continuity matter? It seems pretty clear that spatial continuity is going to have a huge impact on the specific response of this subsurface problem to our exploitation scheme of injected water and flushing. Of course, if it was a water aquifer, it would have spatial continuity could have a big impact to the flow rate suite recover from the aquifer. What part of the aquifer we could produce the radius that we were able to drain from a single well. And in addition to this, so what can we say for the same univariate statistics? It's not enough to just know the variance, the mean of permeability, ferocity. You have to know the spatial pattern in order to fully understand what's going on in the subsurface spatial continuity often impacts our reservoir forecast. So motivated by that, we need to characterize or quantify spatial continuity. Then once we have our quantified spatial continuity measures, we need to then impose them in the spatial models that we built. And if you think about it, that's a big part of what geosatistics is about that characterization of spatial continuity and in position of that spatial continuity into numerical subsurface models for the purpose of assisting with decision making. There's other concepts connected with spatial continuity, such as your ability to estimate or predict away from known locations where you have a well or drill hole or some other type of spatial sample. And those concepts we will talk about more of when we talk about estimation. But right now, I just wanted to motivate you all to get thinking about the impact of spatial continuity on important decisions. Let's define spatial continuity at this time. We will define spatial continuity as correlation between values, any value you're interested in, ferocity, permeability, some mineral composition, density of trees, the number of fish it doesn't matter. Coralation between values over distance. If there's no spatial continuity, there's no correlation between values over distance. And so you're drawing random values at each spatial location from the univariate distribution. Doesn't matter how far away you are from a known location or anything like that. If you're homogeneous phenomenon, you don't have any dispersion in the univariate statistics. Everything everywhere is the same. And since all the values are the same, they're going to have perfect correlation with each other. And so that's also possible. So I went into Excel. I created a random set of values. I used a moving window in order to impose spatial continuity. It's a convolution style for imposing a continuity by just doing a local window averaging. And so I started, first of all, with just take random numbers. And I standardized them by putting them through a Gaussian distribution with a specific mean probably close to 15% standard deviation, probably in the realm of 2% or so. And I went ahead and this is the PDF, I've been to PDF words, bend and we have the probabilities of each point location. And the CDF calculator right here. And so in looking at a completely random phenomenon, we did not impose any spatial continuity whatsoever. I want to train your eye to a concept here. Many people, if you ask them to draw a random phenomenon, they'll go like this, those scribble back and forth. The lot is scribbling. They seem to fill that random means that there won't be any runs or strings of low values, high values, medium values. So they're going to be maximally different as you go from step to step. That would be incorrect. Random does not mean that we would expect just because one values low, the next value has to be high. That would in fact be the opposite of random. That would impose that would be a negative correlation in space. We would expect with random that each value is equally likely for every step with no correlation or information. So if I'm low, I still have a pretty good chance of being low next time. They don't have to be high. There's no information being shared. So when you look at random phenomenon, I invite you to notice the fact that hey, there's just string of low values. They oscillate. Here's another string of low values. We kind of got higher up here, higher here. These ones looked to have more scribbles with more amplitude. These have lower amplitude. You should expect strings with patterns, little bits of the data with certain features of patterns. Why do I want you to train your eye to what random looks like? First of all, it's interesting scientifically speaking to understand that there's patterns of random. We don't even talk about the fact that a random number generator is judged by how these patterns occur, the frequencies and so forth, as to whether they have the statistics of random, the right behaviors of random, which is very interesting. That's fascinating. The other point is that, kind of right up there with the law of small numbers, it can be very much be a cognitive bias that we will identify features and try to diagnose or try to interpret, I should say, interpret meaning from them when they in fact are features that come from random. So training your eye to expect certain types of patterns and random can protect you from that bias of looking for patterns where there are no patterns indeed. We can start imposing spatial continuity using the convolution type of method that I mentioned before. By increasing the window size, we increased the range of spatial continuity from low to medium to high and to very high spatial continuity. Once you get to a very large spatial continuity, you can now see you have persistent information that there's very smooth transitions, things are changing very smoothly, but it's still going to make its way over the range of possible sample data. At some point, if we had a large enough sample set, you'd see it go really low, really high. But clearly, when we make the spatial continuity very long, we can get to a point where we probably don't see the entire distribution. And we'll talk about those phenomena when we talk about volume variance and ergotic fluctuations and so forth later on. Now, you'll also notice that when we have short scale continuity, that we do a really nice job of covering the entire space, that entire distribution is being observed very quickly. The opposite of what I just mentioned. Right. And the case for which we have homogeneous phenomenon, well, there's no dispersion. Everything is equal to the mean constant. It's the PDF would look like this. It's a spike. The CDF is just a step. And so, of course, in this case, buyer definition, a spatial continuity, there's perfect spatial continuity because nothing is changing. Everything is the same. If I sample a single value right here, I can predict any other location within the phenomenon. And that's important. So, we will talk specifically about how we calculate and then how we model while interpret, calculate interpret and model verigrams over the next couple of lectures. I hope this was helpful to you. I'm always happy to discuss. I'm glad that recent Twitter response that there was a lot of enthusiasm around putting this course online. I do appreciate everyone's response and you know, I'm very open to people providing review and input to improve the course for everyone's use. Thank you very much. All right.
Everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin, and I put all of my lectures on YouTube. This is part of my course in introduction to data analytics and geostatistics. And in this lecture, we'll carry on on the topic of sampling bias and methods to mitigate sample bias. Please watch the previous two lectures that discuss sampling bias, its source, general ideas for mitigation, and then declusturing. Now we get into the idea of the biasing with secondary information. So let's introduce this idea of the biasing by showing a very simple example. Plan view again, we've got x and y, and we're looking down an area of interest. This darker outline is the area or maybe it represents a volume of interest over which we're trying to make an estimate. Calculate statistic, it could be average perosity if it's the subsurface. Then we have a location over which we say that in general we expect things to be higher, better quality. It's a better part of the subsurface, and these are all the samples in that region. That can't be good. Then everything else will be lower values, and you notice we have no samples available. None. Not a single sample. How can we make an estimate of the average perosity when we have no samples from the low part of the distribution? This is our setting for the application of spatial de-biasing to address this sampling bias. Let's put a little more details around this idea. Spatial biasing. What is happening here? Well, I'll draw a distribution right here. The shaded is the declustered, and the unshaded is the naive distribution, and we're changing the heights of the bars. Remember, declustering from the previous unit or lecture, allows you to wait the data, change the height of the bars, calculate the CDF, any sample statistic, anything you want to do. But the only corrective mechanism was waiting of the data, so changing the heights of the bars. What do we do when we're just missing data? In this example, we have no porosity values, less than 7.5%. It's just missing. In this case, you can't wait your way out of this problem. You can't use cell-based declustering. We need another method. That's spatial data de-biasing. All right, here's the idea. Now, this is an example modified the image from the book with Clayton Deutsch, appreciation to Clayton Deutsch. I believe this was his figure even from a previous paper. And so what we have here is a plan view of a reservoir. Now, this reservoir right here, it is an anticlimb. And so if you took a piece of paper and you bent it over like this, that's the shape of the reservoir. And what we would have as long the top here is the crest of the structure. Excuse me, I just used a piece of scrap paper to show that I've written all over that. And so imagine that structure in 3D. This is that crest, the highest, the shallowest part of the reservoir. And these parts right here are deeper parts of the reservoir. Now, you'll notice our well data here, here, here, here, here are only on the crest of the reservoir, the most shallowest locations in the reservoir. The problem is, is that often we have what's known as a compaction trend. That is the porosity goes smaller as we go deeper. Things become compacted. All of the void spaces get crushed up and you just don't have as much void space. Okay. So if we were to use these data alone in order to calculate the average porosity, we'd be biased. We would overestimate porosity. But our problem is we never sampled what we anticipate to be the lowest porosity parts of the reservoir. In other words, we never sampled the parts of the reservoir that were down on the fringes. On these sides that are deeper down in the subsurface. Okay. So what are we going to do? What's the process by which we can fix this problem? What we can do is we can look at the relationship between porosity and depth. And if we have a good understanding of that relationship and if depth is available to us at all locations in the reservoir, we can take advantage of that. Depth is now our secondary information. And we'll use it exhaustively mapped in addition to a relationship between depth and porosity to solve the problem to correct the distribution. That's why we call it soft data debiasing. Often it's called soft data debiasing. We'll just call it spatial debiasing or just debiasing is fine. Okay. We're going to extrapolate porosity data using the full depth distribution and the relationship between the two. Now it's a good thing and you might wonder while we were doing it. Why did we cover all of that marginal, conditional, joint distributions and probabilities? Because of here's an example right here why we need that information. What we can do. Okay. So let's first of all imagine what we have. We currently have only these data right here at shallow depths with high porosity. You see this is porosity. This is depth, shallow depth, high porosity. So right now all we have is this porosity distribution right here. Okay. Imagine we have many more data than I show. I've just shown these few circles here, but we'd probably have more. And so we have this distribution right here. What we want is the full porosity distribution, including the porosities we've never sampled. These porosities that we mistowed on. Now what we have also is if we look at the depth distribution, it's right here. We have only sampled the shallowest depths. What can we do? First of all, we have depth available at all locations in the reservoir. That's an easy to get spatial feature. Okay. So we have it everywhere. So we know depths real distribution. What we do next is we form a relationship between porosity and depth. Now in this case, we'll just assume that there's some type of linear compaction curve. That whatever distribution we have of porosity at shallow depths, we can extrapolate it along a linear curve. That's suggesting that as we go deeper and deeper, we have the same distribution. We're just shifting the mean lower. Okay. That's not a big deal. Okay. So what we'll do is we'll take this conditional distribution. We'll extrapolate it in a linear manner to cover the full range of depths. And then we can use that to map across and calculate the full porosity distribution. Okay. Let's give a little more details around that. What we're going to do for this. We got to map the secondary feature, which is depth at all locations. We'll have the full depth distribution. We'll develop a bivariate relationship between depth and porosity. We'll assume that it's some type of linear function. It could form. It could be any functional form. We can generate the distribution of porosity over all locations in the reservoir by combining all of the conditional distributions that have been extrapolated. We have a conditional distribution here. And we could map it down to here. Map it down to here. Map it down to here. You see that we have the conditional distributions over very shallow depths, kind of more deeper depths, getting very deep. And so and we can go ahead and map across all of those conditional distributions to calculate this joint. We've already talked about the concepts of calculating the the marginal distribution or the joint distribution or the conditional distribution. In this case, the operation looks like this. The marginal porosity PDF is going to be simply the integration of the conditional porosity given. See that? That right there. Porosity given a certain range of depths multiplied by the actual density at that range of depths. And we're going to integrate over all possible depths by doing that. We actually solve for the marginal porosity. This is usually accomplished by discretization. What what people will do is they'll take and the discretize the depth into multiple bins. Those solve for the conditional distributions in each one of those bins. So right here, here, here, and here, and maybe here. And then simply apply this as a summation of the conditional multiplied by the individual marginal depth probabilities from this function in order to effectively wait all of these conditional distributions and combine them together. So this is usually done by simple discretization instead of integrating over the full distribution. Now, if you'd like an example of this approach of de-biasing, I actually have an example and a well-documented Excel sheet. And I previously mentioned this in the last lecture for declustering, but it does include a tab that actually has de-biasing. So if you go to this link right here, this is my GitHub account. In my Excel numerical demos, there is a declustering and de-biasing demo. Now, may record a video shortly that actually just works through and talks about that form of de-biasing and its demonstration in Excel. All right. So I hope that this was helpful to you. This concludes the overall set of lectures on spatial bias covered sampling bias and how do we mitigate it declustering and now de-biasing approaches. I hope this was helpful to you. I'm Michael Perch. I'm an associate professor at the University of Texas at Austin where I teach and conduct research on data analytics, geostatistics, and machine learning. If you're working for a company and you want to partner and support student research or you want to learn more or you want to get some of my help, go ahead, just contact me. I've been told by some of my friends in industry that I'm one of their favorite professors. I actually pick up my phone. I'm happy to help out. All right. Everyone, take care. Bye.
Howdy everyone, I'm Michael Perch and I'm associate professor at the University of Texas at Austin and I record all of my lectures and this is part of my course Introduction to data analytics and geostatistics and last lecture we talked about the issue around spatial bias spatial sampling bias and specifically how we have it almost every time we deal with sparsely sampled spatial data common for the subsurface of course where we're bit focused and So what we'll talk about in this lecture is one methodology to correct this problem known as declustering so let's talk about the concept of Clustered samples and how this occurs and how it impacts the statistics Let's make an estimate over an area or volume of interest in the subsurface So it's a spatial problem. We got x and y. This is in plan view two-dimensional representation It could be a three-dimensional body doesn't matter Now we have this area of interest. I drew an arbitrary boundary just like this and I have my samples available to me So I have one through seven samples Now I want to calculate the average velocity. Why would I want that average velocity? I want to calculate the oil in place over the sitting area of interest The entire area of interest. So the average velocity is part of an important predictive model for this volume or area of interest All right, so let's take a look at these samples and ask ourselves a really important question Are you comfortable using The average of the values at each one of these samples as the average Phorosity over this area of interest. Do you have any concerns? Do you think it would be fair? Do you think it would be potentially biased? Okay, let me give you a little bit more information and tell me if that would influence your decision I'll tell you now that this part of the reservoir is Systematically higher. It has a higher porosity and this part is all lower porosity Now if we were to take those samples and just calculate the average we got one two three four values there And we have three values here. This is a smaller area. This is a larger area with only three values Can you see how that would cause a bias? Now Let's go ahead and imagine what would happen if we were to continue drilling And the drilling looked like that Now are you even more concerned about using the average than naive raw average from all of those data as the representative average of that area or volume of interest? I think I'd be concerned too. I'll tell you what we love good news We love drilling where things are really good over and over again because we'll get the best production rates We'll get the highest mineral grades if we're worried about minerals if we're producing water from aquifers We'll drill where we get high production rates of water We love good news and so there's plenty of good reasons why we'd have more samples in this location here We've already abandoned the idea of random sampling and now we're just dealing with the data that we have available to us Now what would happen if we tried to use the well average the naive average porosity To represent the average over that area of interest Well, this dotted line right here could represent the actual true porosity average over that entire area of interest When we drilled the first couple wells we were already somewhat more densely sampled in that high area So we'd imagine we started out with a bias We drilled a couple more wells in the lower area And then as we continue to drill we kept drilling more and more in that high area And so as we had increasing number of wells The well porosity average continued to go up and this difference right here is a bias We were overestimating the average porosity because we kept drilling in that high area We were clustering the samples in the good locations Nyeev's statistic the naive sample average will become even more and more biased as we collect more and more data Clustered even more in the good or the high area We need a method that corrects for clustered samples and gets us a more representatives statistic Okay, so let's talk about how to do that with a methodology will describe as declustering Okay, so what we have in this example right here. Let's give ourselves a data set to demonstrate this concept And so what we have is we have an exhaustive data set with porosity at every location Now recall back from our basics statistics concepts discussions the map of all porosity values At a sufficient resolution is considered to be in fact the population So this map is the populations all possible porosity values And so this right here this distribution is the distribution of porosity over all of those locations This should read porosity population I called it realization because I used a geostatistical method to make the population and we'll talk more about that later Then what we actually have available to us are these samples Now I hope everyone can see these samples here regularly sampled But then we're missing some samples you notice that preferentially We have sampled in a more dense manner in the highs And in the low values Because of those missing samples we have a sparser sampling We're not quite having the same sampling density in the low values So if you look at the sample distribution right here What you can see is that it looks a little bit different now you don't have to just trust me Let's look at the summary statistic the average that's the thing that we're concerned about predicting The true average arithmetic average or mean is 10% porosity So if I calculate the average of this distribution it's 10% porosity The cluster sample these samples that are shown right here at these locations in this distribution right here Is 10.48 10.5% The error is about 5% error in the average of the porosity Now you might ask yourself does an overestimation of 5% In the average of porosity actually matter and what you find from many reservoirs around the world overestimations of average porosity by 5% resulting in an overestimation of oil in place By 5% would probably be significant and could impact the way you plan or even Though decision to produce a reservoir or not or to develop an asset or not And so we don't want to have bias just because the way we sample and it can get worse and worse and worse in some settings I've seen it go up as to as much as 15 or 20% overestimation because of clustered sampling Okay, so what are we going to do about it? In the last lecture I suggested we have two methodologies we can use to correct for bad sampling and this methodology that we'll explain right now is known as declustering declustering is used specifically when the problem with sampling is clustered sampling samples Which I just demonstrated where you have sampled the good stuff or you could sample the bad stuff With greater density So what we're going to do with the clustering techniques is we're going to assign to each data A weight based on the closeness of surrounding data In other words if you're in a location if that data is sampled in a location where there's not very much data rounded it's sparsely sampled in that location It should get a high weight. It's more representative of more area or volume If the data is in a location where it has a lot of data around it. It's very densely sampled around there It's not providing very much new information. It should get less weight It's not as representative of the volume or the area Okay, and then what we can do is we can use these weights We'll calculate a weight for every one of the data. We got a column with all of the values of the data and now we have a column with all of the weights of the data weight for each one of the data And we can calculate any summary statistic like the average With those weights included a weighted average and I'll show you how to do that shortly You can even calculate the entire histogram CDF PDF histogram using those weights too. So let's explain declustering in the next lecture will explain Debiasing techniques Okay, so the first methodology we have for declustering is cell declustering This is a method for calculating the declustering weights by using a mesh of cells So what we're going to do is we're going to go ahead and take our data set Now I hope you don't get confused. I did for this example here I took from my book an example which had different data than what I've shown previously But we'll go back to that original data So don't try to map a motor figure it out But this is just illustrative right now So we take the data the area of interest. This is x. This is y. We put a mesh of cells on top And now we have to calculate how much weight to put on each one of these spatial sample data So what we'll do is if there's a cell and there's only one data inside the cell That data gets full weight. It's going to get one weight If we have a cell with three data it gets one third weight A cell with five data one fifth weight and so forth Now you'll notice that we're doing one divided by the number of data within the cell As a amount of weight and then we're scaling it by the total number of data Divided by the total number of cells with data in them Don't worry about that. It's not a big deal This scaling Just ensures that the summation of all the weights to all of the data Is equal to the number of data And that's a nice consistent way to do this because now what we have Is we know the sum of the weights over all the data is going to be equal to number data And we know that when we have a data with weight equal to one that's nominal weight It means that that data is sampled perhaps in a location where We're not dense We're not too sparse. We're not too dense. We're at the density of sampling We would expect if we had regular sampling that would be weight of one And if we're going to be less than one it tells us we're taking data that's from a location where Perhaps we have very dense sampling And if we've a weight greater than one we're taking data from locations in which we have Perhaps sparser sampling All right, so that's the cell-based methodology that we're going to employ now As I explained just now what we would expect is that if the declusturing weights are equal to one for that data That means nominal weight Less than one reduced weight because the data is densely sampled. It's too densely sampled in that spatial location greater than one it indicates that we have increased weight because sparsely sampled now. Let's go back to the original example that I showed previously If we were to go ahead and take that data set that I showed you with some missing data and calculate the Cell-based declusturing weights will find a weight of one is this I don't know kind of a pink color here And these ones right here might be data locations with a weight of one Then what you'll see is a weight less than one are these blue colors right here darker colors and look in the area where we have very dense sampling we have weights less than one And then what we'll also notice is that when we have Data in very sparsely sampled locations like that's a perfect example and that's a good example there The weights are greater than one We can also take a look at the distribution or the histogram of the weights And we can confirm that we range somewhere between about point five to point one point five And that'll vary depending on the problem that we're working with But right here in the middle about a weight of one these bars here represent nominal weight Okay, now I should mention that some software programs that do declustering Actually use a convention of the sum of the weights equal to one Not a big deal. It's just a different convention in that case the nominal weight is one divided by n You can see that so it's it's not too bad now. Let's look at what happens when we use our declustering weights What we can do is we can update the distribution the histogram using declustering weights Here's the original porosity samples. We would call this a naive histogram We haven't accounted for spatial sampling bias with declustering And we look at that and we have the true mean was porosity was 10% the clustered sample mean was 10.48 error 4.8 percent we saw this before Now what we can do is We can go ahead and take the data values and the column with the weights Now what's fascinating is that many of the methodologies for calculating statistics within software packages or plugins in Python Actually allow you to include weights with the data why Many people have been doing this trying to find spatial Waiting or other types of waiting on data to device data people who work in hydrology do it all of the time and other types of spatial Spatially based scientific fields So we can calculate any statistic weighted in fact if you think about this histogram right here This histogram if you look very carefully it's frequency Frequency of one that means we had one sample at about 6% we'd one sample at about 14.5 percent or so We had six samples remember so a histogram is just showing frequencies And if you look very carefully 1 3 5 4 you see how they're all integer frequencies they all reach These points right here 1 2 3 4 5 6 and so forth and that makes sense. It's a simple frequency plot Now look what happens when we go down here What we've done now is we've included the weights So you see this value right here this bar right here used to be 1 Now it's 2 points something you see it's even going above 2 Because it had a weight of 2 points something You notice how this bar right here used to be about 3 it looks like it's under 3 now. It's gone down just a little bit This has gone up. This has gone down. This has gone down. This has gone down The bars on the left hand side have generally gone up the bars on the right hand side have gone down And the result is when we calculate the declustered mean Now look at this. This is not a big deal We can calculate the declustered mean as simply the sum of the weights multiplied by the data values over all locations i Through n samples and all we have to do is divide by the sum of the weights If we do that we get the weighted average Okay, what's really cool about that if you calculate the declustered mean What you'll actually get is a declustered mean of 10.07 Now that's awesome because if you looked at the if you compare it to the clustered sample mean of 10.48 With 5% error above 4.8% error rounding up And now you look at the true mean of 10.0 We now have an error rounded up to 1% actually 0.7% error We've dramatically decreased the error in our average by using cell-based declustering weights So this is really epic. This is really cool And so by using this methodology we can greatly improve or mitigate the issue with error now I want to give just give you an expert tip And that is I can't guarantee that every time you do declustering That you will in fact improve the accuracy of this statistic There's many pathological data configurations or cases for which it won't improve it But in expectation Statistical expectation now in other words On average as you use declustering you will move in the right direction you will improve the accuracy That's what I can guarantee for you Now there's a really important issue here and a lot of people pick up on this as soon as I Show this figure right here You look at the cells and they say to themselves But what if I change the size of the cell How will that affect the data weights? And you're exactly right it will affect the data weights Let's do a mental experiment right now What would be the weights if I was to make the cell size so big that all of the data are in the same cell Just think about that What would happen is all of the data would get a weight of one And you would estimate with the naive average your weighted average would just be the naive average now Imagine the opposite end of the spectrum make the cells so small That everyone the data have their own cell What would be the data weights? They'd all be one again And you would estimate with the naive average again And everything in between would change from the naive average So what we do in practice is we run the declusturing algorithm over a range of cell sizes And we use a plot to pick the best cell size So the standard declusturing software will actually run over a range of cell sizes Going from very small to very large and we'll calculate the declustured mean At each one of those cell sizes now we know that if the cell size is very small We're going to have a declustured mean equal to the naive mean So that naive mean is right here we could draw a line across right here And as the cell size goes larger we're going to approach that naive mean again Now we didn't go far enough if we'd gone larger and larger these dots would have kept rising up Then what we do is we look at the shape of that curve And we pick the minimizing declustured mean in the case That we believe that we have Preferentially sampled or clustered our samples in the high locations Now don't worry about this when you produce this plot it'll be apparent If you had preferentially sampled the Low locations this plot would be inverted the values would come up and back down again Why do we pick the cell size that minimizes the declustured mean It's a conservative engineering choice It's the most conservative choice we can make Now I'll tell you right now There's no theory to say that that's absolutely the best choice It's an engineering choice that we make At the same time we should recognize the fact that the results may be sensitive to large scale trends So if you do have knowledge by visual inspection of the data knowledge about the way the data was sampled About overall clustering scales or you could employ some other type of sensitivity study that would be very useful I'm not going to get into those details for the purpose of this class Let's stick with the idea of using the minimizing or maximizing cell size Now just to give you a little deeper knowledge about it though can't help myself You could choose the cell size so that In the sparsely sampled areas or parts of the area of interest that you get about one data per cell And in a densely sampled areas you would get more data per cell That would actually be a good choice of cell size But I don't want to get into all of those details and confuse people right now That's a lot of tradecraft. Let's stick with minimizing and maximizing Now many students who see cell-based clustering get a little worried because of this issue of the exact location of the cell mesh Look at this data value right here All happy one weight just a king of the world getting all just getting maximum weight But it's right at the edge of the boundary If this cell mesh was shifted up just a little bit It would share itself with other data and it would be demoted and get less weight Hmm, this is a problem. We don't want our result to be sensitive to the cell mesh origin the location of the cell mesh What are we going to do? We'll use a good statistical solution We'll randomize an average that'll remove the sensitivity. How do we randomize the cell mesh? Easy We'll go ahead and we'll take random locations of the cell mesh just 10 different locations of the cell mesh will repeat the calculation and we'll take the average weight over those multiple Wates calculated for each one of the data So this data right here would have 10 different weights coming from the 10 different cell meshes Calculate the average of that now you have a much more robust measure of declusturing weight That's not sensitive to the location of the cell mesh Let's just make a couple concluding comments about cell-based declusturing It's going to be sensitive to the choice of cell size We're going to use the minimizing declusturing mean methodology You could also try to select based on data configuration I gave a couple tips on that but let me specify further For this class, we're going to use the minimizing maximizing You're going to calculate the cell Based declusturing mean over a wide range of cell sizes and take the minimizing or maximizing based on the shape of the plot We're going to remove the sensitivity of the exact cell mesh location by averaging over multiple cell sizes That's a parameter in the algorithm. It's going to ask how many a big enough number is fine 10 20 is fine Has low or little sensitivity to the data boundary Now that's very very interesting nowhere in this methodology. Does it ask what is the edge of the data What is the edge of the area of interest? For that we're going to use another form of declusturing that is much more sensitive to the boundary That methodology is called polygonal declusturing And there should be no surprises if you were coming to this content from hydrology or other areas spatial sciences Where they do Debiasing through waiting this is the common methodology Now what do we do here? We're going to split up the area of interest Here's our area of interest right here the data points are shown as the black circles that are filled We're going to split it up into voinor polygons or partitions Not a big deal All we have to do is take perpendicular bisectors between all of the data and intersect them up Now another way to think about this is we're actually segmenting the space such that if I picked up Location anywhere in this polygon the closest data and I just mean you chlidean distance the closest data is this data right here Any location here the closest data is that data there That's why we call them polygons of influence. It's Over what areas or volumes of the subsurface would I consider this the closest Piece of information. Okay. Now what we can do then is once we created our polygons and this could be done in full 3d not a problem We can go ahead and calculate weights for each of the data Just proportional to the area of the polygon where you have large area you get greater weight You're representative of more part more of the subset of this of the area of interest So in this case we go ahead and we calculate the weight at a location as the area Divide it by the sum of the areas or in other words the Total area of the area of interest or this can be done for volume of course Now you can see immediately that for this methodology the sum of all of the weights is going to be equal to 1 Just consider all of those areas J equals 1 through n and if you were to go ahead and put them through this equation and sum them up You'd see immediately that's just equal to 1 now another convention just like we talked about before Is that the sum of the weights actually equal to the number of data and for that all you have to do is put n in front here And the result is that the Some of the weights is equal to the number of data and we're back to that situation that we discussed before where nominal weight is going to be equal to 1 Now for this methodology up here recall nominal weight would be equal to 1 divided by n indicating data that are neither Sampled in a manner locally that's two dense Nor two sparse now this methodology is very sensitive to the boundary imagine this Case right here you see how much weight this data right here was getting What would happen if we were to expand the boundary out here These polygon this polygon here would get bigger and bigger and bigger now imagine we made the area of interest come All the way out there now if we were in the classroom right now I would tell you to imagine that we took the boundary all the way to taco joint the taco restaurant I go to almost every day on campus there Those were great days anyway, so in that case we would have a massive All area for that polygon and the weight to that data would be very large Okay, so that's the polygonal based declustering Let me just comment people don't typically calculate the perpendicular bisectors and the intersections and to form these Voronoi Partitions or polygots It's computationally. It's a geometric operation. That's a little expensive What people often do is a numerical form of approximation through discretization Super simple what we do is we just form a nice fine mesh of locations Many many locations like a thousand by a thousand or a hundred by a hundred And then you go ahead at every location you find the nearest data Find the nearest data find the nearest data If you do that and you calculate the number of locations on that fine mesh that have this as nearest data I hope you can see that that would be an approximation For the area or proportion of area or volume within each one of these polygons Okay, so a numerical discretization is often applied Applied to solve that problem practically Okay declustrits statistics. I told you already that we're going to calculate the data And then we're going to also have weights roll of the data and that's great as soon as you've done that You can calculate any sample statistic and our sample statistics going to be our predictive model here And so and that sample statistic will be declustered And you hope that it's reduced the bias Okay, so for means I already showed this already you take all of the data the sum of the weights time the data times the data Divide by the sum of the weights and if you do that you will get the declustered mean Sample variants all we had to do is extend the equation for sample variance by including the weights multiplied by all of the square differences between the values minus their means Here for generality we show the sum of the weights minus one now We're assuming the convention of the sum of the weights equal to n here So not a big deal. This is equivalent to just n minus one here And we can go on we can carry on we can calculate the covariance now in the case of the covariance here We have the products we've expanded the variance talk about how two variables covery or vary together I should say And so we've expanded variants now we have the value the sample Minus the mean for one feature and for another feature the paired sample minus its mean the product of those All we have to do is apply this weight and once again instead of n will put the sum of the weights right there We're assuming once again the concept of these sum of the weights equals to n Now you can do waiting on the entire cdf We already showed the histogram not a big deal if the sum of the weights is equal to one Then we can go ahead and calculate the cumulative distribution function As just taking the sum of the weights for all data values Less than a threshold and if you do that that'll give you the probability of being Less than that value super super cool really easy to do And so we're able to go ahead and calculate cdf. So any statistic can be weighted And so this is very powerful and our statistics are predictive model once again Now just in case you'd like a little bit of practice. I think I'll record a video on this later But I do have on my github account an example of declusturing In excel that actually it's pretty cool. It takes a data set and it'll build a mesh Calculate the weights and give you the naive distribution the declustured distribution the cdf's and also the declustured averages and the naive averages So it can make comparisons and you get to play around the cell size and see how that impacts the result Now a much more rigorous approach would be to demonstrate to you how to do this Using an algorithm that gives you the full outcome the full result the Multiple declustured means over all of the range of cell sizes and so forth the full workflow In fact, I have the exact same workflow and data set that I showed in the course notes in a really nicely Documented Jupiter notebook available on github. So great opportunity to get a little Python practice and to play around with an important Spatial data analytics data preparation step of spatial declusturing Let me just make a couple of comments on Cell-based declusturing as we finish up here We can I showed cell-based declusturing in two dimensions You can do the whole thing in 3d But what I'd say is if the wells are vertical It is a two-dimensional problem You really don't need to do full three-dimensional declusturing you can calculate one weight and use the same weight along the entire vertical well The sampling density is not changing vertically if the wells are vertically Now You could deal with full three-dimensional declusturing not a big deal now you have cells that are Three-dimensional. That's not so bad. We'll do that when we have horizontal wells highly deviated wells The problem is a three-dimensional problem when it comes to clustering The wells may be moving together in a way at different locations vertically So the clustering is going to vary vertically. We've got to count for that I should comment to what we've shown has been isotropic two-dimensional cells Now in 3d we'd want to use Non-isotropic in other words the extent of the cells would vary vertically and horizontally The choice of that Cell size in each dimension is really related to the geometric configuration of the data The sampling rate of the data if you have a lot of data in the vertical direction at every half-foot as we often do Of course, you're going to want to use the cell size Ratio such that in the vertical direction you have much smaller Extent then you do horizontally. You'll use the ratio of the data spacing If the data tends to be a kilometer apart in the horizontal direction But about one meter apart in the vertical direction about a thousand to one ratio That's how we work that out that was kind of more advanced details here But we'll just focus kind of more of these simple two-dimensional examples like shown in the code right here now Every time I teach a subject like this I get students asking questions about Uncommentions often have much more dense data sense And they say while I get declusturing in the deep water or other settings where we don't have a lot of data It seems very sparse but in dense spatial data sets or more dense spatial data sets Do we really need this and so what I have right here is an example I have the Haynesville the Barnette and the Fayetteville and I worked this out I took publicly available data And I looked at the initial production rates averaged over the first three months And I went ahead and I calculated them over modern wells I think they were recently drilled long horizontal mountain multi-frack I did a lot of filtering to make sure I was doing something pretty fair as far as pulling the data I produced the distributions and I looked at naive statistic the average initial production And I looked at the average declustr weighted Initial production and when I compared them what I found was 4% overestimation using naive statistics versus declustrid 5% 8% and I had some other examples that were above 10% So What do we find even in more densely sampled unconventional settings? We still have an issue with spatial sample my advice to you as you go out as a professional geoscientist or engineer Assume all spatial data sets are sampled in a bias manner and address this All right, I hope this was helpful to you That's the end of my discussion around declustering I will next get into the topic of Debiasing where we use secondary information to assist us In correcting for bias remember in the case in which we have failed to sample Part of the distribution we can't use declustering weights. We need a different technology And I'll get into this next All right, I hope this was helpful to you. I'm Michael Perch. I'm an associate professor at the University of Texas at Austin where I teach and work and conduct research on Geostatistics data analytics spatial data analytics and spatial machine learning I hope this was helpful to you everyone Take care
Well, this is a fun lecture. We're starting to get into spatial statistics. And so we got to cover some fundamental principles before we can start getting into quantifying spatial continuity, spatial estimation, spatial simulation, and so forth. We've got to kind of build some basics. And so this lecture will be fun. We'll get to get into those basics that we need to cover. And so let's get into it with some comments. First comment is that this is all going to build on past lectures. I had a student that was actually really interested in it. They asked me last year, they're like, well, how does this all connect up? And it does. Everything we're doing still relies on probability theory. We're still using univariate and bivariate statistics, sampling the bootstrap. All of these things still matter to us as we continue to proceed. Okay. So it's all building up. So let's provide a concise and practical definition of the random variable. We'll get into random functions. And then we'll spend some time looking at pretty pictures of brick walls. It'll be kind of fun. Okay. The random variable. So if we do not know the true or actual value at a location or time, and it can take, therefore it takes a range of possible values that can be fully described by a PDF or conversely by the CDF, that's a random variable. So I'll give you an example. A random variable we will represent it with a capital letter variable. So x capital at location u, u bold is the location vector index one. So our first location will have can take on any one of these possible values between five millimeters to 20 millimeters of grain size. And it is completely defined by its PDF, which is shown here. So this is a random variable at a location in space. We don't know the actual value. We might have some information that helps us constrain what it might be. And so we provide a distribution of possible outcomes at that location. And that is describing that location as a random variable. So that's a random variable. So far so good, right? Just we don't know it's uncertain. It can take on multiple possible outcomes. Okay. What's a random function? A random function is really not that bad. It's a set of random variables correlated over space and or time. We're still going to represent it as a upper case, but now it is a suite of random variables at distinct locations. We're going to now we could sample from that random function or those individual random variables. And when we do that's a realization. And I'll talk about that right away, but that would be expressed as a lower caps or small caps variable. And so in general, our spatial context, we could describe data or realizations x at u1 through un are a number of locations that we have data or that we have realizations. And they could or if they are realizations, they could be sampled from a random function described at these locations. So let's visualize that random variable at location u1 random variable at location u2 random variable at location u3. And if they're all jointly correlated with each other, that would be considered to be a random function. This is very fundamental because the use of random functions and random variables is exactly how we accomplish so much within geostatistical modeling of the subsurface. It's all built on these random variables random functions. What is a realization? An outcome from a random variable or a joint set of outcomes accounting for correlation from a random function would be considered to be a realization. Once again, it's represented as lowercase variable. So little x in the spatial context will continue to use the idea of u with an index to indicate a spatial vector at multiple locations, one through n locations that we're concerned with. So any type of resulting simulation from a Monte Carlo simulation to a sequential ghost in simulation, which is really a set of Monte Carlo simulations, any type of method that samples jointly from a random function or individually from a random variable, this is creating realizations. We will in general consider our realizations to be equal probable, but that is not necessary. We could of course have distinct scenarios which we say some are more likely than others. All kinds of things can happen when put our uncertainty workflows together. We'll talk much more about uncertainty later on. So if we have a random variable and it has its associated PDF indicated here, if we draw a single value just using Monte Carlo simulation, we already talked about Monte Carlo simulation. That drawn value of Monte Carlo simulation we call a realization. Now, if we have a random function and so I've tried to represent this, probably not, it's not a great looking figure, I'm sorry, but we have multiple locations u, one through four, and I indicate it with a box and whisker drawing the concept of a PDF at each single location, one through four, it's getting narrower as we go from one to four. Suggesting that perhaps we have maybe some type of data constraints or something going on over here and it's causing less uncertainty as we closer to data more as we get away from the data. Okay, so a realization from the random function would be jointly sampling from all four of these random variables such that we're counting for correlation. You notice how I've indicated here that we sampled high here for location one, high here for location two, maybe a little bit more medium for location three and four right in the middle. They're not willy-nilly just kind of jumping up and down. I'm trying to show that there's a degree of correlation going on here. Realization number two, Monte Carlo simulation from the random variable, we've got another value, that's realization number two, realization number two from the random function. We've got four more values that are jointly sampled, counting for correlation between each of them from the random function. Kind of cool, I could do this all day, I could keep drawing realizations and we would do that as many are needed in order to perform our calculation, our analysis. We'll talk about that live. Concepts of random variables, random functions, and realizations from a random variable or random function have been described so far. Okay, let's talk about stationarity. What is this stationarity thing? So the problem is this. In order to do any type of modeling prediction so forth, we need statistics. In order to calculate any single statistic, you need replicates, multiple samples that we can work with. And so in many of our problems, air, water, beer, anything that we're monitoring a line, we can turn a tap and we can extract a sample, we can come back later that day, turn the tap, extract a sample, and we can do replicates over time. Many of our problems, we have that opportunity to do that, and we can formulate our statistics, we can talk about predictions and inferences, everything needs statistics to do that. In our geospatial problems, things are different. We don't have repeated samples or replicates available to us in space. If you were to drill a hole in the earth and to extract a core sample that looks like this from that location in the earth, if you go back to that location later that year, or the next day, or a decade later, what will be there? The answer, a hole in the ground. You've extracted the sample, no other samples available for you, you do not have replicates in the spatial context. So what does this mean? It means that we have to make a decision. If we don't make that decision, we're stuck. So instead of time, turn in the tap, get some beer, check the clarity, the color, whatever they measure on beer, or on oil, or gas, or water, and then come back and do it over and over again. Instead, we must pull samples over space to calculate our statistics. This decision to pull samples over space is what's known as the decision of stationery. It is a decision. I don't want to get too technical here on all y'all. It's a decision that all of that stuff that you pull together and sample is the same stuff that we can treat it as the same and we can extract statistics from it. Yeah, very interesting, right? Okay, so let's get into more details about stationery. What can you do if you do not make a decision of stationery? If you say, no, no, no, that's way too much. We don't want to make those types of choices. We can't know that. I'm just going to sample at all of these locations within an area of interest, but I'm not making any decision of stationery. If you do that, what do you have? You have a table of samples at specific locations and you know nothing about what's going on in between. You know nothing about the general relationships between them. You are effectively stuck in the hole, in the well bore, in the drill hole, whatever it is, you're stuck at those locations. You know nothing about what's going on between. You can't calculate any statistics. You can't make any predictions about the behavior of the subsurface between the samples away from the samples and so forth. You are very much stuck. So we can't get away from it. We're going to have to make a decision stationery, but don't worry, I'll be fine. So let's talk about some multiple components to try to clarify furthermore on this decision of stationery. First, we have what I would deem as an import license. That's our decision of what data to pool together in order to evaluate the statistic of interest. Take our example here. One, two, three, four, through seven data. We pool them all together and we get a distribution that we can now use in order to support everything we're doing with decision-making and modeling. That choice of exactly what data we could pool together that one through seven was the import license. That's the decision of what data to put into the statistic. We're saying they all come from the same stuff. They can be pooled together. We could decide that one is different than the rest and we could go ahead and remove it and work with two through seven. We could add 8, 9, 10, 11, 12 and so forth. This is a pretty important decision. The ORC export license is the choice of where we can use that statistic. Once we have that statistic of the property of interest, the metric of the statistic that we're worried about, where can we use it for the purpose of prediction and modeling? I draw a dash line. Maybe we say that we can use the statistics gathered from these data within this dashed area. Perhaps it has to be smaller. Perhaps things are pretty homogenous and well-behaved and we can go even further away. That is the export license. That's the component of stationarity, but where we can use the statistic. Well, so let's give a couple of formal definitions for stationarity. This one, even though I'm an engineer, I do have a strong interest in geology. I have quite a bit of experience and so forth with the geologic context and the data source and so forth, so I will go ahead and write something like this. The geologic definition stationarity is that the ROC over the stationery domain is sourced, comes from the same provenance, deposited, the sedimentary depositional type of processes are very much the same, preserved the post-depositional diagenetic type of alterations are very similar, and that this demean, this part of the surface that I consider to be similar, is mapable and maybe used for local prediction or as information for analogous locations within the subsurface. Therefore, it's useful to pull information over this expertly mapped volume of the subsurface. I think that's a very careful definition of stationarity paying attention to the geologic context. And so I show an outcrop figure here. You can see different units, perhaps a deep water complexes, complex sets, or so forth, and they have distinct different types of rock properties, and so forth. And maybe we identified these as being stationary domains based on what we're trying to measure and what's the problem we're trying to attack, credit to André Fuldani, a good friend of mine over with Equinor, and his other co-authors for this work. I believe it's out of the Chile outcrops, beautiful crops, turned around with them before to look at those. Thank you, André. Now, if we were engineers about it, we would use a more statistical, mathematical definition of stationarity. But I hope you can see that there's not a disconnect between the two. They are mutually supportive. And so stationarity would be the metric, the measure, the statistic of interest, is invariant under translation over this domain, this space. So you could consider a variety of different statistics, and we could write them out mathematically based on the concept of stationarity. The expectation of the variable of interest, Z, it's a random variable, at various different locations, would be equal to M, a constant. It does not change. It is the mean is invariant under translation for all locations that were interested in. The distribution, F of U, U being the location vector, Z being the property of interest is only a function of Z. The location is not important over the locations we look at, the CDF, F capital is the same everywhere for the variable Z. The semi-verogram could also be considered stationarity. That is the semi-verogram of the property Z is only a function of the lag vector H. And we will talk about semi-verograms pretty quickly here. And has nothing to do with the location over the locations we're interested in. So for stationarity, the mathematical definition will always contain two parts. What's the metric? Or I should say the statistic. And what's the volume or the region over which we are considering? We could extend this to any statistic of interest, including FACES proportions by various distributions, multiple point statistics, my favorite, 13th percentile, or Hertosis, whatever statistic we're interested in, transition probabilities, and so forth could all be deemed stationarity. In fact, in the spatial context, we must make that determination in order to calculate those metrics. Stationarity is a decision. It is not a hypothesis. Why do we differentiate that? If it was a hypothesis, we would have a statistical test for it. We do not have a statistical test. The best we can do is we can work with the data with geologic context, expertise, all put together, and we can demonstrate it our assumption. Or a decision, I should say, of stationarity is appropriate. In addition, stationarity depends on scale. This is a figure from the book that Clayton Deutsche and I worked on and published back in 2014 on geostatistical reservoir modeling. In this figure, what you can see is that we have some type of spatial phenomenon, just a one-dimensional spatial phenomenon. We have clearly non-stationaries going on. We map those non-stationaries or transitions over space with a dash line. If you zoom in, what you find is that your evaluation of what was non-stationary, what was variant over space would refine. You would start seeing more, more details. In general, part of the stationarity decision has to do with what is the scale that you're looking at, what scale you're evaluating the spatial phenomenon. Let's go ahead and take an example. We can look at this concept of scale and its impact on the decision of stationarity. Anyone out there who is from the UT Jackson School of Geoscience should immediately recognize this as the wall across from your building when you're sitting out front, having a nice relaxing time sitting in those tables, look across to this wall in the building beside you across the way, and you will see it. It is a really interesting wall. It has a lot of different features. Let's think about this. Is this wall stationary? Just sitting back, looking at the entire wall all at once, is this wall stationary? Is the part where you think about it a bit? Many of my students today or a couple days ago thought that this wall was stationary, and then I asked the question, what was the metric? What were you looking at? It was cool. A lot of students were pointing out that there were these Vogs, which I thought was pretty cool. The engineers knew to say Vogs. It might have been the Geoscientist or two in my class, but there's Vogs. Some students were looking at the colors of the bricks, the size of the bricks, the aspect ratio of the bricks or whatever it might have been, and they looked at the image and they thought, well, it doesn't seem like there's more or less. It just seems kind of more homogeneously, heterogeneous, and so they all agreed after some discussion that it was stationary. Well, zoom in. Let's zoom in right here. What about this? Is this stationary right here? This is the part that was interesting. A lot of my students said, well, no, it's more Vogs over here, less Vogs. Here are these bricks or lighter, long aspect ratios. These bricks are kind of lower aspect ratios and darker colors. They start to say, no, this seems like it's kind of a non-stationary type of image. I said, okay, let's zoom in again. You zoom in on that one brick. Is that stationary? Now everybody recognized that, wow, if you ignore the mortar right there and just look at the brick, that's a very, very stationary image. Then I said one more zoom in. Everybody agreed that this is highly non-stationary. Big Vogs here, kind of smaller Vogs here, a big old inclusion of some sort, some class here within the rock. What everyone realized was that as we go through four different scales looking at this rock across from the Jackson School, we found, or this brick, I should say, across from the Jackson School, you can see that there we go from stationary to non-stationary, to stationary again, to non-stationary again. So it really seems that the scale is very important. Okay, there's many people who hear this discussion about stationarity and they're concerned. And you should be a bit concerned because this is clearly the heart, the kernel of the subjective component of modeling the subsurface. And so people might be disturbed by that or uncomfortable with it. And so here's some words that will not reassure you. Specifically, you cannot avoid the decision of stationary. You really only have a couple of choices, extreme cases. You could say no stationary decision, then you can't move beyond the data, you can't populate, well, you can't pull any data to calculate any statistics. You can't do anything to make predictions. Conversely, you could assume, just say, well, okay, okay, it's just all stationary. I'm not going to be too committed. I'll just say everything is stationary. That's a very, very strong assumption. If you start to say that all data over large volumes the earth is stationary. This is a very naive way to look at the earth. And you'll find that the result will be uncertainty models, which will again turn certain models soon, which will be so broad and not very useful for the purpose of making decision-making. So you've got to walk that line, you've got to do something in the middle, you can't hide your head in the sand, you've got to do something. Once again, geomodeling stationarity is a decision over what region you're going to collect data together, your import license, what region you will then use the resulting statistics from your data that you export license. Now, don't be fearful because the world doesn't have to be stationary. We can find non-stationaryities or things that are varying to space. And like the dash line in that one-dimensional example, we can map that. We can model that. We can use secondary information from seismic. We can use all kinds of things to help us. We can map it, put that into our model, and then work with a stationary residual with our statistical and stochastic as we will talk later stochastic approaches. You could also treat those trend models as somewhat uncertain, have multiple scenarios. So we are not hemmed in. We have a lot of options for dealing with non-stationarity. The point is we have to understand stationary and we have to work with it, work with it's framework, use it to be able to collect statistics, to build statistical models of subsurface. I always like to give this caveat right now. Good geological mapping and data integration is essential. I'm never suggesting that this is the purely statistical problem. And the decisions of stationary will form the framework of any subsurface model. So this is critical to everything we've done. Here's a definition once again of stationary, kind of a nice concise one to the students of my class. This would be the type of thing I would ask about, for sure. And of course, with everything else I've just talked about, the statistic metric is invariant under translation over in interval. Now I made a bunch of comments about replacing space for time because we don't have multiple samples over time. But I don't want to narrow us down. You could in fact work with stationary and in stationary assumption. For a problem that is just one location in space over time, in which you have samples over a time interval and you decide where you can pull those samples of the system changes over time. So of course you could have stationary considerations over just a time system. I just used the common application of replacing time with space to help demonstrate and explain the concepts. Whenever we make a decision of stationary, we got to decide what is stationary. You need a metric. You need to work with statistics at this point. And so we got to know what statistics we're concerned with. Over one interval, we need a time or volume of interest. With those two, you have a complete definition of your stationarity decision. It'll always depend on your model purposes. If you're trying to model a phenomenon for which you only have to understand at a very large scale, you may not need to work with very fine scales. You may make grosser assumptions about stationary. It always depends on the resolution of your data and over what scale you're trying to work. The decision is not a hypothesis. Therefore, it cannot be tested. It can be in fact shown to be inappropriate, given available data and geologic knowledge. And it can of course be supported by geologic knowledge and by available data. Here's a summary slide. We provide a bunch of the information that was talked about as far as stationarity. And in addition to that, if you follow me on Twitter, you'll see that I like to put out summaries, these infograms that have a whole bunch of summary information about an important concept from geostatistics, statistics, machine learning, and so forth. And so this is one on stationarity. It's also available on GitHub. I've started to put lecture material and so forth up there. So I hope that that's useful to those of you who are trying to use the concept of stationarity within your own classes, within your own lectures, and so forth. I hope that this was helpful to you. To be honest, this course is going to get so much more interesting from now on because we are into spatial statistics. We'll be transitioning into machine learning and talking about uncertainty analysis. And those concepts to me are really, really fun to work with. All right. I hope this is useful. I welcome any comments or any ideas for improvement and so forth. Thank you.
Hey, howdy everybody. So we made it through all of that spatial continuity programs calculation interpretation and modeling and so now is our opportunity to get into getting some work done spatial estimation by creaking and spatial simulation We'll talk about our ground-based Gaussian based simulation methods. So let's get into how do we get? to make estimates in space Let's give ourselves a very simple problem where we have non-stationarity and you're gonna see how critical it is for us to account for non-stationarity when we're making estimates and it'll follow what we're trying to do simulations later on and so What we'll find is that in the presence of significant non-stationarity We would not rely we would not want to rely 100% on a spatial estimate based on just data and a spatial continuity model. We'd want to incorporate a trend component So let's give ourselves a very simple problem to demonstrate this we have these shale on the y-axis and we have Location on the x-axis. It's a spatial problem That is fundamentally just one D for ease of visualization the data are the dots at five different locations and the problem is We want to make an estimate of the shell at an unsampled location You not so this location here we want and it could be any value. We don't know what that value is gonna be we need to estimate that So Problem we have those we have a trend in data you look at that data and clearly there's something going on It's rising as we go towards from one through location five and so there's a pretty clear trend if you observe a trend We really should model the trend we need to fit a model to it Everything we're gonna do right now is data driven of course we could have all kinds of other Secondary information geologic information you this go information pitch is so for it to help us model this trend So here's a there's a pretty good model we draw a line so we will inner model going through our data right here And you can see how that could help us make an estimate at this unknown location That we're counting for the fact that in this area Things are typically pretty high if we rest maybe over here We would have things typically pretty low and the trend captures that information that's important information So what we can do is we can now work with residuals you notice the axis here is residual you notice that it centered on zero Where zero would be the case where we match our trend and Positive and negative whether or not we were above or below the trend residuals captured simply is taking the data values minus the trend And so you could pan back and forth and you could commit yourself that With the limits of the quality of my drafting That we really have just Subtract the trend from all the data values and then we form a model we built the model of the residuals And that model does pretty well It has a degree of spatial continuity so at the data locations you get the data values with your model away from the data there's going to be a blending of data and the trend At a certain distance you're going to get back to the trend that'll typically typically be related to the Distance of spatial continuity that would make sense that when we get far enough away from the data that we don't have any information from the data any longer We would make the estimate based on the trend And so we're working with the residuals and so we build our model in the residuals And then what we can do is we can after we model we can add the trend back now This is a pretty good model now because A data locations we get the data values away from the data we go towards the trend and it's pretty well behaved it tracks along that trend very well What's the consequence of applying this type of modeling approach but skipping the step where we model and fit the trend and work with residuals? We do that. We're going to have something like this away from the data locations What would be your best estimate in the absence of a trend? It would be the global meat while the global meme goes something like this this dot dash line right here And so systematically away from the data in areas of Low values we over estimate And away from the data and locations of high values we would underestimate And so we'd be systematically pretty bad When it comes to our estimates away from data if we don't account for the trend We don't account for that additional information about the property systematically shifting or changing we just leave it to a Estimation method with the data. That's not a good idea. Now let me make a couple more comments about trend We have to identify a model the trend that's our job But it's not our job alone as say the residual mother While we discussed data-driven type of trend-mobbing approaches here and in the additional slides after this We do that because that's a very practical way we can model trends and in fact in many companies They are using data-driven methods, but they're always within the bounds and constraints of All of the available information to the subsurface asset team and if this was high Groundwater it would be the Subsurface team that's given the map of groundwater or uranium or mining every time I say reservoir of subsurface It could be any type of mapping of the subsurface or it could even be any spatial mapping of the subsurface You guys know that I just want to make sure that that's good so When we're building these trends we show data driven because those are methods that are commonly used But they're used within the constraints of the entire asset team and so you have the geologists physicists geophysists the petrophysists the reservoir engineers and so forth who are all incorporating and integrating their information together There's a lot of different signs that goes on the formulating these trends trend is in fact very important because it has a direct impact on The resulting uncertainty model. So let's think about this you have a phenomenon a Variable over the area of interest for us to be sure whatever it might be it has a certain degree of variability Now of course if it had an older ability it was all the same everywhere It would be very easy to predict and that variability actually makes it harder to predict If you model a Trend what you're doing is you're taking part of that variability this pie chart represents all the variability And you are allocating it to a known component the trend is something going to model will say deterministically And you'll treat it as being known and Whatever variability is left is treated as unknown a residual and that's part of the uncertainty model So this is very interesting because you're actually partitioning removing part of the variance and putting into bucket and saying I know that but that other component that remains the residual variance I don't know that and just like we just showed the workflow where we subtracted The trend from the data to get residuals. That's exactly what we're talking about here The variance the residual versus the variance of the trend and the total variance before we removed any type of residual So when we are trend modeling we're making a conscious decision of how much variability to put in the known bucket and to then Decrease the amount of uncertainty by how much well if the trend and the residual Are independent of each other then it's just simply in addition the variance total is equal to the variance of the trend plus the variance of the residual and this two times the covariance between the two goes to zero In that case this indicates just that X and Y are independent Actually, I just cleaned that up. We would expect that the Component of trend and the component of residual are independent as denoted by the symbol indicating independence In that case the covariance between the two would be equal to zero and We can simply calculate the variance of the residual as the total variance Minus the variance in the trend and more variability including the trend you take directly from the residual the part that's considered Uncertain so when you trend model you're making a decision about how much Variance put known versus unknown now you might be curious about exactly how we figure out add a tiby of variance and whether or not You know how was that calculated? Well, remember we previously covered concepts of the expectation We previously talked about Variance and so of course we can denote the variance as being equal to the expectation of the square of the variable Minus the square of the expectation for the mean And so you can go ahead and do a substitution instead of Z you put X plus Y and if you expand and you simplify You'll find that the whole thing will simplify down to the variance of X plus Y is equal to the variance of X plus the variance of Y Plus two times the covariance between X and Y All right, so we know additivity of variance for some we can use that we're now thinking about partitioning the total variability between trend and residual Now I said the trend was known and so in that case we were can consider the trend to be a deterministic component of the model It's a deterministic model that's being used What is a deterministic model? It's a model that assumes perfect knowledge There's no one certainty How do we generally build the termistic models? They're generally based on knowledge of the phenomenon For the trend that we're fitting or the trend could be fit to data But generally it suggests that we have some good information in order to justify this perfect knowledge Most subsurface models in fact have a deterministic component in my experience I don't think I can think of a subsurface model I built Or saw or reviewed which did not have some type of a trend component to it usually a locally variable faces proportion Maybe vertical locally variable porosity But of course a trend could include any component any statistic could be modeled non-stitioner with the trend What do they look like in practice while they look it look really cool? They're really quite amazing Here's two examples I took directly from gocads website from paradigm And you can see they're really quite amazing because Here's a trend Where you have cyclicity in the vertical direction if you go perpendicular to those dip and beds You can see that there is cyclicity You can also see that there's a long range Transition from this side to this side going from high proportion of eight bases to a low proportion of that bases So what do we see here? What do we learn? The trend can actually be pretty complicated it can be within Constraint within a gritting framework, which it should be It could have the hierarchical With different directions acting over different scales You could even have the vertical cycle and then a larger scale vertical cycle imposed there thinning up or so forth In fact, they're in this documentation you can see in gocad They're even suggesting the idea of building faces proportion trends by sector And then welding them together over multiple aerial sectors In which you'd have a lot of degrees of freedom or control over this type of a trend So in general Trend modeling they will be quite rich often incorporating data incorporating concepts They would be checked against all of geologic petrophysics geophysical type of information to show their consistent With all of the available information They must must be careful with them because they often are treated deterministically known and we only model And treat unknown these visitors Okay, I think I've said enough. I can I don't I don't need to spend any more time on my soapbox about that Trend modeling practically speaking how could you calculate it? I showed some really beautiful images One idea is you can just take a move in window and move through the data and calculate the average What would it look like? Well in one D this is it this is location Some location coordinate This is going to be the value of the property of interest. There's the data values And you can take a moving window with some type of weight distribution and scan it through the data What happens with the shaped weight distribution? It looks almost Gaussian Is that when a data value is on the edge? It slowly starts at more impact when it gets the center It has the most impact and so you're making estimates of this location right here of the trend So as you move through you don't get jumps If you use a uniform weight distribution What happens when you get a high value something boom the estimate jumps up And when the high value leaves the other end boom jumps down And so that's a pretty erratic trend. If I want something smooth this type of weighting Issue a smooth result So you move this thing across your data space Take the weighted averages of data and sign them to the center and keep moving across your data nice Continue this smooth looking trend model How do you calculate trend models in full 3D? Well, it's fine and dandy when you have dense data Then you can just build a three-dimensional cube keyboard or what in you base this Stand through the data set just like we did with the 1D but you're doing three Of course the curse curse at the mentionality you would need quite a bit of data to do that To have the right coverage to be able to accomplish that So what people do in typical situations where you don't have that type of coverage So they do a 2d plus 1d approach 2d What you do is you take all the well data and you calculate the averages or the vertical drill full data or whatever data You have available you get some type of the average vertically to get points within a two-dimensional framework And then you just interpolate between to get a smooth map You then average layer by layer over z In order to get a vertical one-dimensional curve And you combine them to get you got the vertical trend curve here x bar Of z You can scale the aerial trend By that Notice we've x bar x and y And then you normalize by the global mean So it's kind of interesting if you think about it you can see it a couple of ways One way to see it is that we're rescaling the vertical proportion Based on the aerial map this ratio of whether or not trend early is higher or lower than the global mean or you could shift the Global mean over here and see it as a rescaling of the aerial slice By what's going on with the vertical trend relative to global mean so you could see the other ways It's a rescaling And so if you do that This is what it looks like a vertical slice You can see right here and so you can see The trend features are time are imposed here And if you look at a single aerial slice of the trend model it looks very much like a rescaled version of the aerial Trend curve Okay Let's let's just do a broad trend definition so we can move forward now Observation of non-stitianity in any statistic metric of interest. So if I say I see a trend Means I observe non-stitianity and any statistic Over there of interest It could also be a model of the non-stitianity in any statistic. This is a trend model Typically modeled with supportive data expert knowledge in a deterministic manner without uncertainty For the purpose of this course will leave it like that of course you could build multiple scenarios of trend And so forth accounting for uncertainty, but we don't need to go into that now But if we're going to talk about trend modeling, we should introduce the concept of overfitting So what I did was I gave myself a simple one-dimensional example This is the fraction of fines within some paper of a rock Maybe it's vitamin and oil sands whatever it might be We got fines proportion fines plays or what Then we have eight data And we have depth so this is one-dimensional data set once again easy to look at easy to visualize I built a third-order polynomial using just simple A regression that with a third-order polynomial When minimizing the square difference at the data locations the error square the data locations and calculate the distribution the residual This would this spread right here would be the amount of Verability that I rule then model and consider uncertain Well the amount of variability represented by this curve will be what I consider certain Confidence intervals are valuable because it tells me about the confidence in that model just based on data alone No other information. So that's good to know Change it to a fifth-order polynomial. I can wiggle. I have more degrees of freedom More parameters to fit. I can fit the data more precisely Distribution narrows a bit. I'm claiming to know more But look what happens because of the lack of data my confidence intervals Are growing significant I'm telling myself I know a lot because the residual distribution start in the shrink But the model's telling me based on data alone that I don't actually know that much Now look what happens if I bump that up to an eighth-order polynomial I'm basically one order away from just perfectly fitting the data Just however just getting through every possible data value So get the order polynomial Where's the confidence intervals they're off the plot they're just there's really no certainty about that model anymore You can see that in fact This is actually can Communicate it to you very nicely by just consider how it's interpolating between this data point and this data point Look at that estimate. Do you think that that's not a very robust or reasonable trend value That's not exactly meeting the property of porcimony the Selection of least astonishment. I don't think so not very well. Look at the distribution of residuals very narrow So what do we have here? We the model that clearly is unstable It's communicating to us through the confidence intervals that it has very little prediction action prediction accuracy And the distribution of residuals is very narrow. So if I use that as a trend I'm gonna actually be telling myself that I know much more than actually know There's almost no variability left the model as I know and so this is the fundamental challenge of overfitting We have to be very cognizant when we build trend models not to overfit So definition overfit just so we can kind of move forward from that and have something to share an overly complicated model That explains the eosyncrasies of the data capturing the noise in the data Also another way to think about it more parameters than can be justified by the data And I'd even open that up and say justify by the available information Results likely a very high error away from the data as shown by the confidence intervals But results in a very low variance you are fooling yourself that you actually know more than you actually know And they will result in a high R squared variance explain that makes sense the little is left the residuals They're very accurate at the data locations Almost no there's no variance is left in the residual But they would do terrible at interpolation testing with a new data set. They would be very bad Here's another example. I took directly from Wikipedia on overfitting They have a nice discussion overfitting a two-dimensional segmentation problem And you get sense here of capturing data in the idiosyncrasies Clearly the red and the blue categories have overlap many data sets do have overlap it may be error It may be some other thing the other factors you haven't considered But for one reason another this two-dimensional feature space there's overlap between these two categories And we would not want to use the green model clear clearly the green models capture just that data In the idiosyncrasies or the black model provides us with a better Model that would have better prediction with prediction with any type of future testing data set Okay, so that's all I have to say about the issues around trend One more time just because I know I just have a feeling that this will come up Of course, we show data-driven approaches right now For the purpose of demonstrating general ideas trend model data-driven approaches model data-driven approaches are commonly used In reservoir model but only under the supervision of subject matter experts who understand the geology that you have physics the petrophysics the reservoir engineering and so forth in order to ensure that those are in fact informed by all available information We can have some confidence when we're assigning them to the bin of known versus unknown residual Okay, so that that's trend model Now the next section we'll get into making estimates with simple pricking. We'll do some demos in Excel our Python and so forth so stay tuned all right if you have any questions or comments go ahead and contact me email Through Twitter on the geostats guy. All right. Thank you
All right, so this is one of a series of lectures introducing Python, basic Python syntax. We're going to do it within the context of Jupyter notebooks. So from Jupyter Lab, if you were to open up Python 3 notebook, we're going to then type our Python code in the code cells and execute them one by one. So you'll have to explicitly, if you follow along, you'll have to explicitly execute all of the code. But understand that later on, we can write entire Python programs outside the context of the notebook that don't need to be executed in line by line. However, this line by line execution can be useful for learning. The first thing we'll do is the first thing you typically do when you learn any new program in language. And that's right, what's called a Hello World program. Well, Hello World program is simply a program that prints the words Hello World to the screen. In Python, it couldn't be easier because we simply write print, Hello World, and execute that code cell again to execute a code cell in the Jupyter notebook, you type shift enter. Your print is something called a function. You can always tell a function in Python because it will have open and closed parentheses with some arguments in the middle. In this case, there's only one argument that is the string Hello World. We know Hello World is a string. We can call another function called type on any object in Python and it will return its type. In this case, STR for string. So Hello World or a string is just a collection of characters. We can manipulate strings in various ways. One way is we can take two individual strings and join them together. There's several ways to do this. So if we have the string Hello and we have the string World, the way that we could join them together so that they would be displayed at once would be to use the plus. So typically we think of plus as the addition of two numbers. When we're talking about strings, this is the addition of two strings and it will add the strings together to become one string that's printed to the screen. You notice here I don't call the print function and there's a subtle difference between what's displayed. The natural output of a notebook cell is to replay the last piece of code in the cell. And so in this case, there's only one any code. It's just this collection of strings and so it's printing Hello World. And you can see that there's a subtle difference in that. You see the string tick marks denoting as a string on the outside of it. If we really wanted to get rid of those, we could type the print function around it. And that would be like that. There's another way, a couple other ways we can join strings. My favorite way is to use the format command and this is useful when you have a piece of a string and you want to insert some other information into that string. So for example, if we had this and we wanted to insert the word world right here, here being indicated by the braces there, on the outside of the string we could type dot format. This is a function. This dot notation will make more sense later as we learn more about Python. But for now, just show you how to use it. And this is a function that if I just sort of type world here, would basically replace whatever's in the argument list of this format into this placeholder denoted by the braces. I could add additional placeholder, like for example, if I wanted to place the exclamation point in separately, I could do this or say we simply wanted to have a period there and it could do something like that. So again, format takes multiple arguments separated by a comma and for every argument in between the parentheses in the format command, there should be a corresponding set of braces in the string that will then be replaced in order by those arguments. The last way that I'll show you to join strings would be with the join command. Let's start with a list of strings. So we'll learn more about lists in Python in a different lecture. But for now, you can think of a list as a collection of items. In this case, it's a collection of strings separated by commas and we use the square brace notation to indicate a list in Python. So what we're going to do is join these lists together in a list of strings together in a to a single string separated by a space. And so we'll have an empty space that will indicate what we're going to place in between the join command. So if we run this like this, you'll see a list of, you know, a list is a collection of items. However, instead we wanted to replace instead of having a space between the items, instead if we wanted to say have a vertical bar, we could place that there and then execute the cell and you would get this kind of thing. We also separate strings that are really long. So using this kind of notation. So if we have a string on one line and at the end of that line we have the forward slash here, then on the second line would be the continuation of the first string. And if I execute that, you can see I'm going to type a string there. You can see the output. You should also go ahead and show you what it looks like if you make a syntax error. So if I were to start to type a string but leave off the last quotation mark and execute that cell, you're going to get a syntax error. Sometimes these syntax errors in Python can be very long. What's most useful is to scroll all the way to the end to the last line and read the message of what it's telling you. In this case it's saying that in a line while scanning string literal, you may not know what that means, but it's pointing to the location of the error with this carrot. And so we can see something that's clearly wrong or missing at the end of the string. And we could correct the syntax error like that. I guess I want to introduce variables. So variables are locations in a programming language that store items. So in this case, what hello is going to store is going to be the string hello. And so now you notice it doesn't print to the screen. Unless we explicitly type hello or type print hello, then we'll print the output. Just for demonstration, I want to show you that what stored in hello is arbitrary. For example, I could store the number 22 and it would print the number 22. This is just a temporary storage location for whatever you put in there that can be access later in the code. So I'll return that to hello. I'll create another variable called world and inside that I will put the string world. And then if I wanted to say add them together by referencing their variable names, instead of retieping the string, I could do it that way. So we saw earlier how the plus operator can will add two strings together. And that is the case here. So variables can be updated in the code. For example, I could change hello to something else. And then if I were to execute this 18 again, realizing I'm executing the notebook out of order here, which is usually not desirable, but nevertheless, the connection or connecting those two together, you have something else in world. So the fact that I named it hello, it's completely arbitrary. It doesn't correspond in any way to what's actually stored in the value. It's completely arbitrary. And again, that's why you could store 22 there. Now, if I were to add these two things together with this syntax, it's going to give me an error because it doesn't know how to add 22 to the word world. 22 is an integer, a number, world is a string. And this addition operator is not defined for that kind of operation. So you get a syntax error. However, I can convert hello to a string, just making it a string. And in that case, you'll see that the connection 22 in front of world there. So it's probably useful to go through a couple of other ways or other kind of ways to modify variables. So what I'm going to create a variable called days in a year. And I can update that variable one way to update it would be to say days in a year plus one. That would give me 366. If I wanted to actually store the new value, 366, say it's a leap year, I wanted to store that in a variable called days in a year, I could update it like this. So what I'm saying is days in a year is initially set to 365. This is going to reset it days in a year to 365 plus one. And again, if you print or just type it out, you'll see that is 366. The thing about this is every time I run this code cell, it's going to increment by one. Because every time the variable is getting redefined to itself plus one. There's actually a shorthand notation and Python for this kind of operation. And that is the plus equals. So the plus equals is shorthand when it, at least with respect to integers, for the operation that I just showed. And that is taking the value that is currently stored in days in a year, adding one to it, and resetting that value or storing the new value in days in a year. So you can see that every time I run the cell, it will update the value by one. There's similar operations minus equals would subtract by one. Multiply equals would multiply by one, which is not interesting. So let's change that to two. So every time I run this, whatever the current value of days in a year will be multiplied by two and stored in the new value, similarly divide by two. So even though I've introduced some arithmetic operations in the sense of updating variables, let's go ahead and show what they are here. So if we want to add two numbers together, like that, this is the output of three plus five is stored in the variable addition. We can see what the output is. Likewise, for subtraction, multiplication, and division. I do want to point out something about division if we look at what that variable is. Even though three divided by two, three and two were input as integers, meaning there were no decimal values associated with them. In this case, it does convert to a floating point number to give you the correct output. So if we actually were to do type on division, it would return type of float. Whereas say, for example, the type of subtraction is an integer. Because again, seven minus five is equal to two in a whole number, integer math. It could be done that way. By the way, just to be clear, the fact that I named them subtraction, multiplication, division, has nothing to do with the fact of the operation I'm doing. These are just variables. The names are arbitrary. So I could call it say my subtraction. And print out the value again, too. In addition to those standard operations or in other common operations, we'd want to do is possibly take the power of a number. So the power of a number in Python syntax is using the two asterisks. Like so. There's also the module operator, the remainder operator. So for example, if we take 15 module O2, this is going to basically give us the remainder of the whole division operation, 15 divided by two. So if I divide 15 into two evenly, I have 14 plus one as a remainder. And so that's what we got. This is useful for testing. If say, for example, numbers are even or odd. So if a number is even, that is, it's divisible by two, then this will always return zero. There's other useful things for using the module operator. So far, we've covered or at least seen several different data types, strings, floats, integers. There's another type called a boolean, and that's simply true or false. These are the only two options of type boole, true or false. But with that, these are returned by what we'll call conditional operations. So for example, if we weren't returned up here to 16 module O2, if we wanted to create a test to see if a number was, in fact, even, we could use the double equals operator here and set it equal to zero. And if, in fact, that is the case, it will be true. So again, if I were to say, change this back to 15, like it was originally, then it's false. We can also negate the operation by putting the exclamation point equals. This means not equal to. We also have the greater than, less than type operations there. And the output of these can be stored in a variable. So in this case, I'll create a variable is even return it to this kind of syntax. And then if we print is even, then we get the resulting boolean. Another way to negate boolean operations by simply using the not command. So we know that the current value is even as false. What I can do is say not that will negate the operation. Not false is true. So not false is true. So later we'll learn how to combine this type of boolean logic to control the flow of programs. By essentially testing if some statement is true or false. And having the code proceed in various paths based on those outcomes. What I'd like to leave you with in this lecture is just a short introduction to functions. We'll talk in much more detail of functions later in a different lecture. But for now, I just want you to see the syntax. We've already used functions. The current statement is a function. Functions are always called with parentheses and sometimes with arguments, often with arguments. Also the type is a function. But we can define our own functions. For example, I'm going to define a function that adds two numbers together. The numbers will be replaced A and B will be replaced with numbers. And that will return the addition of those two numbers. A and B. So in Python syntax, function always starts with the def, meaning define a function. And it typically has a return statement. It returns something. Not always functions don't necessarily have to return anything. For example, the function could just print to the screen. The output of A plus B without returning it. So in this case, if I call my add, if I call the function with two numbers, say two and four, it will print to the screen six. Now, there's a subtle difference between what is printed and what is returned from a function. So for example, if I were to have a variable, I'll just call it my variable, and set that equal to the function, you'll see that my variables in fact empty. It doesn't return anything. If we check the type, it's of none type. And that's because this function my add did, it only printed to the screen A plus B. It didn't actually return anything. So if I were to move this back to the return statement, like I originally wrote the function, now if running the function, I'll first run it without it having it return anything. And at first glance, it appears that the function just did what it did before. It printed to the screen the letter six, the number six. However, if I were to have it assign what it returns to a variable, my variable. Now, what's stored in my variable is an integer, and of course, its value is six. And I know this is a very simple function that doesn't do much, but you can think of functions as little programs in their own right. They took, they take in inputs, in this case the inputs are two numbers, A and B, and they return something. And what's in between can be fairly complicated. So, you know, I'll just rename this function, say my math operation. And in this case, I'll do something slightly more complex, just to demonstrate the utility. Still not very complex, but in this case, what this guy is going to do is, A will be passed to here, added to B plus divided by two, and then 56 will be added to everything that's there. All of that will be stored in a variable, A and S. This variable is temporary, it's local to the function, meaning outside of this function, A and S will be undefined. So, if I execute this cell, and then call now my math operation, to get that output, I just want to demonstrate that A and S is not defined. Anywhere except in this function. It's a temporary variable that only lives in the function and is returned, but when the function exits, after this line of code is run, it is destroyed, and it's no longer available, and it's undefined outside of that. So, I just want to give you one another example of calling the function. In this case, we explicitly put in two and four, but we could put in variables, say number one and number two, that are defined elsewhere. So, we'll define number one as... And then run this. And so, now, 67 takes on the value, or 67 is assigned to number one, 89 is assigned to number two. Number one and number two become arguments of the function, so they get become the inputs, number one, number two. And then returning to the definition of the function, number one, again, which is 67 becomes A. And number two, which is 89 becomes B. And those numbers get plugged into this statement. So, for example, in this example, this statement here reads 67 plus 89 divided by 2, plus 56. All of that is stored. The outcome of that is stored in the variable A and S. That variable is then returned, or whatever value that is is returned by the function. So, in this case, it's 167.5. And you can verify that on a piece of paper, if you wish. We'll learn a lot more about functions, but you will see them. And so, I wanted to go ahead and give you a quick demonstration of what functions are, how to define them, and we'll cover more later.
Hey, howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin. I record all of my lectures. So what I'm going to do today is this is going to be a walkthrough of all of the steps that you need in order to calculate a directional verigram and model it with a full, multi-directional, two-dimensional, positive, definite model that you can use for creaking or simulation. There's so many different things that you can do with a verigram. It's a powerful, predictive tool and a tool to just understand spatial continuity. Now if you want to follow along with this, just go ahead to my GitHub account, github.com, Geostatskyne, and have a bunch of repositories. And there is this repository right here, Python numerical demos. If you go there, what you're going to find is a list of many, many demonstrations. And there's some new interactive ones where I use IPI widgets and Metplot Live to automate things so that you can play with the methodologies and visualize the result immediately. It's called interactive verigram modeling, IPIthon notebook, IPYNB. And so go ahead and download that. Now you will also need a data set, but no worries, that's available to you too. There's another repository called right down here, Jew data sets. Now in this directory, there's a whole bunch of data sets you can work with. And so the data set that we'll use is right down there. And you can go ahead and work with that. Okay, and I'm going to walk you through the workflow and show you all of the steps and help you so that you can take this workflow, open up your own data set and go ahead and calculate a model to dimensional verigram. So I think this will be really helpful to you. I specifically coded this yesterday for my students because I had just assigned, or was about to assign a verigram modeling assignment. I thought it'd be really cool for my students to be able to have a nice interactive workflow to get experiential learning about verigram modeling. Rather than something kind of ad hoc where you change a parameter run, change parameter run, I want them to be able to visualize and see the impact of their choices immediately. I think that's very powerful. Okay, so every time I do one of these workflows, I like to put some documentation up front so it's self-contained. It contains some of the information from my typical lecture notes. If you go ahead and look on my YouTube channel, you'll find that there's a bunch of videos here available to you. There's a playlist specifically for this course, data analytics and geostatistics. If you view the entire playlist, you'll find that there are lectures right down here on spatial bias, spatial declustration, spatial declustering, also stationarity, spatial continuity verigrams, verigram calculation, verigram parameters. There's a lot of lectures here available to support you. Okay, so the verigram, what we want to do is calculate the experimental verigram and then we want to go ahead and model it with a positive definite model. And so some general observations about verigram calculation and experimental verigrams. And then we have the concept of using a nested set of positive definite, known positive definite verigram models that we can go ahead and fit and be able to and the ones available to us include spherical exponential Gaussian and nugget. That's why I've included in this method. There's less common ones like whole effect, dampen, whole effect, and power law that have not included. They're typically not used very often. Specifically, power law wouldn't be, wouldn't be appropriate if we're doing simulation or we would not honor the histogram, of course. And the concept of using geometric anastropi where we work with a major and a minor direction or orthogonal to each other. So we just have to in 2D, have a single parameter of asmif of the major. The minor is assumed to be orthogonal to it. And we have this geometric anastropi model where all we have to do is for each structure specify the contribution to the variance and the range of the major and the minor. And so we'll go ahead and we'll we'll work with those and I'll show you the result as we add those together and make a positive definite model. Okay, so geostats pi is the package that we're going to need for the verigram calculation and modeling. When I was teaching spatial data analytics last spring, I realized there wasn't a good Python option for the entire workflows of geostatistics and spatial data analytics. Not one that I felt was reliable and provided what we needed. So I ended up coding this up on the weekends, which was really fun. And I think it's great. We have something. Let's go ahead and the first thing we'll do is we're going to restart and clear all the outputs so that we have a fresh start here. There's no previous runs getting the way and we'll go ahead and import geostats pi. It has two components to it. Geost live is a bunch of visualization and like the geost live programs and also it has simple wrappers to run the for trend. We're not going to do that. We're going to use geostats, which is the methodology, which actually has re-implemented all of the algorithms like verigram calculation modeling in Python for us. So I think that's great. We're going to use a bunch of the standard type of packages like numpy and pandas for NDE arrays, multi-dimensional arrays and pandas for data frames, matplot live for plotting of course. And then we're going to have iPi widgets for building interactive plots at the very end. We'll do an interactive verigram modeling. I like to set a current directory, no exactly where stuff's coming from and then just read the data file. So if you want to work a sample data multivari biased, which is perfectly good to dimensional data set you could work with with prostate and permeability you go ahead and download that from my github repository, put it in your working directory that you'll set there and away you'll go. So when you run this, if you get no error, oh what did I do wrong? I do this all the time and I know my students do too so I kind of do it on purpose just to make sure everybody's recognizing that. It's a common problem. I skipped a block simple. The thing to remember about running a Jupyter notebook is until you've run the code it hasn't happened and so we haven't imported OS and so when I run this command with OS to work with the operating system to change the working directory, I get an error OS is not recognized. So go ahead and run that now the code works. You see that? So you've got to run everything in sequence. I often tell my students if you run into an error back up to the beginning and run back through chances are you didn't run a block. That's the most common mistake I see among my students. Okay, so let's go ahead. We'll read in the data set from the common delimited file and we'll go ahead and put it as a data frame from pandas. Pandas PD, read CSV will turn this into a data frame. DF is now a data frame and the prove it to ourselves we access for our DF data frame object. The built-in function to do a preview of the first five or six samples up front the head command. Okay, so yeah five first samples right there, 0 through 4 are shown and you can see the data that we just load it. Now I should have mentioned I also have acoustic impedance which is kind of cool in that data set. So it's a multivariate spatial data set. It's synthetic. You're free to just use it. Don't worry about it. I made it using multivariate geostatistical methods. So it's a pretty good data set. You'll see. We could separate it by facies but for this workflow we won't and the reason it's not too bad is because the facies do have spatial continuity and porosity and permeability. It's not like a system where we have a discont nudie between the facies. So it's perfectly fine to try to model the varogram between them. We would probably if we were building a real reservoir model we would probably maybe be more thoughtful about that choice but for now we're choosing simplicity and brevity of the workflow. So I hope everyone's cool with that. Let's look at the summary statistics. I love Panda data frames. For the fact we can do that very simply. We can see what's going on with our data sets. The values look reasonable for porosity. Good range of porosity. So we can go ahead and we'll do a normal score transform. The normal score transform is performed. It's turning the porosity, turning it into a standard normal distribution. Mean of 0 variance and standard deviation equal to 1. Now we do that specifically because we're acknowledging that these varograms are going to be used for Gaussian methods like sequential Gaussian simulation. So for consistency you have to work with the data transformed into Gaussian space, the normal score transform. Now it also does help without liars and provides in general cleaner looking varograms. So it's not a bad idea and specifically driven by the theory we should do it. Normal score transform is built into Geostap pi. I like the Gslive Normal Score transform. I find it performs very well and so I'm going to use that instead of a methodology available in other packets. So we go ahead and we can visualize the CDFs original two transformed and no surprises here. It really does. It was almost a Gaussian distribution for porosity but now look at that. Beautiful S curve, negative 3 to positive 3 effectively. It's an unbounded distribution but we'll find practically speaking our samples will likely be bounded there. Okay and if we look at the permeability distribution we've gone from a very strong positive skewed distribution probably getting close to the log normal and we've gone to a nice Gaussian CDF. Okay so we've done our transforms. Now if we want to try to calculate and understand the directionality of our spatial phenomenon plus if we're trying to figure out how to calculate good experimental varograms we really should do some data plotting in space so we can make good choices about lags directionality and so forth. Let's see maybe we'll even see directionality. Okay so let's set back look at that data set that's normal score transform and porosity. Let's focus on porosity. They're highly correlated with each other as you can see by the crossplot so we should expect similar spatial features but we'll just focus on porosity for this workflow. Okay high values are lighter colors darker colors are lower values on the normal score going from 3 to negative 3 and if you look at this hmm do you see it? Are you able to see the directionality? I can totally see the directionality right there. It's it's you can see about if this is 0 0 0 on the y direction and as of 0 9 0 in the x direction you can clearly see about 0 4 5 is a primary direction of continuity that you have this high low high pattern and so we're I would suspect that we'll see 0 4 5 so that's our ocular inspection approach to detect directionality. Okay so we've done that. We can also do verogram maps which are super cool. That's something that I coded up to and added to geostat pi and so geostats var map v which works with irregular data and we can load up the data frame the x and y locations the feature of interest and all we have to do is specify the number of lags going from the center to one side 11 the distance of the lags so 50 so that means we're going to explore an offset of 550 from a center cell in the middle here of size 50 and so we're going to extend 550 over this way plus 25 for half the distance across that cell in the middle so there's a cell in the middle and then 11 cells going this way it's a mesh and then there'll be 11 cells going this way 11 cells going up up so we're going to span from 0 to 550 plus 25 575 down negative 575 and the same in the x direction making a mesh. Then what we've done is we've used the basic contouring program after we've made the mesh and calculate the verogram values for each one of those offsets in the data set and we've kind of contour it up that's why you see some of these weird shapes those shapes are not it's just because we have sparse information here we have 11 plus 11 22 plus one we have a 23 by 23 grid and we've just done some simple contouring between those values right here it's a good idea to look at the number of pairs involved in each one of those cells and you can see in general if we have ill-informed cells getting towards the edges here we have much fewer pairs to work with so that might be a concern you see that darkening color but we're still pretty good it looks it really does look like we're always above 50 or 100 pairs so we could look at the matrix the or two-dimensional array that came up from this diagram map which would be in P number pairs map and you could look at that 23 by 23 array and you could investigate the values make a choice on maybe you should remove some of these values using conditional statements okay but I'm pretty comfortable now let's go ahead and sit back and look at that diagram map what do we see what's going on here okay well what do you think low-verogram values high-verogram values still is kind of an orangey color so if I drew a contour of the sill it would go somewhere like around here 45 degree nail that ocular inspection works the verga map is confirming it and the range if we want to pick the range off of this I could go all the way to the edge of the verga map and I still haven't reached the sill I'm suspecting zonal atoms trape in that direction I can go probably I'm eyeballing it right now 200 or so in this direction before I get to the sill here really guessing right now but you know it's good it's giving us kind of some first evidence to work with okay so I suspect our two-dimensional verga will have a major direction in the 0, 4, 5 azmuth and I suspect that we'll see a about a zonal ash trape in that direction and the orthogonal the 135 azmuth I suspect we will see a probably about a 200 meter range or so all right now another good way to detect directionality and something we should do anyway because we're trying to model veragrams is we should calculate experimental veragrams now going back to the data you can confirm this but if you look at it you'll see that 100 meter spacing and let's just go back here 100 meter spacing would definitely get a lot of pairs you could probably go to 30 50 or so but I think I found it got a little noisy you go ahead and play around it yourself if you like to try something so but I'm going to say a lag distance about 100 meters half extent of the data set if I'm calculating the 45 degree direction or these other directions I'd say I can go about 700 meters because I have about 1400 meters across in the diagonal and I'm expecting I can calculate reliable verga groups up to about half the extent so let's just say around 700 meters or so I'll go out give or take okay and in addition to that I'll go ahead I can set the lag tolerance is one half the lag distance but that's not necessary I could actually increase that and cause overlap between the bins and further smoothing for smooth the result I think I did that because it improved the results a little bit but I welcome you to go ahead and try some differences so I have a lag distance of 100 meters I'm going to calculate seven of those now with the program it's going to go 0 1 2 3 4 5 600 why don't we go ahead and just make that one more lag so we actually get the 700 okay then we'll go ahead and we'll say no bandwidth because I'm not really worried about you know getting too far away from that direction and I'll put the asmr tolerance pretty tight we got pretty dense data so I want some good strong directionality now a great thing with working with python is I make an azimad which is just going to be a list of different asmr's and I can run my program looping over those and I'll calculate all the directional verga going from 0 22.5 45 right here 67.5 90 12.5 135 and all the way back to I don't need to do 180 because that'll be the same as zero verga grams in general are going to be symmetric so we're not too worried about that okay and now I should have mentioned i-cell in the verga calculation program you do have the option to constrain it to standardize the distribution to a variance of 1. now we've done the Gaussian transfer my suspect it should be very close to one it doesn't hurt just to say okay it's kind of like a belt in suspenders you know the distribution should be one I could check it right now I'm sure it'll be one then I could just say i-cell equals 0 but it doesn't hurt to standardize and make sure that the variance is exactly the one it's kind of a safe thing to do because if there is any deviation what will happen in the result is you won't reach the sill or you go above the sill if the variance is less than one you you'll have a lesser sill and you might interpret it as zonal anastropi when in fact it's just your distribution is wrong okay so let's go ahead and we'll set it to 1 and force it to standardize okay so run that block now what we're going to do is we're going to loop over all of those verga grams calculations and we're going to create this array right here will be the lag h offset this will be the verga and values this number of pairs we have that for record if we need now we're going to use a two-dimensional array so I can do all of my asmiffs all of those listed asmiffs and I can also run it so that I'm able to this right here will be all of the lags 0 100 200 300 all the way the 700 so I'll have those lags and we can plot them and when we plot them we'll just say well we'll loop over and plot all of those asmiffs their lags so we're sending every time we do this command right here lag iazzi with this placeholder here we're saying send the one-dimensional array for this specific row which is that specific asmif one-dimensional right here so we can plot that and we'll go ahead and just say well add to the name the asmiffs so we have a nice plot okay let's go ahead and run that enough talking about it look at look at that isn't that awesome i love working on python coding is just powerful because i ran all of those directional verga's look at that asmif 0 22.5 45 okay let's look right here range 300 range 400 range zonalanish trapey undefined effectively infinity really now I wouldn't say it's infinity i'm sure waltz-laws kicking in here and at some distance we'll see it reach the cell but over our study area we don't see all the variability okay 67.5 oh ranges now 550 350 probably about 250 you see what's happening here and then by time we get to 135 about 250 or so so it's fair to say that 45 is the major and by design it has to be 90 degrees but it is consistent 135 will be the minor and it looks fine it actually does look like it has about the lowest range it's similar to these other directions but i'm not worried about that okay so now what we've done now we did ocular inspection and we said the major looks like 45 we did the verga map the major looks like 45 we ran a bunch of verga and the major really is 45 okay so now for this program if you put your own data in you've run all of these pick the directions from the asmif array or list that are the major and the minor now if you come back up here this is the zero because remember python indexes is list by 0123 to n minus 1 so this is the zero direction the one the two okay so our major is the second one second one okay then we have the third the fourth the fifth and the six so my minor direction is the 135 which is the sixth sixth direction in the list so what I have to do is for my workflow to work now and be able to build the models I got to pick the major and the minor from these directions and so I pick up so two is the major six is the minor so down here in the code two and six once you've done that you're set the rest of the code will basically run automatic and the first thing it's going to do just to confirm for you it's going to produce the major minor plots it's the same plots of above but now this is our experimental verga grams in the two directions that we're going to focus on for modeling okay now what we can do is we can use two functions built into G as stats pie GOS that pie so there's the make verga function not a big deal all it's going to do is you provide the parameters for a vergram into this program and it makes a dictionary I know it's so simple but I've done it so it's kind of nice and canned so you run this make vergram with all these parameters it's going to make the dictionary okay and that dictionary is going to be like an object we can now pass for visualization pass for creating pass for simulation it'll be nice compact standardized storage the parameters nugget number of nested structures you can do one or two if you do one it's it's going to assume no contribution for these second structure parameters it's going to say the contribution zero inside and it's going to have nothing now you don't even have to give these parameters if you say nested structure one you don't have to actually list these parameters it will default to zero contribution okay so we will have the type of structure we have spherical exponential and Gaussian okay we have the contribution the contributions should sum to the sill we're working in standardized variables sum of contribution one and two should be 1.0 the azmuth keep the azmuth the same in this is the azmuth the main you keep it the same between the structures you could do that you could have different azmuths for the different structures we're keeping things simple here for ease of interpretation understanding and in most practical circumstances you'll find people will work with the same major direction and just have multiple structures that are consistent in that direction then major range now we have isotropic for the short scale variability structure structure one and then we have 99999 for the major of the second structure that's giving us a little on a copy and men of that so this is our first model of the verigram that we just observed this is a reasonable verigram model right here okay so let's go ahead we can put that into the make verigram function and we'll get that verigram dictionary that we can then visualize the v model function is the same as the v model function from Gs like it's there just simply to take your verigram model that object called verio and to project it in a direction now verio is a two-dimensional verigram model so all we have to do is just tell the model what azmuth direction do you want to see the model do you want to project it now it's not actually it's working with the analytical model so you can actually sample it at whatever rate you want okay so in that direction you give an x lag and you say sample at every 10 meters and do 100 samples and that would give you a 1000 meter long function and so the program would actually give you the index the lag distance at every one of those samples the verigram value at the all those samples the covariance function at all those samples and the correlogram of wow this is a three-for-one sale today for v model really really great okay so if we go ahead and take our reasonable model this these are the parameters right here no surprises I just set the azmuth for the first and second structure to be equal to the major direction i major is the index two for that as he met and so that would be 45 degrees right there and all of the parameters I showed above make the verigram object right here then what I can do is I can visualize 70 lags of it and each lag is going to be 10 so we're going to see 700 0 through 700 right and we can plot that up so we'll go ahead and make those verigrams plot them up and woo look at that awesome okay so it worked the make verigram made a reasonable verigram object that then was sent for display by the v model program and look when we plot it we see a nice function right here now I have lots of students ask questions like why did we worry about the number of lags and the x lag so let me show you something right now let's go ahead and decrease the number of lags to seven but we'll increase the lag distance to 100 now we're going to sample 0 100 200 300 and so forth when we make these verigram models now look at what happens do you see it it's piecewise linear so in other words you're only sampled seven points of the function it's not going to look very good so remember this number of lags and x lag is not a calculation we're not calculating verigrams anymore we're just projecting analytical form so this is just the resolution of that projection not a big deal let's go back to 70 and we'll go ahead and make the lag distance 10 again and we get something nice and smooth we can understand it and so that's it just make it smooth enough and it runs super fast like it wouldn't hurt like you could go ahead and say 700 and lag distance one and look at that that's that's going to be every meter sampled and it didn't really change much you know if you have 10 that's probably that's probably pretty reasonable given the ultimate range of this problem now if you're working very short skill phenomenon you can need tighter sampling not to be deal it's just display now what we're going to do is I'm not going to walk through this because this work this workflow walkthrough is not to teach you I pie widgets and how to do this but I hope you can look at my code and get some ideas of how to do this which is super awesome it's going to take that same plotting we just did but take all the verigram parameters and turn them into sliders and selection toggle boxes or selection boxes so that we're able to go ahead and automate that process just now okay so let's go ahead and run that now we get down to it this display is now going to display our interactive display so here it is okay so let's scroll that up we know these parameters now we got nugget effect the first structure type contribution major major range minor range and the second structure the same thing now notice there's no azmeth here because what this model is going to do it's going to take your major direction and assume this is aligned with that okay so we don't even have to put that in now okay and it's going to automatically display it in the major and minor on the plot okay now I've said it so that as soon as you change something the plot's going to appear okay so we have the plot now now I think it'd be fair to say and given my experience of what we've already done with this data set we could probably start off with like a 50% contribution or 0.5 for the first structure 0.5 for the second structure but our ranges are all zero so this is the major this is the minor ranges are all zero so it all looks like really effectively nugget effect this jumping upright here is at 10 meters the first sample on the v model if we'd sampled every meter this would go up to just one at one meter so you see it's just a resolution issue that we see this jump here it should actually be the nugget effect you know the range we used here is 0.01 should be up there immediately right now you might wonder why I defaulted it to have a minimum range it's because of numerical issues with my code if you put zeros in I use anastropi ratios inside the code and it causes divide by zero errors so we're just avoiding that okay so let's go ahead and make our minor about 300 and that's what we found worked pretty well before now look what's happened we now have a like a nugget effect of 0.5 because with this right here is no range and we have about 300 isotropic right there let's start to give this some range too we're gonna say the minor about 300 also okay now look what happened in the minor direction that's looking pretty good now you see that we're starting to get a pretty good fit right there and let's go ahead and set the major watches we want zonal anastropi let's make it big what happens is I keep making it bigger and bigger and bigger and bigger you see what's happening it's not cool it's leveling off okay so let's go well you know go bigger go home right let's like make this pretty big okay that right there take it all the way up to 10,000 wow that's starting to look pretty good now one thing we could do is we could say well let's tweak the first structure up a little bit to bring the zonal anastropi up we want more contribution of this component right here and so we go go ahead and say make that point six now look this is doing better but look we're now above the sill the reason being is that the sum of point five and point six is 1.1 we're modeling to 1.1 so let's go ahead and drop that down now the contributions point four for that structure we have point four here we point four for this component right here now in the minor direction we have 303 hundred spherical both so it's effective with like a single 300 meter range spherical and so that's why we have that nice continuous shape now of course if I was to increase this range on the second structure you notice how we have an inflection point now because now we have a long range spherical plus a short range spherical spherical resulting in this result okay so effect okay so I think we've done a pretty good job modeling that verigram now you could maybe you'll say why I think there should be nugget effect you could decrease that contribution there and you could get now get nugget effect now you probably want to play around this too now and fix the in the minor direction now maybe you you want to kind of get a little more technical in the short range maybe you think now that there's a bit of a Gaussian structure here so what you could do is you could say that first structure is Gaussian look at that look at that and maybe you shorten up the range a little bit like that look it so now we're getting that kind of Gaussian shape to the first structure because not kind of rise we could remove the nugget effect we go ahead and add the second contribution back up look at that that is starting to look pretty sharp now I wouldn't be super excited about using the Gaussian imparacity typically you do see something more spherical exponential for spatial continuity Gaussian is commonly used in cases of topography thickness of beds things that you really know should be strongly continuous so one thing I haven't talked about here which I think is something kind of special is the fact that we have taken the major direction the verigram model projected in the major direction we plotted it right here we took the minor direction the verigram model projected in the minor direction and we plotted right here and look at what we did I think this is awesome is we incremented between the major so 45 is the major 67.5 90 12.5 and then 135 and so you can actually see the geometric and stripy model we had modeled the major and the minor and then the model geometric and stripy with the two structures was able to interpolate between so now we have a verigram model that works for all directions here's a bunch of different directions and for all distances right here so we can calculate the verigram between any pair of data in space so there before we can also calculate the covariance function the verigram and we can solve creaking and simulation and lots of estimation problems so it's very powerful stuff okay that wraps it up that was the interactive demo the cool thing is you can use this to complete load up any data set right there at the beginning calculate a bunch of different directions pick the major minor direction and now model the major minor direction all in Python nice and interactive and you'll have the parameters right here that you can go ahead and record and use in any other software or you could use these models directly with the very model dictionary verigram model dictionary and apply that to geostat pie creaking and simulation and so forth so we have calculated and modeled a consistent spatial continuity model that's positive definite can be used for all kinds of purposes okay as always I hope that this was helpful to you I'm Michael Perch I'm an associate professor at the University of Texas at Austin where I teach and conduct research in data analytics geostatistics and machine learning spatial statistics and so forth and everybody stay safe thank you
Hey, howdy everyone. In the previous lectures we introduced the idea of spatial continuity and suggested it was very important to measure it and to get it into our subsurface or spatial models of any type. We talked about the Virogram as a convenient methodology for quantifying spatial continuity. We talked about how it's calculated, but what we're going to do this lecture is get into the practical nitty-gritty details about exactly how do you calculate a Virogram given the fact that you typically have sparse data available to you. Well, just a reminder on the Virogram definition. For the purpose of Virogram calculation, we're going to have to take the average of the square difference between values at a tail location and values at a head location separated by a lag vector h. And all we have to do is take one half of that. Now, I mentioned the fact before that we take one half so that this works out right here that the covariance function is just equal to the variance, the cell minus the Virogram value. Therefore, the covariance function is just the mirror image of the Virogram function. And we talked about the fact that the covariance function can be related to a Corellogram simply by standardizing by the cell or the variance. In other words, if the cell is equal to one, the covariance function is equal to the Corellogram. So there we go. We got our measures of spatial continuity. Now, in general, we're going to work with the Virogram. We calculate the Virogram. We model the Virogram. We mentioned all of this covariance because within the algorithms, we're going to be using covariance, but we'll get into more of those details later on. I think that any new geostatician being a geoscientist, data scientist or engineer should have at least one experience in their lives where they've calculated the Virogram by hand. And just doing it today with my class, it was clear that it was a great learning opportunity. Students were making all kinds of interesting mistakes that it elucidated or illustrated their specific intellectual hurdle on the topic. And by working their way through it, I think it was very helpful. So let's do this might seem kind of trivial, but I think it's a good opportunity. What I'm going to do is I'm going to give you a one-dimensional data set equally sampled, equal spacing, a regular sampling on it. And I'm going to ask you to calculate the Virogram for any lag distance you want. You can, since it's regular space, you can pick lag one. And if you did that, you would be calculating the Virogram using pairs going from the first to the second, the second to the third, the third to the fourth, all the way through the data set. And it'd be pretty straightforward. And you'd expect to have kind of a low Virogram value, a high degree of covariance. That should happen at a short distance you can imagine. Or you could pick lag number two, you would skip the middle one. So you'd go from one to three, two to four, from three to five, four to six, and so forth. And you'd work your way through the data set, skipping, going from every possible tail location. But skipping one value going to the next, you could do lag three and skip two, you could do lag four and skip three, and so forth. Just pick a single lag that you'd like to work with. So let's go ahead and look at the data set. The data set that we have is shown right here as a data table. We have depth, and we have porosity that's been normal scored to have a go-sing distribution, a variance of one, and a mean of zero. So the cell is going to be equal to one for this data set that you calculate, that should help you. The data set is available in Excel document called 1D underscore porosity, Excel as x. It's within on GitHub Geostance Guy, the geo data set repository. So you should be able to find this data set ahead and pick a single lag distance and calculate the verogram for that lag distance. Now note, you should end up with a single verogram value for a single lag distance. So just remember that, that should help you with it. I will show you the solution. I'll walk through it with you in three, two, one. So if you calculate the verogram for all the possible lag distances, this is what you would get. So we got the cell equal to one. If we were to pick the first lag, we should get a value somewhere close to about 0.2527 something like that. You should have noticed that if you picked a lag at about 1.25 or greater, that you're going to start getting above the cell, you have a verogram value equal to one or even greater than one indicating negative correlation. You're above the cell now. You should drop down again and do this. People might be concerned by this behavior. Well, first of all, this looks pretty okay, pretty typical, right? We have a general increase in the degree of dissimilarity as we have greater distance. And that was one of our observations. We expected that. We reached the cell. We level out. We got some negative correlation going on here. And then things seem to get a little bit wacky. What's happening there? Well, if you look at this data set, there's only 10 meters of data. For sure, certainly by the time you're getting to almost 10 meters, like 9.75, you now comparing the very first sample with the very last sample for one pair. Well, one replicate does not give you a very stable statistic. We need enough samples to have a good representative statistic. So you'd expect that a lot of this kind of strange behavior is probably a result of just having two few pairs. We're comparing the absolute edges of the data set will not actually using any of the data in the middle, which is kind of interesting. That's the first observation we can make as far as what's going on with this kind of degree of noisiness. The other thing is that maybe it's possible some of this is information. Maybe if we looked at the phenomenon and we go back here, we look at it, there is a degree of cyclicity. And so this general type of behavior does indicate that there's cyclicity and in the next section, we'll talk more about advanced interpretation of diagrams and we'll talk about that. Now, just in case you were stuck or if it can help you, here's how I solve this problem. So I calculated for all possible lags, each one of the columns was a lag. Lag number one, I would go through and I would do this calculation. Take the difference between the first and the second data. They're all equally spaced. I can just scan through. Take the square of that and put it right here. This will be the second and third, fourth and fifth, fifth and sixth and so forth. Just working my way through the data. You've got to be careful. You copy and paste it all the way down. You just have to make sure that you stop when you get, when you run out of data. And what you'll find is, well, I'll show you as we increase the lag, what happens. Once you've got that, all you have to do is take that column and you just have to calculate the summation of, you calculate the number of pairs that are available to you, just using the count command. That's easy to do. And then gamma is just going to be equal to the summation divided by the number of pairs, multiply that by 0.5. That's it. That's all we have to do. Now, the one thing we've got to pay attention to, if you pick, as you pick an increasing number of lag, what will happen is you've got a longer lag vector and you can't go down as far because you're going to run out of data sooner. You're actually reducing the number of pairs available to you. So let's go ahead. We'll pick this one right here. So that's a lag 1, 2, 3, 4, 5, 6, 7. So by the time I get here, I can't, if I was to copy and keep going, I would be comparing data with nothing. I've run out of data. And so that's why we have fewer and fewer number of pairs available to us as you can see in the spreadsheet. All right. So that's how you calculate a verogram by hand. I think it's useful to do that at least once in your life. Let's talk a little bit more about the covariance function. We've already explained how the covariance function is related to the veriramits mirror image. The equation for the covariance function is shown right here. This is very interesting because the covariance is in fact equal to the expectation of the or the average of the product of the variable at the tail location times the variable at the head location. And then there's a subtraction here of the expectation of the tail value, expectation of the head value. And you scan that over all possible locations. Now first of all, if the mean is stationary, the mean at the tail is the same as the mean at the head. You would expect that this term and this term are just equal to a constant. The expectation of the tail and the head locations is just the mean. And so it's the mean square. So sample mean squared. So far so good. You could look at this and realize too that if the mean is equal to zero, you could just remove this component. And now we're just dealing with the expectation or the average of the product between the data values that the head and the tail locations. So that will be that is our covariance function. Now of course, we're assuming we have stationarity of the mean. We're also assuming that we have stationarity of the variance in order to make this calculation. If we calculate a covariance, we'd realize very quickly it is the mirror image of the verogram. So the red would be a verogram and the green could represent a covariance function. And it makes sense because you imagine that as you move further further away, the degree of dissimilarity will go up. But the degree of dissimilarity, which should be maximum when you're very close or at zero distance would be the highest covariance. That as you move further away, you would have less and less similarity. So that's relating them to each other. Let's define it formally. The covariance function is simply going to be a measure of similarity versus distance. It's calculated as as the average product of the values separate by lag vector centered by the square of the mean. This term right here. We could say the covariance function really is very much like the verogram upside down. If we assume stationarity, that works out. We will model verograms. We'll talk about verograms, but we'll recognize that inside of estimation and simulation methods that there is the use of the covariance within those methods because of mathematical niceties. We'll get into that. Let's summarize some of the basic components or definitions that we've really covered until now, but we want to get them really solid. Make sure we're using the right terms. The nugget effect. The discontinuity in the verogram at a distance less than the minimum data spacing, often reported as a ratio or as a relative nugget effect a percentage. Measurement error could be the cause of nugget effect. In many cases or many spatial problems, if there's no expectation of high degrees of discontinuity at distance is shorter than the minimum data spacing, we will tend to ignore the nugget effect. There are some spatial problems where you actually do have that type of discontinuity you want to include it. The sill is equal to the sample variance. It is the point when we reach the sill with the verogram, at that point we have no correlation. We no longer have any information. And if that's important, then we give it a name. That distance, the lag distance in order to reach the sill, the first instance where we approach the sill, we will term that as the range. It's the distance at which you no longer have any information. It's also really important because many of the verogram parametric models that we will talk about in the next two lectures from now when we talk about verogram modeling, they are parameterized by a contribution, the height, but also by the range, which is very useful. You can vary these models to fit different problems that have different distances of spatial continuity. Now at this point in the lecture, as I change gears a little bit, many students get a bit confused because we were talking about verogram calculation and then suddenly we start talking about search templates and then we're going to start talking about valid spatial models and about how do we model in full 3D? And they may not realize it's all part of one thing and the theme is this, how do we practically calculate and model semi-verogram continuity, spatial continuity from the available and often very sparse information? How do we get that done? And we got problems. The first problem is that we have to calculate the verogram with irregularly spaced data. So we need to use search templates with different search parameters in order to get around that problem. What about the issue of how do we ensure that we have a valid spatial continuity model? And so we'll talk about how we can fit spatial continuity models with a nested and additive set of spatial continuity models that are known to be valid, the nugget, the spherical exponential, and the Gaussian and so forth. Another problem we have is how do we model in full 3D? We've only calculated the verogram in 1D. How do we move from 1D to 3D? And the answer is that we're going to model in primary directions. Those directions are going to be mutually orthogonal from each other. They're all going to be 90 degrees from each other. And then we're going to we model them and we'll turn them as major horizontal, minor horizontal, and the vertical. And then we'll combine them together in a consistent manner, assuming in general a assumption of geometric and a stroppy. And we'll cover that. We'll get into more details about that. But for now, let's get into how we calculate a verogram with irregularly spaced data, search templates and parameters. In the next lectures, we'll get into this idea of modeling with these spatial models and the 3D and all of that. So how do we determine what pairs to group together in order to calculate a verogram? Now in the data set we already worked with, all the data was regularly spaced, 0.25 meters per quarter, a quarter of a meter. And so it was very straightforward. Lag number one was 0.25 meters. Lag number two was a half a meter. Lag three, three quarters of a meter and lag four would have been a full meter. And so we could just scan through the data set. It was very straightforward. But often we're going to work with even if we are regularly spaced data, you could specify the offsets as grid offsets. You could do kind of very fast calculations and templates to scan through that. But the diagonal directions get weird. Because this is one, this is one, that distance there is not, in fact, going to be one. If we're working isotropic, that diagonal direction right there would be like 1.41. And so it gets awkward. We don't exactly have the data at the right distances. So that's a problem, even for regularly spaced data. We're going to run into troubles about how to determine pairs. Irregularly spaced data gets even worse. The data might not even, there might be some average distance separation between the data. But in general, there's no reason why data is going to be specifically at exactly at a certain spacing. In spatial context and spatial problems, often because of all kinds of constraints, limited sampling and so forth, and sampling to answer certain questions, data is not going to be regularly sampled. And so the distances are going to vary. So what do we need to do? But if regularly spaced data, we have to make a decision to work with some form of a tolerance. We're going to work with a search template that's full of different tolerances. So let's talk about the one after note. First would be distance tolerance. Now it makes sense, as I mentioned before, that if our lag separation distance was 200 meters, it's quite possible that the data is in fact maybe 175 meters, maybe 225 meters apart, not exactly 200 meters. And so you put a tolerance in. You say, well, I'll take plus or minus from this point, plus or minus, let's say 200 meters, plus or minus 25 meters, or some type of a distance tolerance. So from a tail location along a certain direction, we'll say this is lag number three, but I am willing to accept plus or minus a distance tolerance. Okay, so far so good, right? The next thing we need is, well, what's the chance if I'm calculating a vergram in that direction? That we're going to have the data perfectly lined up in that asmif horizontally or that dip vertically. You might expect that maybe if that was 45 degrees, that looks like it's more like 60 degrees asmif from north. You would expect that maybe the data isn't exactly on the specter, maybe it's off a little so then you put a plus or minus on the asmif also or on the dip. The result is you now have a distance tolerance. You have a asmif tolerance. And so you have a space, a template over which I would consider if there was a tail data point here and a head any data here within that template would be considered to be within the lag three bin for that direction given my tolerance. And so that's what we do. We use the tolerance as to give us the ability to find pairs in the presence of a regularly spaced data or in the presence of even regularly spaced data when we want to work isotropically. And we have to count for the fact that we want multiple different directions. Okay, now you'll look at this template and you realize, well, something's missing here because you don't see these lines just carry onward, carry onward in this direction. And the reason is that there's another parameter and that's the bandwidth. And what we're saying with the bandwidth is that we like the asmif tolerance. We think it makes sense to use it. I agree, it's a good idea. But imagine when we get to lag 20. Imagine just how far away from that vector, I would select data because of that as the tolerance. It would continue to grow the further I go out, the further away from that vector. And so the bandwidth says, well, I'm not going to go any further than this away from this vector. And it prevents us from mixing information that perhaps comes from very far away from that directional vector just because we're really far out. Okay, it's used actually quite a bit vertically because you can imagine if we had a dip tolerance, we're using a dip tolerance. That what you could run into is that we could start mixing data from different stratigraphic units, you know, very quickly. And so we would consider using this bandwidth to constrain ourselves. So we're not taking data vertically to our offset from the current location we're at or current elevation we're at or within our strato correlations, whatever it is. But we constrain ourselves to try to stay within a single unit. Now of course, we want to model by regions, by groups, and we'll talk about that later. But in general, that's how bandwidths are used. So that's the result of our template. So we take that template, we can position it in our data set and identify all possible pairs. So let's take the example of the fourth lag. The fourth lag is going to be this shape right here. That's the outline of it. We position it on a data value. Now each one of these dots, just for illustration purposes, a nice regularly spaced data set. And so we position our template and now we're trying to identify this is the tail, what are all the possible head values that could be compared with the tail value as a for each one of those red circles, that's another pair tail head, tail head, tail head that are going to inform the fourth lag bin in this direction. Now of course, we don't want to just do that. We'd want to move that around to all possible data locations. And as we move it around, we would identify all pairs. So imagine that template can scan all the way over all possible data locations. And that would identify all possible pairs within that direction. When you're done, you take, you've taken the square difference of all those pairs, you sum that up or you calculate the average of it by dividing the, by the number of pairs, you multiply it by one half and that gets you a single barogram value. One, two, three, four. That would have been for this lag bin four. You would have done it for one through three, five through whatever the number of lags you want to look at. And that would give you the experimental barogram. Each one of these points representing a single lag, which is calculated by scanning that template through. Now we should talk about some of the options, some of the other considerations. First of all, think about data transformations. Now there's a variety of different transformations you might be interested in. For instance, you may transform your continuous variable to Gaussian or normal, a normal score distribution with a standard normal distribution with a mean of zero variance of one. If you're working with Gaussian simulation methods. And so you'd want to do that before you calculate the barogram so that you're consistent with the way that you'll be simulating the variable. You may also transform the categorical variable to a series of indicators if you're using indicator simulation or maybe the categorical variable to a continuous centroid for the truncated Gaussian type method. You have to transform the data so that you're consistent with the way that you'll be modeling with the data. Also, you may consider coordinate transformations. The barogram should be calculated aligned with the stratigraphic network. If your stratigraphic network has this types of complexity to it, it's coming up and down. We would have straddle units that are going to be based on a bottom, a datum surface, a top surface, or so forth. And they may be proportional or on-lapping. There's a lot we could talk about as far as what type of coordinate scheme. By transforming first the coordinates, we're flattening the coordinates. So now we're working within a nice flat box. When we calculate the barogram, we ensure that we are calculating along the straddle directions. And this is essential because if you just calculate a barogram using the z elevation and you're crossing all kinds of different straddle layers that are coming up and down, you just calculate right across those layers. You're going to definitely underestimate spatial continuity. You won't get the right spatial continuity model. And then finally, you in fact are going to calculate the barogram. We talked about correlograms, we talked about co-verances, but at the end of day, what we use within estimation simulation, the inputs into these methodologies will be the barogram. I mentioned also previously that you're going to calculate the barogram in a lot of cases accounting for directionality. You're going to count for the fact that spatial continuity is not the same for all directions. And so how do you find out what directions you should calculate the barogram? Well, first, you can inspect the data. You can look at all available interpretations, checks, sections, planned views. Find out what the paleo-flow directions were within the reservoir. Was it a carbonaceous ramp in which you had certain types of paleo environments with different energy levels that were aligned with the ramp margin into the center? There's so many things that can be interpreted by understanding what was going on with regard to the depth position. It resulted in that reservoir. So we should be definitely working directly with geoscience expertise, geophysical information and so forth. Note if we talk about azimuths, we're talking about angles that are clockwise from the north. And so what are you going to do? Well, you can look at different directions. And then what you're going to do is you're going to try to choose a set of three perpendicular directions. So you're going to have the vertical, the horizontal, the major, and then you have the horizontal minor. And so those three directions all together will give you a full three-dimensional model of spatial continuity. So how can you do it? Well, the first thing you could do is you could first try to calculate an omni-directional barogram where you don't account for directionality. Use very big tolerances, looking all directions for the same distance. This is often done in the horizontal direction. And you can determine whether or not you even get a well-behaved spatial continuity model that you can try to analyze or work it. If that works, then you could further refine and start trying to calculate the barogram in a major horizontal. And the two directions perpendicular in 2D would just be the minor horizontal. In 3D, you would add a vertical. You try to see whether or not you're able to actually calculate barograms in those different directions that are consistent with each other. Then what you do is you will then, if you can detect directionality as trape within your phenomenon, you're going to then model all three of those orthogonal directions. And you're going to assume a geometric model of as trape at all directions in between, at all other directions. So what does that look like? Okay, so we got y, x, this is two dimensional problem. Through all of our geologic information and inputs, through looking at geophysics, through looking at data, we identified that the 0, 3, 0, 30 degree asmeth is the principal direction. Okay, so this is going to be our principal direction. By default, since we're in 2D, we're only going to then consider the 120 degrees, which is going to be 90 degrees or orthogonal to our principal directions. That's going to be our horizontal minor. They're both rotated and aligned with each other. We'll calculate the barogram model and specifically parameterized by the range in that major direction. So the range is pretty long. We'll calculate the range in the minor direction and we'll assume an ellipse or ellipse soil in 3D. Range change over different angles. That model is valid. That's a model we can use for spatial continuity. So now we have a full three-dimensional model of spatial continuity by only having to calculate the barogram within the major direction, 30 degrees and the minor direction horizontally, which is 120 degrees asmeth. And from that, we can fill in all of the other possible directions using this geometric model of Amstropping. And we'll talk more about that in the coming discussions. What are some guidance? What's some information I can give you with regard to barogram calculation parameters? First thing, if we were talking about general guidance, it would be the lag separation distance should coincide with the data spacing, specifically the minimum data spacing. There would be no value to calculating the actual barogram at distance shorter than the minimum data spacing. You don't have any information. There would be no possible pairs. So in general, that's what we're going to stuff. The lag tolerance is generally going to be chosen to be one half the lag separation distance. Why do we do that? If this is the lag separation distance from lag one to lag two, if we set 50 percent of that as the lag tolerance, what happens? We have no gaps in our template. The second lag bin is adjacent to it goes right up against the third lag bin and we don't lose any possible pairs. Now, if you have a very noisy barogram, you could increase the lag tolerance and now you have overlap, which means you potentially have data that would be considered in lag three bin and also in lag two bin. There would be overlap between the templates for each one of the lags. That's fine. That will cause a smoothing effect on the barogram to make it a bit more interpretable. Default started at one half of lag separation distance for the lag distance tolerance. So, barogram calculation. Of course, we did it in Excel. It was tedious. We did it. So we got a little bit of hands-on type of experience with it. But let's get a little GsLive experience. Now, students will typically complain about this part because GsLive is a set of 4-trans executables run by parameter files. You have to run them in the command window. There's no GUI. It's pretty old-fashioned. But GsLive is open source. It's standard. In fact, if you look into many, many methodologies or packages for GeoStatistics, for instance, PyGsLive is basically just wrapping the 4-trans GsLive and putting it in Python, which is very useful appreciation to the authors on doing that. But you'll find that many applications for GeoStatistics are actually in fact based on GeoSlide. It's been used in many different places. It's very well tested. It works very well. You have complete expert control. You can build your own workflows. You can do all kinds of neat things with different types of scripting approaches. I've even used Visual Basic to do it. I've scripted it through C++. And in Python, I've actually put together a very straightforward wrapper. It's called GeoStat's Pi. And all it does is it takes the execute goals, runs them, writes out the parameter files, runs them, takes the input back in, re-implements a bit of the graphics program. So you can put things together in a nice Python script. Only works on Windows. I haven't properly wrapped it. Okay. The main point is we go back to GeoSlide. We get back to the fundamental tools and we'll have a better understanding. The method, you have a set of executables, basic algorithms. You put them together in workflow. You have parameter files and they are ASCII files with just the variables. And they'll have comments so you know what each one of those are. And you're going to have the, and you can edit them of course in notepad, textpad or whatever text editor you like to work with. And the data files will be in raw ASCII, their GeoES format. It's a very simple ASCII format for data. And the graphics are going to be in post script, which are really nice. They're vector-based graphics, but you need a viewer and in order to be able to look at them. So this is what it looks like. We have a data file. Our GeoES format is shown right here. The name of the file, the number of columns, each with a variable. The name of the columns are listed. And then the data will be either space or tab-delimited with the end columns of data. And it will carry on until and then the file and empty line will indicate the end of file and the program will stop reading the data file. The parameter file for a program like Gambe, which is the one we'll use for calculating barograms, we could have used Gam because the Gam program works with regularly space data. But I thought it was much more instructive to use Gambe because it's more general to regularly space data. But we have our data here. We can go ahead and look at Gam. We tell it the name of the file, which is 1D underscoreprocity.dat. We tell which columns to look for x, y, and z, x, y, and z, 1, 2, 3. We tell it how many variables we want to work with. We're only working with simple barograms. None of these cross barograms are anything. So we just have one variable to work with. And that's in column number four. Trimming limits in case we want to remove bad data, data that's out of bounds somehow. We have no values. You'd use the training and trimming limits here to remove it. The file for the barogram output will be called gambe.out. The number of lags. Okay, this is going to all should make sense based on what we just talked about. Lag separation distance 25 meters. That makes sense. Lag tolerance, one half lag separation distance. So for so good number of directions, we've got a 1D problem. There's only really one direction to consider. And then the azmeth, that doesn't matter. We're going to be working vertical. You'll notice that the x and the y are all equal to each other. It's only the z that changes. So it doesn't matter. We'll just say zero azmeth, 90 degrees azmeth tolerance, we're not concerned. It's really isotropic in the horizontal direction now. And we'll put the bandwidth doesn't matter here. But we'll say the dip is 90 degrees. The azmeth tolerance is 22.5. They're all in the same line. But we just put some tolerance just in case I didn't want to put zero and then have it all determined by some floating point error or something. So I put a little bit tolerance. And the bandwidth, I put a little bit of bandwidth so that it's not going to be so it won't go far away. But here the problem is completely vertical. So we shouldn't be too worried about that. So in fact, what we're investigating is a cone and then a cylinder going down the well. Then the number, do we want to standardize the sills? Well, the data set already has a variance of one. We should be fine. The number of error grams will be just one verogram. It's going to be one verogram type is just a traditional verogram. And the head and the tail variable is only going to be variable number one. We only have one variable we're working with. It's just it with itself. The output file would be the semi-verogram lag number, the lag distance, the verogram value, the number of pairs, this is important because if you have a point that has no pairs, you should include it. And then the mean of the tail and the mean of the head so you can check whether or not you have a stationary mean for that one. So we go ahead, we can use that information, we can go ahead and try to run GIs Live. And I think just like calculating the verogram by hand, it's useful to run GIs Live at least once you like to understand what tools available to you. You may want to just put a workflow together very quickly. You can do it with GIs Live. I have put 1dpress.dodat on GitHub. And so that's available to you. It is located at, oh, I had it in the previous slide. It's located in my Geo data sets repository so you can download that. We'll fill out parameter files for Gambi PAR, which is the verogram calculation program for regularly spaced data and VARG Plot, which is verogram visualization given the output from Gambi. And we'll make sure we have the GIs Live executables Gambi.exe and VARG Plot.exe which are both available in Windows and Linux. They're available on Web on the internet. You just type in GIs Live. You'll find it very quickly. They're available on the website to forget what it was called. So let's go ahead and I've set that up already and we'll do it very quickly. And so I have my example in a working directory right here. I have my 1dprosy data that I download from GitHub right here. No surprises. I opened it up in Textpad. I have the Gambi.par and I already talked about those parameters, the different types of verograms. We're just doing a traditional semi-verogram. And I have the VARG Plot program, which I'm just telling it that I have one verogram to plot. I tell it what distance limits are, the title and how to put dash lines, dots, experimental verograms. So I want to be dots. Now all I have to do to run this is I just have to get a command prompt. I'll come right here, open up my command prompt and I just have to type Gambi. Now if I run this, it's going to assume Gambi.par in the working directory is the parameter file I want to run. I could do this. And now I'm telling it, Gambi program used the Gambi.par parameter. That's pretty useful if you're scripting with a batch file or something. You can have a bunch of different parameters run them in sequence. So I ran it. I got a little bit of summary statistics from the data set. Yes, the variance is pretty close to the one. I might have thought about standardizing it because it's not quite one. It's not perfect. It's not enough data to have, you know, once again, the belief in the law of small numbers. A small number set isn't going to exactly have the representative statistics of the population, which with the transform should be 0, 1, 0 mean, 1 variance. Here we're off by a little bit. It's about 96.96 instead of 1 for the variance. Okay, so we just ran that. What came out of Gambi was this file right here, which we showed before. The varogram lag, the lag distance, the varogram value, the number of pairs. So this is pretty useful. You can look through it and see number of pairs and so forth. Next to visualize it, we just have to run the Gambi, the bargplot program I mean. And once again, you could do bargplot.par or you could just run bargplot. And if the name of the parameter file is bargplot.par, it will just default to look there. Now, if you don't have a parameter file and you run the program, in fact, it will, if you delete the parameter file and you just hit bargplot. And you get to oops, bargplot. And you get here, if you had no parameter in the work, no parameter file in the working directory, it would generate you a fresh blank parameter file that you could then populate. Okay, so we already ran that. I already have GhostView, good old GhostView installed on my computer. That allows me to view post-credit files. And so if I open it up, I get something like that. And of course, you might look at that and say, well, you know, you shouldn't really have the lines on it because it's an experimental barogram. It's not an barogram model. And so we should get rid of those lines and why were you doing that? So I could come over here, lines 0, 1 and I put 0 there. And just to prove that, in fact, I am running this and go ahead and oops, I'm sorry, I have more than one command window, I had to find it. And so I could run it again. And so you see 0 there now, so the parameter should have changed. And I can go back and open that up again. And there. Now you might say, well, I want to see all those points. We could increase the y-axis. That's one of the parameters. We could look further, we could calculate more lags and so forth. But this is a very straightforward way to calculate a barogram. So once again, I invite you to take that data file. Go back or rewind to the parameters I used if you need help. Go ahead and try running JS Live. I think it's good to just try it out. So if you run it, this is what you should see. I did a little bit better job tweaking the parameters to show all the points. But this is what we'd see. This is the original data set. And this is what we'd get. And so we could talk about how we've interpreted that we have a range of about 1.5. We have some type of cyclicity or trend, but we'll get into those details. So I think it's useful to try an example of calculating or estimating the best barogram search parameters for these three situations. One, two, and three. And so based on our previous discussion, go ahead and pick some search parameters that you would use for each one of these. And I will walk you through my recommendations in three, two, one. So the first one is pretty straightforward. We want to calculate the barogram isotropic. The lag distance or lag size. Look for the minimum data spacing. It's isotropic, so I don't care what direction. Just look at pairs that are close to each other. And I'd estimate that's about 50 meters apart given these axes. So about 50 meters. The lag tolerance should be about 1.5 that. And the data extent is 500 meters in this direction, 200 meters in that direction. So we could really only go up to about 1.5 that maximum extent. And really have reliable barograms. So about 250 meters. Well, 50 meters for each lag, we can go 250 meters. We can do about five lags for 250 meters. The horizontal bandwidth. I'm not really going to be concerned about bandwidths in this situation because I'm working with an isotropic barogram. I don't care about trying to limit the distance offset from the vector because with an isotropic barogram, I'm only concerned about distance. I'm not concerned about directionality. The asmr could be any because I set the search tolerance to 90 degrees. What does that mean? Well, 90 degrees while I'm going zero in north direction. Then I'm saying go 90 degrees. This way or 90 degrees that way. So my tolerance is saying I'm going to search through like this. I'll take all possible values above my set. Now my students, this will result in isotropic, though my students have been concerned about this in the past, they think why not put 190? I mean, 180. Sorry. Why not put some number greater than 90? What will happen is if you're searching this direction, you put more than 90, your template starts to go back on itself, which means that as you're searching forward, you're going to actually double dip. You will double include some of the pairs just if they have the right orientation to do that. And that'll create a bias. So 90, for an asmr tolerance for an isotropic barogram. So what about the asmr tropic case where we want 45 degree direction? So imagine a 45 degree direction going like that. The data is clustered now. So the minimum data spacing is going to be maybe about 20 meters. Our lag tolerance about 10 meters. Number of lags, while we still have about 250 meters of data in the longest direction, the greatest extent. And so we can work up to about 250. Well, 20 meters for lag, 12 of those 240. So that's pretty close. So we could do that. Horson will bandwidth, while we have directionality, we may want to limit that. It'll depend on the problem, but you could pick something like 200 or so. The asmr, well, it's a 45 degree angle that we asmr, if that we want to work with and so 45. And a pretty common asmr tolerance is about 22.5. That gives you about 45 degrees that's going to be included in your tolerance. If you wanted to get more precise, you had the data supported, you could try to decrease that. If you don't have enough data, you could increase that to get smoother barograms in each direction. What about an asmr tropic in the 90 degrees? So now we're trying to go along this bed. It's stratified. So we know that things are kind of changing. We want to kind of focus within a certain area, not mix up data, not mix up information too much. Well, first of all, data is pretty clustered up about maybe 15 meters apart from each other. Lag tolerance will be half of that. Number of lags? Well, look at this problem. It's 300 meters in extent. So we can go about 150 meters in the 90 degree direction. So we get 10 lags at 15 meters each. By putting a 5 meter bandwidth that does provide us some control to ensure we're not mixing pairs from this straddle unit with this strada unit or that strada unit, it keeps things separate. So we could use a vertical bandwidth in order to impose that. Now we would expect that since we're along the x direction that we want to work in an asmr of 90. Remember, y would be 0, x would be 90. We could use an asmr of tolerance if this data could be in and out of that section. And so we could put that in based on what our data configuration is, whether or not we need that tolerance. And I make a note here, I just added this in just now. I apologize for excluding this, but we would want to have control over the dip. We're in the x direction, but we want to say, let's go zero dips. So we're traversing along this direction. So this background would be calculated so that we're going along these layers. We would control that by indicating a specific dip to do that. All right. So that was a lot on barogram calculation along. So I hope that was useful to you. If you have any questions or comments about this lecture or any other lecture, I'm always happy to discuss geostats. I'm available on Twitter. My email is easy to find. I can also be contacted through comments on these videos right here. So I hope it's helpful. Okay. Thank you.
Hey, howdy everybody. Let's go ahead and carry on with the topic of verigram interpretation. Now, last time when we were talking about verigram calculation, in order to support the ideas around verigram calculation, we did cover some very basic interpretation ideas. We talked about the fact that we have a sill, and the sill is the variance of the data that's used for the verigram calculation. And so, if we're dealing with data that has been normalized to have a variance equal to the standard deviation equal to one, then we would expect that the sill will be equal to one. So, a lot of students ask, how do we calculate the sill? The verish trade forward just take the data that you're calculating the verigram, calculate the variance of it, plot the sill to support interpretation. The range is the distance at which the experimental verigram reaches the sill. The range is very important to us. I've talked about it before because beyond the range, we have no correlation. We know nothing is what I often tell the class. In addition, when we get into verigram modeling, you'll find out that the range is a critical parameter of the verigram models that we use. The nugget effect is that variability that occurs at very short scales. In fact, distances less than the minimum data spacing. And we would also note that it would be represented or calculated, therefore, at distances less than the minimum or the smallest experimental lag distance. We'll talk about how we can use the nugget effect too. Now, we've got to always be careful because the nugget effect, as we mentioned before, could be physically real as in the case of gold and gold nuggets. It, in fact, is a very short scale change in the spatial continuity of the grade of gold due to the fact that you encounter naturally forming nuggets of gold that are significantly different than the usual ounces per ton type of measurements of gold. And also, we have to be careful because nugget effect can also be the measurement error. So, if you take a data set and you sprinkle in random, uncorrelated measurement error, that would actually show up directly as nugget effect. So, these were the three fundamental concepts of interpretation that we've covered so far. Let's go ahead and get into much more depth around verigram interpretation. Geometric anestropia is critical for verigram interpretation and modeling. The idea is very straightforward. It simply is that the spatial continuity, the range of spatial continuity, depends on the direction. And so, if we look at this spatial phenomenon right here, it's a 2D example. So, through this lecture, I'll use a lot of 2D. It's very easy to visualize. We have a field of prerocities between 4% and 16%. And we can see the prerocity values of all possible locations. And if you look, you can see, pre-obviously, that there is a greater degree of spatial continuity in the x direction than in the y direction. What I've done is I took that phenomenon. I calculated the experimental verigram. The blue is in the y direction. Well, that will always be 0, 0, 0 asmeth for all of our examples. The y-cornet, 0, 0, 0 assumption is pretty typical when it comes to the use of horizontal asmeths in verigram direction, determination. The 0, 9, 0 will be orthogonal to that direction. It's in the x direction like this. And so, we have green for the experimental verigram points in the 0, 9, 0 x direction. And if you look at that, we can calculate a range where we're at a point where we reach close enough to the sill. We're right there at the sill at around 300 meters. And I went to this image and I drew an arrow with that 300 meters. And then, in the y direction, we reach the sill a little bit of a wobble, a little bit of cyclicity. It's interesting if you look vertically. You can see there's some cycling going on. But we reach the sill at about 150 meters. So about a 2 to 1 ratio of horizontal and is trappy. And so I drew the ellipse. And so effectively, this ellipse right here is our model of geometric and is trappy. It communicates the idea that in two principal directions of continuity, the x, which we will always know as the direction with greater continuity in horizontal, we'll call it the horizontal major. And the direction with orthogonal to it will be the horizontal minor. And then the ellipse is representative of the behavior at all angles in between or off diagonal to the major and the minor. And so this is a model of geometric and is trappy. We calculate it identified and calculated the ranges in the major and the minor. And we can plot it and look at it and we'll use it for modeling later on. It will be central to everything we're doing with modeling. In fact, we commonly find that most of our spatial phenomenon does have geometric and is trappy. It's all over the place and we use it to model. Vertical range of correlation is often much less shorter than the horizontal range due to depositional processes. The way that the rock is being laid down, geologists know this. They talk about the difference between vertical and horizontal type of relationships. And they know it as Walter's law. And they teach it and they recognize it from their own crops in all of the various geo data sets. And once again, juestats is a numerical representation of geology to practical science around geologic modeling. So it makes sense that we represent that numerically. The ratio of horizontal to vertical is commonly known as the horizontal to vertical and is trappy ratio. Geometric and is trappy is also common in the horizontal direction just like in this example. This is all horizontal x, y, 0, 0, 0, 0, 9, 0 in this direction right here. And once again, the ratio of the horizontal major direction, this direction here, to the horizontal minor direction is commonly known as the horizontal major to minor and is trappy ratio. And as we calculate verograms, we will consistently see this type of behavior when we calculate in the two primary directions, or orthogonal to each other, we'll often see the range greater in one direction less than the other direction. And once again, just so nobody gets this wrong in my class, the direction with the greatest special continuity, in other words, the greater range is known as the major and the one with the less range is known as the minor direction. And they must be orthogonal to each other, 90 degrees separated from each other. That's how we parameterize geometric kind of trappy. Now, I want to kind of hit this a little bit harder because when we model, we're going to use geometric and trappy all the time. So what we'll do is we'll zoom into that problem. That's all I did here. I took one of these ellipses, I zoomed into it. Once again, I've drawn this arrow to 300 meters. This is 150 meters. We got the ranges right. This is the major. This is the minor. And this geometric and trappy model allows you to be able to calculate the range in all possible directions, not just the minor and the major. And so I've drawn 0300060. Of course, off diagonal angles, I could have picked 013 degrees or whatever I want to pick. Now, it's going to provide us a valid measure of spatial continuity for all of these directions. And the way it's going to look, it's going to look like an interpolation or a stretching squeezing of the common structure. In fact, if you look at this structure or the shape in the minor and you stretch it out, it's actually equal to what we have in the major. So we're using the same shape we're just stretching it or squeezing it in order to find all of these off diagonal directions. And they're associated ranges and the verogram values for any possible distance. That's the geometric and the stropping model. It's effectively the major, the minor, the shapes are consistent between the major and the minor and then an interpolation between them. And you could look at the ellipse here, this ellipse, the line on this ellipse. So this would be the range in 0100150. This is the range in 090 asmeth, which is 300. And this line is the range in all of the other directions. So it's pretty cool. We have a consistent full two-dimensional. Now, of course, you can take this ellipse. And if you add another axis, you'll get a ellipsoid, which is a three-dimensional representation. And so doing that, you now have a full, all possible direction representation of the anastropi, directional anastropi. And this is all critical because when we start modeling, we're going to build our verogram models by partitioning the total variance of the problem. The sill is the total variance. We're going to break it up into components. And over each component, we're going to have a different spatial structure. The verogram model that has to act over all directions and brought all together, they provide you a full three-dimensional model of all the variances combined. And so it's a very powerful concept. That's what we use with use this geometric anastropi to do that. Cyclicity, another structure that we often see when we're calculating verograms, I gave myself a very simple problem here. If you look at it carefully, you'll notice that there's systematically higher values here, lower, higher, lower, do you see the cycles here? There's heterogeneity. It's not perfectly just cyclic, but this is higher, this lower, higher. And so you can see there's these cycles. So many natural phenomena on have cycles. Geology tends to cycle. There's changes systematically in forcing, allogenic, autogenic, causes all kinds of interest in finding out, pervading out, so forth. Sometimes these cycles are not real. Sometimes if you have sparse data, it's just due to the fact that you just have too few data to calculate a verogram. And so you see a bunch of noise. So we've got to be careful. We've got to make sure that we have a clean signal with the cycles and that we have enough data to support it. So if I gave myself this problem right here and I calculate the verogram in the vertical direction and the horizontal direction, you'd be able to tell which direction was vertical immediately because we have this, the vertical direction is crossing this layering these cycles. So it's resulting in the cyclic behavior in the verogram. So what can we say about this? Well, first of all, the cycles in the verogram have a wavelength. If we go from here to here, just over maybe about 200 meter wavelength in the cycle. Now what's really cool about that is if we look at this data set right here, we'll realize that about every 200 meters, we have a cycle. And so you can interpret the cycles in a lag distance directly as the cycles in the spatial phenomenon that we're studying. Another feature that we often see is Zonal and Strock. Now you'll notice I kept the same problem, the same example where we had the cycles and the actually actually the same experimental verograms that we calculated before. And so in this example, we saw a pairing of cyclicity in one direction and in the other direction, we saw the verogram rise up and fail to reach the sill. It leveled off at what we'll call an apparent sill. So when the experimental verogram does not reach the sill in a direction, we call this Zonal Anastropi. It's often paired with cyclicity or some type of trend in the other orthogonal direction. Now it's very interesting because just like we could interpret directly from the features of cyclicity, we can interpret directly from Zonal Anastropi. In fact, the variability within the layers is in fact about 77 percent. And if you look at it, the verogram has risen to about 0.77. So the proportion of the variance that's experienced within the layers is where the apparent silt will rise to. In addition, the remaining variability is now the variability between the layers, the variability that on average we see in difference between being in and out of layers. And that is this other component right here. And if you look at it, you'll see that the cyclicity occupies that component of the variability. So it's very cool. You can interpret once again the apparent sill will be at the proportion of variability within the layers. And the missing part, the difference between the true sill and the apparent sill is the variability between the layers. Yeah, so another important feature, Zonal Anastropi, trend. Now if you look really careful at this data set, it's heterogeneous. It's got lots of different features in it. But in general, the values are higher and they find in the wide direction. It becomes lower porosity as we go up more blues and purples, more oranges and yellows down here. Trends are everywhere. In fact, in my experience modeling reservoirs, I think we probably use some type of trend model in almost every reservoir model. They occur everywhere in nature. So when you have a trend in your data, the background in that direction, that's experiencing the trends. The trend is in the wide direction here. The wide direction verogram, the blue, notice what it does. It rises up and then it carries on above the sill in this manner. Just takes off and keeps going above the sill. That is trend structure in verogram interpretation. Now when you see that trend structure, if you're kind of keen about geostats, you probably see in the book that there's power law models, there's fractal tech models that can account for that increasing variability as you go to larger and larger scales. But the problem is that we often use verograms for the purpose of simulation. And when we do work with simulation, we have to model to the sill. In addition, if we have a trend, it's much better to model that trend deterministic to remove it from the data. So effectively, we take this data set right here. And on every row, we would calculate the average cross. We'd get a function that would be decreasing. And we would subtract that function, that trend model from every value in this two dimensional model. And the result would be something that be have a stationary mean we wouldn't expect it to be all yellow down here and all blew up here. It would balance note. It would be a stationary residual with the trend removed. Once we removed the trend, then we would model the residual. And when we calculate the verogram on the residual, we would probably see that now it reaches the sill. And it no longer goes exceeds the sill and keeps going. It would in fact be like a more typical type of verogram structure without a trend feature in it. And so that's the typical workflow. But at the same time, it's important to recognize that when we see this type of structure in an experimental verogram, it's indicating that there's trend. And that's very diagnostic. You could go to the data, remove the trend and then test it again by calculating a verogram and establish whether or not the trend has been removed by how the verogram is behaving does it still have trend structures to it. Now, the main point here is that it turns out that rarely are verograms textbook. In other words, they don't typically just have trend, zonal and stroppy, geometric and stroppy and cyclicity. They usually have combinations. And so this image taken directly from the book includes a whole variety. Let's just talk through a couple together right now. So if you had this type of an experimental verogram in one of the directions you calculate the verogram, what would you think of that? Well, I think immediately you'd see that there's cyclicity. But the interesting thing about the cyclicity is it's climbing consistently upwards. And so this is a combination of cyclicity and trend types of type of structure. Here's a nice example of trend structure. That's a beautiful one. That's really nice. Here's another example with cyclicity. Here's an example where we have an inflection point and then we have trend. And so there's some interesting things going on, kind of a multi spatial frequencies going on there. Here's an example of zonal and stroppy. So we have all kinds of different features. Well, zonal and stroppy, perhaps with some cyclicity in it too. So there's often going to be complication. There's going to be a superposition of multiple types of structures that we can interpret and try to understand at different spatial frequencies. And so verogram calculation and interpretation can be pretty rich and very interesting. Now, I want to give Dr. Clayton Deutsch credit. He was my PhD advisor. And you'll notice that some of these slides in this presentation are either taken from one of his courses, which great appreciation for permission to do that and to use them in my class. I do appreciate that. And or also modified. And so of course, you know, being my advisor, you probably find that there's a variety of things I do that are inspired by Clayton Deutsch. And so appreciation to him. And basically in respect to Clayton Deutsch, I thought I showed this example because I think it shows a certain level of his cool nerdiness and Clayton, I hope I said said, I hope you're not bothered by nerdiness, but I think nerdiness is a good term now. And so this example right here was, I don't know if you remember the good old days when people used to go to malls, I think people still do. There was a display with a rock and I think it was a ole in some type of sedimentary rock, which was nicely polished. He bought the rock, took it home, put it on a scanner and scanned it in. The first thing you did was calculate the error of it. And I just think that's so cool. And so if you look at this nice example right here, I think immediately you can pick up which is the wide direction, which is the extra action. These cycles right here clearly are going vertical or in the wide direction, I should say. And this right here, this direction right here would be along the layers, right? And so what's very cool about this example is that you get a really nice example of what we call dampened whole effect. And what causes dampened whole effect? Well, if the cyclicity was persistent and was consistent, I should say, in persistent, like the thickness of the cycles stayed the same and they kept going the whole way through the data set. We would expect the cyclicity to remain pretty consistent. But in this example right here, you can see the thickness of the units are changing. You also see that they're getting kind of disrupted a bit. And so this general kind of irregularity within the cycles leads to a situation where over short distances, you got pretty obvious cycling. Okay. Over longer distances, what we're seeing is that at longer distances, we're getting out of sync. Sometimes we have a good cycle. Sometimes we don't. Sometimes we don't. You see that over that same distance. Things are either lining up or not lining up at longer distances. It starts to get confused. And so that causes the dampened effect. In addition, you don't see zone lines dropping this example. Not really. The reason being is because of these ripple type structures, I think they are. And because of the changes in thickness and so forth and undulations here in the horizontal direction, you do eventually see all the variability. It's not a case where you don't see the variability. So this is a great example. Another point we should make is that the verigram model is a measure of the change the change measured by a barogram. It's an average word difference, one half of pairs separated by a length distance versus distance. So if you think about a geologically speaking, geologic distance is really changed. The way that the rock changes is often interpreted as a geologic distance. And this distance, the right here is just plain old Euclidean distance, separation in space. And so the verigram is like a function that helps us go between Euclidean distance and geologic distance. Let's just summarize reviews some of the verigram interpretation ideas that have come up so far. We've talked about the verigram. It's important to measure spatial continuity. We showed in the last lectures that spatial continuity impacts the response of the subsurface, the things like drilling for hydrocarbons, could be the case if you're trying to drill for water, whatever type of resource grades in mineral deposit or whatever it might be. We mentioned in the last lecture, if we are working in that flat and stratigraphic framework, we have to ensure that we calculate the verigram in that same framework. The coordinate transformations have been applied. If we're working with Gaussian type of modeling methodologies, we have to ensure the data has been transformed to Gaussian. Or if we're working with indicator based methods, we have to transfer the data to indicator or truncation. So whatever it might be, we have to be consistent, calculate the verigram in the same data, important space as where we'll be modeling. The interpretation principles we cover trend, cyclicity, geometric and zonal and stroppy. As we go into modeling, interpretation modeling the verigram, keep in mind that the short scale structure is often most important. A lot of the action happens in short scales. We have to make predictions nearer well. We have to rely on very few wells at very short distances. We don't have a lot of data to help us at short distances. We also have to be cognizant of what's the size of the model cells that we're working with. Because of course we cannot capture structures below the resolution of the model when we're modeling. And so this will influence how we focus with verigram model. Vertical direction, verigrams are typically pretty well informed. That's because our data tends to be in the vertical direction. Even in unconventional where we have very long horizontal reaches, the well logs and other type of detailed information is usually constrained to the original vertical well before the drilling of the associated horizontal reach. Due to the limited number of wells, we are often challenged to calculate and to model horizontal verigrams. And so we will often rely on analog information, mature fields with more data, out crop information. We will often focus on the vertical which is much better informed and then use our geometric and trape model in order to with an anastropia ratio, horizontal to vertical, in order to understand the spatial continuity in the horizontal direction from the vertical and knowledge above this ratio. Okay, so bit of a spoiler alert because I want people to make sure they understand where we're going. This lecture series has been pretty lengthy when it comes to spatial continuity. There's a lot to talk about. What's it all about? What are we doing this all for? We need to practically calculate model spatial continuity. That's it. None of this is for fun. We're doing it for real practical reasons. So we have to do this from available and often sparse subsurface data. And so in the previous lecture, we talked about the use of search templates in order to take care of the part of having sparse data, irregularly sample data, we still want to pull pairs together over a range of distances in order to get a reliable verigram. We can't be way too specific in calculating the verigram. We need to have tolerances in order to get something that's interpretable. So that's complete. That's done. We can check that off our list. Now we've explained interpretation, but our next steps are to get into valid spatial models. How do we fit with a couple different nested additive spatial continuity models, including nuggets, spherical, exponential, Gaussian, and so forth? How do we fit these in order to formulate a valid model? And then fitting these, how do we extend that and understand spatial continuity in full three dimensions? We're going to model in primary directions. We'll talk a lot about major horizontal, minor horizontal, vertical. And we're going to combine them together with the concept of geometric chemistry. So everything we've been talking about is going to build up and in the end, we'll be able to characterize the spatial continuity with full valid three dimensional models. And so that's where we're going. These interpretation skills that we just gathered will be critical to be able to move forward into the next phase of being able to model the verigram. And that's it for interpretation. We'll next cover modeling. If you have any feedback, questions, comments, suggestions, anything, just go ahead and drop me a line. I'm easy to find my email at the school here. I'm Michael Perch. I'm a professor at the University of Texas at Austin. Thank you very much for listening.
Hey, howdy everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin. I record all of my lectures. So what I'm going to do today is this is going to be a walkthrough of all of the steps that you need in order to calculate a directional verigram and model it with a full, multi-directional, two-dimensional, positive, definite model that you can use for creaking or simulation. There's so many different things that you can do with a verigram. It's a powerful, predictive tool and a tool to just understand spatial continuity. Now if you want to follow along with this, just go ahead to my GitHub account, github.com, Geostatskyne, and have a bunch of repositories. And there is this repository right here, Python numerical demos. If you go there, what you're going to find is a list of many, many demonstrations. And there's some new interactive ones where I use IPI widgets and Metplot Live to automate things so that you can play with the methodologies and visualize the result immediately. It's called interactive verigram modeling, IPIthon notebook, IPYNB. And so go ahead and download that. Now you will also need a data set, but no worries, that's available to you too. There's another repository called right down here, Jew data sets. Now in this directory, there's a whole bunch of data sets you can work with. And so the data set that we'll use is right down there. And you can go ahead and work with that. Okay, and I'm going to walk you through the workflow and show you all of the steps and help you so that you can take this workflow, open up your own data set and go ahead and calculate a model to dimensional verigram. So I think this will be really helpful to you. I specifically coded this yesterday for my students because I had just assigned, or was about to assign a verigram modeling assignment. I thought it'd be really cool for my students to be able to have a nice interactive workflow to get experiential learning about verigram modeling. Rather than something kind of ad hoc where you change a parameter run, change parameter run, I want them to be able to visualize and see the impact of their choices immediately. I think that's very powerful. Okay, so every time I do one of these workflows, I like to put some documentation up front so it's self-contained. It contains some of the information from my typical lecture notes. If you go ahead and look on my YouTube channel, you'll find that there's a bunch of videos here available to you. There's a playlist specifically for this course, data analytics and geostatistics. If you view the entire playlist, you'll find that there are lectures right down here on spatial bias, spatial declustration, spatial declustering, also stationarity, spatial continuity verigrams, verigram calculation, verigram parameters. There's a lot of lectures here available to support you. Okay, so the verigram, what we want to do is calculate the experimental verigram and then we want to go ahead and model it with a positive definite model. And so some general observations about verigram calculation and experimental verigrams. And then we have the concept of using a nested set of positive definite, known positive definite verigram models that we can go ahead and fit and be able to and the ones available to us include spherical exponential Gaussian and nugget. That's why I've included in this method. There's less common ones like whole effect, dampen, whole effect, and power law that have not included. They're typically not used very often. Specifically, power law wouldn't be, wouldn't be appropriate if we're doing simulation or we would not honor the histogram, of course. And the concept of using geometric anastropi where we work with a major and a minor direction or orthogonal to each other. So we just have to in 2D, have a single parameter of asmif of the major. The minor is assumed to be orthogonal to it. And we have this geometric anastropi model where all we have to do is for each structure specify the contribution to the variance and the range of the major and the minor. And so we'll go ahead and we'll we'll work with those and I'll show you the result as we add those together and make a positive definite model. Okay, so geostats pi is the package that we're going to need for the verigram calculation and modeling. When I was teaching spatial data analytics last spring, I realized there wasn't a good Python option for the entire workflows of geostatistics and spatial data analytics. Not one that I felt was reliable and provided what we needed. So I ended up coding this up on the weekends, which was really fun. And I think it's great. We have something. Let's go ahead and the first thing we'll do is we're going to restart and clear all the outputs so that we have a fresh start here. There's no previous runs getting the way and we'll go ahead and import geostats pi. It has two components to it. Geost live is a bunch of visualization and like the geost live programs and also it has simple wrappers to run the for trend. We're not going to do that. We're going to use geostats, which is the methodology, which actually has re-implemented all of the algorithms like verigram calculation modeling in Python for us. So I think that's great. We're going to use a bunch of the standard type of packages like numpy and pandas for NDE arrays, multi-dimensional arrays and pandas for data frames, matplot live for plotting of course. And then we're going to have iPi widgets for building interactive plots at the very end. We'll do an interactive verigram modeling. I like to set a current directory, no exactly where stuff's coming from and then just read the data file. So if you want to work a sample data multivari biased, which is perfectly good to dimensional data set you could work with with prostate and permeability you go ahead and download that from my github repository, put it in your working directory that you'll set there and away you'll go. So when you run this, if you get no error, oh what did I do wrong? I do this all the time and I know my students do too so I kind of do it on purpose just to make sure everybody's recognizing that. It's a common problem. I skipped a block simple. The thing to remember about running a Jupyter notebook is until you've run the code it hasn't happened and so we haven't imported OS and so when I run this command with OS to work with the operating system to change the working directory, I get an error OS is not recognized. So go ahead and run that now the code works. You see that? So you've got to run everything in sequence. I often tell my students if you run into an error back up to the beginning and run back through chances are you didn't run a block. That's the most common mistake I see among my students. Okay, so let's go ahead. We'll read in the data set from the common delimited file and we'll go ahead and put it as a data frame from pandas. Pandas PD, read CSV will turn this into a data frame. DF is now a data frame and the prove it to ourselves we access for our DF data frame object. The built-in function to do a preview of the first five or six samples up front the head command. Okay, so yeah five first samples right there, 0 through 4 are shown and you can see the data that we just load it. Now I should have mentioned I also have acoustic impedance which is kind of cool in that data set. So it's a multivariate spatial data set. It's synthetic. You're free to just use it. Don't worry about it. I made it using multivariate geostatistical methods. So it's a pretty good data set. You'll see. We could separate it by facies but for this workflow we won't and the reason it's not too bad is because the facies do have spatial continuity and porosity and permeability. It's not like a system where we have a discont nudie between the facies. So it's perfectly fine to try to model the varogram between them. We would probably if we were building a real reservoir model we would probably maybe be more thoughtful about that choice but for now we're choosing simplicity and brevity of the workflow. So I hope everyone's cool with that. Let's look at the summary statistics. I love Panda data frames. For the fact we can do that very simply. We can see what's going on with our data sets. The values look reasonable for porosity. Good range of porosity. So we can go ahead and we'll do a normal score transform. The normal score transform is performed. It's turning the porosity, turning it into a standard normal distribution. Mean of 0 variance and standard deviation equal to 1. Now we do that specifically because we're acknowledging that these varograms are going to be used for Gaussian methods like sequential Gaussian simulation. So for consistency you have to work with the data transformed into Gaussian space, the normal score transform. Now it also does help without liars and provides in general cleaner looking varograms. So it's not a bad idea and specifically driven by the theory we should do it. Normal score transform is built into Geostap pi. I like the Gslive Normal Score transform. I find it performs very well and so I'm going to use that instead of a methodology available in other packets. So we go ahead and we can visualize the CDFs original two transformed and no surprises here. It really does. It was almost a Gaussian distribution for porosity but now look at that. Beautiful S curve, negative 3 to positive 3 effectively. It's an unbounded distribution but we'll find practically speaking our samples will likely be bounded there. Okay and if we look at the permeability distribution we've gone from a very strong positive skewed distribution probably getting close to the log normal and we've gone to a nice Gaussian CDF. Okay so we've done our transforms. Now if we want to try to calculate and understand the directionality of our spatial phenomenon plus if we're trying to figure out how to calculate good experimental varograms we really should do some data plotting in space so we can make good choices about lags directionality and so forth. Let's see maybe we'll even see directionality. Okay so let's set back look at that data set that's normal score transform and porosity. Let's focus on porosity. They're highly correlated with each other as you can see by the crossplot so we should expect similar spatial features but we'll just focus on porosity for this workflow. Okay high values are lighter colors darker colors are lower values on the normal score going from 3 to negative 3 and if you look at this hmm do you see it? Are you able to see the directionality? I can totally see the directionality right there. It's it's you can see about if this is 0 0 0 on the y direction and as of 0 9 0 in the x direction you can clearly see about 0 4 5 is a primary direction of continuity that you have this high low high pattern and so we're I would suspect that we'll see 0 4 5 so that's our ocular inspection approach to detect directionality. Okay so we've done that. We can also do verogram maps which are super cool. That's something that I coded up to and added to geostat pi and so geostats var map v which works with irregular data and we can load up the data frame the x and y locations the feature of interest and all we have to do is specify the number of lags going from the center to one side 11 the distance of the lags so 50 so that means we're going to explore an offset of 550 from a center cell in the middle here of size 50 and so we're going to extend 550 over this way plus 25 for half the distance across that cell in the middle so there's a cell in the middle and then 11 cells going this way it's a mesh and then there'll be 11 cells going this way 11 cells going up up so we're going to span from 0 to 550 plus 25 575 down negative 575 and the same in the x direction making a mesh. Then what we've done is we've used the basic contouring program after we've made the mesh and calculate the verogram values for each one of those offsets in the data set and we've kind of contour it up that's why you see some of these weird shapes those shapes are not it's just because we have sparse information here we have 11 plus 11 22 plus one we have a 23 by 23 grid and we've just done some simple contouring between those values right here it's a good idea to look at the number of pairs involved in each one of those cells and you can see in general if we have ill-informed cells getting towards the edges here we have much fewer pairs to work with so that might be a concern you see that darkening color but we're still pretty good it looks it really does look like we're always above 50 or 100 pairs so we could look at the matrix the or two-dimensional array that came up from this diagram map which would be in P number pairs map and you could look at that 23 by 23 array and you could investigate the values make a choice on maybe you should remove some of these values using conditional statements okay but I'm pretty comfortable now let's go ahead and sit back and look at that diagram map what do we see what's going on here okay well what do you think low-verogram values high-verogram values still is kind of an orangey color so if I drew a contour of the sill it would go somewhere like around here 45 degree nail that ocular inspection works the verga map is confirming it and the range if we want to pick the range off of this I could go all the way to the edge of the verga map and I still haven't reached the sill I'm suspecting zonal atoms trape in that direction I can go probably I'm eyeballing it right now 200 or so in this direction before I get to the sill here really guessing right now but you know it's good it's giving us kind of some first evidence to work with okay so I suspect our two-dimensional verga will have a major direction in the 0, 4, 5 azmuth and I suspect that we'll see a about a zonal ash trape in that direction and the orthogonal the 135 azmuth I suspect we will see a probably about a 200 meter range or so all right now another good way to detect directionality and something we should do anyway because we're trying to model veragrams is we should calculate experimental veragrams now going back to the data you can confirm this but if you look at it you'll see that 100 meter spacing and let's just go back here 100 meter spacing would definitely get a lot of pairs you could probably go to 30 50 or so but I think I found it got a little noisy you go ahead and play around it yourself if you like to try something so but I'm going to say a lag distance about 100 meters half extent of the data set if I'm calculating the 45 degree direction or these other directions I'd say I can go about 700 meters because I have about 1400 meters across in the diagonal and I'm expecting I can calculate reliable verga groups up to about half the extent so let's just say around 700 meters or so I'll go out give or take okay and in addition to that I'll go ahead I can set the lag tolerance is one half the lag distance but that's not necessary I could actually increase that and cause overlap between the bins and further smoothing for smooth the result I think I did that because it improved the results a little bit but I welcome you to go ahead and try some differences so I have a lag distance of 100 meters I'm going to calculate seven of those now with the program it's going to go 0 1 2 3 4 5 600 why don't we go ahead and just make that one more lag so we actually get the 700 okay then we'll go ahead and we'll say no bandwidth because I'm not really worried about you know getting too far away from that direction and I'll put the asmr tolerance pretty tight we got pretty dense data so I want some good strong directionality now a great thing with working with python is I make an azimad which is just going to be a list of different asmr's and I can run my program looping over those and I'll calculate all the directional verga going from 0 22.5 45 right here 67.5 90 12.5 135 and all the way back to I don't need to do 180 because that'll be the same as zero verga grams in general are going to be symmetric so we're not too worried about that okay and now I should have mentioned i-cell in the verga calculation program you do have the option to constrain it to standardize the distribution to a variance of 1. now we've done the Gaussian transfer my suspect it should be very close to one it doesn't hurt just to say okay it's kind of like a belt in suspenders you know the distribution should be one I could check it right now I'm sure it'll be one then I could just say i-cell equals 0 but it doesn't hurt to standardize and make sure that the variance is exactly the one it's kind of a safe thing to do because if there is any deviation what will happen in the result is you won't reach the sill or you go above the sill if the variance is less than one you you'll have a lesser sill and you might interpret it as zonal anastropi when in fact it's just your distribution is wrong okay so let's go ahead and we'll set it to 1 and force it to standardize okay so run that block now what we're going to do is we're going to loop over all of those verga grams calculations and we're going to create this array right here will be the lag h offset this will be the verga and values this number of pairs we have that for record if we need now we're going to use a two-dimensional array so I can do all of my asmiffs all of those listed asmiffs and I can also run it so that I'm able to this right here will be all of the lags 0 100 200 300 all the way the 700 so I'll have those lags and we can plot them and when we plot them we'll just say well we'll loop over and plot all of those asmiffs their lags so we're sending every time we do this command right here lag iazzi with this placeholder here we're saying send the one-dimensional array for this specific row which is that specific asmif one-dimensional right here so we can plot that and we'll go ahead and just say well add to the name the asmiffs so we have a nice plot okay let's go ahead and run that enough talking about it look at look at that isn't that awesome i love working on python coding is just powerful because i ran all of those directional verga's look at that asmif 0 22.5 45 okay let's look right here range 300 range 400 range zonalanish trapey undefined effectively infinity really now I wouldn't say it's infinity i'm sure waltz-laws kicking in here and at some distance we'll see it reach the cell but over our study area we don't see all the variability okay 67.5 oh ranges now 550 350 probably about 250 you see what's happening here and then by time we get to 135 about 250 or so so it's fair to say that 45 is the major and by design it has to be 90 degrees but it is consistent 135 will be the minor and it looks fine it actually does look like it has about the lowest range it's similar to these other directions but i'm not worried about that okay so now what we've done now we did ocular inspection and we said the major looks like 45 we did the verga map the major looks like 45 we ran a bunch of verga and the major really is 45 okay so now for this program if you put your own data in you've run all of these pick the directions from the asmif array or list that are the major and the minor now if you come back up here this is the zero because remember python indexes is list by 0123 to n minus 1 so this is the zero direction the one the two okay so our major is the second one second one okay then we have the third the fourth the fifth and the six so my minor direction is the 135 which is the sixth sixth direction in the list so what I have to do is for my workflow to work now and be able to build the models I got to pick the major and the minor from these directions and so I pick up so two is the major six is the minor so down here in the code two and six once you've done that you're set the rest of the code will basically run automatic and the first thing it's going to do just to confirm for you it's going to produce the major minor plots it's the same plots of above but now this is our experimental verga grams in the two directions that we're going to focus on for modeling okay now what we can do is we can use two functions built into G as stats pie GOS that pie so there's the make verga function not a big deal all it's going to do is you provide the parameters for a vergram into this program and it makes a dictionary I know it's so simple but I've done it so it's kind of nice and canned so you run this make vergram with all these parameters it's going to make the dictionary okay and that dictionary is going to be like an object we can now pass for visualization pass for creating pass for simulation it'll be nice compact standardized storage the parameters nugget number of nested structures you can do one or two if you do one it's it's going to assume no contribution for these second structure parameters it's going to say the contribution zero inside and it's going to have nothing now you don't even have to give these parameters if you say nested structure one you don't have to actually list these parameters it will default to zero contribution okay so we will have the type of structure we have spherical exponential and Gaussian okay we have the contribution the contributions should sum to the sill we're working in standardized variables sum of contribution one and two should be 1.0 the azmuth keep the azmuth the same in this is the azmuth the main you keep it the same between the structures you could do that you could have different azmuths for the different structures we're keeping things simple here for ease of interpretation understanding and in most practical circumstances you'll find people will work with the same major direction and just have multiple structures that are consistent in that direction then major range now we have isotropic for the short scale variability structure structure one and then we have 99999 for the major of the second structure that's giving us a little on a copy and men of that so this is our first model of the verigram that we just observed this is a reasonable verigram model right here okay so let's go ahead we can put that into the make verigram function and we'll get that verigram dictionary that we can then visualize the v model function is the same as the v model function from Gs like it's there just simply to take your verigram model that object called verio and to project it in a direction now verio is a two-dimensional verigram model so all we have to do is just tell the model what azmuth direction do you want to see the model do you want to project it now it's not actually it's working with the analytical model so you can actually sample it at whatever rate you want okay so in that direction you give an x lag and you say sample at every 10 meters and do 100 samples and that would give you a 1000 meter long function and so the program would actually give you the index the lag distance at every one of those samples the verigram value at the all those samples the covariance function at all those samples and the correlogram of wow this is a three-for-one sale today for v model really really great okay so if we go ahead and take our reasonable model this these are the parameters right here no surprises I just set the azmuth for the first and second structure to be equal to the major direction i major is the index two for that as he met and so that would be 45 degrees right there and all of the parameters I showed above make the verigram object right here then what I can do is I can visualize 70 lags of it and each lag is going to be 10 so we're going to see 700 0 through 700 right and we can plot that up so we'll go ahead and make those verigrams plot them up and woo look at that awesome okay so it worked the make verigram made a reasonable verigram object that then was sent for display by the v model program and look when we plot it we see a nice function right here now I have lots of students ask questions like why did we worry about the number of lags and the x lag so let me show you something right now let's go ahead and decrease the number of lags to seven but we'll increase the lag distance to 100 now we're going to sample 0 100 200 300 and so forth when we make these verigram models now look at what happens do you see it it's piecewise linear so in other words you're only sampled seven points of the function it's not going to look very good so remember this number of lags and x lag is not a calculation we're not calculating verigrams anymore we're just projecting analytical form so this is just the resolution of that projection not a big deal let's go back to 70 and we'll go ahead and make the lag distance 10 again and we get something nice and smooth we can understand it and so that's it just make it smooth enough and it runs super fast like it wouldn't hurt like you could go ahead and say 700 and lag distance one and look at that that's that's going to be every meter sampled and it didn't really change much you know if you have 10 that's probably that's probably pretty reasonable given the ultimate range of this problem now if you're working very short skill phenomenon you can need tighter sampling not to be deal it's just display now what we're going to do is I'm not going to walk through this because this work this workflow walkthrough is not to teach you I pie widgets and how to do this but I hope you can look at my code and get some ideas of how to do this which is super awesome it's going to take that same plotting we just did but take all the verigram parameters and turn them into sliders and selection toggle boxes or selection boxes so that we're able to go ahead and automate that process just now okay so let's go ahead and run that now we get down to it this display is now going to display our interactive display so here it is okay so let's scroll that up we know these parameters now we got nugget effect the first structure type contribution major major range minor range and the second structure the same thing now notice there's no azmeth here because what this model is going to do it's going to take your major direction and assume this is aligned with that okay so we don't even have to put that in now okay and it's going to automatically display it in the major and minor on the plot okay now I've said it so that as soon as you change something the plot's going to appear okay so we have the plot now now I think it'd be fair to say and given my experience of what we've already done with this data set we could probably start off with like a 50% contribution or 0.5 for the first structure 0.5 for the second structure but our ranges are all zero so this is the major this is the minor ranges are all zero so it all looks like really effectively nugget effect this jumping upright here is at 10 meters the first sample on the v model if we'd sampled every meter this would go up to just one at one meter so you see it's just a resolution issue that we see this jump here it should actually be the nugget effect you know the range we used here is 0.01 should be up there immediately right now you might wonder why I defaulted it to have a minimum range it's because of numerical issues with my code if you put zeros in I use anastropi ratios inside the code and it causes divide by zero errors so we're just avoiding that okay so let's go ahead and make our minor about 300 and that's what we found worked pretty well before now look what's happened we now have a like a nugget effect of 0.5 because with this right here is no range and we have about 300 isotropic right there let's start to give this some range too we're gonna say the minor about 300 also okay now look what happened in the minor direction that's looking pretty good now you see that we're starting to get a pretty good fit right there and let's go ahead and set the major watches we want zonal anastropi let's make it big what happens is I keep making it bigger and bigger and bigger and bigger you see what's happening it's not cool it's leveling off okay so let's go well you know go bigger go home right let's like make this pretty big okay that right there take it all the way up to 10,000 wow that's starting to look pretty good now one thing we could do is we could say well let's tweak the first structure up a little bit to bring the zonal anastropi up we want more contribution of this component right here and so we go go ahead and say make that point six now look this is doing better but look we're now above the sill the reason being is that the sum of point five and point six is 1.1 we're modeling to 1.1 so let's go ahead and drop that down now the contributions point four for that structure we have point four here we point four for this component right here now in the minor direction we have 303 hundred spherical both so it's effective with like a single 300 meter range spherical and so that's why we have that nice continuous shape now of course if I was to increase this range on the second structure you notice how we have an inflection point now because now we have a long range spherical plus a short range spherical spherical resulting in this result okay so effect okay so I think we've done a pretty good job modeling that verigram now you could maybe you'll say why I think there should be nugget effect you could decrease that contribution there and you could get now get nugget effect now you probably want to play around this too now and fix the in the minor direction now maybe you you want to kind of get a little more technical in the short range maybe you think now that there's a bit of a Gaussian structure here so what you could do is you could say that first structure is Gaussian look at that look at that and maybe you shorten up the range a little bit like that look it so now we're getting that kind of Gaussian shape to the first structure because not kind of rise we could remove the nugget effect we go ahead and add the second contribution back up look at that that is starting to look pretty sharp now I wouldn't be super excited about using the Gaussian imparacity typically you do see something more spherical exponential for spatial continuity Gaussian is commonly used in cases of topography thickness of beds things that you really know should be strongly continuous so one thing I haven't talked about here which I think is something kind of special is the fact that we have taken the major direction the verigram model projected in the major direction we plotted it right here we took the minor direction the verigram model projected in the minor direction and we plotted right here and look at what we did I think this is awesome is we incremented between the major so 45 is the major 67.5 90 12.5 and then 135 and so you can actually see the geometric and stripy model we had modeled the major and the minor and then the model geometric and stripy with the two structures was able to interpolate between so now we have a verigram model that works for all directions here's a bunch of different directions and for all distances right here so we can calculate the verigram between any pair of data in space so there before we can also calculate the covariance function the verigram and we can solve creaking and simulation and lots of estimation problems so it's very powerful stuff okay that wraps it up that was the interactive demo the cool thing is you can use this to complete load up any data set right there at the beginning calculate a bunch of different directions pick the major minor direction and now model the major minor direction all in Python nice and interactive and you'll have the parameters right here that you can go ahead and record and use in any other software or you could use these models directly with the very model dictionary verigram model dictionary and apply that to geostat pie creaking and simulation and so forth so we have calculated and modeled a consistent spatial continuity model that's positive definite can be used for all kinds of purposes okay as always I hope that this was helpful to you I'm Michael Perch I'm an associate professor at the University of Texas at Austin where I teach and conduct research in data analytics geostatistics and machine learning spatial statistics and so forth and everybody stay safe thank you
Out of everyone, I'm Michael Perch. I'm an associate professor at the University of Texas at Austin and I record all my lectures. And I put them on YouTube for anyone in the world to be able to learn about data analytics, geo statistics and machine learning. So what I'm going to do right now is just have a simple whiteboard discussion of the topic of a verogram calculation. And specifically, how do we work with regular data? And why do we need to use all of these search tolerances? Now, just a reminder of solves, what we need to do here is we need to be able to populate this equation. This equation right here is the calculation for a semi-verogram. Now, I'm going to keep saying verogram in my class. We just say verogram. And semi just means divide by two. And what we know here is that the verogram is just one half the average square difference and average because we're doing a divide by n h, which is the number of pairs that are available to us for a specific like vector h shown right here. So for this equation, we're going to have to average over the square difference of a bunch of pairs. So let's go ahead and calculate a verogram and demonstrate that. Now, first, I've got my area of interest right over here. I'm going to go ahead and give myself a couple of data that I can work with. Now, just to keep this simple, just give myself two data. Now, of course, we're going to have a whole bunch of data everywhere, but we need to identify a pair. Let's just work with two data to see if we can identify these as a pair. Now, we're going to calculate the barogram. It's a bin statistic. We'll calculate it at certain direction and distances. And it'll be bin. Now, first of all, let's identify the direction. Let's go ahead and say that this verogram is going to be calculated for the 45 degree asmif. Okay. So 45 degree asmif would mean that we're actually calculating in this direction right here. That would be the 0, 4, 5. Okay. Asmif. Okay. We want to work in that direction. We're going to identify certain distance bins. Let's go ahead and say our fundamental or elemental distance is going to be lag distances going to be 100 meters. That's simple. So we want to calculate the verogram at 100 meters, 200 meters, 300 meters, and so forth. Okay. So 45 degrees asmif 100 meters. Let's go ahead and draw that. Now, we've got to anchor on a data point for our tail location. Let's anchor on this data point right here. And we're going to go 100 meters. Okay. 100 meters in that direction. Now, what's happened here? We identified a pair. Nope. No pairs yet. What's going on? Well, what's the chance that two data are going to be exactly 100 meters apart in that 45 degree asmif. Actually, if I use a very precise number of 100.000, I can start to say that there's really zero probability that any of my data in space could be that far apart from each other. So I'm going to say, well, let's go ahead and put a tolerance on the distance. Now, let me draw it over here. We'll say we want 100 meters. But I'm going to be okay with plus or minus 50 meters. Okay. Now, what that means is I'm going to take anything between 50 and 150. And I'll say that that gets to be counted as a possible pair to put into this calculation up here. Okay. So I'm going to do that. That was 100 meters. So I'll go 50 through 150. And now we've got this as our distance. So 150 is here. 50 is here. Now, here's a problem. Now, you're the line. It looks like it's going through the point. But what's the chance that we could expect two data being exactly zero four five aligned with each other. You know, this might actually be if I drew the line there and calculate it might be 43 asmif 46 asmif and so forth. So we had a distance tolerance. Let's go ahead and put in an angular asmif tolerance. Okay. So I'm going to be accepting of anything that falls within. Let's say plus or minus 20 degrees. Okay. 20 degrees on my asmif. And so now I'm willing to take anything that is within 2225 and 65 asmif as counting to be in that 45. So right here, we said 45 asmif. But effectively what we've done is we've done a plus or minus 20 on that asmif right there. Okay. So now if I try to draw it, I can draw exactly what I have for a search template. And what it is is here is my 50 meter distance. There is my 150 meter distance. I drew an arc. I hope I got a pretty good arc. This is the angle tolerance. And so I'm willing to take anything within that shape as being a pair. Okay. So now I have a template for that first lag in the 45 degree direction at the 100 meter offset. And I can take that that search template and anchor it on any data and identify all pairs that fall within that template. Now to further explain this, let's go ahead and extend this template to the next bit. Okay. So I that's my 45 degree vector. And that right there was 150. Now right about here would be about 200. And this would be about 250. Now I hope I haven't drawn this perfectly the scale, but I tried. And so we could continue to look at that asmif tolerance. Now look at the asmif tolerance. As we go further away, it gets further away from that original 45 degree vector. Now you might be disturbed if you're working in a certain type of setting, you might like stratigraphic setting. If you're looking vertically, you don't want to go too far off that vector. But you start to mix things that are not should not be mixed together spatially. And so the next thing we can do is we can say, well, I'm going to put a tolerance here, which is basically called a bandwidth. And that bandwidth is going to say, I'm not going to allow myself to go too far away. Sorry. I'm not going to allow myself to go too far away from that vector. And so the bandwidth will limit that how far I can go. And so if I apply the bandwidth and I draw that next bin, the 200 bin in the 45 degree direction, we use plus or minus 50 meters. So it's going to be touching the previous bin. There's no gaps. If I use the smaller number, there'd be a gap. Then it would track along this line right here, the bandwidth maximum. And it would get all the way out to 250 right there. Okay. So I want a little too far with that line. But if I drew it kind of more carefully, get rid of that, then you can see that I have a shape, which is now the 200. Meter bin in the 45 degree with a bandwidth limiting how far away you can go. Okay. So if I apply that bin and I search through my data, now let's just pretend we had a bunch more data. Here's the original data, put data here, data here, data here, lots of data. I can take that search template in the 45 degree direction and I can anchor it on move it, take that entire template, move it, anchor it on each point, find each possible pair, put them all together, comparing all pairs together with the equation for the semi-vera-gram. And if I do that, I would calculate the point right here. And I should draw that in blue because that's my first experimental vera-gram point. Now I can go for the second bin and I'll get a point maybe right here. And I can continue doing that and I will calculate the vera-gram in that one direction. Now that might have been the major direction. I could also apply the same template. And I could work in the minor direction, which would be orthogonal to that direction. I'm considering just working in 2D here. So I could apply the asm of tolerance, I could look at the distance, get the distance tolerance, and I could calculate that search template in the minor direction. And I could carry on and calculate the vera-gram in the minor direction. I would expect the vera-gram values to probably be higher because I'm reaching the sill sooner. I've got weaker spatial continuity. And I could proceed and go ahead and calculate the vera-gram by bending all data in that direction. Let me draw just to finish up here. I have to draw this so I can interpret my vera-gram. And that right there will simply be the variance equal to the sill, the variance of the data, which is equal to the sill of the vera-gram. Now I can interpret it. Okay, so this was a basic explanation of the concepts of distance tolerance within a vera-gram calculation, the concept of a asm of tolerance. And the final one was this idea of a bandwidth to restrict how far I can go away from the vector when finding pairs. Alright, I hope this was helpful to you. With all of this, you should be ready to go ahead and calculate vera-grams with GS-LIVE or Petrol or any other type of modeling software, or even my package in Geostats Pi and Python. Alright, I hope this was helpful to you. I'm Michael Perch. I'm a Associate Professor at the University of Texas at Austin. I record all my lectures and put my content out to support my students and working professionals and anyone who wants to learn about science, spatial data analytics, geostatistics, machine learning. Alright, I hope this was helpful to you. Take care.
Hey, howdy everyone. In the last lecture, we motivated the need to calculate and quantify and model spatial continuity. I tried to show an example where we looked at how the flow response in the subsurface would change dramatically with different spatial contentities. So given that, we're now motivated, we now know we need to calculate spatial continuity and use it in modeling. Let's go ahead and talk about how we do that. For this lecture, we will limit ourselves to the use of the verigram. And we will then, of course, we'll talk about how to calculate it, how to model with it. We'll go into estimation simulation with it. We will also get into other types of spatial continuity measures. Those such as used in Boolean modeling, our mark point process modeling, or maybe more commonly known as object-based modeling approaches, multiple points, statistics, simulation, and so forth. But for now, we will keep it to the verigram. So we need to quantify spatial continuity. And so we want a function, a statistic, measure that can tell us about how things are similar or dissimilar over space. And so there we have the semi-verigram. It is a function of the difference over distance. This axis is the verigram value for a specific lag or distance in space. This is the distance in space. And so this measure is showing you how things become more different. There's more difference as we increase the distance. So we're going to make this calculation over offsets in space, defined as a lag vector h. The lag vector h is bold because it is a vector. It has a direction and it has a distance. We use you to indicate a location. So you gives us the location in the context of your spatial problem to a tail location. So we have a tail. And then we have the lag vector h that describes the separation from tail to the head. And we are going to make this calculation of the verigram by scanning through our data set, looking for all data that are offset by that lag vector h. We're looking for all possible pairs of data that are separated based on that lag vector h. That's our distance and direction offset. And so in scanning for all the possible pairs, we'll go ahead and use this equation here. Where if you look at it, if you've ignored the two right now, what you observe is that this is simply going to be the tail values scanning around. This will be the paired head values. We're scanning around jointly together. We're taking the difference between the two. We're squaring that difference. And the one over n function of h is simply going to be take the average of all of those pairs. n parenthesis h is simply going to be the number of pairs available at that specific lag vector h offset from tail to head. So it's simply the average of the squared difference of all the values that we find that are separated by that vector. The one half is included. And that's what makes it a semi verigram. We did not have the one half. It would simply be a verigram. You'll notice in my notes and commonly within geostatistics literature, there is a degree of laziness about reporting semi verigram versus verigram. In fact, during most of the lectures from now on, I will assume one half of the average squared difference. And I will simply term it as the verigram. That's sufficient. And that's what used within practice. So that's how we calculate the verigram. Now what's very interesting is that every time you take a lag vector h and you scan around, you've got the tail and the head and you're pairing them up, you could create an h scatter plot. The h scatter plot would simply be take the value at the tail location on this axis, take the value at the head location on this axis for which they're paired up and we're differencing them and just plot them. And if we plot them, it's very interesting because if we have a phenomenon for which the values are constant over separation from the tail to the head over that h lag separation, we would expect that those dots would fall on the 45 degree line. And so if we calculate the correlation coefficient, we'll find out that this would be a good measure of how things are correlated in space. I'll show shortly that's equivalent to a cherella gram and can be related directly to chelverins, which can be related directly to the verigram. So it is informative and useful, but it's also nice to look at an h scatter plot and to think about the degree of similarity or correlation over separation distance described by lag vector h. Furthermore about the verigram, once again the verigram equation is shown right here. In words, we would say that it is one half the average grad difference of values separated by a lag vector h. Now you may wonder why remove the one half, why take one half and multiply it by this? It's done so that this works right here. By taking one half the verigram and calculate the semi-verigram representing that as gamma, by tucking that one half in there, we ensure that this equation works out, which means the covariance function, which we will define in a little bit a function or a measure of this degree of similarity versus distance, is going to be equal to the variance of the variable, which we'll call the sill, subtract the verigram value at that h lag vector. If you look at that, the sill is a constant. These are both going to vary over the lag distance h. And so this means that the semi-verigram and the covariance function are simply going to be mirror images of each other. We get the covariance by simply taking the constant sill and subtracting the verigram, flipping the upside down. Now, we'll define covariance a little bit more rigorously, shortly, but it is very interesting to note, to note, as I mentioned on the previous slide, that if you take the covariance function over h and you standardize by the variance, or if you're working with a variable that has a variance of equal to one, such as if we've done a Gaussian transform to standard normal, mean of zero, variance of one, we would be able to get the correlogram. And the correlogram is a measure of similarity over distance for which its value is equal to the h scatterplot correlation coefficient. We've covered correlation coefficients within the previous discussion about bivariate statistics. We're talking about the Pearson correlation coefficient, pretty straightforward. And so that's really nice because now we have a function that relates the degree of similarity to a correlation coefficient, which is something that's very natural for us. So we've defined the verigram. We've talked about spatial continuity and so forth. Let's make some observations about the verigram. Let's talk about general things we would see if we looked at a verigram. Now, first of all, what we'll do is we'll give ourselves a very simple data set. It's exhaustive. It's on a mesh. It's very simple. We're able to go through and calculate the verigram at different lag distances and we'll only concern ourselves with the horizontal, the x direction, just to keep things very simple. So now imagine you go to that data set and you pan through two points, tail and head, tail and head, tail and head, and they're all very close to each other. In fact, there are only four cells apart. There's three cells in between. And if you pan through that data set, I hope what you observe very quickly is that the degree of dissimilarity will be low. That you will both be tail and head will be high or high-ish. Once again, this phenomenon, this property could be any property indeed. It could be a porosity field. It could be a saturation of a fluid, a contaminant concentration. It could be a density of trees. It doesn't matter. It's a spatial property. And so, and you'll notice that as that lag separation distance increases, you would expect that as you go further apart from tail to head, tail to head, you have a more of a likelihood to have a significant change or transition between the tail to head, going from low to high, high to low or high to high. Many different things can happen. So that degree of similarity, or I should say the degree of dissimilarity goes up, degree of similarity goes down. And so, what does that mean? What's observation number one? That in general, as the distance increases, we would expect that the semi-verogram value would increase. Okay? So that's our first observations. That in general, I'm not saying that that's a rule, but that's something you would observe in general. The other point is that to calculate a verogram, you would use every possible pair. Remember when we talked about statistics and sampling and standard error and all of these concepts, the more samples you have, the better or more reliable your statistics. Its confidence interval will decrease. You have more certainty about your statistic. It's more reliable. It has less randomness. And so, we talked about the law of small numbers. We want to have large numbers for more robust statistical representation. So, we're going to calculate the verogram over all possible pairs. Now, I have many students who think that when they calculate a verogram on a mesh like this, they will anchor the tail, and you'll take a close value, the next value, the next value, the next value, the next value, and they're done. Or maybe they'll take a lag separation like this, and they'll skip ahead to the next bin, the next one, the next one. They won't slide across and get every possible value. So, in other words, if you're trying to calculate this case right here for which we have about a 10, a lag equals to 10, you would expect that you would anchor the tail here, go ahead here, then you move over one and do the next one. So, you'd go from for 10, you would go one to data point 11, you'd use this 10 in the middle, then you would go from 2 to 12, 3, 13, and so forth. You would scan across with your two points and then scan down, scan across, you would scan over all possible pairs of data with that separation. That way you have the most reliable measure. Now, once you scan through this entire data set, calculate the square difference between all of those offsets, you summed it all together, divided by number of pairs which is effectively just averaging, or an expectation, you would divide that by 2 all of that hard work and you get one single point on your background plot. So, that's a lot of work to get one point. That's a good thing computers are doing that, right? And then you would increase the lag separation repeat, repeat, repeat, repeat for every possible lag that you want to calculate for. So, you're going to scan through and get all possible pairs from your data set, the most reliable statistics. Observation number three, you need to plot the sill to know the degree of correlation, and I would say in order to actually interpret a background, you need a sill. The sill is equal to the variance as I mentioned before, so we plot the sill. That's the first thing you do, plot the sill. Now you start calculating the variance. Now, I mentioned before, the covariance or degree of similarity is going to be equal to the sill minus the background value. So, this distance from the sill down to the background value is in fact the degree of similarity. Now, if we're working with a standardized variable, in other words, the variance is equal to one, that covariance function is actually equal to the core releggram, which means that that value right there is exactly equal to the correlation coefficient of the H scatter plot. So, now you're starting to understand by looking just looking at this plot, you can see that at this point right here, if you had a data value right here, what would be its correlation? Well, the varogram value at distance of zero would be equal to zero, because there's no difference if you compare points with themselves tail and head are on top of each other, there's going to be no difference whatsoever. So, we would expect that the correlation would be the variance minus zero. And so, in that case, where we're working with a variance equals one, if we have a point down here, the distance right here is equal to one, the correlation is equal to one, which is pretty cool. Now, what happens if we're at the sill? At the sill, there's zero difference or zero distance, I should say, between the varogram value and the sill value, that would mean that the corellogram is equal to zero, the correlation is equal to zero at the sill. That's pretty important information. So, when we reach the sill with a varogram, our experimental varogram values that we're calculating for each lag offset, that's the distance at which we no longer have correlation between tail and head, there's no correlation anymore. So, what does that look like? Let's extend that observation number three. If you were to plot the H scatter plot at the point at which we have varogram value equal to point four and the sill is equal to one, the correlation coefficient at that distance right there, which is probably around four meters or whatever that is right there above four meters, three points something, would be equal to point six. Six is the distance from the sill down here. So, you can interpret that directly. At the sill, at this point right here, the correlation coefficient will be very close to zero. What does that mean? There is no correlation between what's at the tail and what's at the head vocation. They're uncoordinated with each other. What I like to tell my class and tell people is that if you go to that distance away, so you drill the well, you gather data right here in space and you go to this distance and it's equal to that distance at which you reach the sill, you no longer know anything. You don't know anything. There's no correlation, you have no information. So, I'm from spatial continuity that can help you with regard to making an estimate or prediction. And if you go above the sill, at that point, you have negative correlation. So, you go from having no information. Now, you have negative correlation. Is negative correlation information? Yeah, it's a constraint. It is information to help you with making a prediction at that unknown location. So, if you go above the sill, now you have negative correlation, which is also information. Observation number four. The lag distance at which the barogram reaches the sill is known as the range. As we already mentioned, at the range, you don't have any information. That's the maximum distance you can go to which you can be informed about what's going on, to which you can make an estimate and say, hey, that local data is telling me something that helps me constrain my estimate beyond some global distribution of possible outcomes. And so, that is the range. That's the maximum distance you can go away from a sample location and still have some information from that local data that can help you. At the range, once again, the correlation is going to be equal to zero. And I like to draw something on this. If I go well, I go to the range. Pass that distance. I no longer have any information. The range is also an important parameter. When we model barograms, we'll in fact use the range as a parameter that will fit valid barogram models to our experimental barogram results. But we'll talk more about that later. Observation five. There are sometimes a discontinuity in the barogram at distances less than the minimum data spacing. This is known as the nugget effect. Now, it's pretty common that if you have a nugget effect, you'll report it as a ratio. I have relative nugget effect as a percent or a ratio that's just simply going to be a fraction. And so, if I have a Cyl equals one, the nugget effect is equal to 0.4. I would report 40% relative nugget effect, or I would report it as simply as a 0.4 nugget effect. And that's fine. Either one of them is fine. The nugget effect is a structure that has no correlation at lags greater than epsilon. Some infinitesimal distance beyond 0. At 0, the barogram must be equal to 0. If you imagine, once again, at 0, the tail and the head are the same location. You're comparing data. I mean, you're comparing the values with themselves. And so, that difference must be equal to 0. The barogram must be the must equal 0 by definition. But at that very short distance, it jumps up and expresses a degree of dissimilar, where does that come from? The nugget effect term actually comes from gold mining in South Africa. If you imagine a long drill hole and you were saying it for the grade of gold, or the composition as far as the amount of gold in each one of the individual samples along, that's a drill hole, every once in a while you can help with gold deposits naturally forming a large, large, large relative to the size of the samples that we're asking. And so you're going kind of low gold, you know, grams per ton, grams per ton, and then suddenly you have a big gold nugget. If you were to calculate the barogram going through all possible pairs, then suddenly you have a gold nugget that extreme jump at a very, very short distance. A distance shorter than the minimum data spacing would result in a level of variability at very short scales that would be reflected as a nugget effect. Now, we've got to be careful of nugget effect because nugget effect could also be measurement error. In fact, if you were to take a just random error and scatter it using Monte Carlo simulation, just scatter it over all of your data everywhere, vote any spatial structure. That random error would be reflected by a nugget effect. In fact, it's a proportionate relative nugget effect would be based on what amount of variability you put in as random compared to the total amount of variability of the problem. And so you can actually create nugget effect doing that. So what does that mean? It means that many expert models around the world will ignore nugget effect within a barogram that calculate because they know it's likely to be measurement error they fill. Or from their experience, they know that the geologic phenomenon, the subsurface phenomenon, the spatial phenomenon they work with does not naturally have nugget effect and so they don't put it in. The main point is to try to understand the natural system not to be biased by a measurement error. So given all of this knowledge, all of these observations, we can start looking at examples. And so we have three different examples of spatial continuity features. You can see the gray scale, could be the indication of porosity, brain size, fraction of shell, whatever you want it to be. And we can look at the overall types of spatial patterns. From this example, the one at top, this one right here and this one right here, they're all the same histograms. They all have in fact the same range of correlations. They're all very very similar. We'll talk about the Gaussian type structure here, but we will term its ranges when we get within I think about 95% of the sill. So they actually in fact do all have the same ranges. Now, if you look at them, they all have distinctly different degrees of continuity over short, medium, and long scale. Short, medium, and long scale. And if you look at it, you can actually see how each one of these play out and impact the overall type of image. First of all, let's look at the short scale. This structure right here, which we'll talk about later as being the Gaussian type of barogram structure. Results in very high degree of short scale continuity. Lots of short scale continuity. Look at the image. Everything over short distances is very continuous. This model here has some nugget effect, but has pretty good short scale continuity. And you can see the salt and pepper little bit of a discontinuity going on their short scale caused by the nugget effect, but overall good continuity. This model has terrible or poor short scale continuity. As a result, when you look at the image, you'll see a lot of disruption as short distances, going from dark to very light, gray to white, very fast transitions within colors. This is very poor short scale continuity. Now, let's look at the long scale continuity. In general, they have the same range. They're very similar somehow. At these longer scales, the continuity is pretty similar. If you look at the image and you concentrate focus on it, what you'll notice is that this general area of high, this general area here, this area here, these general large scale structures are in fact very similar between the images. That's really interesting. So what does it tell us about the verigram? Well, as we become more accustomed to interpreting verigrams, we should be thinking about a verigram as breaking apart multiple scales of continuity. And being able to recognize that they, it's like a superposition that they're all combined as multiple frequencies that are combined together and we can interpret each one of them separately. And so we have that opportunity as we move forward with verigrams, we'll go back to this idea of interpreting verigrams based on short scale, medium scale, and long scale types of continuity structures. The other thing, of course, is this should reinforce the idea that even with the same distribution, you can dramatically different spatial continuity. So that happens all of the time. The other thing too is it should make sense that these functions, these representations of these spatial continuity, the verigrams, actually do have a pretty big impact in spatial continuity. It's not like it's very subtle. The difference between this, this, this are all very different, they're just thinkably different phenomenon speaking about spatial continuity. So we will get more into interpretation and modeling in the next couple of sessions. I hope that this was useful to you. We've now defined the verigram and we'll carry on with how to practically speaking, how to calculate them in the challenging sparse, irregularly spaced data types of settings that we often work with, get more into advanced interpretation, and then how do we build valid models that we can use in fact for the purpose of estimation and simulation. Thank you very much for your interest. As always, shoot me a question or a suggestion, email, comments, Twitter, Facebook, even YouTube, whatever you like. My email is very easy to get. I'm a professor here at University of Texas. Thank you.